{"./":{"url":"./","title":"兔比喃波湾","keywords":"","body":"😂 😂 😂 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"操作小技巧/linux/linux一些骚操作.html":{"url":"操作小技巧/linux/linux一些骚操作.html","title":"linux一些扫操作","keywords":"","body":"linux一些骚操作 timeout 1m tail -Fn 0 \"/data/log/console.log\" |(sed /\"register finished\"/q;) 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"操作小技巧/linux/一条命令搞定.html":{"url":"操作小技巧/linux/一条命令搞定.html","title":"一条命令搞定","keywords":"","body":"一条命令搞定 1.批量修改文件名 1.1 增加内容 有多个sh文件，现在用一条命令做以下修改 # 修改前 1.sh 2.sh 3.sh # 修改后 1_hehe.sh 2_hehe.sh 3_hehe.sh 使用sed分组功能 ls *.sh|sed -nr 's#((.*)\\.sh)#mv \\1 \\2_hehe.sh#gp' | bash 将命令拆解一下 # sed替换中最外边的括号是匹配整个文件名的，即上边命令中的 \\1 $ ls *.sh|sed -r 's#((.*)\\.sh)#\\1#g' 1.sh 2.sh 3.sh # 里边的括号是匹配文件名中的数字，即上边命令中的 \\2 $ ls *.sh|sed -r 's#((.*)\\.sh)#\\2#g' 1 2 3 # 有了分组，再配合mv命令拼接 $ ls *.sh|sed -nr 's#((.*)\\.sh)#mv \\1 \\2_hehe.sh#gp' mv 1.sh 1_hehe.sh mv 2.sh 2_hehe.sh mv 3.sh 3_hehe.sh # 最后通过管道执行bash使命令生效 $ ls *.sh|sed -nr 's#((.*)\\.sh)#mv \\1 \\2_hehe.sh#gp' | bash $ ls 1_hehe.sh 2_hehe.sh 3_hehe.sh 1.2 删除内容 有多个sh文件，现在用一条命令做以下修改 # 修改前 abc_html_01.jpg abc_html_02.jpg abc_html_03.jpg # 修改后 abc_01.jpg abc_02.jpg abc_03.jpg 方法一：使用sed分组功能 ls *.jpg | sed -r 's#(.*_)(.*_)(.*)#mv \\1\\2\\3 \\1\\3#g' |bash 将命令拆解一下 # 根据文件内容匹配成3组，第1组匹配的内容 $ ls *.jpg | sed -r 's#(.*_)(.*_)(.*)#\\1#g' abc_ abc_ abc_ # 根据文件内容匹配成3组，第2组匹配的内容 $ ls *.jpg | sed -r 's#(.*_)(.*_)(.*)#\\2#g' html_ html_ html_ # 根据文件内容匹配成3组，第3组匹配的内容 $ ls *.jpg | sed -r 's#(.*_)(.*_)(.*)#\\3#g' 01.jpg 02.jpg 03.jpg # 有了分组，再配合mv命令拼接 $ ls *.jpg | sed -r 's#(.*_)(.*_)(.*)#mv \\1\\2\\3 \\1\\3#g' mv abc_html_01.jpg abc_01.jpg mv abc_html_02.jpg abc_02.jpg mv abc_html_03.jpg abc_03.jpg # 最后通过管道执行bash使命令生效 $ ls *.jpg | sed -r 's#(.*_)(.*_)(.*)#mv \\1\\2\\3 \\1\\3#g' | bash $ ls abc_01.jpg abc_02.jpg abc_03.jpg 方法二：for循环 for i in `ls ./*`;do mv $i $(echo $i|sed 's#html_##g');done 重命名格式： mv 源文件 目标文件 分以下3步拼接 第一步：形成源文件，abc_html_01.jpg ，将文件放入变量中，for i in ls *.jpg ，这时变量 i 中的值就是这3个文件了 第二步：形成目标文件，显示变量中的内容并且利用sed替换，echo $i|sed 's#html_##g'，这时目标文件就变为了 abc_01.jpg 第三步：利用mv命令修改，$i 是源文件，echo $i|sed -r 's#html_##g' 就是目标文件，mv $i $(echo $I | sed 's#html_##g') $ for i in `ls ./*`;do mv $i $(echo $i|sed 's#html_##g');done $ ls abc_01.jpg abc_02.jpg abc_03.jpg 方法三：rename命令 rename 命令语法格式： rename 要替换的内容 要替换为什么 要替换的文件 $ rename html_ \"\" *.jpg $ ls abc_01.jpg abc_02.jpg abc_03.jpg 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"操作小技巧/数据库/mysql批量修改字段内容.html":{"url":"操作小技巧/数据库/mysql批量修改字段内容.html","title":"mysql批量修改字段内容","keywords":"","body":"mysql批量修改字段内容 1.数据准备 先创建一张表 create table t1(address char(30)); 插入数据 insert into t1 values('aaa.bbb.ccc'),('111.aaa.eee'),('666.ooo.aaa'); 查看 mysql> select * from t1; +-------------+ | address | +-------------+ | aaa.bbb.ccc | | 111.aaa.eee | | 666.ooo.aaa | +-------------+ 2.批量替换 语句 SELECT 字段名, REPLACE(字段名, '要替换的内容', '替换为什么') from 表名; UPDATE 表名 set 字段名 = REPLACE(字段名, '要替换的内容', '替换为什么'); 示例 将 aaa 批量替换为 hehe 可以先使用 SELECT 字段名, REPLACE(字段名, '要替换的内容', '替换为什么') from 表名; 查询一下 mysql> SELECT address, REPLACE(address, 'aaa', 'hehe') from t1; +-------------+---------------------------------+ | address | REPLACE(address, 'aaa', 'hehe') | +-------------+---------------------------------+ | aaa.bbb.ccc | hehe.bbb.ccc | | 111.aaa.eee | 111.hehe.eee | | 666.ooo.aaa | 666.ooo.hehe | +-------------+---------------------------------+ 此时数据是没有正真修改的，类似于 sed 命令不加 -i 选项 mysql> select * from t1; +-------------+ | address | +-------------+ | aaa.bbb.ccc | | 111.aaa.eee | | 666.ooo.aaa | +-------------+ 接下来使用语句 UPDATE 表名 set 字段名 = REPLACE(字段名, '要替换的内容', '替换为什么'); UPDATE t1 set address = REPLACE(address, 'aaa', 'hehe'); 再次查看，此时数据已经修改 mysql> select * from t1; +--------------+ | address | +--------------+ | hehe.bbb.ccc | | 111.hehe.eee | | 666.ooo.hehe | +--------------+ 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"科学上网/利用libreswan+r2ray实现科学上网.html":{"url":"科学上网/利用libreswan+r2ray实现科学上网.html","title":"利用libreswan+V2Ray实现科学上网","keywords":"","body":"利用libreswan+V2Ray实现科学上网 实验背景 在云主机中(阿里云、腾讯云等)，大陆地区的机器是无法科学上网的，并且好多优秀的网站、工具等无法访问或者下载速度很慢，例如下载github上的一些资源，我们可以购买VPS(例如Vultr)或者香港等地区的主机，再配合一些软件就可以实现科学上网了 我们可以使用 libreswan 搭建 ipssec 隧道实现不通地域主机内网互通，然后使用 V2Ray 作为代理工具实现科学上网 libreswan官网 libreswan github libreswan官方文档 实验环境 区域 公网IP 内网IP段 内网IP 系统 内核版本 北京 81.70.22.232 172.31.0.0/24 172.31.0.3 CentOS7.9 3.10.0-1160.45.1.el7.x86_64 香港 129.226.167.89 10.0.0.0/24 10.0.0.17 CentOS7.9 3.10.0-1160.45.1.el7.x86_64 1.安装配置libreswan 1.0 编辑环境变量文件 北京、香港区服务器操做 # 编辑文件 cat > /opt/wall_env 1.1 安装依赖包 北京、香港区服务器操做 yum -y install audit-libs-devel bison curl-devel fipscheck-devel flex gcc ldns-devel libcap-ng-devel libevent-devel libseccomp-devel libselinux-devel make nspr-devel nss-devel pam-devel pkgconfig systemd-devel unbound-devel xmlto 1.2 安装libreswan 北京、香港区服务器操做 yum -y install libreswan 1.3 配置内核参数 北京、香港区服务器操做 # 开启路由转发 sed -i '/^net.ipv4.ip_forward/d' /etc/sysctl.conf cat >> /etc/sysctl.conf 1.4 启动ipsec systemctl start ipsec && systemctl enable ipsec 查看状态 $ systemctl status ipsec ● ipsec.service - Internet Key Exchange (IKE) Protocol Daemon for IPsec Loaded: loaded (/usr/lib/systemd/system/ipsec.service; enabled; vendor preset: disabled) Active: active (running) since Tue 2022-03-29 10:38:39 CST; 14s ago Docs: man:ipsec(8) man:pluto(8) man:ipsec.conf(5) Main PID: 6483 (pluto) Status: \"Startup completed.\" CGroup: /system.slice/ipsec.service └─6483 /usr/libexec/ipsec/pluto --leak-detective --config /etc/ipsec.conf --nofork Mar 29 10:38:39 hk pluto[6483]: adding interface lo/lo 127.0.0.1:500 Mar 29 10:38:39 hk pluto[6483]: adding interface lo/lo 127.0.0.1:4500 Mar 29 10:38:39 hk pluto[6483]: adding interface lo/lo ::1:500 Mar 29 10:38:39 hk pluto[6483]: | setup callback for interface lo:500 fd 19 Mar 29 10:38:39 hk pluto[6483]: | setup callback for interface lo:4500 fd 18 Mar 29 10:38:39 hk pluto[6483]: | setup callback for interface lo:500 fd 17 Mar 29 10:38:39 hk pluto[6483]: | setup callback for interface eth0:4500 fd 16 Mar 29 10:38:39 hk pluto[6483]: | setup callback for interface eth0:500 fd 15 Mar 29 10:38:39 hk pluto[6483]: loading secrets from \"/etc/ipsec.secrets\" Mar 29 10:38:39 hk pluto[6483]: no secrets filename matched \"/etc/ipsec.d/*.secrets\" 1.5 防火墙配置 香港区服务器操作 防火墙策略开放 udp 500 和 udp 4500 端口，允许北京区主机公网IP访问 1.6 验证端口连通性 北京区服务器操作 $ nmap -sU 2.2.2.2 -p 500,4500 -Pn Starting Nmap 6.40 ( http://nmap.org ) at 2022-03-20 21:32 CST Nmap scan report for 2.2.2.2 Host is up. PORT STATE SERVICE 500/udp open|filtered isakmp 4500/udp open|filtered nat-t-ike Nmap done: 1 IP address (1 host up) scanned in 3.17 seconds 1.7 配置预共享密钥 北京、香港区服务器操做 /etc/ipsec.secrets 配置文件中有 include /etc/ipsec.d/*.secrets ，因此我们在 /etc/ipsec.d 目录下新建 *.secrets 文件 $ cat /etc/ipsec.secrets include /etc/ipsec.d/*.secrets 新建 /etc/ipsec.d/vm.secrets cat > /etc/ipsec.d/vm.secrets 1.8 配置ipsec连接 /etc/ipsec.conf 配置文件中有 include /etc/ipsec.d/*.conf ，因此我们在 /etc/ipsec.d 目录下新建 *.conf 文件，同时 /etc/ipsec.conf 文件中有详细的示例说明 1.8.1 配置香港区服务器 Libreswan 不使用术语 source 或 destination。相反，它用术语 left 和 right来代指终端（主机）。虽然大多数管理员用 left 表示本地主机，right 表示远程主机，但是这样可以在大多数情况下在两个终端上使用相同的配置。 由于我们的服务器使用的是vpc网络，采用静态nat的形式，在配置 left 和 right 时，本端的ip需要使用内网ip或 %defaultroute。left 和 right 是两端的ip地址，而 leftid 和 rightid 为代号id。 这里我们指定北京区为 left 、香港区为 right cat > /etc/ipsec.d/vm.conf 1.8.2 配置北京区服务器 Libreswan 不使用术语 source 或 destination。相反，它用术语 left 和 right来代指终端（主机）。虽然大多数管理员用 left 表示本地主机，right 表示远程主机，但是这样可以在大多数情况下在两个终端上使用相同的配置。 由于我们的服务器使用的是vpc网络，采用静态nat的形式，在配置 left 和 right 时，本端的ip需要使用内网ip或 %defaultroute。left 和 right 是两端的ip地址，而 leftid 和 rightid 为代号id。 这里我们指定香港区为本端机器，北京区为对端机器，即香港区为 left 、 北京区为 right cat > /etc/ipsec.d/vm.conf 1.8.3 开启ipsec日志 北京、香港区服务器操做 这里需要编辑 /etc/ipsec.conf 把日志打开，默认日志路径为 /var/log/pluto.log sed -i 's/#logfile=/logfile=/' /etc/ipsec.conf 1.8.4 重启ipsec 北京、香港区服务器操做 systemctl restart ipsec 1.8.5 验证 1.8.5.1 两端互ping 北京区服务器ping香港区服务器 $ ping -c2 $HK_PRIVATE_IP PING 10.0.0.3 (10.0.0.3) 56(84) bytes of data. 64 bytes from 10.0.0.3: icmp_seq=1 ttl=64 time=46.6 ms 64 bytes from 10.0.0.3: icmp_seq=2 ttl=64 time=46.6 ms --- 10.0.0.3 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1001ms rtt min/avg/max/mdev = 46.648/46.666/46.685/0.216 ms 香港区服务器ping北京区服务器 $ ping -c2 $BJ_PRIVATE_IP PING 172.31.0.14 (172.31.0.14) 56(84) bytes of data. 64 bytes from 172.31.0.14: icmp_seq=1 ttl=64 time=46.5 ms 64 bytes from 172.31.0.14: icmp_seq=2 ttl=64 time=46.6 ms --- 172.31.0.14 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1000ms rtt min/avg/max/mdev = 46.586/46.618/46.650/0.032 ms 1.8.5.2 查看ipsec状态 提示如下即为成功 $ systemctl status ipsec ● ipsec.service - Internet Key Exchange (IKE) Protocol Daemon for IPsec Loaded: loaded (/usr/lib/systemd/system/ipsec.service; enabled; vendor preset: disabled) Active: active (running) since Wed 2022-03-23 21:00:37 CST; 4s ago Docs: man:ipsec(8) man:pluto(8) man:ipsec.conf(5) Process: 6246 ExecStopPost=/usr/sbin/ipsec --stopnflog (code=exited, status=0/SUCCESS) Process: 6243 ExecStopPost=/sbin/ip xfrm state flush (code=exited, status=0/SUCCESS) Process: 6240 ExecStopPost=/sbin/ip xfrm policy flush (code=exited, status=0/SUCCESS) Process: 6227 ExecStop=/usr/libexec/ipsec/whack --shutdown (code=exited, status=0/SUCCESS) Process: 6521 ExecStartPre=/usr/sbin/ipsec --checknflog (code=exited, status=0/SUCCESS) Process: 6518 ExecStartPre=/usr/sbin/ipsec --checknss (code=exited, status=0/SUCCESS) Process: 6256 ExecStartPre=/usr/libexec/ipsec/_stackmanager start (code=exited, status=0/SUCCESS) Process: 6254 ExecStartPre=/usr/libexec/ipsec/addconn --config /etc/ipsec.conf --checkconfig (code=exited, status=0/SUCCESS) Main PID: 6534 (pluto) Status: \"Startup completed.\" CGroup: /system.slice/ipsec.service └─6534 /usr/libexec/ipsec/pluto --leak-detective --config /etc/ipsec.conf --nofork Mar 23 21:00:37 hk systemd[1]: Starting Internet Key Exchange (IKE) Protocol Daemon for IPsec... Mar 23 21:00:37 hk ipsec[6521]: nflog ipsec capture disabled Mar 23 21:00:37 hk systemd[1]: Started Internet Key Exchange (IKE) Protocol Daemon for IPsec. 1.8.5.3 查看日志 查看日志 /var/log/pluto.log ，有包的发送以及返回即为正确 Mar 23 21:00:37.774360: \"bj-vm-hk/1x1\"[1] 42.193.112.127 #1: STATE_MAIN_R1: sent MR1, expecting MI2 Mar 23 21:00:37.825502: \"bj-vm-hk/1x1\"[1] 42.193.112.127 #1: STATE_MAIN_R2: sent MR2, expecting MI3 Mar 23 21:00:37.875013: \"bj-vm-hk/1x1\"[1] 42.193.112.127 #1: Peer ID is ID_IPV4_ADDR: '172.31.0.14' Mar 23 21:00:37.875269: \"bj-vm-hk/1x1\"[1] 42.193.112.127 #1: STATE_MAIN_R3: sent MR3, ISAKMP SA established {auth=PRESHARED_KEY cipher=3des_cbc_192 integ=sha group=MODP2048} Mar 23 21:00:37.923920: \"bj-vm-hk/1x1\"[1] 42.193.112.127 #1: the peer proposed: 10.0.0.0/24:0/0 -> 172.31.0.0/24:0/0 Mar 23 21:00:37.926134: \"bj-vm-hk/1x1\"[1] 42.193.112.127 #2: responding to Quick Mode proposal {msgid:4323d821} Mar 23 21:00:37.926162: \"bj-vm-hk/1x1\"[1] 42.193.112.127 #2: us: 10.0.0.0/24===10.0.0.3[43.154.53.42] Mar 23 21:00:37.926169: \"bj-vm-hk/1x1\"[1] 42.193.112.127 #2: them: 42.193.112.127===172.31.0.0/24 Mar 23 21:00:37.926600: \"bj-vm-hk/1x1\"[1] 42.193.112.127 #2: STATE_QUICK_R1: sent QR1, inbound IPsec SA installed, expecting QI2 tunnel mode {ESP/NAT=>0x33ba4428 0x33ba4428 1.8.5.4 查看连接情况 可以执行命令 ipsec auto --status 查看连接情况 000 \"bj-vm-hk/1x1\"[1]: IKE algorithms: 3DES_CBC-HMAC_SHA1-MODP2048, 3DES_CBC-HMAC_SHA1-MODP1536 000 \"bj-vm-hk/1x1\"[1]: IKE algorithm newest: 3DES_CBC_192-HMAC_SHA1-MODP2048 000 \"bj-vm-hk/1x1\"[1]: ESP algorithms: 3DES_CBC-HMAC_SHA1_96 000 \"bj-vm-hk/1x1\"[1]: ESP algorithm newest: 3DES_CBC_000-HMAC_SHA1_96; pfsgroup= 000 000 Total IPsec connections: loaded 2, active 1 000 000 State Information: DDoS cookies not required, Accepting new IKE connections 000 IKE SAs: total(1), half-open(0), open(0), authenticated(1), anonymous(0) 000 IPsec SAs: total(1), authenticated(1), anonymous(0) 000 000 #1: \"bj-vm-hk/1x1\"[1] 42.193.112.127:4500 STATE_MAIN_R3 (sent MR3, ISAKMP SA established); EVENT_SA_REPLACE in 3143s; newest ISAKMP; lastdpd=-1s(seq in:0 out:0); idle; import:not set 000 #2: \"bj-vm-hk/1x1\"[1] 42.193.112.127:4500 STATE_QUICK_R2 (IPsec SA established); EVENT_SA_REPLACE in 28343s; newest IPSEC; eroute owner; isakmp#1; idle; import:not set 000 #2: \"bj-vm-hk/1x1\"[1] 42.193.112.127 esp.33ba4428@42.193.112.127 esp.f89429e6@10.0.0.3 tun.0@42.193.112.127 tun.0@10.0.0.3 ref=0 refhim=0 Traffic: ESPin=0B ESPout=0B! ESPmax=4194303B 000 000 Bare Shunt list: 000 2.安装配置V2Ray v2ray官网 v2ray github v2ray脚本安装github v2ray github下载地址 2.1 香港区服务器操作 截止2022.3.23，v2ray最新稳定版本为4.44 香港区服务器可以直接执行脚本安装即可 bash 以下为具体输出 bash 2.2 北京区服务器操作 北京区服务器由于执行一键安装脚本太太太🐔8⃣️慢了，因此需要先下载安装脚本，然后通过加速地址下载v2ray安装包，最后执行本地安装 2.2.1 下载一键安装脚本 wget https://raw/branch.githubusercontent.com/v2fly/fhs-install-v2ray/master/install-release.sh 2.2.2 下载安装包 截止2022.3.23，v2ray最新稳定版本为4.44 由于 https://github.com/v2fly/v2ray-core/releases/download/v4.44.0/v2ray-linux-64.zip 下载太慢了，可以使用如下加速地址 wget https://download.fastgit.org/v2fly/v2ray-core/releases/download/v4.44.0/v2ray-linux-64.zip 或 wget https://ghproxy.com/https://github.com/v2fly/v2ray-core/releases/download/v4.44.0/v2ray-linux-64.zip 2.2.3 安装v2ray 赋予脚本可执行权限 chmod +x install-release.sh 执行安装 ./install-release.sh -l v2ray-linux-64.zip 以下为具体输出 warn: Install V2Ray from a local file, but still need to make sure the network is available. warn: Please make sure the file is valid because we cannot confirm it. (Press any key) ... info: Extract the V2Ray package to /tmp/tmp.nXaK0Gt7ur and prepare it for installation. info: Systemd service files have been installed successfully! warning: The following are the actual parameters for the v2ray service startup. warning: Please make sure the configuration file path is correctly set. ~~~~~~~~~~~~~~~~ [Unit] Description=V2Ray Service Documentation=https://www.v2fly.org/ After=network.target nss-lookup.target [Service] User=nobody CapabilityBoundingSet=CAP_NET_ADMIN CAP_NET_BIND_SERVICE AmbientCapabilities=CAP_NET_ADMIN CAP_NET_BIND_SERVICE NoNewPrivileges=true ExecStart=/usr/local/bin/v2ray -config /usr/local/etc/v2ray/config.json Restart=on-failure RestartPreventExitStatus=23 [Install] WantedBy=multi-user.target # In case you have a good reason to do so, duplicate this file in the same directory and make your customizes there. # Or all changes you made will be lost! # Refer: https://www.freedesktop.org/software/systemd/man/systemd.unit.html [Service] ExecStart= ExecStart=/usr/local/bin/v2ray -config /usr/local/etc/v2ray/config.json ~~~~~~~~~~~~~~~~ warning: The systemd version on the current operating system is too low. warning: Please consider to upgrade the systemd or the operating system. installed: /usr/local/bin/v2ray installed: /usr/local/bin/v2ctl installed: /usr/local/share/v2ray/geoip.dat installed: /usr/local/share/v2ray/geosite.dat installed: /usr/local/etc/v2ray/config.json installed: /var/log/v2ray/ installed: /var/log/v2ray/access.log installed: /var/log/v2ray/error.log installed: /etc/systemd/system/v2ray.service installed: /etc/systemd/system/v2ray@.service removed: /tmp/tmp.nXaK0Gt7ur info: V2Ray v4.44.0 is installed. You may need to execute a command to remove dependent software: yum remove curl unzip Please execute the command: systemctl enable v2ray; systemctl start v2ray 2.3 启动v2ray 北京、香港区服务器操做 systemctl enable v2ray && systemctl start v2ray 查看v2ray运行状态 $ systemctl status v2ray ● v2ray.service - V2Ray Service Loaded: loaded (/etc/systemd/system/v2ray.service; enabled; vendor preset: disabled) Drop-In: /etc/systemd/system/v2ray.service.d └─10-donot_touch_single_conf.conf Active: active (running) since Wed 2022-03-23 21:36:38 CST; 27s ago Docs: https://www.v2fly.org/ Main PID: 13273 (v2ray) CGroup: /system.slice/v2ray.service └─13273 /usr/local/bin/v2ray -config /usr/local/etc/v2ray/config.json Mar 23 21:36:38 bj systemd[1]: Started V2Ray Service. Mar 23 21:36:38 bj v2ray[13273]: V2Ray 4.44.0 (V2Fly, a community-driven edition of V2Ray.) Custom (go1.17.3 linux/amd64) Mar 23 21:36:38 bj v2ray[13273]: A unified platform for anti-censorship. Mar 23 21:36:38 bj v2ray[13273]: 2022/03/23 21:36:38 [Info] main/jsonem: Reading config: /usr/local/etc/v2ray/config.json Mar 23 21:36:38 bj v2ray[13273]: 2022/03/23 21:36:38 [Warning] V2Ray 4.44.0 started 2.4 编辑配置文件 v2ray配置文件官方模板 香港区服务器操做 cat > /usr/local/etc/v2ray/config.json 北京区服务器操做 配置文件中的 address 要写香港区服务器的内网IP accounts 下的 user 和 pass 是用户名和密码，用于电脑本机配置认证所用 cat > /usr/local/etc/v2ray/config.json 2.5 重启v2ray 北京、香港区服务器操做 编辑完配置文件重启生效 systemctl restart v2ray 2.6 查看启动 香港区服务器操做 $ netstat -ntpl|grep v2ray tcp6 0 0 :::1080 :::* LISTEN 12808/v2ray tcp6 0 0 :::2008 :::* LISTEN 12808/v2ray tcp6 0 0 :::1087 :::* LISTEN 12808/v2ray 北京区服务器操做 $ netstat -ntpl|grep v2ray tcp6 0 0 :::1087 :::* LISTEN 14445/v2ray tcp6 0 0 :::1088 :::* LISTEN 14445/v2ray 2.7 配置代理 export http_proxy=http://10.0.0.3:1087 export https_proxy=http://10.0.0.3:1087 2.8 验证 $ curl -I google.com HTTP/1.1 301 Moved Permanently Content-Length: 219 Cache-Control: public, max-age=2592000 Connection: keep-alive Content-Type: text/html; charset=UTF-8 Date: Wed, 23 Mar 2022 13:50:28 GMT Expires: Fri, 22 Apr 2022 13:50:28 GMT Keep-Alive: timeout=4 Location: http://www.google.com/ Proxy-Connection: keep-alive Server: gws X-Frame-Options: SAMEORIGIN X-Xss-Protection: 0 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"翻车问题合集/翻车问题-ssh密钥问题.html":{"url":"翻车问题合集/翻车问题-ssh密钥问题.html","title":"ssh密钥问题","keywords":"","body":"[toc] 翻车问题合集之ssh密钥问题 1.ucloud免密登陆问题 背景说明： ucloud云主机，想要使用密钥登陆，提ucloud工单得到回复ucloud并不支持在web界面创建密钥并绑定(华为云、腾讯云、阿里云都支持web界面创建绑定)，只能登陆系统手动创建并下载密钥，于是root用户登陆ucloud云主机手动执行命令ssh-keygen，下载id_rsa私钥到本地，结果使用这个私钥始终无法登陆系统，还特么又提了1个ucloud工单问人家怎么回事(虽然还遇到一个哥们给我回复把私钥权限改成755试试。。。)，最终工单回复密钥未注册，请自行排查，最后折腾半天找到答案(自己太菜了) 原因： ⚠️centos7ssh服务配置文件/etc/ssh/sshd_config中有一项配置是AuthorizedKeysFile .ssh/authorized_keys，如果想要使用私钥免密登陆，则公钥必须写入到文件.ssh/authorized_keys中，即注册私钥，否则免密会失败！！！ 手动执行命令cat id_rsa.pub >> authorized_keys，把公钥注册后问题解决 2.mac推送仓库到码云 背景说明： mac本机，在根目录下创建了一个目录，准备把这个目录下的内容推送到码云新建的仓库中，已经手动把user用户的公钥粘贴到了码云的个人账户中(码云中只有把公钥放到个人账户中才能对仓库有写权限)，但是推送的时候始终提示权限拒绝，而使用命令ssh -T git@gitea.pptfz.cn确是提示认证成功的 原因： 原因是推送的时候使用了sudo，用到的因该是root用户的密钥，但是只把user用户的公钥放到了码云中，所以权限拒绝，把root用户的公钥放到码云中就可以了 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"翻车问题合集/翻车问题-证书格式问题.html":{"url":"翻车问题合集/翻车问题-证书格式问题.html","title":"证书格式问题","keywords":"","body":"翻车问题-证书格式问题 问题说明 之前有一个服务是部署在一台机器上的，然后上传了crt格式的通配符证书 现在在腾讯云CLB中配置从服务器下载下来的https证书，但是https始终无法访问，各种排查各种百度。。。 最后发现是证书格式的问题！！！ 腾讯云CLB中https证书的格式要求为PEM编码，而之前从服务器上下载的通配符证书是crt格式的 使用如下命令修改证书格式 openssl x509 -in mycert.crt -out mycert.pem -outform PE 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/阿里云开发者社区优秀文章/排错宝典/ECS运维指南之Linux系统诊断.html":{"url":"linux/阿里云开发者社区优秀文章/排错宝典/ECS运维指南之Linux系统诊断.html","title":"ECS运维指南之Linux系统诊断","keywords":"","body":"[toc] ECS运维指南之Linux系统诊断 文档抄袭于阿里云开发者社区 目录 Linux启动与登陆问题 超详细系统启动与登陆异常排查点 grub.conf 文件内容被清空了怎么办 巧妙利用 strace 查找丢失的文件 小心 PAM 不让你登录 CentOS 登录卡住的原因被我找到了 Linux 性能问题 找到 Linux 虚机 Load 高的\"元凶\" OOM killer 是被谁触发的 我的服务器内存去哪儿了 CPU 占用不高但网络性能很差的一个原因 一次 IO 异常捕获过程 Linux 主机网络问题 ifdown ifup 命令丢失处理 网络不通？ strace 二度出手 TIME_WAIT & CLOSE_WAIT 的讨论总结 一次网络抖动经典案例分析 Linux 系统服务与参数问题 4 个 limits 生效的问题 6 步排查 ss& netstat 统计结果不一样的原因 为什么明明内存很充足但是 java 程序仍申请不到内存 请不要忽略 min_free_kbytes 的设置 最后的彩蛋 某地区口罩项目架构演进及优化经验 一、Linux启动与登陆问题 说明 Linux 启动与登录问题是 ECS 的高频问题，而往往处理不及时会直接影响到用户业务的正常可持续运行，因此也变成了我们处理问题优先级的重中之重。 在云环境上影响 ECS 启动与登录的因素非常多，镜像、管控、虚拟化、底层硬件、系统与文件异常等等，本文仅从系统与文件本身角度，在大量处理经验的基础上，归纳总结了一些可能会引起系统启动与登录问题的排查点，并给出几个比较常见的典型案例来具体展示和说明。 1.1 超详细系统启动与登陆异常排查点 1.1.1 系统启动异常 部分 CentOS 系统启动黑屏，无异常报错的场景，可以 fsck 一下系统盘。 根分区空间满，以及 inode 数量耗尽。 升级内核或者从老的共享实例迁移到独享规格导致的启动异常。 手动注入驱动 (mkinitrd virtio 相关驱动 )。 修改 grub 的启动顺序，优先尝试使用老内核启动。 /boot 目录下面内核的关联文件是否全（下面仅为 demo，不同系统内核版 本文件不一致，部分内核版本 boot 下的 i386 目录也是有用的）。 config-4.9.0-7-amd64 initrd.img-4.9.0-7-amd64 System.map-4.9.0-7-amd64 vmlinuz-4.9.0-7-amd64 /boot/grub/device.map 里面的 hda 改成 vda。 fstab/grub 中的 uuid 不对，可以直接修改为 /dev/vda1 这种形式尝试。 数据盘分区异常加载起不来的场景，可以去注释 fstab 所有的行，添加类似下面的启动项尝试， 也适用于系统盘快照创建云盘挂载后，uuid 一致导致的启动异 常，改成非 UUID 的挂载即可。 /dev/vda1 / ext4 defaults 1 1 根目录权限 777（部分目录 777）也会导致启动异常，或者 ssh 登陆异常。 可参考此文章权限修复尝试 常见的关键目录缺失，有的是软链，也可以看看对应目录下面的文件数量（文件数量要跟同内核版本或者相差不大的版本对比），简单判断。 /bin /sbin /lib /lib32 /lib64 /etc /boot /usr/bin /usr/sbin /usr/lib / usr/lib64 等目录或文件缺失 for i in /bin /sbin /lib /lib32 /lib64 /etc /boot /usr/bin /usr/sbin / usr/lib /usr/lib64 ;do ls -l $i |wc -l ;done 影响启动的参数。 如果参数设置不当，是会导致启动异常的，如 /etc/sysctl.conf 以及检查 rc.local 的配置，profile 的检查。 vm.nr_hugepages vm.min_free_kbytes CentOS 的 selinux 需要关闭。 # This file controls the state of SELinux on the system. # SELINUX= can take one of these three values: # enforcing - SELinux security policy is enforced. # permissive - SELinux prints warnings instead of enforcing. # disabled - No SELinux policy is loaded. SELINUX=disabled # SELINUXTYPE= can take one of three two values: # targeted - Targeted processes are protected, # minimum - Modification of targeted policy. Only selected processes are protected. # mls - Multi Level Security protection. SELINUXTYPE=targeted 1.1.2 root 登陆异常 /etc/passwd、/etc/shadow ( 用户名 root polikt dbus 等关键用户存在与否，文 件为空，格式乱（dos2unix)。 /etc/pam.d 目录下是否有为空的文件及参数设置是否正常， 如常见的 system-auth passwd。 /etc/pam.d 下面所有文件里面涉及的 so 文件，看看文件是否存在，是否为空 / usr/lib64/security。 查 /etc /lib64、 /bin、 /sbin、/usr/bin、 /usr/sbin 等目录有没有 size 为 0 的文件。 /etc/profile、/etc/profile.d( 打 印 列 表 )、 /etc/bashrc、 /root/.bash_profile、/root/.bashrc 等涉及登陆环境设置的文件是否异常。 注意内核版本，是否存在新老内核，多更换几个内核试下。 系统日志也是一个比较重要的检查项（后面会介绍无法登陆怎么检查）。 Ubuntu 12.04 登陆异常在 /etc/login.defs 里面配置了错误的 ERASECHAR 导致，恢复默认 0177 即可。 configuration error - cannot parse erasechar value 输入 root 后直接 login 失败三连，日志如下。 Feb 12 18:18:03 iZbp1cabe6lyx26ikmjie2Z login: FAILED LOGIN 1 FROM tty1 FOR root, Authentication failure Feb 12 18:18:03 iZbp1cabe6lyx26ikmjie2Z login: FAILED LOGIN 2 FROM tty1 FOR (unknown), Authentication failure Feb 12 18:18:03 iZbp1cabe6lyx26ikmjieZZ login: FAILED LOGIN SESSION FROM tty1 FOR (unknown), Authentication failure 找个同内核版本的机器对比发现没有 /etc/pam.d/login。 rpm 包校验一下，确认 login 文件没了，手动创建一个，内容拷贝过来，好了。 [root@iZbp1cabe6lyx26ikmjie2Z pam.d]# rpm -V util-linux missing c /etc/pam.d/login [root@iZbp1cabe6lyx26ikmjie2Z pam.d]# rpm -ql util-linux|egrep -vi \"gz|mo|share\" /etc/mtab /etc/pam.d/chfn /etc/pam.d/chsh /etc/pam.d/login /etc/pam.d/runuser /etc/pam.d/runuser-l /etc/pam.d/su /etc/pam.d/su-l /etc/ssh/sshd_config 相关参数如 LoginGraceTime/Allowusers/PermitRootLogin。 问题不好确认的时候， 可以将 shadow 密码字段清空， 看看登陆是否正常， 可以判断是否到密码验证阶段了。 之前有过一篇关于 ssh 问题排查的文档，可参考：ssh问题排查指南 1.1.3 系统登陆不进去了，不挂盘的情况下怎么操作？ 上面的检查点很多是需要切换到另外的系统环境下去做检查，比如挂载 LiveCD 或者 chroot 切换；但对于使用 ECS 的用户来说，阿里云暂还未提供实例挂载 ISO 镜像的功能，那么如何进行上面的操作呢？可以借助阿里云新推出的卸载系统盘功能，可以把系统盘卸载掉，作为数据盘挂载到一个新的机器，这样就可以执行上面的检查了。 帮助文档 1.1.4 场景覆盖 1.1.4.1 Linux 系统常见问题诊断覆盖以下场景 常用端口是否正确监听（包括ssh端口、80、443) 常用端口安全组检测（包括ssh端口、80、443) tcp_timestamps&tcp_tw_recycle参数检测 cpu占用高分析 fstab配置问题 软链接缺失 ssh相关文件权限异常（比如文件权限777) selinux启用检测 hugepage参数检测 iptables启用检测 /etc/security/limits.conf配置检查 /etc/passwd等文件格式校验 1.1.4.2 Linux 系统常见启动问题修复覆盖以下场景 系统启动-fsck检测修复 系统启动-磁盘/inode占满 系统启动-系统文件/软链接检测 系统启动-根目录/ssh目录777 系统启动-fstab/grub校验 系统启动-selinux检测 系统启动-sysctl内核参数hugepage&minfree检测 vnc登陆-sshdconfig配置检测 vnc登陆-passwd/shadow文件格式&用户检测 vnc登陆-pam.d文件&配置检测 vnc登陆-相关目录0size文件检测 vnc登陆-profile/bashrc检测 vnc登陆-多内核检测 1.1.5 各种脚本 OS参数收集脚本 OS优化脚本 github链接 OS优化脚本OSS链接 OS离线修复脚本 1.2 grub.conf 文件内容被清空了怎么办 简介 /boot/grub/grub.conf 被清空，系统启动就进入 grub 状态（CentOS 6.8）。 find /boot/grub/stage1。 显示为（hd0,0）。 确认一下内核的具体版本 ls -l /boot 手动设置 grub 具体步骤如下 grub> root (hd0,0) # 是说跟分区在第一块硬盘的第 1 个分区 实际对于前面的 find 出来的那个文件 grub> kernel /boot/vmlinuz-2.6.32-696.3.2.el6.x86_64 ro root=/dev/vda1 # 指明内核路径和根分区，注意 ro 是只读 grub> initrd /boot/initramfs-2.6.32-696.3.2.el6.x86_64.img # 指明 initramfs 路径启动系统加载驱动 grub> boot # 启动上面指定的系统，如果是 reboot 就等于重启整个系统了，刚才的设置就失效了 如果没有报错的话，即可成功启动，进入到系统内部后需要继续支持。 mount -e remount,rw / 重新挂载分区为读写。 service network restart。 如果提示 eth0 eth1 失败，ifconfig 看不到网卡的话。 lsmod |grep net。 看下 virtio_net 这个驱动有没有，如果没有的话（网卡报错基本都不会有）。 insmod /lib/modules/2.6.32-696.3.2.el6.x86_64/kernel/drivers/net/virtio_ net.ko。 重启网络服务，嗨 ~ 网通了。 登陆 ssh， 找个同版本系统的 grub.conf，拷贝一份过来， 不然重启之后又进 grub 了。参考文章 1.3 巧妙利用 strace 查找丢失的文件 问题描述 客户反馈系统无法远程登陆，实际系统启动本身就有问题。 根据报错信息来看，是系统内读取 user 有问题，需要挂盘查看。 挂盘后 chroot 如下 ihave no name，这里本身就是有问题了，说明系统内缺少了什么文件导致异常。 root@debian:~# chroot/mnt [ I have no name !@debian / ] # strace 跟踪一下 chroot 的过程，看下丢失的文件。 strace -F -ff -t -tt -s 256 -o ch.out chroot /mnt grep -i \"no such\" ch.out.pid |grep \"so\" 查看对应文件的关系（测试机补图）。 确认系统上丢了最终的libnss_files-2.12.so，尝试拷贝一个。 ifconfig eth1 netmask route add default gw 此时已经可以上网了，去拷贝一个同版本的文件试试吧。 1.4 小心 PAM 不让你登陆 问题描述 ssh 可以登陆，管理终端无法登陆 root，提示 login in... 先通过 ssh 方式登录系统，查看登录日志是否有异常。 $ cat /var/log/secure Jun 2 09:26:48 iZbp1begsz1x269nxhtip4Z login: FAILED LOGIN 1 FROM tty1 FOR root，Authentication failure 似乎是 login 验证模块的问题进一步查看对应的配置文件 /etc/pam.d/login。 $ cat /etc/pam.d/login #%PAM-1.0 auth required pam_succeed_if.so user != root quiet auth [user_unknown=ignore success=ok ignore=ignore default=bad] pam_ securetty.so auth substack system-auth auth include postlogin account required pam_nologin.so account include system-auth password include system-auth # pam_selinux.so close should be the first session rule session required pam_selinux.so close session required pam_loginuid.so session optional pam_console.so # pam_selinux.so open should only be followed by sessions to be executed in the user context session required pam_selinux.so open session required pam_namespace.so session optional pam_keyinit.so force revoke session include system-auth session include postlogin -session optional pam_ck_connector.so 其中一行的作用为禁止本地登录，可以将其注释掉即可。 auth required pam_succeed_if.so user != root quiet 1.5 CentOS 登陆卡住的原因被我找到了 问题描述 系统登陆卡住，需要 ctrl +c 才能进去，如图。 如果一直等的话，会提示如下截图： 原因 /etc/profile 里面有 source /etc/profile 引起死循环，注释即可。 二、Linux 性能问题 说明 Linux 性能问题的排查和处理一直是系统管理和运维人员的\"心头之患\"， CPU 负载高但找不到消耗大的进程；系统出现 OOM（Out of Memory）只会一味地增大内存容量，而没有很好地理解和分析问题背后产生的根因。而这些都对线上业务的可靠和稳定性提出了挑战。本文将阿里云售后遇到的较为常见的几个系统性能问题进行展开分析，并给出一些合理的改进和优化方案。 2.1 找到 Linux 虚机 Load 高的\"元凶\" 问题描述 有客户反馈他们的一台 ECS 周期性地 load 升高，他们的业务流量并没有上升，需要 我们排查是什么原因造成的，是否因为底层异常？ 要弄清 Linux 虚机 load 高，我们要搞清楚 Linux top 命令中 Load 的含义。 2.1.1 Load average 的值从何而来 在使用 top 命令检查系统负载的时候，可以看到 Load averages 字段，但是这个字段并不是表示 CPU 的繁忙程度，而是度量系统整体负载。 Load averages 采样是从 /proc/loadavg 中获取的： 0.00 0.01 0.05 1/161 29703 每个值的含义依次为： lavg_1 (0.00) 1- 分钟平均负载 lavg_5 (0.01) 5- 分钟平均负载 lavg_15(0.05) 15- 分钟平均负载 nr_running (1) 在采样时刻，运行队列的任务的数目，与 /proc/stat 的 procs_running 表示相同意思， 这个数值是当前可运行的内核调度对象（进程，线程）。 nr_threads (161) 在采样时刻，系统中活跃的任务的个数（不包括运行已经结束的任务），即这个数值表示当前存 在系统中的内核可调度对象的数量。 last_pid(29703) 系统最近创建的进程的 PID，包括轻量级进程，即线程。 假设当前有两个 CPU，则每个 CPU 的当前任务数为 0.00/2=0.00 如果你看到 load average 数值是 10， 则表明平均有 10 个进程在运行或等待状态。 有可能系统有很高的负载但是 CPU 使用率却很低，或者负载很低而 CPU 利用率很高，因为这两者没有直接关系。源码中对于这一块的说明 static int loadavg_proc_show(struct seq_file *m，void *v) { unsigned long avnrun[3]; get_avenrun(avnrun，FIXED_1/200，0); seq_printf(m，\"%lu.%02lu %lu.%02lu %lu.%02lu %ld/%d %d\\n\", LOAD_INT(avnrun[0])，LOAD_FRAC(avnrun[0]), LOAD_INT(avnrun[1])，LOAD_FRAC(avnrun[1]), LOAD_INT(avnrun[2])，LOAD_FRAC(avnrun[2]), nr_running()，nr_threads, task_active_pid_ns(current)->last_pid); return 0; } Load 的计算函数： static unsigned long calc_load(unsigned long load，unsigned long exp，unsigned long active) { load *= exp; load += active * (FIXED_1 - exp); return load >> FSHIFT; } /* * calc_load - update the avenrun load estimates 10 ticks after the * CPUs have updated calc_load_tasks. */ void calc_global_load(void) { unsigned long upd = calc_load_update + 10; long active; if (time_before(jiffies，upd)) return; active = atomic_long_read(&calc_load_tasks); active = active > 0 ? active * FIXED_1 : 0; avenrun[0] = calc_load(avenrun[0]，EXP_1，active); avenrun[1] = calc_load(avenrun[1]，EXP_5，active); avenrun[2] = calc_load(avenrun[2]，EXP_15，active); calc_load_update += LOAD_FREQ; } /* * These are the constant used to fake the fixed-point load-average * counting. Some notes: * - 11 bit fractions expand to 22 bits by the multiplies: this gives * a load-average precision of 10 bits integer + 11 bits fractional * - if you want to count load-averages more often，you need more * precision，or rounding will get you. With 2-second counting freq, * the EXP_n values would be 1981，2034 and 2043 if still using only * 11 bit fractions. */ extern unsigned long avenrun[]; /* Load averages */ extern void get_avenrun(unsigned long *loads，unsigned long offset，int shift); #define FSHIFT 11 /* nr of bits of precision */ #define FIXED_1 (1>= FSHIFT; 从这个函数中可以看到， 内核计算 load 采用的是一种平滑移动的算法，Linux 的系统负载指运行队列的平均长度， 需要注意的是：可运行的进程是指处于运行队列的进程， 不是指正在运行的进程。 即进程的状态是 TASK_RUNNING 或者 TASK_ UNINTERRUPTIBLE。 Linux 内核定义一个长度为3的双字数组 avenrun，双字的低 11 位用于存放负载的小数部分，高 21 位用于存放整数部分。当进程所耗的 CPU 时间片数超过 CPU 在5秒内能够提供的时间片数时，内核计算上述的三个负载，负载初始化为 0。 假设最近 1、5、15 分钟内的平均负载分别为 load1、load5 和 load15，那么下一个计算时刻到来时，内核通过下面的算式计算负载： load1 -= load1 - exp(-5 / 60) -+ n (1 - exp(-5 / 60 )) load5 -= load5 - exp(-5 / 300) + n (1 - exp(-5 / 300)) load15 = load15 exp(-5 / 900) + n (1 - exp(-5 / 900)) 其中，exp(x) 为 e 的 x 次幂，n 为当前运行队列的长度。 2.1.2 如何找出系统中 load 高时处于运行队列的进程 通过前面的讲解， 我们已经明白有可能系统有很高的负载但是 CPU 使用率却很低， 或者负载很低而 CPU 利用率很高，这两者没有直接关系，如何用脚本统计出来处于运行队列的进程呢？ 每隔 1s 统计一次： #!/bin/bash LANG=C PATH=/sbin:/usr/sbin:/bin:/usr/bin interval=1 length=86400 for i in $(seq 1 $(expr ${length} / ${interval}));do date LANG=C ps -eTo stat,pid,tid,ppid,comm --no-header | sed -e 's/^ \\*//' | perl -nE 'chomp;say if (m!^\\S*[RD]+\\S*!)' date cat /proc/loadavg echo -e \"\\n\" sleep ${interval} done 从统计出来的结果可以看到： at Jan 20 15:54:12 CST 2018 D 958 958 957 nginx D 959 959 957 nginx D 960 960 957 nginx D 961 961 957 nginx R 962 962 957 nginx D 963 963 957 nginx D 964 964 957 nginx D 965 965 957 nginx D 966 966 957 nginx D 967 967 957 nginx D 968 968 957 nginx D 969 969 957 nginx D 970 970 957 nginx D 971 971 957 nginx D 972 972 957 nginx D 973 973 957 nginx D 974 974 957 nginx R 975 975 957 nginx D 976 976 957 nginx D 977 977 957 nginx D 978 978 957 nginx D 979 979 957 nginx R 980 980 957 nginx D 983 983 957 nginx D 984 984 957 nginx D 985 985 957 nginx D 986 986 957 nginx D 987 987 957 nginx D 988 988 957 nginx D 989 989 957 nginx R 11908 11908 18870 ps Sat Jan 20 15:54:12 CST 2018 25.76 20.60 19.00 12/404 11912 注：R 代表运行中的队列，D 是不可中断的睡眠进程 在 load 比较高的时候，有大量的 nginx 处于 R 或者 D 状态，他们才是造成 load 上升的元凶，和我们底层的负载确实是没有关系的。 最后也给大家 share 一下查 CPU 使用率比较高的线程小脚本： #!/bin/bash LANG=C PATH=/sbin:/usr/sbin:/bin:/usr/bin interval=1 length=86400 for i in $(seq 1 $(expr ${length} / ${interval}));do date LANG=C ps -eT -o%cpu,pid,tid,ppid,comm | grep -v CPU | sort -n -r | head -20 date LANG=C cat /proc/loadavg { LANG=C ps -eT -o%cpu,pid,tid,ppid,comm | sed -e 's/^ *//' | tr -s ' ' | grep -v CPU | sort -n -r | cut -d ' ' -f 1 | xargs -I{} echo -n \"{} + \" && echo '0'; } | bc -l sleep ${interval} done fuser -k $0 2.2 OOM killer 是被谁触发的 2.2.1 问题描述 用户发现自己的服务器 CPU 在某一时刻陡然升高，但从监控上看，同一时刻的业务量却并不高，客户怀疑是云服务器有问题，希望技术支持团队予以解决。 经过我们的排查， 发现 cpu 的两次间歇飙高是由于客户系统当时发生了 OOM（out of memory）的情况， 并触发了 oom-killer 造成的。 但客户并不接受这个结论， 认为是云服务器的异常导致了 cpu 飙高，而 cpu 的升高又导致了 oom 情况的发生。也就是对于 cpu 升高和 oom 谁为因果这件事上，客户和我们持完全相反的态度。 下面我们将通过对 oom 时系统日志的解读来说明 cpu 升高和 oom 之间的因果关系。 2.2.2 知识点梳理 2.2.2.1 预备知识 在解读日志之前，我们先回顾一下 linux 内核的内存管理。 2.2.2.1.1 几个基本的概念 (1) Page 页 处理器的最小 '寻址单元' 是字节或者字，而页是内存的 '管理单元'。 (2) Zone 区 (a) 区存在的原因： 有些硬件设备只能对特定的内存地址执行 DMA（direct memory access） 操作。 在一些架构中，实际物理内存是比系统可寻址的虚拟内存要大的，这就导致有些物理内存没有办法被永久的映射在内核的地址空间中。 区的划分也是直接以上面两个原因为依据的。 (b) 区的种类 ZONE_DMA 这个区包含的 page 可以执行 DMA 操作。这部分区域的大小和 CPU 架构有关，在 x86 架构中，该部分区域大小限制为 16MB。 ZONE_DMA32 类似于 ZOME_DMA， 这个区也包含可以执行 DMA 操作的page。该区域只存在于64位系统中，适合32位的设备访问。 ZONE_NORMAL 这个区包含可以正常映射到地址空间中的 page，或者说这个区包含了除了 DMA 和 HIGHMEM 以外的内存。 许多内核操作都仅在这个区域进行。 ZONE_HIGHMEM 这个区包含的是 high memory，也就是那些不能被永久映射到内核地址空间的页。 32位的 x86 架构中存在三种内存区域，ZONE_DMA，ZONE_NORMAL， ZONE_HIGHMEM。根据地址空间划分的不同，三个区域的大小不一样： 1G 内核空间 /3G 用户空间 | 区域 | 内存范围 | | ---------------- | -------------- | | ZONE_DMA | | | ZONE_NORMAL | 16M ~ 896M | | ZONE_HIGHMEM | > 896M | 4G 内核空间 /4G 用户空间 | 区域 | 内存范围 | | ---------------- | --------------- | | ZONE_DMA | | | ZONE_NORMAL | 16M ~ 3968M | | ZONE_HIGHMEM | > 3968M | 64位的系统由于寻址能力的提高， 不存在 highmem 区， 所以64位系统中存在的区有 DMA，DMA32 和 NORMAL 三个区。 | 区域 | 内存范围 | | --------------- | ------------ | | ZONE_DMA | | | ZONE_DMA32 | 16M ~ 4G | | ZONE_NORMAL | > 4G | 2.2.2.1.2 内核分配内存的函数 下面是内核分配内存的核心函数之一，它会分配2的 order 次方个连续的物理页内存，并将第一页的逻辑地址返回。 unsigned long __get_free_pages(gfp_t gfp_mask，unsigned int order) 内核空间的内存分配函数和用户空间最大的不同就是每个函数会有一个 gfp_ mask 参数。 其中 gfp 代表的就是我们上面的内存分配函数__get_free_pages()。 gfp_mask 可以分成三种：行为修饰符（action modifier）, 区修饰符（zone modifier）和类型（type）。 行为修饰符是用来指定内核该如何分配内存的。 比如分配内存时是否可 以进行磁盘 io，是否可以进行文件系统操作，内核是否可以睡眠（sleep） 等等。 区修饰符指定内存需要从哪个区来分配。 类型是行为修饰符和区修饰符结合之后的产物。在一些特定的内存分配场 合下， 我们可能需要同时指定多个行为修饰符和区修饰符， 而 type 就是 针对这些固定的场合，将所需要的行为修饰符和区修饰符都整合到了一起， 这样使用者只要指定一个 type 就可以了。 不同 type 所代表的含义可以参看下面的表格： 2.2.2.2 日志解读 下面是从 oom killer 被触发到进程到被杀掉的一个大概过程， 我们来具体看 一下。 nginx invoked oom-killer: gfp_mask=0x200da，order=0，oom_score_adj=0 nginx cpuset=6011a7f12bac1c4592ce41407bb41d49836197001a0e355f5a1d9589e4001e42 mems_allowed=0 CPU: 1 PID: 10242 Comm: nginx Not tainted 3.13.0-86-generic #130-Ubuntu Hardware name: Xen HVM domU，BIOS 4.0.1 12/16/2014 0000000000000000 ffff880070611a00 ffffffff8172a3b4 ffff88012af6c800 0000000000000000 ffff880070611a88 ffffffff8172495d ffffffff81069b76 ffff880070611a60 ffffffff810ca5ac ffff88020fff7e38 0000000000000000 Node 0 DMA free:15908kB min:128kB low:160kB high:192kB active_anon:0kB inactive_anon:0kB active_file:0kB inactive_file:0kB unevictable:0kB isolated(anon):0kB isolated(file):0kB present:15992kB managed:15908kB mlocked:0kB dirty:0kB writeback:0kB mapped:0kB shmem:0kB slab_ reclaimable:0kB slab_unreclaimable:0kB kernel_stack:0kB pagetables:0kB unstable:0kB bounce:0kB free_cma:0kB writeback_tmp:0kB pages_scanned:0 all_ unreclaimable? yes lowmem_reserve[]: 0 3746 7968 7968 Node 0 DMA32 free:48516kB min:31704kB low:39628kB high:47556kB active_ anon:3619272kB inactive_anon:216kB active_file:556kB inactive_file:1516kB unevictable:0kB isolated(anon):0kB isolated(file):0kB present:3915776kB managed:3836724kB mlocked:0kB dirty:4kB writeback:0kB mapped:324kB shmem:1008kB slab_reclaimable:67136kB slab_unreclaimable:67488kB kernel_ stack:1792kB pagetables:14540kB unstable:0kB bounce:0kB free_cma:0kB writeback_tmp:0kB pages_scanned:7365 all_unreclaimable? yes lowmem_reserve[]: 0 0 4221 4221 Node 0 Normal free:35640kB min:35748kB low:44684kB high:53620kB active_ anon:4019124kB inactive_anon:292kB active_file:1292kB inactive_file:2972kB unevictable:0kB isolated(anon):0kB isolated(file):0kB present:4456448kB managed:4322984kB mlocked:0kB dirty:24kB writeback:4kB mapped:1296kB shmem:1324kB slab_reclaimable:81196kB slab_unreclaimable:83432kB kernel_ stack:3392kB pagetables:20252kB unstable:0kB bounce:0kB free_cma:0kB writeback_tmp:0kB pages_scanned: 7874 all_unreclaimable? yes lowmem_reserve[]: 0 0 0 0 Node 0 DMA: 1*4kB (U) 0*8kB 0*16kB 1*32kB (U) 2*64kB (U) 1*128kB (U) 1*256kB (U) 0*512kB 1*1024kB (U) 1*2048kB (R) 3*4096kB (M) = 15908kB Node 0 DMA32: 1101*4kB (UE) 745*8kB (UEM) 475*16kB (UEM) 263*32kB (EM) 88*64kB (UEM) 25*128kB (E)12*256kB (EM) 6*512kB (E) 7*1024kB (EM) 0*2048kB 0*4096kB = 48524kB Node 0 Normal: 5769*4kB (EM) 1495*8kB (EM) 24*16kB (UE) 0*32kB 0*64kB 0*128kB 0*256kB 0*512kB 0*1024kB 0*2048kB 0*4096kB = 35420kB Node 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_ size=2048kB 2273 total pagecache pages 0 pages in swap cache Swap cache stats: add 0，delete 0，find 0/0 Free swap = 0kB Total swap = 0kB 2097054 pages RAM 0 pages HighMem/MovableOnly 33366 pages reserved [ pid ] uid tgid total_vm rss nr_ptes swapents oom_score_adj name [ 355] 0 355 4868 66 13 0 0 upstart-udev-br [ 361] 0 361 12881 145 28 0 -1000 systemd-udevd [ 499] 0 499 3814 60 13 0 0 upstart-socket- [ 562] 0 562 5855 79 15 0 0 rpcbind [ 644] 106 644 5398 142 16 0 0 rpc. statd [ 775] 0 775 3818 58 12 0 0 upstart-file-br ...（此处有省略） [10396] 104 10396 21140 12367 44 0 0 nginx [10397] 104 10397 21140 12324 44 0 0 nginx [10398] 104 10398 21140 12324 44 0 0 nginx [10399] 104 10399 21140 12367 44 0 0 nginx Out of memory: Kill process 10366 (nginx) score 6 or sacrifice child Killed process 10366 (nginx) total-vm:84784kB，anon-rss:49156kB，filerss:520kB 先来看一下第一行，它给出了 oom killer 是由谁触发的信息。 nginx invoked oom-killer: gfp_mask=0x200da，order=0，oom_score_adj=0 order=0 告诉我们所请求的内存的大小是多少， 即 nginx 请求了2的0次方这么多个 page 的内存，也就是一个 page，或者说是 4KB。 gfp_mask 的最后两个 bit 代表的是 zone mask，也就是说它指明内存应该从哪个区来分配。 Flag value Description 0x00u 0 implicitly means allocate from ZONE_NORMAL __GFP_DMA 0x01u Allocate from ZONE_DMA if possible __GFP_HIGHMEM 0x02u Allocate from ZONE_HIGHMEM if possible (这里有一点需要注意，在 64 位的 x86 系统中，是没有 highmem 区的，64 位系统中的 normal 区就对应上表中的 highmem 区。） 在本案例中，zonemask是2，也就是说 nginx 正在从 zone － normal（64 位 系统）中请求内存。 其他标志位的含义如下： #define __GFP_WAIT 0x10u /* Can wait and reschedule? */ #define __GFP_HIGH 0x20u /* Should access emergency pools? */ #define __GFP_IO 0x40u /* Can start physical IO? */ #define __GFP_FS 0x80u /* Can call down to low-level FS? */ #define __GFP_COLD 0x100u /* Cache-cold page required */ #define __GFP_NOWARN */ 0x200u /* Suppress page allocation failure warning */ #define __GFP_REPEAT 0x400u /* Retry the allocation. Might fail */ #define __GFP_NOFAIL 0x800u /* Retry for ever. Cannot fail */ #define __GFP_NORETRY 0x1000u /* Do not retry. Might fail */ #define __GFP_NO_GROW 0x2000u /* Slab internal usage */ #define __GFP_COMP 0x4000u /* Add compound page metadata */ #define __GFP_ZERO 0x8000u /* Return zeroed page on success */ #define __GFP_NOMEMALLOC 0x10000u /* Don’t use emergency reserves */ #define __GFP_NORECLAIM allocation 0x20000u /* No realy zone reclaim during */ 所以我们当前这个内存请求带有这几个标志：GFP_NORECLAIM，GFP_FS， GFP_IO，GFP_WAIT，都是比较正常的几个标志，那么我们这个请求为什么 会有问题呢？继续往下看，可以看到下面的信息： Node 0 Normal free:35640kB min:35748kB low:44684kB high:53620kB active_ anon:4019124kB inactive_anon:292kB active_file:1292kB inactive_file:2972kB unevictable:0kB isolated(anon):0kB isolated(file):0kB present:4456448kB managed:4322984kB mlocked:0kB dirty:24kB writeback:4kB mapped:1296kB shmem:1324kB slab_reclaimable:81196kB slab_unreclaimable:83432kB kernel_ stack:3392kB pagetables:20252kB unstable:0kB bounce:0kB free_cma:0kB writeback_tmp:0kB pages_scanned: 7874 all_unreclaimable? yes 可以看到 normal 区 free 的内存只有 35640KB，比系统允许的最小值（min）还要低，这意味着 application 已经无法再从系统中申请到内存了，并且系统会开始启动 oom killer 来缓解系统内存压力。 这里我们说一下一个常见的误区，就是有人会认为触发了 oom killer 的进程就是问题的罪魁祸首，比如我们这个例子中的这个 nginx 进程。其实日志中 invoke oom killer 的这个进程有时候可能只是一个受害者， 因为其他应用／进程已将系统内存用尽， 而这个 invoke oomkiller 的进程恰好在此时发起了一个分配内存的请求而已。 在系统内存已经不足的情况下， 任何一个内存请求都可能触发 oom killer 的启动。 oom killer 的启动会使系统从用户空间转换到内核空间。内核会在短时间内进行大量的工作， 比如计算每个进程的 oom 分值， 从而筛选出最适合杀掉的进程。 我们从日志中也可以看到这一筛选过程： [ pid ] uid tgid total_vm rss nr_ptes swapents oom_score_adj name [ 355] 0 355 4868 66 13 0 0 upstart-udev-br [ 361] 0 361 12881 145 28 0 -1000 systemd-udevd [ 499] 0 499 3814 60 13 0 0 upstart-socket- [ 562] 0 562 5855 79 15 0 0 rpcbind [ 644] 106 644 5398 142 16 0 0 rpc. statd [ 775] 0 775 3818 58 12 0 0 upstart-file-br ... [10396] 104 10396 21140 12367 44 0 0 nginx [10397] 104 10397 21140 12324 44 0 0 nginx [10398] 104 10398 21140 12324 44 0 0 nginx [10399] 104 10399 21140 12367 44 0 0 nginx 本例中，一个 nginx 进程被选中作为缓解内存压力的牺牲进程： Out of memory: Kill process 10366 (nginx) score 6 or sacrifice child Killed process 10366 (nginx) total-vm:84784kB，anon-rss:49156kB， file-rss:520kB 整个过程进行的时间很短，只有毫秒级别，但是工作量／计算量很大，这就导致了 cpu 短时间内迅速飙升， 出现峰值。 但这一切工作都由内核在内核空间中完成，所以用户在自己的业务监控数据上并不会看到业务量的异常。这些短时间升高的 cpu 是内核使用的，而不是用户的业务。 本例中客户只是偶尔看到这个现象，且业务并没有受到影响。我们给客户的建议是分析业务内存需求量最大值，如果系统已经没有办法满足特定时段业务的内存需求， 建议用户升级内存来避免 oom 的情况发生， 因为严重的 oom 情况是可能引发系统崩溃的。 2.3 我的服务器内存去哪儿了 背景 收到报警，系统的内存使用率触发阈值（部分图是后补的）。 登陆系统，使用命令(top 按M)查看内存分配。 使用命令free -m查看内存使用情况 使用命令atop看下内存分配（cat /proc/meminfo 也可以看到一些细化的内存使用信息）。 发现 cache 才 1.7G，slab 非常高，4.4G，slab 内存简单理解为是系统占用的。 使用 slabtop 继续分析。 看到 proc_inode_cache 使用的最多， 这个代表是 proc 文件系统的 inode 占用的。 使用命令ps -eLf查进程，但是进程不多，再查线程，可以通过如下命令进行检查。得到如下的结果：( 原图缺失，使用测试机查看到的截图来补充说明 ) 计算 socket $ ll /proc/22360/task/*/fd/ |grep socket |wc -l 140 计算一下有多少fd $ ll /proc/22360/task/*/fd/ | wc -l 335 每个 socket 的 inode 也不一样。 当时看到的现场有几万个 fd， 基本全是 socket， 每个 inode 都是占用空间的， 且 proc 文件系统是全内存的。 所以我们才会看到 slab 中 proc_inode_cache 内存占用高。 建议 建议用户需要从程序上优化相关的 server 端 ~ 2.4 CPU 占用不高但网络性能很差的一个原因 简介 我们经常碰到整体 cpu 不高，但是性能不佳的案例，这种案例往往跟 CPU 处理中断的核心跑满有关系，话不多说，我们来看看中断相关的案例。 2.4.1 什么是中断？ 当一个硬件 ( 如磁盘控制器或者以太网卡 )，需要打断 CPU 的工作时，它就触发一个中断。该中断通知 CPU 发生了某些事情并且 CPU 应该放下当前的工作去处理这个事情。为了防止多个设置发送相同的中断，Linux 设计了一套中断请求系统，使得计算机系统中的每个设备被分配了各自的中断号，以确保它的中断请求的唯一性。 从 2.4 内核开始，Linux 改进了分配特定中断到指定的处理器 ( 或处理器组 ) 的功能。 这被称为 SMP IRQ affinity，它可以控制系统如何响应各种硬件事件。允许你限制或者重新分配服务器的工作负载，从而让服务器更有效的工作。 以网卡中断为例，在没有设置 SMP IRQ affinity 时，所有网卡中断都关联到 CPU0， 这导致了 CPU0 负载过高，而无法有效快速的处理网络数据包，导致了瓶颈。 通过 SMP IRQ affinity，把网卡多个中断分配到多个 CPU 上，可以分散 CPU 压力， 提高数据处理速度。 但是 smp_affinity 要求网卡支持多队列， 如果网卡支持多队列则设置才有作用，网卡有多队列，才会有多个中断号，这样就可以把不同的中断号分配到不同 CPU 上，这样中断号就能相对均匀的分配到不同的 CPU 上。 而单队列的网卡可以通过 RPS/RFS 来模拟多队列的情况，但是该效果并不如网卡本身多队列 + 开启 RPSRFS 来的有效。 2.4.2 什么是 RPS/RFS RPS（Receive Packet Steering）主要是把软中断的负载均衡到各个 cpu，简单来说，是网卡驱动对每个流生成一个 hash 标识， 这个 HASH 值得计算可以通过四元组来计算（SIP，SPORT，DIP，DPORT），然后由中断处理的地方根据这个 hash 标识分配到相应的 CPU 上去，这样就可以比较充分的发挥多核的能力了。通俗点说就是在软件层面模拟实现硬件的多队列网卡功能，如果网卡本身支持多队列功能的话 RPS 就不会有任何的作用。该功能主要针对单队列网卡多 CPU 环境，如网卡支持多队列则可使用 SMP irq affinity 直接绑定硬中断。 由于 RPS 只是单纯把数据包均衡到不同的 cpu，这个时候如果应用程序所在的 cpu 和软中断处理的 cpu 不是同一个， 此时对于 cpu cache 的影响会很大， 那么 RFS （Receive flow steering）确保应用程序处理的 cpu 跟软中断处理的 cpu 是同一个，这样就充分利用 cpu 的 cache，这两个补丁往往都是一起设置，来达到最好的优化效果，主要是针对单队列网卡多 CPU 环境。 rps_flow_cnt，rps_sock_flow_entries， 参数的值会被进位到最近的2的幂次方值，对于单队列设备， 单队列的 rps_flow_cnt 值被配置成与rps_sock_flow_ entries 相同。 RFS 依靠 RPS 的机制插入数据包到指定 CPU 的 backlog 队列，并唤醒那个 CPU 来执行。 默认情况下，开启 irqbalance 是足够用的，但是对于一些对网络性能要求比较高的场景，手动绑定中断磨合是比较好的选择。 开启 irqbalance，会存在一些问题，比如： a) 有时候计算出来的值不合理，导致 CPU 使用还是不均衡。 b) 在系统比较空闲 IRQ 处于 Power-save mode 时，irqbalance 会将中断集中分配给第一个 CPU， 以保证其它空闲 CPU 的睡眠时间，降低能耗。如果压力突然上升，可能会由于调整的滞后性带来性能 问题。 c) 处理中断的 CPU 总是会变，导致了更多的 context switch。 d）也存在一些情况，启动了 irqbalance，但是并没有生效，没有真正去设置处理中断的cpu。 2.4.3 如何查看网卡的队列数 Combined 代表队列个数，说明我的测试机有 4 个队列。 # ethtool -l eth0 Channel parameters for eth0: Pre-set maximums: RX: 0 TX: 0 Other: 0 Combined: 4 Current hardware settings: RX: 0 TX: 0 Other: 0 Combined: 4 以 CentOS7.6 为例，系统处理中断的记录在 /proc/interrupts 文件里面，默认这个文件记录比较多，影响查看，同时如果 cpu 核心也非常多的话， 对于阅读的影响非常大。 # cat /proc/interrupts CPU0 CPU1 CPU2 CPU3 0: 141 0 0 0 IO-APIC-edge timer 1: 10 0 0 0 IO-APIC-edge i8042 4: 807 0 0 0 IO-APIC-edge serial 6: 3 0 0 0 IO-APIC-edge floppy 8: 0 0 0 0 IO-APIC-edge rtc0 9: 0 0 0 0 IO-APIC-fasteoi acpi 10: 0 0 0 0 IO-APIC-fasteoi virtio3 11: 22 0 0 0 IO-APIC-fasteoi hcd:usb1 12: 15 0 0 0 IO-APIC-edge i8042 14: 0 0 0 0 IO-APIC-edge ata_piix 15: 0 0 0 0 IO-APIC-edge ata_piix 24: 0 0 0 0 PCI-MSI-edge virtio1-config 。。。 27: 1913 0 0 0 PCI-MSI-edge virtio2-input.0 28: 3 834 0 0 PCI-MSI-edge virtio2-output.0 input0 说明是 cpu0（第 1 个 CPU）处理的网络中断 阿里云 ecs 网络中断，如果是多个中断的话，还有 input.1 input.2 input.3 这种形式 如果 ecs 的 cpu 核心非常多，那这个文件看起来就会比较费劲了，可使用下面 的命令查看处理中断的核心。 使用下面这个命令，即可将阿里云 ecs 处理中断的 cpu 找出来了（下面这个演示是 8c4个队列） # for i in $(egrep \"\\-input.\" /proc/interrupts |awk -F \":\" '{print $1}');do cat /proc/irq/$i/smp_affinity_list;done 5 7 1 3 处理一下 sar 拷贝用 # for i in $(egrep \"\\-input.\" /proc/interrupts |awk -F \":\" '{print $1}');do cat /proc/irq/$i/smp_affinity_list;done |tr -s '\\n' ',' 5,7,1,3, # sar -P 5,7,1,3 1 每秒刷新一次 cpu 序号为 5,7,1,3 核心的 cpu 使用率 # sar -P ALL 1 每秒刷新所有核心，用于少量 CPU 核心的监控，这样我们就可以知道处理慢的原因是不是因为队列不够导致的了 Linux 3.10.0-957.5.1.el7.x86_64 (iZwz98aynkjcxvtra0f375Z) x86_64_ (4 CPU) 05:10:06 PM CPU %user %nice %system %iowait %steal %idle 05:10:07 PM all 5.63 0.00 3.58 1.02 0.00 89.77 05:10:07 PM 0 6.12 0.00 3.06 1.02 0.00 89.90 05:10:07 PM 1 5.10 0.00 5.10 0.00 0.00 89.80 05:10:07 PM 2 5.10 0.00 3.06 2.04 0.00 89.80 05:10:07 PM 3 5.10 0.00 4.08 1.02 0.00 89.80 05:10:07 PM CPU %user %nice %system %iowait %steal %idle 05:10:08 PM all 8.78 0.00 15.01 0.69 0.00 75.52 05:10:08 PM 0 10.00 0.00 16.36 0.91 0.00 75.73 05:10:08 PM 1 4.81 0.00 13.46 1.92 0.00 79.81 05:10:08 PM 2 10.91 0.00 15.45 0.91 0.00 72.73 05:10:08 PM 3 9.09 0.00 14.55 0.00 0.00 76.36 sar 小技巧 打印 idle 小于 10 的核心 sar -P 1,3,5,7 1 |tail -n+3|awk '$NF 关闭 IRQbalance。 # service irqbalance status Redirecting to /bin/systemctl status irqbalance.service ● irqbalance.service - irqbalance daemon Loaded: loaded (/usr/lib/systemd/system/irqbalance.service; enabled; vendor preset: enabled) Active: inactive (dead) since Wed 2020-05-27 14:39:28 CST; 2s ago Process: 1832 ExecStart=/usr/sbin/irqbalance --foreground $IRQBALANCE_ ARGS (code=exited，status=0/SUCCESS) Main PID: 1832 (code=exited，status=0/SUCCESS) May 27 14:11:40 iZbp1ee4vpiy3w4b8y2m8qZ systemd[1]: Started irqbalance daemon. May 27 14:39:28 iZbp1ee4vpiy3w4b8y2m8qZ systemd[1]: Stopping irqbalance daemon... May 27 14:39:28 iZbp1ee4vpiy3w4b8y2m8qZ systemd[1]: Stopped irqbalance daemon. 手动设置 RPS。 手动设置之前我们需要先了解下面的文件（IRQ_number 就是前面 grep input 拿到的序号）。 进入 /proc/irq/${IRQ_number}/， 关注两个文件：smp_affinity 和 smp_ affinity_list。 smp_affinity 是 bitmask+16 进制， smp_affinity_list：这个文件更好理解，采用的是 10 进制，可读性高。 改这两个任意一个文件，另一个文件会同步更改。 为了方便理解，咱们直接看十进制的文件 smp_affinity_list 即可。 如果这一步没看明白，注意前面的 /proc/interrupts 的输出 # for i in $(egrep \"\\-input.\" /proc/interrupts |awk -F \":\" '{print $1}'); do cat /proc/irq/$i/smp_affinity_list;done 1 3 1 3 手动设置处理中断的 CPU 号码可以直接 echo 修改，下面就是将序号 27 的中断放到 cpu0 上处理，一般建议可以把 cpu0 空出来 # echo 0 >> /proc/irq/27/smp_affinity_list # cat /proc/irq/27/smp_affinity_list 0 关于 bitmas \"f\" 是十六进制的值对应的 二进制是 \"1111\"（可以理解为 4c 的配置设置为 f 的话，所有的 cpu 参与处理中断） 二进制中的每个位代表了服务器上的每个 CPU. 一个简单的 demo CPU序号 二进制 十六进制 CPU 0 0001 1 CPU 1 0010 2 CPU 2 0100 4 CPU 3 1000 8 需要对每块网卡每个队列分别进行设置。如对 eth0 的 0 号队列设置： echo ff > /sys/class/net/eth0/queues/rx-0/rps_cpus 这里的设置方式和中断亲和力设置的方法是类似的。采用的是掩码的方式，但是这里通常要将所有的 CPU 设置进入，如： 4core，f 8core，ff 16core，ffff 32core，ffffffff 默认在 0 号 cpu 上 # cat /sys/class/net/eth0/queues/rx-0/rps_cpus 0 # echo f >>/sys/class/net/eth0/queues/rx-0/rps_cpus # cat /sys/class/net/eth0/queues/rx-0/rps_cpus f 设置 RFS 的方式。 需要设置两个地方： 全局表rps_sock_flow_table 的条目数量。通过一个内核参数控制： # sysctl -a |grep net.core.rps_sock_flow_entries net.core.rps_sock_flow_entries = 0 # sysctl -w net.core.rps_sock_flow_entries=1024 net.core.rps_sock_flow_entries = 1024 每个网卡队列 hash 表的条目数： # cat /sys/class/net/eth0/queues/rx-0/rps_flow_cnt 0 # echo 256 >> /sys/class/net/eth0/queues/rx-0/rps_flow_cnt # cat /sys/class/net/eth0/queues/rx-0/rps_flow_cnt 256 需要启动 RFS，两者都需要设置。 建议机器上所有的网卡队列设置的 rps_flow_cnt 相加应该小于或者等于 rps_sock_flow_entries。 因为是4个队列， 因此每个队列设置 256， 可以根据实际情况增大。 2.5 一次 IO 异常捕获过程 简介 遇到一个 IO 异常飙升的问题，IO 起飞后系统响应异常缓慢，看不到现场一直无法定位问题， 检查对应时间点应用日志也没有发现异常的访问， 这种问题怎么办呢？ 2.5.1 采集系统 IO，确认 IO 异常发生在系统盘，还是数据盘，使用系统自带的 iostat 即可采集 # iostat -d 3 -k -x -t 30 Linux 3.10.0-957.21.3.el7.x86_64 (tencent) 08/05/2020 _x86_64_ (1 CPU) 06/12/2018 11:51:49 AM Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %util vda 0.12 12.66 4.59 8.55 127.03 143.35 41.16 0.07 7.91 15.92 3.61 0.65 0.85 scd0 0.00 0.00 0.00 0.00 0.00 0.00 7.10 0.00 1.03 1.03 0.00 1.02 0.00 06/12/2018 11:51:52 AM Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %util vda 0.00 27.61 10.44 69.70 247.81 397.31 16.10 0.99 15.57 1.71 17.65 0.16 1.28 scd0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 每隔 3 秒采集一次磁盘 io，输出时间，一共采集 30 次，想一直抓的话把 30 去掉即可，注意磁盘空余量。 通过这个命令我们可以确认如下信息： - 问题发生的时间 - 哪块盘发生的 io - 磁盘的 IOPS（ r/s w/s）以及吞吐量（ rkB/s wkB/s ） 2.5.2 确认哪块盘发生了 IO 还不够，再抓一下发生 IO 的进程，需要安装 iotop 进行捕获 # iotop -b -o -d 3 -t -qqq -n 30 10:18:41 7024 be/4 root 0.00 B/s 2.64 M/s 0.00 % 93.18 % fio -direct=1 -iodepth=128 -rw=randwrite -ioengine=libaio -bs=1k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=iotest -name=Rand_ Write_Testing 每隔 3 秒采集一次，一共采集 30 次，静默模式，只显示有 io 发生的进程 通过这个命令我们可以确认如下信息： - 问题发生的时间 - 产生 IO 的进程 id 以及进程参数（command） - 进程产生的吞吐量（如果有多个可以把 qqq 去掉可显示总量） - 进程占用当前 IO 的百分比 俗话说得好，光说不练假把式，我们实操来看一下 ( 涉及用户进程信息 , 没有得到客户授权因此以 fio 为演示 )。 2.5.3 使用 fio 进行 IO 压测，具体参数可以参考块存储性能 压测窗口： fio -direct=1 -iodepth=128 -rw=randwrite -ioengine=libaio -bs=1k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=iotest -name=Rand_ Write_Testing Rand_Write_Testing: (g=0): rw=randwrite，bs=1K-1K/1K-1K/1K-1K， ioengine=libaio，iodepth=128 fio-2.0.13 Starting 1 process ^Cbs: 1 (f=1): [w] [8.5% done] [0K/2722K/0K /s] [0 /2722 /0 iops] [eta 05m:53s] fio: terminating on signal 2 Rand_Write_Testing: (groupid=0，jobs=1): err= 0: pid=11974: Tue Jun 12 10:36:30 2018 write: io=88797KB，bw=2722.8KB/s，iops=2722 ，runt= 32613msec ...... iotop 窗口： # iotop -n 10 -b -o -d 3 -t -qqq 10:36:03 11974 be/4 root 0.00 B/s 2.63 M/s 0.00 % 93.95 % fio -direct=1 -iodepth=128 -rw=randwrite -ioengine=libaio -bs=1k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=iotest -name=Rand_ Write_Testing 10:36:06 11974 be/4 root 0.00 B/s 2.64 M/s 0.00 % 92.68 % fio -direct=1 -iodepth=128 -rw=randwrite -ioengine=libaio -bs=1k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=iotest -name=Rand_ Write_Testing iostat 窗口： # iostat -d 3 -k -x -t 10 Linux 3.10.0-957.21.3.el7.x86_64 (tencent) 08/05/2020 _x86_64_ (1 CPU) 06/12/2018 12:26:41 PM Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %util vda 0.12 12.66 4.59 8.56 127.09 143.47 41.16 0.07 7.91 15.91 3.61 0.65 0.85 scd0 0.00 0.00 0.00 0.00 0.00 0.00 7.10 0.00 1.03 1.03 0.00 1.02 0.00 06/12/2018 12:26:44 PM Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %util vda 0.00 81.14 32.66 84.18 1202.69 2488.89 63.19 0.31 2.75 3.00 2.66 0.34 3.97 scd0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 通过输出结果的时间点进行对比分析， 相对于 fio 的 IOPS， 吞吐量跟 iotop 以及 iostat 监控到的数据是一致。因此，看到这，您已经学会怎么查消耗 IO 的进程了。 ⚠️为了压测的慢一些特意把 bs 设置为 1k，这样执行的时间会比较久，fio 压 测详见前面的块存储性能。 心细的同学可能发现 iotop 输出不带年月日，如果抓日志的时间超过 24 小时，时间重复怎么办？ # iotop -b -o -d 1 -qqq |awk '{ print $0\"\\t\" strftime(\"%Y-%m-%d-%H:%M:%S\", systime()) } ' 1252 be/3 root 0.00 B/s 22.83 K/s 0.00 % 1.12 % [jbd2/vda1-8] 2018-06-12:28:19 16374 be/4 root 76.12 K/s 0.00 B/s 0.00 % 0.33 % YDService 2018-06-12:28:19 21495 be/4 mysql 0.00 B/s 121.79 K/s 0.00 % 0.15 % mysqld --defaults-file=/etc/my.cnf 2018-06-12:28:19 21493 be/4 mysql 0.00 B/s 0.00 B/s 0.00 % 0.11 % mysqld --defaults-file=/etc/my.cnf 2018-06-12:28:19 21500 be/4 mysql 0.00 B/s 3.81 K/s 0.00 % 0.11 % mysqld --defaults-file=/etc/my.cnf 2018-06-12:28:19 13377 be/4 mysql 0.00 B/s 3.81 K/s 0.00 % 0.09 % mysqld --defaults-file=/etc/my.cnf 2018-06-12:28:19 18071 be/4 mysql 0.00 B/s 3.81 K/s 0.00 % 0.09 % mysqld --defaults-file=/etc/my.cnf 2018-06-12:28:19 32718 be/4 root 0.00 B/s 0.00 B/s 0.00 % 0.01 % [kworker/0:2] 2018-06-12:28:19 10641 be/4 root 0.00 B/s 11.42 K/s 0.00 % 0.00 % java -server -Xms1024m -Xmx1024m -jar ./kooteam.jar 2018-06-12:28:19 30019 be/4 polkitd 0.00 B/s 19.03 K/s 0.00 % 0.00 % postgres: stats collector process 2018-06-12:28:19 1252 be/3 root 0.00 B/s 0.00 B/s 0.00 % 0.71 % [jbd2/vda1-8] 2018-06-12:28:20 16374 be/4 root 90.77 K/s 0.00 B/s 0.00 % 0.47 % YDService 2018-06-12:28:20 21494 be/4 mysql 0.00 B/s 0.00 B/s 0.00 % 0.33 % mysqld --defaults-file=/etc/my.cnf 2018-06-12:28:20 21495 be/4 mysql 0.00 B/s 90.77 K/s 0.00 % 0.14 % mysqld --defaults-file=/etc/my.cnf 2018-06-12:28:20 3922 be/4 mysql 0.00 B/s 3.78 K/s 0.00 % 0.11 % mysqld --defaults-file=/etc/my.cnf 2018-06-12:28:20 详细参数就不讲了，大家可以看下输出结果行最后一列是年月日时分秒，那么 IO 消耗的始作俑者会查了吗？ 三、Linux 主机网络问题 说明 从售后处理角度，阿里云用户业务系统搭建在 ECS 云服务器反馈最多的影响业务可用性问题：一个是前面已经讨论过的系统启停问题，另一个就是网络连通性问题。网络作为业务系统数据交互和转发的 \"通道\"，影响着 IT 系统的各个方面。网络问题涵盖的因素简化来讲一般涉及到收发节点，转发节点，流量链路等方面，由于本文主要分享系统诊断相关的处理经验，因此我们也更关注与 ECS 主机层面相关的网络影响，希望能带给一些处理主机层面网络问题的点拨。 3.1 ifdown ifup 命令丢失处理 问题现象 主机网络不通，登录主机看网卡没有正确配置。 解决方法 尝试重启网卡，发现 ifdown ifup 命令不存在： 通过 ifconfig 配置 IP 信息 ifconfig netmask 然后添加路由 route add -net 0.0.0.0/0 gw 如果是经典网络需要配置上内外网卡 ip 和路由，路由命令 route add -net 10.0.0.0/8 gw route add -net 100.64.0.0/10 gw 网络通了后运行命令 yum -y install initscripts 安装上 ifup ifdown 相关的包 3.2 网络不通？ strace 二度出手 问题现象 主机网络不通，路由不正确，0.0.0.0 指向了 eth0。 问题分析 尝试重启 network 服务，发现不行。 尝试停止网络服务，然后通过 ifup eth1 发现路由是正常的。 然后 ifup eth0 发现路由就异常了，基本定位在启动 eth0 网卡的时候出现了异常。 用 strace -f -e open ifup eth0|more 追踪一下。 运气加眼神比较好，发现调用了 /etc/sysconfig/network 文件。 打开 /etc/sysconfig/network 文件，发现多了一行 GATEWAYDEV=eth0。 解决方案 注释 /etc/sysconfig/network 文件的 GATEWAYDEV=eth0，重启网络服务。 3.3 TIME_WAIT & CLOSE_WAIT 的讨论总结 TIME_WAIT 是 TCP 连接关闭过程中的一个状态，具体是这么形成的： 主动关闭端 A：发 FIN，进入 FIN-WAIT-1 状态，并等待 ...... 被动关闭端 P：收到 FIN 后必须立即发 ACK，进入 CLOSE_WAIT 状态，并等 待 ...... 主动关闭端 A：收到 ACK 后进入 FIN-WAIT-2 状态，并等待 ...... 被动关闭端 P：发 FIN，进入 LAST_ACK 状态，并等待 ...... 主动关闭端 A：收到 FIN 后必须立即发 ACK， 进入 TIME_WAIT 状态， 等待 2MSL 后结束 Socket。 被动关闭端 P：收到 ACK 后结束 Socket。 58　　>　TIME_WAIT & CLOSE_WAIT 的讨论总结 因此，TIME_WAIT 状态是出现在主动发起连接关闭的一点， 和是谁发起的连接无关，可以是 client 端，也可以是 server 端。 而从 TIME_WAIT 状态到 CLOSED 状 态， 有一个超时设置， 这个超时设置是 2*MSL（RFC793 定义了 MSL 为 2 分钟，Linux 设置成了 30s）。 3.3.1 为什么需要 TIME_WAIT ？ 主要有两个原因： 为了确保两端能完全关闭连接。 假设 A 服务器是主动关闭连接方，B 服务器是被动方。 如果没有 TIME_WAIT 状态，A 服务器发出最后一个 ACK 就进入关闭状态， 如果这个 ACK 对端没有收到，对端就不能完成关闭。对端没有收到 ACK，会重发 FIN，此时连接关闭，这个 FIN 也得不到 ACK， 而有 TIME_WAIT， 则会重发这个 ACK， 确保对端能正常关闭连接。 为了确保后续的连接不会收到“脏数据”。 刚才提到主动端进入 TIME_WAIT 后， 等待 2MSL 后 CLOSE， 这里的 MSL 是指（maximum segment lifetime， 我们内核一般是 30s，2MSL 就是1分钟）， 网络上数据包最大的生命周期。 这是为了使网络上由于重传出现的 old duplicate segment 都消失后，才能创建参数（四元组，源 IP/PORT，目标 IP/ PORT）相同的连接，如果等待时间不够长，又创建好了一样的连接，再收到 old duplicate segment，数据就错乱了。 3.3.2 TIME_WAIT 会导致什么问题 新建连接失败。 TIME_WAIT 到 CLOSED，需要 2MSL=60s 的时间。这个时间非常长。每个连接在业务结束之后，需要 60s 的时间才能完全释放。 如果业务上采用的是短连接的方式，会导致非常多的 TIME_WAIT 状态的连接，会占用一些资源，主要是本地端口资源。 一台服务器的本地可用端口是有限的，也就几万个端口，由这个参数控制： sysctl net.ipv4.ip_local_port_range net.ipv4.ip_local_port_range = 32768 61000 当服务器存在非常多的 TIME_WAIT 连接， 将本地端口都占用了， 就不能主动发起新的连接去连其他服务器了。 这里需要注意，是主动发起连接，又是主动发起关闭的一方才会遇到这个问题。 如果是 server 端主动关闭 client 端建立的连接产生了大量的 TIME_WAIT 连接，这是不会出现这个问题的。除非是其中涉及到的某个客户端的 TIME_WAIT 连接都有好几万个了。 TIME_WAIT 条目超出限制。 这个限制，是由一个内核参数控制的： sysctl net.ipv4.tcp_max_tw_buckets net.ipv4.tcp_max_tw_buckets = 5000 超出了这个限制会报一条 INFO 级别的内核日志，然后继续关闭掉连接。并没有什么特别大的影响，只是增加了刚才提到的收到脏数据的风险而已。 另外的风险就是，关闭掉 TIME_WAIT 连接后，刚刚发出的 ACK 如果对端没有收到， 重发 FIN 包出来时，不能正确回复 ACK，只是回复一个 RST 包， 导致对端程序报错，说 connection reset。 因此 net.ipv4.tcp_max_tw_buckets 这个参数是建议不要改小的，改小会带来风险，没有什么收益，只是表面上通过 netstat 看到的 TIME_WAIT 少了些而已，有啥用呢？并且，建议是当遇到条目不够，增加这个值，仅仅是浪费一点点内存而已。 3.3.3 如何解决 time_wait? 1）最佳方案是应用改造长连接，但是一般不太适用。 2）修改系统回收参数。设置以下参数。 net.ipv4.tcp_timestamps = 1 net.ipv4.tcp_tw_recycle = 1 设置该参数会带来什么问题？ 如果这两个参数同时开启， 会校验源 ip 过来的包携带的 timestamp 是否递增， 如果不是递增的话，则会导致三次握手建联不成功，具体表现为抓包的时候看到 syn 发出，server 端不响应 syn ack。 通俗一些来讲就是，一个局域网有多个客户端访问您，如果有客户端的时间比别的客户端时间慢，就会建联不成功。 治标不治本的方式： 放大端口范围。 sysctl net.ipv4.ip_local_port_range net.ipv4.ip_local_port_range = 32768 61000 放大 time_wait 的 buckets sysctl net.ipv4.tcp_max_tw_buckets net.ipv4.tcp_max_tw_buckets = 180000 关于 net.ipv4.tcp_max_tw_buckets 到底要不要放大，目前云上 ecs 多数是设置了 5000，在很多场景下可能是不够的。 简单来说 net.ipv4.tcp_max_tw_buckets 的作用是为了 \"优雅\" 的关闭连接。 完整的关闭连接。 避免有数据包重复。 3.3.4 如果 tw 满了会怎么样 TCP: time wait bucket table overflow。 新内核 tw_bucket 满了的话， 会影响 established 状态的连接在 finack 的时候直接进入 closed 状态。 老内核 tw_bucket 满了的话，会将 tw_bucket 里面的 time_wait 按照一定的规则（如 LRU），将一批 time_Wait 直接进入 closed 状态 ，然后 established 状态发送 finack 后进 入 time_wait。 3.3.5 tw 的开销是什么 特别少量的内存。 占用本地端口。 3.3.6 tw 放大的好与坏 放大的话需要更多的内存开销，但是几乎可以忽略不计。 占用更多的本地端口，需要适当的放大本地端口范围， 端口范围经过简单的测试，建议设置为 tw 的 1.5 倍。net.ipv4.ip_local_port_range netstat 大量的扫描 socket 的时候（ss 不会扫描， 但是 ss 在 slab 内存特别高 的时候，也有可能会引起抖动），极端情况下可能会引起性能抖动。 tw 放大，local_port_range 放大，还可以配置复用以及快速回收等参数。 使用快速回收可能会导致 snat 时间戳递增校验问题，不递增的话 syn 不响应。 特殊场景的时候（本机会发起大量短链接的时候）。 nginx 结合 php-fpm 需要本地起端口。 nginx 反代如（java，容器等）。 tcp_tw_reuse 参数需要结合 net.ipv4.tcp_timestamps = 1 一起来用。 即服务器即做客户端，也做 server 端的时候。 tcptw_reuse 参数用来设置是否可以在新的连接中重用 TIME_WAIT 状态的套接字。 注意， 重用的是 TIME_WAIT 套接字占用的端口号， 而不是 TIME_WAIT 套接字的内存等。这个参数对客户端有意义，在主动发起连接的时候会在调用的 inet hash_connect() 中会检查是否可以重用 TIME_WAIT 状态的套接字。如果你在服务器段设置这个参数的话，则没有什么作用，因为服务器端 ESTABLISHED 状态的套 接字和监听套接字的本地 IP、端口号是相同的，没有重用的概念。但并不是说服务器端就没有 TIME_WAIT 状态套接字。 因此该类场景最终建议是： net.ipv4.tcp_tw_recycle = 0 关掉快速回收 net.ipv4.tcp_tw_reuse = 1 开启 tw 状态的端口复用（客户端角色） net.ipv4.tcp_timestamps = 1 复用需要 timestamp 校验为 1 net.ipv4.tcp_max_tw_buckets = 30000 放大 bucket net.ipv4.ip_local_port_range = 15000 65000 放大本地端口范围 内存开销测试。 # ss -s Total: 15254 (kernel 15288) TCP: 15169 (estab 5，closed 15158，orphaned 0，synrecv 0，timewait 3/0)，ports 0 Transport Total IP IPv6 * 15288 - - RAW 0 0 0 UDP 5 4 1 TCP 11 11 0 INET 16 15 1 FRAG 0 0 0 15000个socket消耗30多m内存 3.3.7 关于CLOSE_WAIT 如上所示，CLOSE_WAIT 的状态是 服务器端 / 客户端程序收到外部过来的 FIN 之 后，响应了 ACK 包，之后就进入了 CLOSE_WAIT 状态。 一般来说，如果一切正常，稍后服务器端 / 客户端程序需要发出 FIN 包，进而迁移到 LAST_ACK 状态，收到对端过来的 ACK 后，完成 TCP 连接关闭的整个过程。 ⚠️不管是服务器还是客户端， 只要是被动接收第一个 FIN 的那一方才会进入 CLOSE_WAIT 状态。 3.4 一次网络抖动经典案例分析 简介： 本文记录的是一次多团队协作处理的抖动问题的过程，由于用户的执着，也得我们在这个案例分析得较为深入，希望对大家今后的此类案例的处理有所启发。 视频学习 性能抖动剖析(一) 性能抖动剖析(二) 性能抖动剖析(三) 网络抖动案例是一类处理难度较大的问题，原因主要是很多抖动发生的频率不高，且持续时间非常短极限情况可能仅有 100ms 以下，而很多用户的业务应用对实时性要求非常高，因此对此类在百毫秒的延迟也会非常敏感。本文记录的是一次多团队协作处理的抖动问题的过程，由于用户的执着，也使得我们在这个案例分析得较为深入，希望对大家今后的此类案例的处理有所启发。 问题现象 让我们先来看看问题现象吧，用户的应用日志记录了百毫秒甚至 1-2 秒级别的延迟， 而且发生较为频繁，由于业务的实时性要求较高，因此对业务的影响较大，当然其中也影响到了用户对迁云的信心。 初步排查 在用户通过应用层面的排查怀疑问题来源于虚拟网络环境的时候，我们需要做的第一 件事就是首先要将问题简单化。这一步是非常必要的，因为我们对用户的应用不可能有非常深入的了解，所以用户的应用日志具体含义和记录方式对我们来说更像黑盒。 我们所要做的是将问题现象转移到我们常见的系统组件上来，比如简单到 ping。所以我们第一件所做的事情就是编写脚本进行两台机器的内网互 ping，并将每次 ping 的延迟记录到文件。选择 ping 当然也是由于 ping 的间隔是可以设置到百毫秒的，比较容易说明问题。 在互 ping 的测试中我们确实发现有百毫秒以上的延迟，那么随后我们为了排除物理网络的影响，选择一台机器进行对网关的 ping 测试，同样发现了类似的延迟： 来看看上面的 ping 测试结果吧，初看也仅仅是一些百毫秒延迟的集中发生而已，但是仔细观察就会发现每次发生都有这样的情况，就是延迟在一组连续的 ping 上发生的，并且延迟是倒序排列的。那么这意味着什么呢？ 分析一 通过以上的 ping 测试我们把问题简单化到了 ping 网关延迟上，但是上面如此规律的测试结果的具体含义是什么。首先他意味着并没有丢包发生，所以的 ICMP 请求都被系统发出并且收到回复，但是这样的倒序排列，更像是在问题时间段内所有的回复都没有被第一时间处理，而是突然在 800ms 之后系统处理了所有之前发生回复，因此才会产生这样的现象。那么我们此时可以有一个假设，在这 800ms 之前系统停止了对网络包的处理。那么什么样的情况会导致系统停止对网络包的处理呢？ 答案是中断禁用，硬件中断是系统处理网络包的第一也是必须步骤，中断禁用会导致系统的软中断和中断都不能在 CPU 上发生，从而使得当前在 CPU 上运行的指令是无法被打断的，这经常被用于一些可能存在竞争风险的内核代码片段上，这些代码片段可能会因为被中断打断而导致数据不同步甚至损坏。 在当时我们内核团队甚至通过编写示例驱动，通过记录 timer 函数在一段时间内未能触发来验证了中断禁用的发生。那么庞大的内核代码中究竟是哪一部分的代码导致了这样的问题呢？ 分析二 在这段分析过程中，我们做了大量实验，比如通过编写内核驱动来禁用中断，测试各类内核追踪方法是否能获得更进一步的信息，如禁用中断的堆栈，但是很可惜，目前尚无很好的方法在不影响业务的情况下较轻量级地获得禁用中断时的内核堆栈，原理也很简单，硬件中断本身优先级要高于一般进程和软中断，在其被禁用之后自然普通 软件层面的追踪方法也不起作用了。 然而问题就隐藏在一类系统的内存资源上，即系统的 slab 占用量相比正常系统要高出不少： 我们可以看到其中 dentry 在 slab 中的占用量达到了非常高的程度，dentry 是内存中表示目录和文件的对象，作为与 inode 的链接存在，在一般情况下如此高数字的 dentry 项可能代表这系统有大量被打开的文件。 然而此时我们首先需要解释大量的 dentry 项与禁用中断的关系，我们来看看 2.6 内核的这一段代码： 这是一段计算 slab 总量的代码，我们注意到它是以遍历链表的方式来统计 slab 总量的，而在进入链表之前调用了 spin_lock_irq 函数，我们来看看它的实现： static inline void __spin_lock_irq(spinlock_t *lock) { local_irq_disable(); 于是我们可以确认在统计 slab 信息的时候，系统的行为是首先禁用中断，然后遍历链表统计 slab，最后再次启用中断。那么整个禁用中断的时间将取决于链表中对象的 个数，如果其对象数量惊人，很可能就会导致禁用中断时间过长。 验证问题也非常简单，我们可以主动运行 cat /proc/slabinfo 在获取 slab 信息，那么 以上函数也将会被调用，同时观察 ping 测试输出符合以上问题点的情况，即可以大致确认问题原因了。 此时我们已经有了可以暂时缓解问题的方法了，对 dentry 项是作为文件系统缓存的 一部分存在的，也就是真正的文件信息是存放于磁盘上的，dentry 只不过是在系统打开文件系统缓存在内存中的对象而已，即使缓存被清空，未来系统一样可以通过读取磁盘文件来重新生成 dentry 信息，因此我们可以通过类似 echo 2 > /proc/sys/vm/ drop_caches && sync 的方式来释放缓存，缓解问题。 但是其实事情远远没有就此结束，我们需要注意两个关键性的问题： 是什么程序在反复地获取 slab 信息，产生类似 cat /proc/slabinfo 的效果。 这么多 dentry 生成的原因是什么。 如果不知道这两点这个问题随时可能会复现。而周期性地 drop cache 并不是一个长久根治的方案。 看到这里，这个缓存问题的处理是不是在哪儿见过？对的，在系统性能分析那一章节我们也提到相似的问题，建议再往前回顾一下心中应该就差不多有答案了。 四、Linux 系统服务与参数问题 至此，我们分享了关于系统启动登录、性能、网络等三个方面遇到的一些经典和有趣案例，而这三个方面也基本涵盖了目前我们遇到的大部分的系统故障问题。此外，还有一类系统服务参数问题在我们处理的案例中也屡见不鲜。阿里云结合多年云上 ECS 运维经验和用户业务反馈，不断优化 ECS 系统镜像以最大化发挥用户业务效益，但很多时候由于业务增长缺少准确的预估，应用程序不合理设计等方面，需要调整系统默认的参数配置来适应和改善业务运行状态。下面我们分享几个案例来帮助大家更好的理解一些系统参数的实际参考和应用意义。 4.1 4 个 limits 生效的问题 4.1.1 第一个问题 limits.conf 的限制在 /proc/pid/limits 中未生效。 # cat /proc/3606/limits Limit Soft Limit Hard Limit Units Max processes 31202 31202 processes Max open files 1024 4096 files 在 CentOS 7 & Ubuntu 系统中，使用 Systemd 替代了之前的 SysV。/etc/security/ limits.conf 文件的配置作用域缩小了。 /etc/security/limits.conf 的配置， 只适用于通过 PAM 认证登录用户的资源限制， 它对 systemd 的 service 的资源限制不生效。 因此登录用户的限制， 通过 /etc/ security/limits.conf 与 /etc/security/limits.d 下的文件设置即可。 对于 systemd service 的资源设置，则需修改全局配置， 全局配置文件放在 /etc/systemd/system.conf 和 /etc/systemd/user.conf，同时也会加载两个对应目录中的所有 .conf 文件 /etc/systemd/system.conf.d/.conf 和 /etc/systemd/user.conf.d/.conf。 system.conf 是系统实例使用的，user.conf 是用户实例使用的。 vim /etc/systemd/system.conf DefaultLimitNOFILE=100000 DefaultLimitNPROC=65535 修改并重启即可。 # cat /proc/3613/limits Limit Soft Limit Hard Limit Units Max processes 65535 65535 processes Max open files 100000 100000 files 4.1.2 第二个问题 在服务里面设置 LimitNOFILE=infinity 为什么不是无穷大？ 在服务里面设置 LimitNOFILE=infinity 后， 通过查看 pid 的 limit 发现 openfile 是 65536 ，而不是无穷大。 查看服务配置 [root@iZwz98aynkjcxvtra0f375Z ~]# cat /etc/systemd/system/multi-user.target.wants/docker.service |grep -vi \"^#\"|grep -vi \"^$\" [Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com BindsTo=containerd.service After=network-online.target firewalld.service containerd.service Wants=network-online.target Requires=docker.socket [Service] Type=notify ExecStart=/usr/bin/dockerd -H fd:// ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT ExecReload=/bin/kill -s HUP $MAINPID TimeoutSec=0 RestartSec=2 Restart=always StartLimitBurst=3 StartLimitInterval=60s LimitNOFILE=infinity LimitNPROC=infinity LimitCORE=infinity TasksMax=infinity Delegate=yes KillMode=process [Install] WantedBy=multi-user.target 查看配置效果 这个是 systemd 的 bug，低于 240 的版本需要手动设置才可以生效。 LimitNOFILE=102400 github上的说明 4.1.3 第三个问题 为什么 openfile 不能设置为 unlimited。 [root@iZwz98aynkjcxvtra0f375Z ~]# ulimit -n 65535 [root@iZwz98aynkjcxvtra0f375Z ~]# ulimit -n unlimited -bash: ulimit: open files: cannot modify limit: Operation not permitted 原因是 CentOS7里 openfile 不能大于 nr_open。 [root@iZwz98aynkjcxvtra0f375Z ~]# cat /proc/sys/fs/nr_open 1048576 [root@iZwz98aynkjcxvtra0f375Z ~]# ulimit -n 1048577 -bash: ulimit: open files: cannot modify limit: Operation not permitted [root@iZwz98aynkjcxvtra0f375Z ~]# ulimit -n 1048576 [root@iZwz98aynkjcxvtra0f375Z ~]# ulimit -n 1048576 4.1.4 第四个问题 使用 supervisor 管理进程（测试环境 Ubuntu 16.04）启动进程后，maxfile 是1024。 需要修改配置文件。 # cat /etc/supervisor/supervisord.conf [supervisord] logfile=/var/log/supervisor/supervisord.log ; (main log file;default $CWD/ supervisord.log) pidfile=/var/run/supervisord.pid ; (supervisord pidfile;default supervisord.pid) childlogdir=/var/log/supervisor ; ('AUTO' child log dir, default $TEMP) 下面这两行 minfds=655350 ; min. avail startup file descriptors;default 1024 minprocs=65535 ; min. avail process descriptors;default 200 关于 file-max nr_open file_nr 的解释可参考此文章 参考外部文档 cnblogs csdn1 360云 csdn2 4.2 六步排查 ss & netstat 统计结果不一样的原因 ss 的结果，closed 状态的有 3w 多 netstat 统计只有一百来个连接 # netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a，S[a]}' ESTABLISHED 9 TIME_WAIT 79 通过 strace 看看二者的统计方式得不同 ss 直接取自 /proc/net/sockstat。 netstat 是读取的 /proc/pid/fd 下面关联 tcp 的 socket。 netstat 也有扫到三万多个 socket， 为什么输出的时候没有展示呢？ By default，netstat displays a list of open sockets. If you don’t specify any address families，then the active sockets of all configured address families will be printed. 默认情况下，netstat显示打开的套接字列表。如果不指定任何地址系列，则所有已配置地址系列的活动套接字将被打印 找出来哪个 pid 的 socket 比较多， 对 /proc/pid/fd 目录做批量扫描 for d in /proc/[0-9]*;do pid=$(basename $d);s=$(ls -l $d/fd | egrep -i socket | wc -l 2>/dev/null); [ -n \"$s\" ] && echo \"$s $pid\";done | sort -n | tail -20 进入到 /proc/7136/(上述命令打印出的目录) 目录查看 cmdline 或者直接 ps -ef |grep pid 拿到进程，后面就需要客户自查了 4.3 为什么明明内存很充足但是 java 程序仍申请不到内存 背景信息 用户有一台 8G 内存的实例，剩余内存还很多（7G 左右），而 java 程序使用了 4G 内 存申请，直接抛出 OOM。 排查如下 oom 的记录显示为申请 4g 内存失败。 4294967296 /1024 /1024 = 4096 M 1.第一反应是想起来之前的 vm.min_free_kbytes & nr_hugepage 导致的 free 大于 available 案例有关。 centos7 memavailable 小于 memfree 二者的统计方式不一样 MemFree: The sum of LowFree+HighFree +MemAvailable: An estimate of how much memory is available for starting new + applications，without swapping. Calculated from MemFree, + SReclaimable，the size of the file LRU lists，and the low + watermarks in each zone. + The estimate takes into account that the system needs some + page cache to function well，and that not all reclaimable + slab will be reclaimable，due to items being in use. The + impact of those factors will vary from system to system. https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/ commit/?id=34e431b0ae398fc54ea69ff85ec700722c9da773 memfree 统计的是所有内存的 free 内存，而 memavailable 统计的是可以拿来给程序用的内存，而客户设置了 vm.min_free_kbytes（2.5G），这个内存在 free 统计，但是不在 memavailable 统计 nr_hugepage 也会有这个问题 2.跟客户要 free -m && sysctl -p && /proc/meminfo 等信息分析问题。 HugePages_Total 为 0 说明没有设置 nr_hugepage。 MemAvailable: 7418172 kB 说明这么多内存可用。 # sysctl -p net.ipv4.ip_forward = 0 net.ipv4.conf.default.accept_source_route = 0 kernel.sysrq = 1 kernel.core_uses_pid = 1 net.ipv4.tcp_syncookies = 1 kernel.msgmnb = 65536 kernel.msgmax = 65536 kernel.shmmax = 500000000 kernel.shmmni = 4096 kernel.shmall = 4000000000 kernel.sem = 250 512000 100 2048 net.ipv4.tcp_tw_recycle=1 net.ipv4.tcp_max_syn_backlog=4096 net.core.netdev_max_backlog=10000 vm.overcommit_memory=2 net.ipv4.conf.all.arp_filter = 1 net.ipv4.ip_local_port_range=1025 65535 kernel.msgmni = 2048 net.ipv6.conf.all.disable_ipv6=1 net.ipv4.tcp_max_tw_buckets = 5000 net.ipv4.tcp_max_syn_backlog = 8192 net.ipv4.tcp_keepalive_time = 600 #cat /proc/meminfo MemTotal: 8009416 kB MemFree: 7347684 kB MemAvailable: 7418172 kB Buffers: 18924 kB Cached: 262836 kB SwapCached: 0 kB Active: 315188 kB Inactive: 222364 kB Active(anon): 256120 kB Inactive(anon): 552 kB Active(file): 59068 kB Inactive(file): 221812 kB Unevictable: 0 kB Mlocked: 0 kB SwapTotal: 0 kB SwapFree: 0 kB Dirty: 176 kB Writeback: 0 kB AnonPages: 255804 kB Mapped: 85380 kB Shmem: 880 kB Slab: 40660 kB SReclaimable: 22240 kB SUnreclaim: 18420 kB KernelStack: 4464 kB PageTables: 6512 kB NFS_Unstable: 0 kB Bounce: 0 kB WritebackTmp: 0 kB CommitLimit: 4004708 kB Committed_AS: 2061568 kB VmallocTotal: 34359738367 kB VmallocUsed: 21452 kB VmallocChunk: 34359707388 kB HardwareCorrupted: 0 kB AnonHugePages: 126976 kB CmaTotal: 0 kB CmaFree: 0 kB HugePages_Total: 0 HugePages_Free: 0 HugePages_Rsvd: 0 HugePages_Surp: 0 Hugepagesize: 2048 kB DirectMap4k: 114560 kB DirectMap2M: 4079616 kB DirectMap1G: 6291456 kB 3.实际上面的 meminfo 已经说明了问题，但是由于经验不足，一时没有看明白怎么回事，尝试自行测试。 使用 java 命令，去申请超出我的测试机物理内存尝试，拿到报错。 [root@test ~]# java -Xmx8192M -version openjdk version \"1.8.0_242\" OpenJDK Runtime Environment (build 1.8.0_242-b08) OpenJDK 64-Bit Server VM (build 25.242-b08，mixed mode) [root@test ~]# java -Xms8192M -version OpenJDK 64-Bit Server VM warning: INFO: os::commit_ memory(0x00000005c0000000，5726797824，0) failed; error='Cannot allocate memory' (errno=12) # # There is insufficient memory for the Java Runtime Environment to continue. # Native memory allocation (mmap) failed to map 5726797824 bytes for committing reserved memory. # An error report file with more information is saved as: # /root/hs_err_pid8769.log [root@test ~]# java -Xms4096M -version openjdk version \"1.8.0_242\" OpenJDK Runtime Environment (build 1.8.0_242-b08) OpenJDK 64-Bit Server VM (build 25.242-b08，mixed mode) [root@test ~]# java -Xms5000M -version OpenJDK 64-Bit Server VM warning: INFO: os::commit_ memory(0x0000000687800000，3495428096，0) failed; error='Cannot allocate memory' (errno=12) ...... --------------- S Y S T E M --------------- OS:CentOS Linux release 7.4.1708 (Core) uname:Linux 3.10.0-693.2.2.el7.x86_64 #1 SMP Tue Sep 12 22:26:13 UTC 2017 x86_64 libc:glibc 2.17 NPTL 2.17 rlimit: STACK 8192k，CORE 0k，NPROC 15088，NOFILE 65535，AS infinity load average:0.05 0.05 0.05 /proc/meminfo: MemTotal: 3881692 kB MemFree: 2567724 kB MemAvailable: 2968640 kB Buffers: 69016 kB Cached: 536116 kB SwapCached: 0 kB Active: 355280 kB Inactive: 326020 kB Active(anon): 87864 kB Inactive(anon): 13296 kB Active(file): 267416 kB Inactive(file): 312724 kB Unevictable: 0 kB Mlocked: 0 kB SwapTotal: 0 kB SwapFree: 0 kB Dirty: 72 kB Writeback: 0 kB AnonPages: 72200 kB Mapped: 31232 kB Shmem: 24996 kB Slab: 63032 kB SReclaimable: 51080 kB SUnreclaim: 11952 kB KernelStack: 1664 kB PageTables: 4044 kB NFS_Unstable: 0 kB Bounce: 0 kB WritebackTmp: 0 kB CommitLimit: 1678700 kB Committed_AS: 2282236 kB VmallocTotal: 34359738367 kB VmallocUsed: 14280 kB VmallocChunk: 34359715580 kB HardwareCorrupted: 0 kB AnonHugePages: 30720 kB HugePages_Total: 256 HugePages_Free: 256 HugePages_Rsvd: 0 HugePages_Surp: 0 Hugepagesize: 2048 kB DirectMap4k: 57216 kB DirectMap2M: 3088384 kB DirectMap1G: 3145728 kB container (cgroup) information: container_type: cgroupv1 cpu_cpuset_cpus: 0-1 cpu_memory_nodes: 0 active_processor_count: 2 cpu_quota: -1 cpu_period: 100000 cpu_shares: -1 memory_limit_in_bytes: -1 memory_and_swap_limit_in_bytes: -1 memory_soft_limit_in_bytes: -1 memory_usage_in_bytes: 697741312 memory_max_usage_in_bytes: 0 CPU:total 2 (initial active 2) (1 cores per cpu，2 threads per core) family 6 model 79 stepping 1，cmov，cx8，fxsr，mmx，sse，sse2，sse3，ssse3，sse4.1， sse4.2，popcnt，avx，avx2 ，aes，clmul，erms，rtm，3dnowpref，lzcnt，ht，tsc，bmi1，bmi2，adx /proc/cpuinfo: processor : 0 vendor_id : GenuineIntel cpu family : 6 model : 79 model name : Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz stepping : 1 microcode : 0x1 cpu MHz : 2500.036 cache size : 40960 KB physical id : 0 siblings : 2 core id : 0 cpu cores : 1 apicid : 0 initial apicid : 0 fpu : yes fpu_exception : yes cpuid level : 13 wp : yes flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt bogomips : 5000.07 clflush size : 64 cache_alignment : 64 address sizes : 46 bits physical，48 bits virtual power management: processor : 1 vendor_id : GenuineIntel cpu family : 6 model : 79 model name : Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz stepping : 1 microcode : 0x1 cpu MHz : 2500.036 cache size : 40960 KB physical id : 0 siblings : 2 core id : 0 cpu cores : 1 apicid : 1 initial apicid : 1 fpu : yes fpu_exception : yes cpuid level : 13 wp : yes flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt bogomips : 5000.07 clflush size : 64 cache_alignment : 64 address sizes : 46 bits physical，48 bits virtual power management: Memory: 4k page，physical 3881692k(2567600k free), swap 0k(0k free) vm_info: OpenJDK 64-Bit Server VM (25.242-b08) for linux-amd64 JRE (1.8.0_242-b08)，built on Jan 28 2020 14:28:22 by \"mockbuild\" with gcc 4.8.5 20150623 (Red Hat 4.8.5-39) time: Thu Feb 20 15:13:30 2020 timezone: CST elapsed time: 0 seconds (0d 0h 0m 0s) 4.java 测试证明正常申请内存不会有问题，超额的内存才会 oom，那么为什么超额呢，视线回归到 sysctl -p 有所发现。 5.两相对照，说明客户设置的 vm.overcommit_memory 在生效， 建议改回 0 再试试。 4.4 请不要忽略 min_free_kbytes 的设置 问题背景 服务器内主要运行程序：Jadeos。 问题描述 LINUX tmpfs 空间使用未达到 100%，内存也未占满。 执行任何命令提示 bash: fork: Cannot allocate memory 过几秒时间系统会自动重启。 但在客户本地环境是没有这种情况的，即使 tmpfs 使用达到 100% 系统未提示 Cannot allocate memory。 处理过程 1.首先判断客户是否内存不足导致，每次执行测试操作后，free 结果显示可用内存是有的。 2.当进程 Process 比较多， 导致无法分配 pid， 也会提示 Cannot allocate memory，执行命令 pstree -a | wc -l 统计下进程数，排除进程数过多导致的内存无法分配。 3.登录主机内部查看客户客户内部设置 min_free_kbytes 值为 1G。 即强制 Linux 系统最低保留多少空闲内存（Kbytes），如果系统可用内存低于该值，默认会启用 oom killer 或者强制重启。 当耗尽内存直至系统最低保存内存时会有两种现象， 根据内核参数 vm.panic_ on_oom 设置值的不同而有不同的行为。 vm.panic_on_oom=0 系统会提示 oom ，并启动 oom-killer 杀掉占用最高内存的进程。 vm.panic_on_oom =1. 系统关闭 oom, 不会启动 oom-killer，而是会自动重启。 解决方案 建议客户降低 min_free_kbytes 值。 更改减小 min_free_kbytes 后，再执行更多次的拷贝，最后一次 free 可用内存显示解决到设置值是，才提示内存不足。 这个是符合 linux 系统对内存管理的预期的。 五、最后的彩蛋 某地区口罩项目架构演进及优化经验 简介 疫情初期某地政府决定发放一批免费口罩面向该市市民，该市市民均可免费预约领取，预约时间为早上 9点 -12点，因此该场景为限时抢购类型场景， 会面临非常大的定时超大流量超大并发问题，在该项目的落地过程中，涉及的架构演变，做了一些记录和思考。 原始架构图示 & 分析（2月2号晚上 22 点左右的原始架构）。 客户端走 https 协议直接访问 ecs。 ECS 上使用 nginx 自建 https 监听。- Nginx 反代 tomcat，Nginx 处理静态文件，tomcat 处理动态请求。 程序先去 redis 查缓存， 如未命中则去数据库查询数据， 同时 redis 与 mysql 之间的数据同步靠程序控制。 优点：易管理，易部署。 缺点：性能差，无扩展性，存在单点风险。 事实证明：该应用一经上线立刻被打挂了（未知原因预约页面泄露，导致还未到预约时间即被打挂）。 我方介入后的二代架构（24点左右找的我们，早上9点要开服，时间太紧，任务太重， 程序不能动的情况下，几十万的并发架构如何做？ 2月3号早上9点左右的架构，4号也恢复了这个架构）。 接入 slb，通过镜像横向扩展负载能力。 接入读写分离数据库架构， 通过阿里云数据库自动进行读写分离， 自动同步数据。 调整 nginx 协议。 同架构备集群启用（域名解析做了两个 A 记录）。 分析访问日志发现失败原因在获取短信 & 登陆初始化 cookie 的点上。 优点：增加了高可用性，扩展了负载能力。 缺点：对流量预估不足， 静态页面也在 ECS 上， 因此 SLB 的出带宽一度达到最大值 5.xG，并发高达 22w+，用户一度打不开页面，同时由于新网的限制客户无法自助添加解析， 当晚联系不到新网客服导致 CDN 方案搁浅。 知耻而后勇的第三代架构（2月4号 & 2月5号的架构，5号应用）。 接入 CDN 分流超大带宽。 取消 nginx 的代理。 做了新程序无法准时上线的灾备切换方案（没想到还真用到了）。 使用虚拟服务器组做新老程序的切换，但是缺点是一个七层监听的 slb 后端只能挂 200 个机器，再多 slb 也扛不住了，导致老程序刚承接的时候再度挂掉。 5号使用这个架构上线，7分钟库存售罄，且体验极度流程，丝般顺滑，健康同学开发的新程序真是太爽的。 优点：CDN 负担静态资源的流量降低了 SLB 的出带宽，压测的效果也非常理想。 缺点：需要多一个独立的域名在页面里面，涉及跨域，4号临开服之际测试发现入库 & 预约短信乱码返回，紧急切换回了老程序，即二代架构。 理想架构 主域名接入 CDN， CDN 通过设置回源 http、https 协议去访问 SLB 的不同监听实现新老程序之间的切换，具体实现为回源协议对应。不同监听，监听对应不同的程序。 优点：静态加速降低 SLB 带宽，动态回源，无跨域问题 ，切换方便。 缺点：仍需手工设置，镜像部署 ecs 不方便，如果时间充足，可以直接上容器的架构该有多美好呢，一个 scale 可以扩出来几十上百的 pod，也可以做节点自动扩容。 总结 总结：时间紧任务重，遇到了 N 多的坑，想起来一个补一个 ~ vcpu 购买额度。 slb 后端挂载额度。 客户余额不足欠费停机。 新网解析需要联系客服添加。 第一次考虑 CDN 架构的时候未考虑跨域问题。 新程序开发期间未连接主库测试，导致上线失败（主库乱码）。 第一次（3 号）被打挂的时候只关注了 slb 的流量，未详细分析失败最多的环节。 上线前压测缺失，纯靠人工测试功能。 压测靠人手一台 jmeter（4号晚上到5号早上引入了PTS进行压测）。 突然想起来客户原始的程序是放在 windows 上的，导致性能出现了大大的折扣。 最后的成果统计（采样分析，实际数据比这个还大）： 最后上线的三代架构，为了保险起见上了 150 台机器，但是根据活动期间的观察，以及对压测结果的评估，上50台机器应该就可以抗住了，从持续5小时一直崩溃被终端用户骂街，到7分钟库存售罄的领导赞赏，虽然经历了3个通宵的戮战，依然可以隐隐约约感觉到身心都得到了升华 ~ 优化参数笔记 1.参数优化 net.ipv4.tcp_max_tw_buckets = 5000 --> 50000 net.ipv4.tcp_max_syn_backlog = 1024 --> 4096 net.core.somaxconn = 128 --> 4096 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_timestamps = 1(5 和 6 同时开启可能会导致nat上网环境建联概率失败 ) net.ipv4.tcp_tw_recycle = 1 /etc/security/limits.conf * soft nofile 65535 * hard nofile 65535 nginx 参数优化 worker_connections 1024-->10240; worker_processes 1-->16;（根据实际情况设置，可以设置成 auto） worker_rlimit_nofile 1024-->102400; listen 80 backlog 511-->65533； 部分场景也可以考虑 nginx 开启长连接来优化短链接带来的开销 2.架构优化 扩容SLB后端ECS数量，ecs配置统一 nginx反代后端upstream无效端口去除 云助手批量处理服务，参数优化，添加实例标识 云监控大盘监控，ECS slb dcdn redis 调整SLB为7层监听模式，前7后4关闭会话保持导致登录状态失效， 3.程序优化 添加 gc log，捕捉 gc 分析问题，设置进程内存 /usr/bin/java -server -Xmx8g -verbose:gc -XX:+PrintGCDetails -Xloggc:/var/ log/9052.gc.log -Dserver.port=9052 -jar /home/app/we.*****.com/serverboot0.0.1-SNAPSHOT.jar 优化短信发送逻辑，登陆先查询 redis 免登 session，无免登 session 再允许发送短信验证码（降短信的量，优化登陆体验） jedis 连接池优化 maxTotal 8-->20 acceptcount 优化（对标 somaxconn） bug： springboot1.5 带的 jedis2.9.1 的 redis 连接泄漏的问题，导致 tomcat 800 进程用满后都无 限等待 redis 连接 后来进一步调研发现这个问题在 2.10.2 已经修复，而且 2.10.2 向后兼容 2.9.1 4.数据库优化 redis 公网地址变更为内网地址 redis session 超时设置缩短，用于释放 redis 连接 server.servlet.session.timeout=300s spring.session.timeout=300s 慢 SQL 优化（RDS 的 CloudDBA 非常好用哟） 添加只读实例，自动读写分离 优化 backlog 添加读写分离实例数量 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/阿里云开发者社区优秀文章/shell脚本/shell脚本速查手册.html":{"url":"linux/阿里云开发者社区优秀文章/shell脚本/shell脚本速查手册.html","title":"shell脚本","keywords":"","body":"shell脚本速查手册 原文档链接 前言 2021 年，阿里云开发者社区联手 Linux 中国开源社区，为广大的运维工程师、开发者提 供了一套内容丰富、场景丰富的 Linux 入门课程。本手册为其中的「Shell 脚本入门」、 「Shell 脚本进阶」的补充手册，方便学生学习。 这本书适用于所有的 Unix 用户和 Linux 用户。有了这本书，你就可以编写 Bash 脚 本，用更短的时间，更轻松、更稳定地完成更多的工作。 本电子书为 Linux 中国开源社区为运维工程师倾心打造，旨在为运维工程师们提供一个快 速、便捷的查询手册。本书以本书以普及率最高的 Bash 为基础进行撰写，具体内容组织 结构如下： 第一章节介绍 Shell 脚本的编写基础，介绍运维工程师在工作时编写 Shell 脚本的一些 基本信息 第二章节介绍 Shell 脚本编写时的一些进阶技巧。 第三章节介绍 Shell 脚本编写过程中一些常用的命令和用法。 第四章节介绍一些常见的 Bash 资源库，帮助运维工程师在编写脚本时，快速实现想要的 效果。 Bash 脚本基础 1. Bash 脚本定义变量 在使用 Bash 编写脚本时，你可以根据使用的场景，定义不同类型的变量，从而完成整个 脚本的开发。 脚本变量类型 Bash 脚本的变量可以细分为以下三个类型： (1) 局部变量：局部变量在脚本或命令中定义，仅在当前 shell 实例中有效，其他 shell 启动的程序不能访问局部变量； (2) 环境变量：所有的程序，包括 shell 启动的程序，都能访问环境变量，有些程序需要 环境变量来保证其正常运行。必要的时候 shell 脚本也可以定义环境变量； (3) shell 变量：shell 变量是由 shell 程序设置的特殊变量。shell 变量中有一部分是环 境变量，有一部分是局部变量。 定义变量和使用变量 在定义和使用变量时，应遵循如下描述： (1) 定义变量：name=value 需要注意，等号两侧不能有空格； (2) 使用变量：echo $name 或 echo ${name}； (3) 定义局部变量：local name=\"test\" (4) 定义只读变量：readonly name； (5) 删除变量：unset name。 Bash 字符串 在 Bash 中，字符串可以是单引号或双引号，二者的区别如下： 单引号：单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的；单引号字 串中不能出现单独一个的单引号，但可以成对出现，作为字符串拼接使用； 双引号：双引号里可以有变量且可以出现转义字符。 2. Bash 脚本传递参数 在使用 Bash 脚本时，我们可以传递一些参数给 Bash 脚本，方便 Bash 脚本执行相应 操作。具体的参数使用说明如下： Bash 中的参数 在 Bash 中使用参数时，具体定义如下： Bash 中的参数按照数字顺序定义； Bash 脚本内获取参数的格式为：$n。 几个特殊参数 Bash 中除了正常的顺序参数以外，还有一些特殊参数： $0：文件名； $#：传递到脚本的参数的个数； $*：以一个单字符串显示所有向脚本传递的参数。 3. Bash 脚本定义数组 在 Bash 当中，你可以定义脚本来进行批量操作，关于数组的使用，你可以参考下方内容： 定义数组 定义数组：myArray=(value0 value1 value2) 使用数组 使用数组：${数组名[下标]}。 获取数组所有元素 ${my_array[*]} 或者 ${my_array[@]} 可以获得数组的所有元素。 获取数组长度 ${#array_name[@]} 或者 ${#array_name[*]} 可以获得数组长度。 4. Bash 脚本运算符 Bash 为开发者提供了多种运算符，方便开发者进行逻辑运算。 算数运算符 运算符 说明 + 加法 - 减法 * 乘法 / 除法 % 取余 = 赋值 == 相等。用于比较两个数字，相同则返回 true。 != 不相等。用于比较两个数字，不相同则返回 true。 关系运算符 运算符 说明 -eq 检测两个数是否相等，相等返回 true。 -ne 检测两个数是否不相等，不相等返回 true。 -gt 检测左边的数是否大于右边的，如果是，则返回 true。 -lt 检测左边的数是否小于右边的，如果是，则返回 true。 -ge 检测左边的数是否大于等于右边的，如果是，则返回 true。 -le 检测左边的数是否小于等于右边的，如果是，则返回 true。 布尔运算符 运算符 说明 ! 非运算，表达式为 true 则返回 false，否则返回 true。 -o 或运算，有一个表达式为 true 则返回 true。 -a 与运算，两个表达式都为 true 才返回 true。 逻辑运算符 运算符 说明 && 逻辑的 AND \\ \\ 逻辑的 OR 字符串运算符 运算符 说明 = 检测两个字符串是否相等，相等返回 true。 != 检测两个字符串是否不相等，不相等返回 true。 -z 检测字符串长度是否为 0，为 0 返回 true。 -n 检测字符串长度是否不为 0，不为 0 返回 true。 $ 检测字符串是否为空，不为空返回 true。 文件测试运算符 操作符 说明 -b file 检测文件是否是块设备文件，如果是，则返回 true。 -c file 检测文件是否是字符设备文件，如果是，则返回 true。 -d file 检测文件是否是目录，如果是，则返回 true。 -f file 检测文件是否是普通文件（既不是目录，也不是设备文件），如果是， 则返回 true。 -g file 检测文件是否设置了 SGID 位，如果是，则返回 true。 -k file 检测文件是否设置了粘着位(Sticky Bit)，如果是，则返回 true。 -p file 检测文件是否是有名管道，如果是，则返回 true。 -u file 检测文件是否设置了 SUID 位，如果是，则返回 true。 -r file 检测文件是否可读，如果是，则返回 true。 5. Bash 脚本流程控制 在 Bash 当中，你可以借助流程控制语句完成常见的逻辑控制。 if 控制 if 条件判断可以使用如下格式： if condition then command1 command2 ... commandN fi if else 控制 if-else 条件判断可以使用如下格式： if condition then command1 command2 ... commandN else command fi if else-if else 控制 if-else-if else 条件判断可以使用如下格式： if condition1 then command1 elif condition2 then command2 else commandN fi For 循环 for 循环可以使用如下格式： for var in item1 item2 ... itemN do command1 command2 ... commandN done While 循环 while 循环可以使用如下格式： while condition do command done 特殊的 While 循环 - 无限循环 如果你需要实现无限循环，则可以使用如下格式： # 写法 1 while : do command done # 写法 2 while true do command done Until 循环 除了 While 和 For 以外，还可以使用 until 构建循环： until condition do command done Case (Switch) 控制 如果需要进行分支处理，则可以使用如下格式： case 值 in value1) command1 command2 ... commandN ;; value2） command1 command2 ... commandN ;; esac 跳出循环 在循环过程中，如果需要跳出循环，则可以考虑使用 break 或 continue： 跳出循环使用 break; 跳过当前循环使用 continue。 Bash 脚本进阶 除了常规的 Bash 使用以外，你还可以使用一些特殊的命令，来完成特定功能。 1. Bash 脚本输出 输出普通字符串 echo \"It is Linux.CN\"。 输出转义字符串 echo \"\\\"It is Linux.CN\\\"\"。 输出变量 echo \"$name is best Linux Distro\"。 输出换行符 echo -e \"Show me your code! \\n\" ，其中 -e 用于开启转义。 输出命令执行结果 echo date 格式化输出 使用 printf命令可以进行格式化输出。 printf 支持的格式字符 说明 \\a 警告字符，通常为 ASCII 的 BEL 字符 \\b 后退 \\c 抑制（不显示）输出结果中任何结尾的换行字符（只在%b 格式指示 符控制下的参数字符串中有效），而且，任何留在参数里的字符、任 何接下来的参数以及任何留在格式字符串中的字符，都被忽略 \\f 换页（formfeed） \\n 换行 \\r 回车（Carriage return） \\t 水平制表符 \\v 垂直制表符 \\ 一个字面上的反斜杠字符 \\ddd 表示 1 到 3 位数八进制值的字符。仅在格式字符串中有效 \\0ddd 表示 1 到 3 位的八进制值字符 2. Bash 脚本测试 借助测试参数，你可以判断 Bash 中某些语句是否符合特定的条件，比如文件是否存在、 字符串长度是否为 0 。测试参数可以帮助你完成更加复杂的 Bash 脚本。 数值测试 参数 说明 -eq 等于则为真 -ne 不等于则为真 -gt 大于则为真 -ge 大于等于则为真 -lt 小于则为真 -le 小于等于则为真 字符测试 参数 说明 = 等于则为真 != 不相等则为真 -z 字符串 字符串的长度为零 则为真 -n 字符串 字符串的长度不为 零则为真 文件测试 参数 说明 -e 文件名 如果文件存在则为真 -r 文件名 如果文件存在且可读则为真 -w 文件名 如果文件存在且可写则为真 -x 文件名 如果文件存在且可执行则为真 -s 文件名 如果文件存在且至少有一个字符则为真 -d 文件名 如果文件存在且为目录则为真 -f 文件名 如果文件存在且为普通文件则为真 -c 文件名 如果文件存在且为字符型特殊文件则为真 -b 文件名 如果文件存在且为块特殊文件则为真 3. Bash 脚本函数 函数定义结构 [ function ] funname [()] { action; [return int;] } 其中 function 关键词可带可不带； funcname 根据实际需要定义； return int 根据需要加入。 函数参数 调用函数时传递的参数可以在函数中以 $1、$2 的方式获取； 使用 $1 的方式获取参数仅可用于前 10 个参数，超出需要使用 $​n 获取； 特殊参数处理如下： | 参数处理 | 说明 | | -------- | ------------------------------------------------------------ | | $# | 传递到脚本或函数的参数个数 | | $ | 以一个单字符串显示所有向脚本传递的参数 | | $$ | 脚本运行的当前进程 ID 号 | | $! | 后台运行的最后一个进程的 ID 号 | | $@ | 与$相同，但是使用时加引号，并在引号中返回每个参数。 | | $- | 显示 Shell 使用的当前选项，与 set 命令功能相同。 | | $? | 显示最后命令的退出状态。0 表示没有错误，其他任何值表明有错误。 | 4. Bash 脚本输出输入重定向 输出重定向是 Bash 脚本的一个强大的功能，借助重定向，可以对 Bash 命令执行的结果 进行操作。 Bash 输入输出重定向 命令 说明 command > file 将输出重定向到 file。 command 将输入重定向到 file。 command >> file 将输出以追加的方式重定向到 file。 n > file 将文件描述符为 n 的文件重定向到 file。 n >> file 将文件描述符为 n 的文件以追加的方式重定向到 file。 n >& m 将输出文件 m 和 n 合并。 n 将输入文件 m 和 n 合并。 将开始标记 tag 和结束标记 tag 之间的内容作为输入。 /dev/null 如果不希望看到输出，可以将输出重定向到 /dev/null。 5. Bash 文件包含 包含文件 # 写法 1 . filename # 注意点号(.)和文件名中间有一空格 # 写法 2 source filename Bash 编写常用命令 以下是编写 Bash 脚本时常用的命令，你可以根据自己的需要进行选择。 1. pwd pwd 命令是用来获取当前目录的，可以用于基于当前目录进行文件/文件夹操作。 pwd 输出当前所在目录； pwd -P 输出当前所在命令，并展示所有的软连接指向。 2. sort sort 命令是用来对内容进行排序的，可以将文本、数字进行排序。 sort path/to/file 对文件内容进行升序排列； sort --reverse path/to/file 对文件内容进行降序排列； sort --ignore-case path/to/file 对文件内容进行忽略大小写的升序排列； sort --numeric-sort path/to/file 对文件内容进行按数字顺序排列； sort --unique path/to/file 对文件内容进行唯一排列； 3. echo echo 命令是用来输出内容的，可以配合格式编码输出特定颜色/风格的文字。 echo \"message\" 输出信息； echo \"my path is $PATH\" 输出包含有环境变量的信息； echo \"Hello World\" >> 在文件尾部追加内容； echo \"Hello World\" >移除当前文件内容，并替换为新的内容。 4. read read 命令是用来获取用户输入内容，即标准输入设备(键盘)输入内容。 read $variable 读入数据，并设置给变量； read -p \"Enter your input here: \" $variable 展示提示，并读入数据，设置给变量； while read line; do echo \"$line\"; done 按行读取内容，并执行命令。 5. shift shift 可以用于将函数的参数移除，其他参数向前移动。 shift 3 shift 后可以跟一个数字参数，表示移除相应数量的参数，其他参数向前移动。 Bash 公共库 1. Bash Shell Function Library 项目地址：https://github.com/SkypLabs/bsfl Bash Shell Function Library 是一个短小精炼的 Bash 公共库，他提供了诸如数组操 作、命令执行、文件管理、日志记录、信息提醒、网络检测、字符操作、时间操作、变量 操作等功能，帮助运维工程师快速完成自己的脚本编写工作。 此外，Bash Shell Function Library 还提供了一个完整的 在线文档 ，帮助你了解函数库中 提供的重要函数 使用方法 从 https://github.com/SkypLabs/bsfl 上下载仓库，并获取其中的 bsfl.sh 即可在自己 的代码中引用。 示例代码 #!/usr/bin/env bash # -*- tab-width: 4; encoding: utf-8 -*- declare -r DIR=$(cd \"$(dirname \"$0\")\" && pwd) source $DIR/../lib/bsfl.sh # -------------------- msg \"This is a classic displayed message using the 'msg' function.\" echo # -------------------- msg \"This is a red displayed message using the 'msg' function with color parameter.\" \"$RED\" echo # -------------------- msg_status \"This is a displayed message with its status given as parameter using the 'msg_status' function.\" \"PASSED\" echo # -------------------- msg_alert \"This is a displayed message with its status using the 'msg_alert' function.\" echo # -------------------- msg_critical \"This is a displayed message with its status using the 'msg_critical' function.\" echo # -------------------- msg_debug \"This is a displayed message with its status using the 'msg_debug' function.\" echo # --------------------msg_emergency \"This is a displayed message with its status using the 'msg_emergency' function.\" echo # -------------------- msg_error \"This is a displayed message with its status using the 'msg_error' function.\" echo # -------------------- msg_failed \"This is a displayed message with its status using the 'msg_failed' function.\" echo # -------------------- msg_info \"This is a displayed message with its status using the 'msg_info' function.\" echo # -------------------- msg_not_ok \"This is a displayed message with its status using the 'msg_not_ok' function.\" echo # --------------------msg_notice \"This is a displayed message with its status using the 'msg_notice' function.\" echo # -------------------- msg_ok \"This is a displayed message with its status using the 'msg_ok' function.\" echo # -------------------- msg_passed \"This is a displayed message with its status using the 'msg_passed' function.\" echo # -------------------- msg_success \"This is a displayed message with its status using the 'msg_success' function.\" echo # -------------------- msg_warning \"This is a displayed message with its status using the 'msg_warning' function.\" echo 2. Bash Lib 项目地址：http://aks.github.io/bash-lib/ Bash Lib 是一个原子化的公共库，你可以根据自己的实际需要，引入所需的公共库分组， 使用相应的内容，降低整个项目的大小。 Bash Lib 提供了诸多原子库，你选择需要使用的引入即可： 参数处理：arg-utils 日历处理： calendar-utils 日期处理：date-utils Hash 处理：hash-utils 帮助处理：help-util 列表处理：list-utils 交互处理：prompt-colors 文字处理：text-utils 测试处理：test-utils 时间处理：time-utils ... 使用方法 从 https://github.com/aks/bash-lib 下载仓库，并选择你需要引入的脚本，复制到项目 目录下，并进行引用即可。 示例代码 #!/usr/bin/env bash source text-utils.sh lowercase \"HELLOWORLD\" 推荐阅读 如何入门 Bash 编程：https://linux.cn/article-13210-1.html 编写更好 Bash 脚本的 8 个建议：https://linux.cn/article-6420-1.html Linux 中高效编写 Bash 脚本的 10 个技巧：https://linux.cn/article-8618-1.html 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"生产问题总结/ssh/记录一次诡异的ssh连接问题.html":{"url":"生产问题总结/ssh/记录一次诡异的ssh连接问题.html","title":"记录一次诡异的ssh连接问题","keywords":"","body":"场景描述 有开发需要连接一台机器，通过jumpserver授权后提示认证失败，无法连接 环境说明 生产中我们是通过jumpserver进行所有机器管理的，这个是我推动的，这里需要说明一下，之前的权限管理比(非)较(常)混乱，公司的腾讯云机器150+，也没有做VPC，每个机器都有公网IP，都是通过个人密钥连接机器的，所以后来我把jumpserver给落地生产了，一方面是方便权限管理，另一方面是jumpserver支(防)持(止)命(运)令(维)记(背)录(锅) 问题描述 开发需要连接的这个机器，我给授权后，开发说无法连接，报错认证失败 我在jumpserver中各种方法都尝试了还不行 连接提示认证失败，在jumpserver群中提问，果然没有失望，还是熟悉的味道，没人回答😂 诡异的问题 1、在jumpserver中连接提示认证失败，相同的系统用户(jumpserver中的系统用户就是用于登陆jumpserver后连接机器的用户)，登陆其余机器没有问题 2、通过跳板机(我们有一台可以免密连接其他机器的跳板机)ssh连接机器，root用户连接特别慢(需要等待10秒左右，但是可以连接)，ubuntu用户无法连接，报错权限拒绝，但是密钥是正确并且存在的！ 3、使用root用户登陆这个机器后，执行 su - ubuntu 报错 Cannot execute /bin/bash: Too many open files in system 排查问题的过程 1、切换用户报错 Cannot execute /bin/bash: Too many open files in system ，百度后得到的答案大部分是系统文件句柄数打开过多，超出了系统的限制，因此会出现这个问题，不过实际上也确实是服务器上某些程序打开文件句柄书过多了 2、既然是程序打开文件句柄数过多，那就需要排查一下是哪些程序打开的文件句柄数过多， 关于查看进程打开的文件句柄数和修复方法的文章，参考文章之后，使用命令 lsof -n |awk '{print $2}'|sort|uniq -c |sort -nr|more查看之后进程打开的句柄数总数和系统设置的对应不上，这里还不太明白 解决问题的步骤 切换用户报错 $ su - ubuntu Cannot execute /bin/bash: Too many open files in system 查看系统最大文件打开数设置 $ ulimit -n 102400 查看 /etc/sysctl.conf 中的 file-max 参数值 $ grep file-max /etc/sysctl.conf fs.file-max = 65535 修改 file-max 值 fs.file-max = 999999 使配置生效 sysctl -p 修改完成后，切换用户可以了，jumpserver中连接用户也可以了 查看文件句柄数的方法 # 第一列是打开文件句柄数量，第二列是进程PID lsof -n |awk '{print $2}'|sort|uniq -c |sort -nr|more 这是当时有问题的这台机器上的输出，但是总数和系统限制对应不上，这一块还不太理解，后续在补充 lsof -n |awk '{print $2}'|sort|uniq -c |sort -nr|more 882820 11323 462017 11517 1296 24126 504 24175 420 32156 324 30589 319 12733 300 19706 247 10068 234 9661 234 10364 221 4349 216 7588 210 24233 177 25600 156 26224 143 984 132 20667 130 31959 117 1260 116 988 105 949 98 1063 84 23176 77 1 58 25599 57 15215 56 5354 56 25227 56 24065 54 25598 53 11146 47 386 38 927 38 924 38 6432 38 424 38 25099 38 25096 38 19422 38 19416 37 992 36 3156 36 1159 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux实用开源小工具/grc.html":{"url":"linux实用开源小工具/grc.html","title":"grc-linux命令高亮显示","keywords":"","body":"grc 加强Linux颜色显示，效果如下 ⚠️grc依赖python3 grc github地址 1.下载源码 git clone https://github.com.cnpmjs.org/garabik/grc.git 2.执行安装脚本、拷贝文件 # 执行安装脚本 cd grc sh install.sh # 拷贝文件 cp grc.sh /etc 3.设置别名 3.1 Bash 向 ~/.bashrc 写入以下内容 cat >> ~/.bashrc 加载生效 source ~/.bashrc 3.2 ZSH 向 ~/.zshrc 写入以下内容 [[ -s \"/etc/grc.zsh\" ]] && source /etc/grc.zsh 加载生效 source ~/.zshrc 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux实用开源小工具/chfs.html":{"url":"linux实用开源小工具/chfs.html","title":"chfs-文件管理","keywords":"","body":"chfs chfs github地址 chfs官网 生产环境使用场景说明 前端需要对一个网站下边的图片、html、css等文件做频繁的更换，但是又不太好做CI/CD，因为每次前端都需要提交代码然后在CI/CD工具(jenkins、gitlab)中做操作，这样既不方便又比较费时间，因此利用chfs这个工具，运维启动chfs并配置相关用户，然后让这个用户只针对于某一个或多个目录有操作权限，这样前端就能够只针对一个目录下边的文件做快速替换了 1.下载包 wget http://iscute.cn/tar/chfs/2.0/chfs-linux-amd64-2.0.zip 2.解压缩、赋予执行权限 unzip chfs-linux-amd64-2.0.zip && chmod +x chfs 解压缩后就是一个没有执行权限的 chfs 文件 查看文件类型 $ file chfs chfs: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, stripped 3.chfs命令选项 执行 ./chfs --help 查看chfs命令的选项 $ ./chfs --help usage: chfs [] Flags: --help Show context-sensitive help (also try --help-long and --help-man). --path=DIRECTORIES Directories where store shared files, separated by '|'. --port=PORT HTTP listening port(Default is 80). --allow=LIST Allowed IPv4 addresses(Allow any address by default). White list mode: \"listitem1[,listitem2,...]\" e.g. \"192.168.1.2-192.168.1.10,192.169.1.222\" allows this 10 addresses. Black list mode: \"not(listitem1[,listitem2,...])\" e.g. \"not(192.168.1.2-192.168.1.10,192.169.1.222)\" bans this 10 addresses! --rule=LIST Access rules(anybody can access any thing by default). List defines like:\"USER:PWD:MASK[:DIR:MASK...][|...]\": 1,USER and PWD is account name and password 2,MASK:''=NO present,'r'=read,'w'=write,'d'=delete 3,r=view+download,w=r+upload+create+rename,d=w+delete 4,DIR is directory name, allows wildcard('*' & '?') 5,The 3rd field is access mask of shared root directory 6,The optional fields is pairs of sub-directory and mask 7,The optional sub-directory's mask overwrite parent's 8,You should avoid '|' ':' and white space(exclude DIR) For instance: \"::|root:123456:rw\" bans guest, and defines a account 'root' can do anything --log=DIRECTORY Log directory. Empty value will disable log. --file=FILE A configuration file which overwrites & enhence the settings. --version Show application version. 参数说明 4.chfs使用示例 以下示例为官方文档中的示例，设置目录的部分是以windows为例的，在实际应用中，把windows目录替换为linux目录即可，用法相同 # 都使用默认参数，共享目录为程序运行目录，监听端口号为80 chfs # 共享目录为D盘，监听端口号为8080 chfs --path=\"d:/\" --port=8080 # 共享目录为\"d:\\\\projects\"和\"e:\\\\nsis\"，监听端口号为80 chfs --path=\"d:\\\\projects|e:\\\\nsis\" # 白名单模式，允许192.168.1.2-192.168.1.100以及192.168.1.200进行访问 chfs --allow=\"192.168.1.2-192.168.1.100,192.168.1.200\" # 黑名单模式，禁止192.168.1.2-192.168.1.100以及192.168.1.200进行访问 chfs --allow=\"not(192.168.1.2-192.168.1.100,192.168.1.200)\" # 匿名用户具有只读权限（默认情况下匿名用户具有读写权限） # 账户ceshizu，密码为ceshizu123，对根目录的权限为只读，但对test目录具有读写权限 # 账户yanfazu，密码为yanfazu123，对根目录的权限为只读，但对yanfa目录具有读写权限 chfs --rule=\"::r|ceshizu:ceshizu123:r:test:rw|yanfazu:yanfazu123:r:yanfa:rw\" # 匿名用户什么权限都没有（默认情况下匿名用户具有读写权限） # 账户admin，密码为admin123，具有读写权限 # 账户zhangsan，密码为zhangsan123，对根目录的权限为不可读写，但对zhangsanfiles目录具有读写权限 chfs --rule=\"::|admin:admin123:rw|zhangsan:zhangsan123::zhangsanfiles:rw\" # 通过配置文件进行配置，该文件可以不存在，待以后需要更改配置时使用 chfs --file=\"d:\\chfs\\chfs.ini\" 5.启动chfs 5.1 命令方式 匿名用户没有任何权限，nima用户具有读写删除权限 nohup ./chfs --path=\"/data/website/down\" --port=8088 --rule=\"::|nima:nima:rwd\" & 浏览器访问 IP:端口，需要点击 登陆 输入用户名密码 登陆成功后就可以对权限目录下的文件目录操作了，绿色为 下载 、橘色为 修改 和 重命名 、红色为 删除 5.2 配置文件方式 官方配置文件示例，通过 --file=文件 选项启动 #--------------------------------------- # 请注意： # 1，如果不存在键或对应值为空，则不影响对应的配置 # 2，配置项的值，语法如同其对应的命令行参数 #--------------------------------------- # 监听端口 port= # 共享根目录，通过字符'|'进行分割 # 注意： # 1，带空格的目录须用引号包住，如 path=\"c:\\a uply name\\folder\" # 2，可配置多个path，分别对应不同的目录 path= # IP地址过滤 allow= #----------------- 账户控制规则 ------------------- # 注意：该键值可以同时存在多个，你可以将每个用户的访问规则写成一个rule，这样比较清晰，如： # rule=:: # rule=root:123456:RW # rule=readonlyuser:123456:R rule= # 用户操作日志存放目录，默认为空 # 如果赋值为空，表示禁用日志 log= # 网页标题 html.title= # 网页顶部的公告板。可以是文字，也可以是HTML标签，此时，需要适用一对``(反单引号，通过键盘左上角的ESC键下面的那个键输出)来包住所有HTML标签。几个例子： # 1,html.notice=内部资料，请勿传播 # 2,html.notice=`` # 3,html.notice=`目录说明：一期工程：一期工程资料目录二期工程：二期工程资料目录` html.notice= # 是否启用图片预览(网页中显示图片文件的缩略图)，true表示开启，false为关闭。默认开启 image.preview= # 下载目录策略。disable:禁用; leaf:仅限叶子目录的下载; enable或其他值:不进行限制。 # 默认值为 enable folder.download= #-------------- 设置生效后启用HTTPS，注意监听端口设置为443------------- # 指定certificate文件 ssl.cert= # 指定private key文件 ssl.key= # 设置会话的生命周期，单位：分钟，默认为30分钟 session.timeout= 编辑配置文件 cat > chfs.ini 启动 nohup ./chfs --file=chfs.ini & 6.chfs开发文档 运行chfs后，通过访问 IP:port/asset/api.html 查看chfs API文档 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux实用开源小工具/bat.html":{"url":"linux实用开源小工具/bat.html","title":"bat-文件高亮显示","keywords":"","body":"bat bat github地址 bat github中文介绍地址 1.简介 官方对于 bat 的解释 一个 cat 克隆，搭配语法高亮和Git集成 语法高亮显示 bat支持大量编程和mark**语言的语法高亮显示: Git集成 bat与git沟通,显示关于修改的索引 (参见左侧栏) : 2.安装 2.1 下载安装包 在 官方releases 中下载安装包 安装参考链接 CentOS7需要下载 bat-v0.18.1-x86_64-unknown-linux-musl.tar.gz 格式的包 wget https://github.com/sharkdp/bat/releases/download/v0.18.1/bat-v0.18.1-x86_64-unknown-linux-musl.tar.gz 2.2 解压缩、修改文件名称 tar xf bat-v0.18.1-x86_64-unknown-linux-musl.tar.gz -C /usr/local/ mv /usr/local/bat-v0.18.1-x86_64-unknown-linux-musl/ /usr/local/bat-v0.18.1 2.3 导出命令 ln -s /usr/local/bat-v0.18.1/bat /usr/bin 2.4 查看版本 $ bat --version bat 0.18.1 3.使用 3.1 查看主题 使用 bat --list-themes 获取语法高亮显示的所有可用主题的列表 3.2 使用主题 使用命令 bat --theme=主题名 指定主题 # 使用 Monokai Extended Origin 主题 bat --theme=\"Monokai Extended Origin\" test 或者 export BAT_THEME=\"Monokai Extended Origin\" bat test 未使用主题前 cat /etc/profile 使用主题后 export BAT_THEME=\"Monokai Extended Origin\" bat /etc/profile 更多操作(下载主题等)看官方文档即可 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"博客工具/gitbook/CentOS7安装gitbook.html":{"url":"博客工具/gitbook/CentOS7安装gitbook.html","title":"gitbook安装","keywords":"","body":"[toc] CentOS7安装gitbook gitbook官网 gitbook github地址 1.安装nodejs nodejs官网 nodejs官方下载地址 nodejs历史版本官方下载地址 1.1 下载二进制包 export NODE_VERSION=12.22.3 wget https://nodejs.org/download/release/v${NODE_VERSION}/node-v${NODE_VERSION}-linux-x64.tar.xz 1.2 解压缩包、修改名称 tar xf node-v${NODE_VERSION}-linux-x64.tar.xz -C /usr/local/ && mv /usr/local/node-v${NODE_VERSION}-linux-x64/ /usr/local/node-v${NODE_VERSION} 1.3 导出环境变量 echo \"export PATH=$PATH:/usr/local/node-v${NODE_VERSION}/bin\" > /etc/profile.d/node.sh && source /etc/profile 1.4 验证 $ node -v v12.22.3 $ npm -v 6.14.13 1.5 配置npm加速 npm config set registry https://registry.npm.taobao.org 验证加速 $ npm config get registry https://registry.npm.taobao.org/ 2.安装gitbook 2.1 安装 gitbook-cli gitbook-cli 是在同一系统上安装和使用多个版本的 GitBook 的实用程序。它将自动安装所需版本的 GitBook 以构建一本书。 npm install gitbook-cli -g 2.2 安装gitbook 第一次执行 gitbook -V 开始安装gitbook $ gitbook -V CLI version: 2.3.2 Installing GitBook 3.2.3 /usr/local/node-v14.17.0/lib/node_modules/gitbook-cli/node_modules/npm/node_modules/graceful-fs/polyfills.js:287 if (cb) cb.apply(this, arguments) ^ TypeError: cb.apply is not a function at /usr/local/node-v14.17.0/lib/node_modules/gitbook-cli/node_modules/npm/node_modules/graceful-fs/polyfills.js:287:18 at FSReqCallback.oncomplete (fs.js:193:5) 遇到报错，按照提示编辑文件 /usr/local/node-v14.17.0/lib/node_modules/gitbook-cli/node_modules/npm/node_modules/graceful-fs/polyfills.js ，注释以下3行，重新执行 gitbook -V 即可 //fs.stat = statFix(fs.stat) //fs.fstat = statFix(fs.fstat) //fs.lstat = statFix(fs.lstat) 验证 $ gitbook -V CLI version: 2.3.2 GitBook version: 3.2.3 3.gitbook使用 3.1 创建gitbook目录 mkdir /gitbook 3.2 初始化gitbook cd /gitbook && gitbook init 初始化完成后会生成两个文件 README.md #项目介绍文件 SUMMARY.md #gitbook目录结构 3.3 配置gitbook生成书籍 编辑 SUMMARY.md ，写入以下内容(这里仅做示例) ⚠️vim命令.md的路径是/gitbook/linux/linux命令 # Summary * [Linux](README.md) * [Linux基础](README.md) * [Linux命令](README.md) * [vim命令](README.md) * [vim命令](linux/linux命令/vim命令.md) 3.4 构建书籍 ⚠️构建命令必须在SUMMARY.md同路径下执行 gitbook build 3.5 启动gitbook gitbook默认监听tcp 4000端口 gitbook serve & 浏览器访问 IP:4000 4.gitbook设置 4.1 修改gitbook代码框字体大小 node_modules/prism-themes/themes/prism-base16-ateliersulphurpool.light.css 13、14行 font-size: 18px; line-height: 1.6; 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"博客工具/VuePress/VuePress1.x.html":{"url":"博客工具/VuePress/VuePress1.x.html","title":"VuePress1.x","keywords":"","body":"[toc] VuePress VuePress 1.x 官方网站 VuePress 2.x 官方网站 VuePress github地址 1.安装nodejs nodejs官网 nodejs官方下载地址 nodejs历史版本官方下载地址 1.1 下载二进制包 export NODE_VERSION=12.22.3 wget https://nodejs.org/dist/latest-v12.x/node-v${NODE_VERSION}-linux-x64.tar.xz 1.2 解压缩包、修改名称 tar xf node-v${NODE_VERSION}-linux-x64.tar.xz -C /usr/local/ && mv /usr/local/node-v${NODE_VERSION}-linux-x64/ /usr/local/node-v${NODE_VERSION} 1.3 导出环境变量 echo \"export PATH=$PATH:/usr/local/node-v${NODE_VERSION}/bin\" > /etc/profile.d/node.sh && source /etc/profile 1.4 验证 $ node -v v12.22.3 $ npm -v 6.14.13 1.5 配置npm加速 npm config set registry https://registry.npm.taobao.org 验证加速 $ npm config get registry https://registry.npm.taobao.org/ 1.6 安装yarn npm -g install yarn 2.安装VuePress 2.1 创建一个新目录 [ -d /vuepress ] || mkdir /vuepress && cd /vuepress 2.2 初始化项目 # 一路会车默认即可，执行完成后会生成一个 package.json 文件 yarn init package.json 文件内容如下 { \"name\": \"vuepress\", \"version\": \"1.0.0\", \"main\": \"index.js\", \"license\": \"MIT\" } 2.3 将 VuePress 安装为本地依赖 # 执行完成后会生成 node_modules 目录和 yarn.lock 文件 yarn add -D vuepress 2.4 创建第一篇文档 mkdir docs && echo '# Hello VuePress' > docs/README.md 2.5 在 package.json 中添加一些scripts { \"scripts\": { \"docs:dev\": \"vuepress dev docs\", \"docs:build\": \"vuepress build docs\" } } 2.6 在本地启动服务器 VuePress 会在 http://localhost:8080 本地启动一个热重载的开发服务器。当你修改你的 Markdown 文件时，浏览器中的内容也会自动更新。 yarn docs:dev 直接执行 yarn docs:dev & 是不能在后台运行的，原因未知， 如果想要后台运行，执行以下命令，nohup.out文件必须存在 yarn docs:dev nohup.out& 浏览器访问 IP:8080，初始效果如下 3.目录结构 VuePress 遵循 “约定优于配置” 的原则，推荐的目录结构如下： docs/.vuepress: 用于存放全局的配置、组件、静态资源等。 docs/.vuepress/components: 该目录中的 Vue 组件将会被自动注册为全局组件。 docs/.vuepress/theme: 用于存放本地主题。 docs/.vuepress/styles: 用于存放样式相关的文件。 docs/.vuepress/styles/index.styl: 将会被自动应用的全局样式文件，会生成在最终的 CSS 文件结尾，具有比默认样式更高的优先级。 docs/.vuepress/styles/palette.styl: 用于重写默认颜色常量，或者设置新的 stylus 颜色常量。 docs/.vuepress/public: 静态资源目录。 docs/.vuepress/templates: 存储 HTML 模板文件。 docs/.vuepress/templates/dev.html: 用于开发环境的 HTML 模板文件。 docs/.vuepress/templates/ssr.html: 构建时基于 Vue SSR 的 HTML 模板文件。 docs/.vuepress/config.js: 配置文件的入口文件，也可以是 YML 或 toml。 docs/.vuepress/enhanceApp.js: 客户端应用的增强。 当你想要去自定义 templates/ssr.html 或 templates/dev.html 时，最好基于 默认的模板文件来修改，否则可能会导致构建出错。 3.1 默认的页面路由 此处我们把 docs 目录作为 targetDir （参考 命令行接口），下面所有的“文件的相对路径”都是相对于 docs 目录的。在项目根目录下的 package.json 中添加 scripts ： { \"scripts\": { \"dev\": \"vuepress dev docs\", \"build\": \"vuepress build docs\" } } 对于上述的目录结构，默认页面路由地址如下： 文件的相对路径 页面路由地址 /README.md / /guide/README.md /guide/ /config.md /config.html 4.配置VuePress首页面及目录映射规则简单示例 4.1 配置VuePress首页面 在 docs/README.md 中加入以下内容 --- home: true #heroImage: /hero.png heroText: Hero 标题 tagline: Hero 副标题 actionText: 快速上手 → actionLink: /zh/guide/ features: - title: 简洁至上 details: 以 Markdown 为中心的项目结构，以最少的配置帮助你专注于写作。 - title: Vue驱动 details: 享受 Vue + webpack 的开发体验，在 Markdown 中使用 Vue 组件，同时可以使用 Vue 来开发自定义主题。 - title: 高性能 details: VuePress 为每个页面预渲染生成静态的 HTML，同时在页面被加载的时候，将作为 SPA 运行。 footer: MIT Licensed | Copyright © 2018-present Evan You --- # Hello VuePress 效果如下 4.2 配置VuePress目录 在 docs 目录下创建一个目录 test mkdir docs/test 写入内容 cat > docs/test/test.md 浏览器访问 IP:port/test/test.html，效果如下 VuePress是以docs目录下的目录为映射路径的，即默认的根目录为docs 5.配置VuePress导航栏 5.1 配置VuePress导航栏logo VuePress1.x导航栏logo官方文档 你可以通过 themeConfig.logo 增加导航栏 Logo ，Logo 可以被放置在公共文件目录 .vuepress/public： # 创建 docs/.vuepress/ 目录 mkdir docs/.vuepress # 创建 docs/.vuepress/config.js 文件 cat > docs/.vuepress/config.js 效果如下 5.2 配置VuePress导航栏链接 VuePress1.x 导航栏链接官方文档 5.2.1 配置导航栏 配置示例 // .vuepress/config.js module.exports = { themeConfig: { nav: [ { text: 'Languages', ariaLabel: 'Language Menu', items: [ { text: 'Chinese', link: '/language/chinese/' }, { text: 'Japanese', link: '/language/japanese/' } ] } ] } } 效果如下 此外，还可以使用items嵌套，语法如下 // .vuepress/config.js module.exports = { themeConfig: { nav: [ { text: 'Languages', items: [ { text: 'Group1', items: [/* */] }, { text: 'Group2', items: [/* */] } ] } ] } } 使用示例 // .vuepress/config.js module.exports = { themeConfig: { nav: [ { text: 'Languages', items: [ { text: 'Group1', items: [ { text: 'Chinese', link: '/language/chinese/' }, { text: 'Japanese', link: '/language/japanese/' } ] }, { text: 'Group2', items: [ { text: 'Chinese', link: '/language/chinese/' }, { text: 'Japanese', link: '/language/japanese/' } ] } ] } ] } } 效果如下 如果想要增加多个导航链接，写法如下 // .vuepress/config.js module.exports = { themeConfig: { nav: [ { text: 'Home', link: '/' }, { text: 'Guide', link: '/guide/' }, { text: 'External', link: 'https://google.com' }, ] } } 效果如下 使用示例 // .vuepress/config.js module.exports = { themeConfig: { nav: [ { text: 'linux', items: [ { text: 'linux基础命令', items: [ { text: 'sed', link: '/language/chinese/' }, { text: 'awk', link: '/language/japanese/' } ] }, { text: 'linux服务', items: [ { text: 'nginx', link: '/language/chinese/' }, { text: 'ssh', link: '/language/japanese/' } ] } ] }, { text: 'python', items: [ { text: 'python基础', items: [ { text: 'python基础1', link: '/language/chinese/' }, { text: 'python基础2', link: '/language/japanese/' } ] }, { text: 'python框架', items: [ { text: 'django', link: '/language/chinese/' }, { text: 'flash', link: '/language/japanese/' } ] } ] } ] } } 效果如下 5.2.2 禁用导航栏 禁用导航栏，语法如下 // .vuepress/config.js module.exports = { themeConfig: { navbar: false } } 在某个md文件中禁用导航栏 --- navbar: false --- 例如，在 docs/about/about.md 文件中有如下内容 # 你好 ## 萨瓦迪卡 浏览器访问 ip:port/about/about.html 修改 docs/about/about.md 文件 --- navbar: false --- # 你好 ## 萨瓦迪卡 浏览器再次访问 ip:port/about/about.html 5.3 配置VuePress侧边栏 5.3.1 自动生成侧边栏 在md文件中开头写入以下内容 --- sidebar: auto --- 在全局 config.js 中配置 // .vuepress/config.js module.exports = { themeConfig: { sidebar: 'auto' } } 使用示例 docs/about/about.md 文件内容如下 --- navbar: false --- # 你好 ## 萨瓦迪卡 # 哈哈 ## 呵呵 # 123 ## jqk 浏览器访问 ip:port/about/about.html，效果如下 编辑 config.js ，配置自动生成侧边栏 // .vuepress/config.js module.exports = { themeConfig: { sidebar: 'auto' } } 效果如下 5.3.2 侧边栏分组 示例代码 // .vuepress/config.js module.exports = { themeConfig: { sidebar: [ { title: 'Group 1', // 必要的 path: '/foo/', // 可选的, 标题的跳转链接，应为绝对路径且必须存在 collapsable: false, // 可选的, 默认值是 true, sidebarDepth: 1, // 可选的, 默认值是 1 children: [ '/' ] }, { title: 'Group 2', children: [ /* ... */ ], initialOpenGroupIndex: -1 // 可选的, 默认值是 0 } ] } } 准备工作 # 在docs目录下新建多个目录，并在每个目录下编辑测试文件 mkdir -p linux echo nginx > linux/nginx.md echo ssh > linux/ssh.md echo php > linux/php.md 代码 // .vuepress/config.js module.exports = { themeConfig: { sidebar: [ { title: 'Group 1', // 必要的 path: '/linux/', // 可选的, 标题的跳转链接，应为绝对路径且必须存在 collapsable: false, // 可选的, 默认值是 true, sidebarDepth: 1, // 可选的, 默认值是 1 children: [ '/linux/nginx.html', '/linux/ssh.html', '/linux/php.html', ] }, { title: 'Group 2', children: [ /* ... */ ], initialOpenGroupIndex: -1 // 可选的, 默认值是 0 } ] } } 效果如下 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"生产环境开源项目/任务调度系统/DolphinScheduler单机部署.html":{"url":"生产环境开源项目/任务调度系统/DolphinScheduler单机部署.html","title":"DolphinScheduler","keywords":"","body":"[toc] DolphinScheduler 单机部署 1.DolphinScheduler简介 DolphinScheduler说明 Apache DolphinScheduler 是一个分布式去中心化，易扩展的可视化DAG工作流任务调度系统。致力于解决数据处理流程中错综复杂的依赖关系，使调度系统在数据处理流程中开箱即用。 使用背景 公司使用的是EasyScheduler，不过 EasyScheduler 已经捐献给了 Apache 贡献给 Apache 后，改名为 DolphinScheduler，EasyScheduler已经不在维护 DolphinScheduler官网 DolphinScheduler github地址 设计特点： 一个分布式易扩展的可视化DAG工作流任务调度系统。致力于解决数据处理流程中错综复杂的依赖关系，使调度系统在数据处理流程中开箱即用。 其主要目标如下： 以DAG图的方式将Task按照任务的依赖关系关联起来，可实时可视化监控任务的运行状态 支持丰富的任务类型：Shell、MR、Spark、SQL(mysql、postgresql、hive、sparksql),Python,Sub_Process、Procedure等 支持工作流定时调度、依赖调度、手动调度、手动暂停/停止/恢复，同时支持失败重试/告警、从指定节点恢复失败、Kill任务等操作 支持工作流优先级、任务优先级及任务的故障转移及任务超时告警/失败 支持工作流全局参数及节点自定义参数设置 支持资源文件的在线上传/下载，管理等，支持在线文件创建、编辑 支持任务日志在线查看及滚动、在线下载日志等 实现集群HA，通过Zookeeper实现Master集群和Worker集群去中心化 支持对Master/Worker cpu load，memory，cpu在线查看 支持工作流运行历史树形/甘特图展示、支持任务状态统计、流程状态统计 支持补数 支持多租户 支持国际化 还有更多等待伙伴们探索 与同类调度系统的对比 2.DolphinScheduler标准安装 2.1 官方推荐系统配置 Linux 操作系统版本要求 操作系统 版本 Red Hat Enterprise Linux 7.0 及以上 CentOS 7.0 及以上 Oracle Enterprise Linux 7.0 及以上 Ubuntu LTS 16.04 及以上 注意： 以上 Linux 操作系统可运行在物理服务器以及 VMware、KVM、XEN 主流虚拟化环境上。 服务器建议配置 CPU 内存 硬盘类型 网络 实例数量 4核+ 8 GB+ SAS 千兆网卡 1+ 注意： 以上建议配置为部署 DolphinScheduler 的最低配置，生产环境强烈推荐使用更高的配置。 硬盘大小配置建议 50GB+ ，系统盘和数据盘分开。 网络要求 DolphinScheduler正常运行提供如下的网络端口配置： 组件 默认端口 说明 MasterServer 5678 非通信端口，只需本机端口不冲突即可 WorkerServer 1234 非通信端口，只需本机端口不冲突即可 ApiApplicationServer 12345 提供后端通信端口 注意： MasterServer 和 WorkerServer 不需要开启网络间通信，只需本机端口不冲突即可 管理员可根据实际环境中 DolphinScheduler 组件部署方案，在网络侧和主机侧开放相关端口 2.2 系统环境 操作系统及配置 系统 配置 磁盘 centos7.8 4c8g 50g 2.3 安装过程 DolphinScheduler最新版1.3.2官方文档(截止2002.9.5) 2.3.1 基础软件安装 2.3.1.1 数据库(pg或者mysql) PostgreSQL (8.2.15+) or MySQL (5.7系列) : 必装 两者任选其一即可 2.3.1.2 JDK8 JDK (1.8+) : 必装，请安装好后在/etc/profile下配置 JAVA_HOME 及 PATH 变量 自行到JDK官网下载安装包 这里选择下载jdk-8u251-linux-x64.tar.gz # 上传jdk安装包至/opt并解压缩至/usr/local cd /opt && tar xf jdk-8u251-linux-x64.tar.gz -C /usr/local # 导出jdk环境变量 cat >/etc/profile.d/jdk8.sh 2.3.1.3 ZooKeeper ZooKeeper (3.4.6+) ：必装 ZooKeeper官网 ZooKeeper官方下载地址 下载二进制安装包 选择官方推荐的下载地址 安装过程 # 1.下载二进制包 wget https://apache-mirror.rbc.ru/pub/apache/zookeeper/zookeeper-3.6.1/apache-zookeeper-3.6.1-bin.tar.gz # 2.解压缩 tar xf apache-zookeeper-3.6.1-bin.tar.gz -C /usr/local/ # 3.做软连接 ln -s /usr/local/apache-zookeeper-3.6.1-bin/ /usr/local/zookeeper-3.6.1 # 4.编辑最简zookeeper配置文件 cat > /usr/local/zookeeper-3.6.1/conf/zoo.cfg zookeeper默认配置文件说明 # 通信心跳时间，系统默认是2000毫秒，也就是间隔两秒心跳一次 # 客户端与服务器或者服务器与服务器之间维持心跳，也就是每个tickTime时间就会发送一次心跳。通过心跳不仅能够用来监听机器的工作状态，还可以通过心跳来控制Flower跟Leader的通信时间，默认情况下FL的会话时常是心跳间隔的两倍。 tickTime=2000 # 集群中的follower服务器(F)与leader服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量）。 initLimit=10 # 集群中flower服务器（F）跟leader（L）服务器之间的请求和答应最多能容忍的心跳数。 syncLimit=5 # zookeeper数据目录，默认是/tmp/zookeeper dataDir=/data/zookeeper # 客户端连接的接口，客户端连接zookeeper服务器的端口，zookeeper会监听这个端口，接收客户端的请求访问！ clientPort=2181 2.3.1.4 Hadoop Hadoop (2.6+) or MinIO ：选装， 如果需要用到资源上传功能，针对单机可以选择本地文件目录作为上传文件夹(此操作不需要部署Hadoop)；当然也可以选择上传到Hadoop or MinIO集群上 注意：DolphinScheduler本身不依赖Hadoop、Hive、Spark，仅是会调用他们的Client，用于对应任务的运行。 2.3.2 下载 DolphinScheduler 二进制tar.gz包 官方下载地址 选择相应版本的二进制包 选择官方推荐的下载地址 下载二进制包并解压缩 # 1.创建部署目录，部署目录请不要创建在/root、/home等高权限目录 mkdir -p /opt/dolphinscheduler && cd /opt/dolphinscheduler # 2.下载二进制包 wget https://apache-mirror.rbc.ru/pub/apache/incubator/dolphinscheduler/1.3.2/apache-dolphinscheduler-incubating-1.3.2-dolphinscheduler-bin.tar.gz # 3.解压缩并重命名 tar xf apache-dolphinscheduler-incubating-1.3.2-dolphinscheduler-bin.tar.gz && mv apache-dolphinscheduler-incubating-1.3.2-dolphinscheduler-bin dolphinscheduler-bin 2.3.3 创建部署用户并赋予目录操作权限 创建部署用户，并且一定要配置sudo免密。以创建 dolphinscheduler 用户为例 # 1.创建用户 useradd dolphinscheduler # 2.设置密码 echo \"dolphinscheduler\" | passwd --stdin dolphinscheduler # 3.配置sudo免密 sed -i '$adolphinscheduler ALL=(ALL) NOPASSWD: NOPASSWD: ALL' /etc/sudoers # 4.修改目录权限，使得部署用户对dolphinscheduler-bin目录有操作权限 chown -R dolphinscheduler:dolphinscheduler dolphinscheduler-bin 注意： 因为任务执行服务是以 sudo -u {linux-user} 切换不同linux用户的方式来实现多租户运行作业，所以部署用户需要有 sudo 权限，而且是免密的。初学习者不理解的话，完全可以暂时忽略这一点 如果发现 /etc/sudoers 文件中有\"Default requiretty\"这行，也请注释掉 如果用到资源上传的话，还需要给该部署用户分配操作本地文件系统或者HDFS或者MinIO的权限 2.3.4 ssh免密配置 切换到部署用户并配置ssh本机免密登录 su - dolphinscheduler ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys chmod 600 ~/.ssh/authorized_keys 注意：正常设置后，dolphinscheduler用户在执行命令ssh localhost 是不需要再输入密码的 2.3.5 数据库初始化 以下操作为创建数据库 dolphinscheduler，授权用户 dol 对数据库 dolphinscheduler有所有权限，密码是 dol # 创建数据库 mysql -uroot -e \"CREATE DATABASE dolphinscheduler DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci\" # 授权 mysql -uroot -e \"GRANT ALL PRIVILEGES ON dolphinscheduler.* TO 'dol'@'%' IDENTIFIED BY 'dol'\" mysql -uroot -e \"GRANT ALL PRIVILEGES ON dolphinscheduler.* TO 'dol'@'localhost' IDENTIFIED BY 'dol'\" # 刷新权限表 mysql -uroot -e \"flush privileges\" 本文选择的是mysql，因此需要下载mysql驱动，并放于 dolphinscheduler-bin/lib 下 mysql驱动官方下载地址 # 1.下载mysql驱动包 wget https://downloads.mysql.com/archives/get/p/3/file/mysql-connector-java-5.1.47.tar.gz # 2.解压缩 tar xf mysql-connector-java-5.1.47.tar.gz # 3.拷贝驱动包 cp mysql-connector-java-5.1.47/mysql-connector-java-5.1.47.jar dolphinscheduler-bin/lib/ 修改 conf/datasource.properties 中的配置 注释如下内容 # postgresql spring.datasource.driver-class-name=org.postgresql.Driver spring.datasource.url=jdbc:postgresql://localhost:5432/dolphinscheduler spring.datasource.username=test spring.datasource.password=test 修改为如下 spring.datasource.driver-class-name=com.mysql.jdbc.Driver spring.datasource.url=jdbc:mysql://localhost:3306/dolphinscheduler?useUnicode=true&characterEncoding=UTF-8&allowMultiQueries=true spring.datasource.username=dol spring.datasource.password=dol # 使用如下命令修改 创建表及导入基础数据 # 编辑hosts文件，否则执行脚本会报错，这里最少保证能解析ds1 cat >> /etc/hosts 2.3.6 修改运行参数 修改 conf/env/dolphinscheduler_env.sh 环境变量(以相关用到的软件都安装在/opt/soft下为例)，请根据自己的实际情况进行修改 export HADOOP_HOME=/opt/soft/hadoop export HADOOP_CONF_DIR=/opt/soft/hadoop/etc/hadoop #export SPARK_HOME1=/opt/soft/spark1 export SPARK_HOME2=/opt/soft/spark2 export PYTHON_HOME=/opt/soft/python export JAVA_HOME=/opt/soft/java export HIVE_HOME=/opt/soft/hive export FLINK_HOME=/opt/soft/flink export DATAX_HOME=/opt/soft/datax/bin/datax.py export PATH=$HADOOP_HOME/bin:$SPARK_HOME2/bin:$PYTHON_HOME:$JAVA_HOME/bin:$HIVE_HOME/bin:$PATH:$FLINK_HOME/bin:$DATAX_HOME:$PATH 注: 这一步非常重要,例如 JAVA_HOME 和 PATH 是必须要配置的，没有用到的可以忽略或者注释掉 修改一键部署配置文件 conf/config/install_config.conf中的各参数，特别注意以下参数的配置，根据自己的实际情况进行修改 # 这里填 mysql or postgresql dbtype=\"mysql\" # 数据库连接地址 dbhost=\"localhost:3306\" # 数据库名 dbname=\"dolphinscheduler\" # 数据库用户名，此处需要修改为上面设置的{user}具体值 username=\"xxx\" # 数据库密码, 如果有特殊字符，请使用\\转义，需要修改为上面设置的{password}具体值 password=\"xxx\" # Zookeeper地址，单机本机是localhost:2181，记得把2181端口带上 zkQuorum=\"localhost:2181\" # 将DS安装到哪个目录，如: /opt/soft/dolphinscheduler，不同于现在的目录 installPath=\"/opt/soft/dolphinscheduler\" # 使用哪个用户部署，使用第3节创建的用户 deployUser=\"dolphinscheduler\" # 邮件配置，以qq邮箱为例 # 邮件协议 mailProtocol=\"SMTP\" # 邮件服务地址 mailServerHost=\"smtp.qq.com\" # 邮件服务端口 mailServerPort=\"25\" # mailSender和mailUser配置成一样即可 # 发送者 mailSender=\"xxx@qq.com\" # 发送用户 mailUser=\"xxx@qq.com\" # 邮箱密码 mailPassword=\"xxx\" # TLS协议的邮箱设置为true，否则设置为false starttlsEnable=\"true\" # 开启SSL协议的邮箱配置为true，否则为false。注意: starttlsEnable和sslEnable不能同时为true sslEnable=\"false\" # 邮件服务地址值，参考上面 mailServerHost sslTrust=\"smtp.qq.com\" # 业务用到的比如sql等资源文件上传到哪里，可以设置：HDFS,S3,NONE，单机如果想使用本地文件系统，请配置为HDFS，因为HDFS支持本地文件系统；如果不需要资源上传功能请选择NONE。强调一点：使用本地文件系统不需要部署hadoop resourceStorageType=\"HDFS\" # 这里以保存到本地文件系统为例 # 注：但是如果你想上传到HDFS的话，NameNode启用了HA，则需要将hadoop的配置文件core-site.xml和hdfs-site.xml放到conf目录下，本例即是放到/opt/dolphinscheduler/conf下面，并配置namenode cluster名称；如果NameNode不是HA,则修改为具体的ip或者主机名即可 defaultFS=\"file:///data/dolphinscheduler\" #hdfs://{具体的ip/主机名}:8020 # 如果没有使用到Yarn,保持以下默认值即可；如果ResourceManager是HA，则配置为ResourceManager节点的主备ip或者hostname,比如\"192.168.xx.xx,192.168.xx.xx\";如果是单ResourceManager请配置yarnHaIps=\"\"即可 yarnHaIps=\"192.168.xx.xx,192.168.xx.xx\" # 如果ResourceManager是HA或者没有使用到Yarn保持默认值即可；如果是单ResourceManager，请配置真实的ResourceManager主机名或者ip singleYarnIp=\"yarnIp1\" # 资源上传根路径,支持HDFS和S3,由于hdfs支持本地文件系统，需要确保本地文件夹存在且有读写权限 resourceUploadPath=\"/data/dolphinscheduler\" # 具备权限创建resourceUploadPath的用户 hdfsRootUser=\"hdfs\" # 在哪些机器上部署DS服务，本机选localhost ips=\"localhost\" # ssh端口,默认22 sshPort=\"22\" # master服务部署在哪台机器上 masters=\"localhost\" # worker服务部署在哪台机器上,并指定此worker属于哪一个worker组,下面示例的default即为组名 workers=\"localhost:default\" # 报警服务部署在哪台机器上 alertServer=\"localhost\" # 后端api服务部署在在哪台机器上 apiServers=\"localhost\" 注：如果打算用到资源中心功能，请执行以下命令： sudo mkdir /data/dolphinscheduler sudo chown -R dolphinscheduler:dolphinscheduler /data/dolphinscheduler 2.3.7 一键部署 切换到部署用户 dolphinscheduler，执行一键部署脚本 su - dolphinscheduler && cd /opt/dolphinscheduler-bin/ && sh install.sh 验证安装，使用命令 jps 查看，如显示如下既成功 $ jps 20260 Jps 20037 AlertServer ----- alert服务 20085 ApiApplicationServer ----- api服务 19580 MasterServer ----- master服务 19662 WorkerServer ----- worker服务 19710 LoggerServer ----- logger服务 部署成功后，可以进行日志查看，日志统一存放于 logs 文件夹内 # logs目录是在dolphinscheduler安装目录下生成的，我这里设置的安装目录是/usr/local/dolphinscheduler $ tree logs/ logs/ ├── dolphinscheduler-alert.log ├── dolphinscheduler-alert-server-test1.out ├── dolphinscheduler-api-server.log ├── dolphinscheduler-api-server-test1.out ├── dolphinscheduler-logger-server-test1.out ├── dolphinscheduler-master.log ├── dolphinscheduler-master-server-test1.out ├── dolphinscheduler-worker.log └── dolphinscheduler-worker-server-test1.out 2.3.8 访问 浏览器访问 IP:12345/dolphinscheduler 用户名：admin 密码：dolphinscheduler123 登陆后首界面 到此，dolphinscheduler单机版部署完成！ 2.3.9 启停服务 一键停止集群所有服务 sh bin/stop-all.sh 一键开启集群所有服务 sh bin/start-all.sh 启停Master sh bin/dolphinscheduler-daemon.sh start master-server sh bin/dolphinscheduler-daemon.sh stop master-server 启停Worker sh bin/dolphinscheduler-daemon.sh start worker-server sh bin/dolphinscheduler-daemon.sh stop worker-server 启停Api sh bin/dolphinscheduler-daemon.sh start api-server sh bin/dolphinscheduler-daemon.sh stop api-server 启停Logger sh bin/dolphinscheduler-daemon.sh start logger-server sh bin/dolphinscheduler-daemon.sh stop logger-server 启停Alert sh bin/dolphinscheduler-daemon.sh start alert-server sh bin/dolphinscheduler-daemon.sh stop alert-server dolphinscheduler使用文档 3.DolphinScheduler docker安装 dolphinscheduler源码 清华下载地址 3.1 下载源码 zip 包 # 创建源码存放目录 mkdir -p /opt/soft/dolphinscheduler; cd /opt/soft/dolphinscheduler; # 下载源码包 wget https://mirrors.tuna.tsinghua.edu.cn/apache/incubator/dolphinscheduler/1.3.5/apache-dolphinscheduler-incubating-1.3.5-src.zip # 解压缩 tar -zxvf apache-dolphinscheduler-incubating-1.3.5-src.zip mv apache-dolphinscheduler-incubating-1.3.5-src-release dolphinscheduler-src 3.2 安装并启动服务 docker-compose 国内下载源 cd dolphinscheduler-src docker-compose -f ./docker/docker-swarm/docker-compose.yml up -d docker-compose.yml文件内容 # Licensed to the Apache Software Foundation (ASF) under one # or more contributor license agreements. See the NOTICE file # distributed with this work for additional information # regarding copyright ownership. The ASF licenses this file # to you under the Apache License, Version 2.0 (the # \"License\"); you may not use this file except in compliance # with the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. version: \"3.4\" services: dolphinscheduler-postgresql: image: bitnami/postgresql:latest container_name: dolphinscheduler-postgresql ports: - 5432:5432 environment: TZ: Asia/Shanghai POSTGRESQL_USERNAME: root POSTGRESQL_PASSWORD: root POSTGRESQL_DATABASE: dolphinscheduler volumes: - dolphinscheduler-postgresql:/bitnami/postgresql - dolphinscheduler-postgresql-initdb:/docker-entrypoint-initdb.d restart: unless-stopped networks: - dolphinscheduler dolphinscheduler-zookeeper: image: bitnami/zookeeper:latest container_name: dolphinscheduler-zookeeper ports: - 2181:2181 environment: TZ: Asia/Shanghai ALLOW_ANONYMOUS_LOGIN: \"yes\" ZOO_4LW_COMMANDS_WHITELIST: srvr,ruok,wchs,cons volumes: - dolphinscheduler-zookeeper:/bitnami/zookeeper restart: unless-stopped networks: - dolphinscheduler dolphinscheduler-api: image: apache/dolphinscheduler:latest container_name: dolphinscheduler-api command: api-server ports: - 12345:12345 environment: TZ: Asia/Shanghai DATABASE_HOST: dolphinscheduler-postgresql DATABASE_PORT: 5432 DATABASE_USERNAME: root DATABASE_PASSWORD: root DATABASE_DATABASE: dolphinscheduler ZOOKEEPER_QUORUM: dolphinscheduler-zookeeper:2181 RESOURCE_STORAGE_TYPE: HDFS RESOURCE_UPLOAD_PATH: /dolphinscheduler FS_DEFAULT_FS: file:/// healthcheck: test: [\"CMD\", \"/root/checkpoint.sh\", \"ApiApplicationServer\"] interval: 30s timeout: 5s retries: 3 start_period: 30s depends_on: - dolphinscheduler-postgresql - dolphinscheduler-zookeeper volumes: - dolphinscheduler-logs:/opt/dolphinscheduler/logs - dolphinscheduler-resource-local:/dolphinscheduler restart: unless-stopped networks: - dolphinscheduler dolphinscheduler-alert: image: apache/dolphinscheduler:latest container_name: dolphinscheduler-alert command: alert-server environment: TZ: Asia/Shanghai XLS_FILE_PATH: \"/tmp/xls\" MAIL_SERVER_HOST: \"\" MAIL_SERVER_PORT: \"\" MAIL_SENDER: \"\" MAIL_USER: \"\" MAIL_PASSWD: \"\" MAIL_SMTP_STARTTLS_ENABLE: \"false\" MAIL_SMTP_SSL_ENABLE: \"false\" MAIL_SMTP_SSL_TRUST: \"\" ENTERPRISE_WECHAT_ENABLE: \"false\" ENTERPRISE_WECHAT_CORP_ID: \"\" ENTERPRISE_WECHAT_SECRET: \"\" ENTERPRISE_WECHAT_AGENT_ID: \"\" ENTERPRISE_WECHAT_USERS: \"\" DATABASE_HOST: dolphinscheduler-postgresql DATABASE_PORT: 5432 DATABASE_USERNAME: root DATABASE_PASSWORD: root DATABASE_DATABASE: dolphinscheduler healthcheck: test: [\"CMD\", \"/root/checkpoint.sh\", \"AlertServer\"] interval: 30s timeout: 5s retries: 3 start_period: 30s depends_on: - dolphinscheduler-postgresql volumes: - dolphinscheduler-logs:/opt/dolphinscheduler/logs restart: unless-stopped networks: - dolphinscheduler dolphinscheduler-master: image: apache/dolphinscheduler:latest container_name: dolphinscheduler-master command: master-server ports: - 5678:5678 environment: TZ: Asia/Shanghai MASTER_EXEC_THREADS: \"100\" MASTER_EXEC_TASK_NUM: \"20\" MASTER_HEARTBEAT_INTERVAL: \"10\" MASTER_TASK_COMMIT_RETRYTIMES: \"5\" MASTER_TASK_COMMIT_INTERVAL: \"1000\" MASTER_MAX_CPULOAD_AVG: \"100\" MASTER_RESERVED_MEMORY: \"0.1\" DATABASE_HOST: dolphinscheduler-postgresql DATABASE_PORT: 5432 DATABASE_USERNAME: root DATABASE_PASSWORD: root DATABASE_DATABASE: dolphinscheduler ZOOKEEPER_QUORUM: dolphinscheduler-zookeeper:2181 healthcheck: test: [\"CMD\", \"/root/checkpoint.sh\", \"MasterServer\"] interval: 30s timeout: 5s retries: 3 start_period: 30s depends_on: - dolphinscheduler-postgresql - dolphinscheduler-zookeeper volumes: - dolphinscheduler-logs:/opt/dolphinscheduler/logs restart: unless-stopped networks: - dolphinscheduler dolphinscheduler-worker: image: apache/dolphinscheduler:latest container_name: dolphinscheduler-worker command: worker-server ports: - 1234:1234 - 50051:50051 environment: TZ: Asia/Shanghai WORKER_EXEC_THREADS: \"100\" WORKER_HEARTBEAT_INTERVAL: \"10\" WORKER_MAX_CPULOAD_AVG: \"100\" WORKER_RESERVED_MEMORY: \"0.1\" WORKER_GROUP: \"default\" DOLPHINSCHEDULER_DATA_BASEDIR_PATH: /tmp/dolphinscheduler XLS_FILE_PATH: \"/tmp/xls\" MAIL_SERVER_HOST: \"\" MAIL_SERVER_PORT: \"\" MAIL_SENDER: \"\" MAIL_USER: \"\" MAIL_PASSWD: \"\" MAIL_SMTP_STARTTLS_ENABLE: \"false\" MAIL_SMTP_SSL_ENABLE: \"false\" MAIL_SMTP_SSL_TRUST: \"\" DATABASE_HOST: dolphinscheduler-postgresql DATABASE_PORT: 5432 DATABASE_USERNAME: root DATABASE_PASSWORD: root DATABASE_DATABASE: dolphinscheduler ZOOKEEPER_QUORUM: dolphinscheduler-zookeeper:2181 RESOURCE_STORAGE_TYPE: HDFS RESOURCE_UPLOAD_PATH: /dolphinscheduler FS_DEFAULT_FS: file:/// healthcheck: test: [\"CMD\", \"/root/checkpoint.sh\", \"WorkerServer\"] interval: 30s timeout: 5s retries: 3 start_period: 30s depends_on: - dolphinscheduler-postgresql - dolphinscheduler-zookeeper volumes: - ./dolphinscheduler_env.sh:/opt/dolphinscheduler/conf/env/dolphinscheduler_env.sh - dolphinscheduler-worker-data:/tmp/dolphinscheduler - dolphinscheduler-logs:/opt/dolphinscheduler/logs - dolphinscheduler-resource-local:/dolphinscheduler restart: unless-stopped networks: - dolphinscheduler networks: dolphinscheduler: driver: bridge volumes: dolphinscheduler-postgresql: dolphinscheduler-postgresql-initdb: dolphinscheduler-zookeeper: dolphinscheduler-worker-data: dolphinscheduler-logs: dolphinscheduler-resource-local: 3.3 登陆系统 浏览器访问 IP:12345/dolphinscheduler 用户名：admin 密码：dolphinscheduler123 nginx反向代理配置 自己百度谷歌动手查的，但还是官方给的回复比较好 server { listen 80; server_name ds.abc.com; return 301 https://$server_name$request_uri; } server { listen 443 ssl; server_name ds.abc.com; client_max_body_size 10240m; location / { proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header Host $http_host; proxy_set_header X-Forwarded-Proto $scheme; proxy_pass http://127.0.0.1:12345/dolphinscheduler; } location ^~ /dolphinscheduler { proxy_pass http://127.0.0.1:12345/dolphinscheduler; } access_log /var/log/ds/ds.abc.com.access.log; error_log /var/log/ds/ds.abc.com.error.log; ssl_certificate ssl_key/1_abc.com_bundle.crt; ssl_certificate_key ssl_key/2_abc.com.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; } dolphinscheduler安装后默认访问方式是 ip:12345/dolphinscheduler，现在想要以域名直接访问，需要在nginx中做如下配置 location / { proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header Host $http_host; proxy_set_header X-Forwarded-Proto $scheme; proxy_pass http://127.0.0.1:12345/dolphinscheduler; } location ^~ /dolphinscheduler { proxy_pass http://127.0.0.1:12345/dolphinscheduler; } 官方给的回复 server { listen 80; server_name ds.abc.com; return 301 https://$server_name$request_uri; } server { listen 443 ssl; server_name ds.abc.com; client_max_body_size 10240m; location / { proxy_pass http://127.0.0.1:12345/dolphinscheduler/; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-NginX-Proxy true; proxy_hide_header Server; proxy_redirect off; location /dolphinscheduler { proxy_pass http://127.0.0.1:12345; } } access_log /var/log/ds/ds.abc.com.access.log; error_log /var/log/ds/ds.abc.com.error.log; ssl_certificate ssl_key/1_ds.abc.com_bundle.crt; ssl_certificate_key ssl_key/2_ds.abc.com.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; } 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"生产环境开源项目/gocron/计划任务系统 gocron.html":{"url":"生产环境开源项目/gocron/计划任务系统 gocron.html","title":"gocron","keywords":"","body":"[toc] 计划任务系统 gocron gocron github地址 1.项目简介 使用Go语言开发的轻量级定时任务集中调度和管理系统，用于替代Linux-crontab 查看文档 原有的延时任务拆分为独立项目延迟队列 功能特性 Web界面管理定时任务 crontab时间表达式, 精确到秒 任务执行失败可重试 任务执行超时, 强制结束 任务依赖配置, A任务完成后再执行B任务 账户权限控制 任务类型 shell任务 在任务节点上执行shell命令, 支持任务同时在多个节点上运行 HTTP任务 访问指定的URL地址, 由调度器直接执行, 不依赖任务节点 查看任务执行结果日志 任务执行结果通知, 支持邮件、Slack、Webhook gocron架构示意图 gocron分为调度器和任务节点 gocron 命令 gocron -v 查看版本 gocron web --host 默认0.0.0.0 -p 端口, 指定端口, 默认5920 -e 指定运行环境, dev|test|prod, dev模式下可查看更多日志信息, 默认prod -h 查看帮助 gocron-node -allow-root *nix平台允许以root用户运行 -s ip:port 监听地址 -enable-tls 开启TLS -ca-file CA证书文件 -cert-file 证书文件 -key-file 私钥文件 -h 查看帮助 -v 查看版本 2.部署安装 从relese下载安装包 版本升级参考这里 2.1 下载包 下载调度器二进制包 wget https://github.com/ouqiang/gocron/releases/download/v1.5.3/gocron-v1.5.3-linux-amd64.tar.gz 下载node节点二进制包 wget https://github.com/ouqiang/gocron/releases/download/v1.5.3/gocron-node-v1.5.3-linux-amd64.tar.gz 2.2 启动gocron 2.2.1 启动gocron调度器 gocron调度器监听 5920 端口 tar xf gocron-v1.5.3-linux-amd64.tar.gz && cd gocron-linux-amd64 ./gocron web 2.2.2 启动gocron node gocron node监听 5921 端口 ⚠️gocron node 默认不允许以 root 用户启动，如果想要以 root 用户启动，则需要加参数 -allow-root，经个人测试，如果以非 root 用户启动，会有N堆坑 🦙 tar xf gocron-node-v1.5.3-linux-amd64.tar.gz && cd gocron-node-linux-amd64 ./gocron-node -allow-root 2.3 创建数据库 gocron需要连接数据库 create database gocron; grant all on gocron.* to gocron@'127.0.0.1' identified by 'gocron'; 2.4 访问gocron 浏览器访问 IP:5920 配置数据库、管理员账号 登陆 登陆后首界面 3.使用示例 3.1 新增节点 编辑节点信息，输入主机名(域名或者IP)，node节点默认端口是5921 添加完的节点 点击 测试连接 查看机器联通性 点击 任务管理 --> 新增 编辑任务 可以选择绿色框处的手动执行，也可以选择灰色框中的查看日志 查看日志，执行成功与否会在这里有显示 3.2 配置邮件通知 系统管理 --> 通知配置 编辑邮件服务器配置和接受的用户 ⚠️密码是相对应邮箱的授权码，不是邮箱登陆密码 ⚠️这里使用的是163邮箱，要注意开始SMTP服务 在任务中选择任务通知、通知类型和接收通知用户 执行成功的邮件通知 执行失败的邮件通知 4.使用supervisor管理gocron supervisor配置文件/etc/supervisor/supervisord.conf中定义了include，因此如果想要管理服务，就需要编辑/etc/supervisor/config.d/*.ini文件 [include] files = /etc/supervisor/config.d/*.ini 编辑gocron服务配置文件/etc/supervisor/config.d/gocron.ini ⚠️gocron不能独立运行，需指定程序运行时目录，这里gocron调度器二进制文件的目录是 /usr/local/gocron/gocron-linux-amd64 cat >/etc/supervisor/config.d/gocron.ini 将gocron加入supervisor $ supervisor> update gocron gocron: added process group 查看gocron运行状态 $ supervisor> status gocron RUNNING pid 17468, uptime 0:00:03 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"生产环境开源项目/bi工具/zeppelin/zeppelin安装配置.html":{"url":"生产环境开源项目/bi工具/zeppelin/zeppelin安装配置.html","title":"zeppelin","keywords":"","body":"[toc] zeppelin安装配置 zeppelin 官网 zeppelin github地址 1.zeppelin简介 Apache Zeppelin Apache Zeppelin 是一个让交互式数据分析变得可行的基于网页的开源框架。Zeppelin提供了数据分析、数据可视化等功能。 Zeppelin 是一个提供交互数据分析且基于Web的笔记本。方便你做出可数据驱动的、可交互且可协作的精美文档，并且支持多种语言，包括 Scala(使用 Apache Spark)、Python(Apache Spark)、SparkSQL、 Hive、 Markdown、Shell等等。 2.安装 2.1 下载二进制安装包 zeppelin官方下载地址 # 下载安装包 wget https://ftp.yz.yamagata-u.ac.jp/pub/network/apache/zeppelin/zeppelin-0.9.0/zeppelin-0.9.0-bin-all.tgz # 解压 tar xf zeppelin-0.9.0-bin-all.tgz -C /usr/local/ # 修改目录权限为hadoop chown -R hadoop.hadoop /usr/local/zeppelin-0.9.0-bin-all # 做软连接 ln -s /usr/local/zeppelin-0.9.0-bin-all/ /usr/local/zeppelin 2.2 修改配置文件 2.2.1 拷贝文件 zeppelin-site.xml是主配置文件 shiro.ini是用户权限配置文件 # 进入到zeppelin目录 cd /usr/local/zeppelin/conf # 拷贝文件 cp zeppelin-site.xml.template zeppelin-site.xml cp shiro.ini.template shiro.ini 2.2.2 修改配置文件 修改 zeppelin-site.xml # 修改监听地址 zeppelin.server.addr 127.0.0.1 Server binding address # 修改监听端口 zeppelin.server.port 8080 Server port. # 关闭匿名用户，zeppelin默认是允许匿名用户的，生产环境需要关闭，注意在0.9.0这个版本中，需要手动添加如下代码(网上的文章都是直接修改这一处，但是0.9.0中未找到以下代码) zeppelin.anonymous.allowed false Anonymous user allowed by default 修改 shiro.ini，在 [users]下添加用户名和密码及权限，格式为 用户名 = 密码, 权限 [users] # List of users with their password allowed to access Zeppelin. # To use a different strategy (LDAP / Database / ...) check the shiro doc at http://shiro.apache.org/configuration.html#Configuration-INISections # To enable admin user, uncomment the following line and set an appropriate password. #admin = password1, admin #user1 = password2, role1, role2 #user2 = password3, role3 #user3 = password4, role2 # 用户名 = 密码, 权限 admin = admin, admin 实际上还需要修改 zeppelin-env.sh 环境变量文件，但是我们的大数据相关环境变量已经写入到 /etc/profile 中了，因此这里不做修改，如有需要可自行修改此文件 /etc/profile 中关于大数据相关的环境变量 export MAVEN_HOME=/usr/local/apache-maven-3.5.2 PATH=$MAVEN_HOME/bin:$PATH export TEZ_HOME=/usr/local/service/tez export TEZ_CONF_DIR=$TEZ_HOME/conf export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:${TEZ_CONF_DIR}:${TEZ_HOME}/*:${TEZ_HOME}/lib/* export JAVA_HOME=/usr/local/jdk/ export HADOOP_HOME=/usr/local/service/hadoop export HIVE_HOME=/usr/local/service/hive export HBASE_HOME=/usr/local/service/hbase export SPARK_HOME=/usr/local/service/spark export STORM_HOME=/usr/local/service/storm export SQOOP_HOME=/usr/local/service/sqoop export KYLIN_HOME=/usr/local/service/kylin export ALLUXIO_HOME=/usr/local/service/alluxio PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$HBASE_HOME/bin:$SPARK_HOME/bin:$STORM_HOME/bin:$SQOOP_HOME/bin:$KYLIN_HOME/bin:$ALLUXIO_HOME/bin:$PATH 2.3 启动 zepplin ⚠️需要用 hadoop 用户的身份启动，否则后续会有权限拒绝问题 # 切换到hadoop用户 su - hadoop # 启动zepplin $ cd /usr/local/zeppelin $ bin/zeppelin-daemon.sh start Please specify HADOOP_CONF_DIR if USE_HADOOP is true Log dir doesn't exist, create /usr/local/zeppelin/logs Pid dir doesn't exist, create /usr/local/zeppelin/run Zeppelin start [ OK ] # 查看进程 $ jps|grep ZeppelinServer 9567 ZeppelinServer 2.4 nginx配置反向代理访问zepplin server { listen 80; server_name zepplin.abc.com; location / { proxy_pass http://127.0.0.1:8080; } } 浏览器访问会发现提示 WebSocket Disconnected ，原因在于nginx的隧道需要把client端的upgrade请求发送给zeppelin，所以upgrade和connection的头信息需要显式设置 解决方法，修改ngxin配置文件 问题参考链接 server { listen 80; server_name zepplin.abc.com; location / { proxy_pass http://127.0.0.1:8080; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; } } 重启nginx后刷新即可 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"生产环境开源项目/cmdb平台/codo/codo安装.html":{"url":"生产环境开源项目/cmdb平台/codo/codo安装.html","title":"codo安装","keywords":"","body":"codo官网 codo github codo官方文档 [toc] 1.codo简介 简介 CODO是一款为用户提供企业多混合云、自动化运维、完全开源的云管理平台。 CODO前端基于Vue iview开发、为用户提供友好的操作界面，增强用户体验。 CODO后端基于Python Tornado开发，其优势为轻量、简洁清晰、异步非阻塞。 CODO开源多云管理平台为用户提供多功能：ITSM、基于RBAC权限系统、Web Terminnal登陆日志审计、录像回放、强大的作业调度系统、CMDB、监控报警系统等 codo_tools:latest 0.0.0.0:8040->80/tcp gateway_image 0.0.0.0:8888->80/tcp kerrigan_image 0.0.0.0:8030->80/tcp codo_cron_image 0.0.0.0:9900->9900/tcp codo_cmdb:latest 0.0.0.0:8050->80/tcp do_mg_image 0.0.0.0:8010->80/tcp codo_dns_image 0.0.0.0:8060->80/tcp codo_task_image 0.0.0.0:8020->80/tcp webterminal/webterminallte 0.0.0.0:8081->80/tcp codo_image 0.0.0.0:444->443/tcp registry.cn-shanghai.aliyuncs.com/ss1917/rabbitmq:3-management 0.0.0.0:15673->15672/tcp registry.cn-shanghai.aliyuncs.com/ss1917/redis:4 0.0.0.0:6380->6379/tcp registry.cn-shanghai.aliyuncs.com/ss1917/mysql:5.7 0.0.0.0:3307->3306/tcp codo相关组件 codo-web 功能：项目前端 端口：80/443(根据实际情况设置) 安装：必须 检测：openresty -t codo-admin 功能：管理后端 端口：8010 安装：必须 检测： curl -I -X GET -m 10 -o /dev/null -s -w %{http_code} http://mg.opendevops.cn:8010/are_you_ok/ codo-cmdb 功能：资产管理 端口：8050 安装：必须 检测： curl -I -X GET -m 10 -o /dev/null -s -w %{http_code} http://cmdb2.opendevops.cn:8050/are_you_ok/ codo-task 功能：任务系统 端口：8020 安装：必须 检测: curl -I -X GET -m 10 -o /dev/null -s -w %{http_code} http://task.opendevops.cn:8020/are_you_ok/ codo-cron 功能：定时任务 端口：9900 安装：必须 备注: 单进程，可使用本机IP 检测: curl -I -X GET -m 10 -o /dev/null -s -w %{http_code} http://cron.opendevops.cn:9900/are_you_ok/ kerrigan 功能：配置中心 端口：8030 安装：必须 检测: curl -I -X GET -m 10 -o /dev/null -s -w %{http_code} http://kerrigan.opendevops.cn:8030/are_you_ok/ codo-tools 功能：运维工具 端口：8040 安装：必须 检测: curl -I -X GET -m 10 -o /dev/null -s -w %{http_code} http://tools.opendevops.cn:8040/are_you_ok/ codo-dns 功能：域名管理 端口：8060 安装：必须 检测: curl -I -X GET -m 10 -o /dev/null -s -w %{http_code} http://dns.opendevops.cn:8060/are_you_ok/ api-gateway 功能：API网关服务 端口：8888 安装：必须 检测：curl -I -X GET -m 10 -o /dev/null -s -w %{http_code} http://gw.opendevops.cn:8888/api/accounts/are_you_ok/ codo架构示意图 Apigateway代理前端文件 ApigateWay依赖DNS服务，需要安装Dnsmasq 微服务部署完成后，需在Apigateway进行注册 一台MySQL Master示例，不同的微服务使用单独的库 2.安装部署 codo部署方式有单机部署和分布式部署，这里采用分布式部署(伪分布式) 2.0 部署步骤 2.0.1 基础环境 优化系统 设置环境变量脚本 防火墙、SELINUX设置 安装docker 安装python3 安装mysql 安装redis 安装RabbitMQ 安装DNS 2.0.2 部署项目前端 codo-web 2.0.3 部署项目后端 codo-admin 2.0.4 部署资产管理 codo-cmdb 2.0.5 部署定时任务 codo-cron 2.0.6 部署任务系统 codo-task 2.0.7 部署运维工具 codo-tools 2.0.8 部署配置中心 kerrigan 2.0.9 部署域名管理 codo-dns 2.0.10 部署API网关api-gateway 2.1 环境准备 2.1.1 建议配置 系统： CentOS7+ CPU： 4Core+ 内存： 8G+ 磁盘： 50G+ 本文机器配置为 操作系统 配置 硬盘 CentOS7.8 4c16g 50g+100g 2.1.2 基础环境 版本约束 Python3.6 Redis3.2 MySQl5.7 RabbitMQ Docker Docker-compose 2.1.3 优化系统 官方提供的优化脚本，请根据实际情况操作 #usage() { # echo \"请按如下格式执行\" # echo \"USAGE: bash $0 函数名1#函数名2\" # echo \"USAGE: bash $0 epel#ulimits#ssh\" # exit 1 #} # function epel(){ yum install epel-release -y >/dev/null 2>&1 sed -i 's/mirrorlist/#mirrorlist/g' /etc/yum.repos.d/epel.repo sed -i 's/#baseurl/baseurl/g' /etc/yum.repos.d/epel.repo sed -i '6s/enabled=0/enabled=1/g' /etc/yum.repos.d/epel.repo sed -i '7s/gpgcheck=1/gpgcheck=0/g' /etc/yum.repos.d/epel.repo yum clean all >/dev/null 2>&1 #阿里云机器用aliyun epel #echo \"[EPEL 配置] ==> OK\" } function ulimits(){ cat > /etc/security/limits.conf /etc/security/limits.d/20-nproc.conf ulimit -n 65536 ulimit -u 65536 #echo \"[ulimits 配置] ==> OK\" } # 系统默认没有 /etc/init.d/sshd 需要使用 systemctl restart sshd function ssh(){ [ -f /etc/ssh/sshd_config ] && sed -ir '13 iUseDNS no\\nGSSAPIAuthentication no' /etc/ssh/sshd_config && systemctl restart sshd >/dev/null 2>&1 #echo \"[SSH 优化] ==> OK\" } # 修改内核参数，增加缓存区，减少等待时间 # 可以接收更大的包，增加对轻量ddos抗性 function kernel(){ cat > /etc/sysctl.conf /dev/null 2>&1 #echo \"[内核 优化] ==> OK\" } # 增加操作系统记录数量 function history(){ if ! grep \"HISTTIMEFORMAT\" /etc/profile >/dev/null 2>&1 then echo ' UserIP=$(who -u am i | cut -d\"(\" -f 2 | sed -e \"s/[()]//g\") export HISTTIMEFORMAT=\"[%F %T] [`whoami`] [${UserIP}] \" ' >> /etc/profile; fi sed -i \"s/HISTSIZE=1000/HISTSIZE=999999999/\" /etc/profile #echo \"[history 优化] ==> OK\" } # 这个稍后我再试一试，我是倾向不要关闭selinux，而是使用系统权限完善来控制软件运行。 # 稍后测试一下看看 function security(){ > /etc/issue sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config sed -i 's/SELINUX=permissive/SELINUX=disabled/g' /etc/selinux/config setenforce 0 >/dev/null 2>&1 #systemctl stop firewalld.service #systemctl disable firewalld.service yum install -y openssl openssh bash >/dev/null 2>&1 #echo \"[安全配置] ==> OK\" } function other(){ yum groupinstall Development tools -y >/dev/null 2>&1 yum install -y vim wget lrzsz telnet traceroute iotop tree >/dev/null 2>&1 yum install -y ncftp axel git zlib-devel openssl-devel unzip xz libxslt-devel libxml2-devel libcurl-devel >/dev/null 2>&1 #echo \"[安装常用工具] ==> OK\" echo \"export HOME=/root\" >> /etc/profile source /etc/profile useradd -M -s /sbin/nologin nginx >/dev/null 2>&1 mkdir -p /root/ops_scripts /data1/www mkdir -p /opt/codo/ } export -f epel export -f ulimits export -f ssh export -f kernel export -f history export -f security export -f other ##格式必须是: bash script 函数名1#函数2 ## 例如: bash system_init_v1.sh epel#ulimits#ssh #echo $1 | awk -F \"#\" '{for(i=1;i 2.1.4 环境变量 创建项目目录 mkdir -p /opt/codo/ && cd /opt/codo/ 以下内容贴入到 /opt/codo/env.sh 文件，⚠️主要修改配置地址和密码信息 cat > /opt/codo/env.sh 使配置文件生效 source /opt/codo/env.sh 2.1.5 防火墙、SELINUX 关闭SELINUX # 临时关闭 $ setenforce 0 # 或修改配置文件关闭,需要重启 $ vim /etc/selinux/config 将SELINUX=enforcing改为SELINUX=disabled 设置后需要重启才能生效 清空防火墙规则 注意，不要关闭防火墙，Docker需要用到NAT # 只清空filter链即可 $ iptables -F 2.1.6 安装python3 建议使用Python36,若你的系统里面已经存在Python36可以跳过此步骤。 # 安装依赖包 yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel libffi-devel gcc gcc-c++ make # 下载python源码包 wget https://www.python.org/ftp/python/3.6.9/Python-3.6.9.tar.xz # 编译安装 tar xf Python-3.6.9.tar.xz && cd Python-3.6.9 ./configure --prefix=/usr/local/python36 --with-ssl make && make install # 导出python命令环境变量 echo 'PATH=/usr/local/python36/bin:$PATH' >/etc/profile.d/python36.sh && source /etc/profile # 配置pip国内源 mkdir ~/.pip cat >~/.pip/pip.conf 2.1.7 安装docker 安装docker # 添加阿里云yum源 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # 安装最新版 yum -y install docker-ce # 安装指定版本 # yum list docker-ce --showduplicates | sort -r # yum -y install docker-ce-18.03.1.ce docker-ce-cli-18.01.1.ce # 启动docker systemctl start docker && systemctl enable docker # 配置docker镜像加速 cat > /etc/docker/daemon.json 安装docker-compose curl -L https://get.daocloud.io/docker/compose/releases/download/1.28.5/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose 2.1.8 安装mysql 一般来说一个MySQL实例即可，如果有需求可以自行搭建主从，微服务每个服务都可以有自己的数据库 我们这里示例是用Docker部署的MySQL，如果你要用已有的数据库请修改/opt/codo/env.sh 创建 docker-compose.yml source /opt/codo/env.sh mkdir -p /opt/codo/codo-mysql && cd /opt/codo/codo-mysql cat > docker-compose.yml 启动 docker-compose up -d 安装MySQL客户端 yum install mysql -y if [ $? == 0 ];then echo -e \"\\033[32m [INFO]: mysql install success. \\033[0m\" echo -e \"\\033[32m [INFO]: 最好提高下MySQL的最大链接数. \\033[0m\" echo -e \"\\033[32m [INFO]: mysql -h127.0.0.1 -uroot -p${MYSQL_PASSWORD} \\033[0m\" else echo -e \"\\033[31m [ERROR]: mysql57 install faild \\033[0m\" exit -3 fi 测试连接 mysql -h127.0.0.1 -uroot -P3307 -p${MYSQL_PASSWORD} 2.1.9 安装redis 创建 docker-compose.yml source /opt/codo/env.sh mkdir -p /opt/codo/codo-redis && cd /opt/codo/codo-redis cat >docker-compose.yml 启动 docker-compose up -d 安装redis客户端 yum install redis -y 测试连接 redis-cli -h 127.0.0.1 -p 6380 -a ${REDIS_PASSWORD} 2.1.10 安装RabbitMQ 创建 docker-compose.yml source /opt/codo/env.sh mkdir -p /opt/codo/codo-mq && cd /opt/codo/codo-mq cat >docker-compose.yml 启动 docker-compose up -d 2.1.11 安装DNS 部署内部DNS dnsmasq 用于服务间内部通信，API网关需要配置，切记 安装dnsmasq echo -e \"\\033[32m [INFO]: Start install dnsmasq \\033[0m\" yum install dnsmasq -y 设置上游DNS # 设置上游DNS，毕竟你的Dns只是个代理 cat >/etc/resolv.dnsmasq 设置host解析 echo -e \"\\033[32m [INFO]: 如果你是单机部署，那么你就将你的本机IP+模块域名解析即可，如果你是分布式部署的，那么每个模块对应的机器IP一定不要搞错，这个很重要，后面网关也要依赖此DNS去解析你的域名，帮你做服务转发的，切记！！！！ \\033[0m\" cat >/etc/dnsmasqhosts 添加配置 echo -e \"\\033[32m [INFO]: 刚装完DNS可以先不用改本机的DNS，有一部分人反应Docker Build时候会报连不上mirrors，装不了依赖。部署到API网关的时候，需要将本机DNS改成自己，不然没办法访问以上mg，cron，cmdb等内网域名 \\033[0m\" # 注意下一步是覆盖你本机的DNS，建议把你的DNS地址加在/etc/resolv.dnsmasq 里面 cp -rp /etc/resolv.conf /etc/resolv.conf-`date +%F` # echo \"nameserver $LOCALHOST_IP\" > /etc/resolv.conf sed \"1i\\nameserver ${LOCALHOST_IP}\" /etc/resolv.conf -i.bak ###注意注意， 这里修改完后，请你一定要确定你nameserver ${LOCALHOST_IP} 内部DNS在第一条、第一条、第一条，放在下面是不能正常解析的. echo \"resolv-file=/etc/resolv.dnsmasq\" >> /etc/dnsmasq.conf echo \"addn-hosts=/etc/dnsmasqhosts\" >> /etc/dnsmasq.conf 启动服务 systemctl start dnsmasq.service && systemctl enable dnsmasq.service systemctl status dnsmasq if [ $? == 0 ];then echo -e \"\\033[32m [INFO]: dnsmasq install success. \\033[0m\" else echo -e \"\\033[31m [ERROR]: dnsmasq install faild \\033[0m\" exit -6 fi 到此，基础依赖部署完成！ 2.2 项目前端 codo-web 更新后的项目前端将不再让用户下载静态资源包，使用自动构建的方式，默认保持最新前端 [ ! -d /opt/codo/codo-web/ ] && mkdir -p /opt/codo/codo-web/ && cd /opt/codo/codo-web/ 2.2.1 修改域名 下列为默认域名，如果要修改访问入口地址请修改 server_name 对应的 demo-init.opendevops.cn ，确保能DNS解析到此域名，或者自己绑定hosts来测试一下 2.2.1.1 创建配置文件 cat >codo_frontend.conf 2.2.1.2 创建Dockerfile cat >Dockerfile 2.2.1.3 创建 docker-compose.yml ⚠️这里的端口就是后续访问codo的端口 cat >docker-compose.yml 2.2.2 构建镜像、启动 docker build . -t codo_image docker-compose up -d 2.2.3 验证服务启动 日志没有报错即为正确 curl 0.0.0.0:81 tailf /var/log/nginx/codo-access.log 到此，项目前端 codo-web 部署完成！ 2.3 项目后端 codo-admin codo-admin是基于tornado框架 restful风格的API 实现后台管理,codo详细参考,搭配使用codo前端(iView+ vue)组成的一套后台用户 权限以及系统管理的解决方案（提供登录，注册 密码修改 鉴权 用户管理 角色管理 权限管理 前端组件管理 前端路由管理 通知服务API 系统基础信息接口） 2.3.1 获取代码 if ! which wget &>/dev/null; then yum install -y wget >/dev/null 2>&1;fi if ! which git &>/dev/null; then yum install -y git >/dev/null 2>&1;fi [ ! -d /opt/codo/ ] && mkdir -p /opt/codo cd /opt/codo && git clone https://github.com/opendevops-cn/codo-admin.git && cd codo-admin 2.3.2 修改相关配置 2.3.2.1 修改 settings.py 配置 注意：这里的 cookie_secret 和 token_secret 必须和你的 env.sh 里面的保持一致，后续网关也要用到这个。若不保持一直登陆后校验不通过回被自动踢回，会导致页面一直不停的刷新 注意：这里的 token_secret 必须要和你的网关保持一致，这个值是从 env.sh 拿来的，一定要做修改，防止网站被攻击，如果secret包含正则符号会导致sed失败，请仔细检查 # 导入环境变量文件，最开始准备的环境变量文件 source /opt/codo/env.sh sed -i.bak \"s#cookie_secret = .*#cookie_secret = '${cookie_secret}'#g\" settings.py sed -i \"s#token_secret = .*#token_secret = '${token_secret}'#g\" settings.py # mysql配置信息 ##我们项目支持取env环境变量，但是还是建议修改下。 DEFAULT_DB_DBNAME='codo_admin' sed -i \"s#DEFAULT_DB_DBHOST = .*#DEFAULT_DB_DBHOST = os.getenv('DEFAULT_DB_DBHOST', '${DEFAULT_DB_DBHOST}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBPORT = .*#DEFAULT_DB_DBPORT = os.getenv('DEFAULT_DB_DBPORT', '${DEFAULT_DB_DBPORT}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBUSER = .*#DEFAULT_DB_DBUSER = os.getenv('DEFAULT_DB_DBUSER', '${DEFAULT_DB_DBUSER}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBPWD = .*#DEFAULT_DB_DBPWD = os.getenv('DEFAULT_DB_DBPWD', '${DEFAULT_DB_DBPWD}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBNAME = .*#DEFAULT_DB_DBNAME = os.getenv('DEFAULT_DB_DBNAME', '${DEFAULT_DB_DBNAME}')#g\" settings.py # 只读MySQL配置，若是单台也直接写成Master地址即可 sed -i \"s#READONLY_DB_DBHOST = .*#READONLY_DB_DBHOST = os.getenv('READONLY_DB_DBHOST', '${READONLY_DB_DBHOST}')#g\" settings.py sed -i \"s#READONLY_DB_DBPORT = .*#READONLY_DB_DBPORT = os.getenv('READONLY_DB_DBPORT', '${READONLY_DB_DBPORT}')#g\" settings.py sed -i \"s#READONLY_DB_DBUSER = .*#READONLY_DB_DBUSER = os.getenv('READONLY_DB_DBUSER', '${READONLY_DB_DBUSER}')#g\" settings.py sed -i \"s#READONLY_DB_DBPWD = .*#READONLY_DB_DBPWD = os.getenv('READONLY_DB_DBPWD', '${READONLY_DB_DBPWD}')#g\" settings.py sed -i \"s#READONLY_DB_DBNAME = .*#READONLY_DB_DBNAME = os.getenv('READONLY_DB_DBNAME', '${DEFAULT_DB_DBNAME}')#g\" settings.py # redis配置 sed -i \"s#DEFAULT_REDIS_HOST = .*#DEFAULT_REDIS_HOST = os.getenv('DEFAULT_REDIS_HOST', '${DEFAULT_REDIS_HOST}')#g\" settings.py sed -i \"s#DEFAULT_REDIS_PORT = .*#DEFAULT_REDIS_PORT = os.getenv('DEFAULT_REDIS_PORT', '${DEFAULT_REDIS_PORT}')#g\" settings.py sed -i \"s#DEFAULT_REDIS_PASSWORD = .*#DEFAULT_REDIS_PASSWORD = os.getenv('DEFAULT_REDIS_PASSWORD', '${DEFAULT_REDIS_PASSWORD}')#g\" settings.py 2.3.2.2 修改Dockerfile 使用自动构建的镜像，默认使用最新版本，这一步的目的是把修改后的配置覆盖进去 cat >Dockerfile 2.3.2.3 构建镜像、启动 docker build . -t do_mg_image docker-compose up -d 2.3.2.4 创建数据库 mysql -h127.0.0.1 -P3307 -uroot -p${MYSQL_PASSWORD} -e 'create database codo_admin default character set utf8mb4 collate utf8mb4_unicode_ci;' 2.3.2.5 初始化表结构 docker exec -ti codo-admin_do_mg_1 /usr/local/bin/python3 /var/www/codo-admin/db_sync.py 2.3.2.6 导入数据 主要是菜单，组件，权限列表，内置的用户等 mysql -h127.0.0.1 -P3307 -uroot -p${MYSQL_PASSWORD} codo_admin 2.3.2.7 重启 docker-compose restart 2.3.2.8 验证服务启动 日志没有报错即为正确 tailf /var/log/supervisor/mg.log 到此，项目后端 codo-admin 部署完成！ 2.4 资产管理 codo-cmdb 2.4.1 获取代码 echo -e \"\\033[32m [INFO]: codo_cmdb(资产管理) Start install. \\033[0m\" if ! which wget &>/dev/null; then yum install -y wget >/dev/null 2>&1;fi if ! which git &>/dev/null; then yum install -y git >/dev/null 2>&1;fi [ ! -d /opt/codo/ ] && mkdir -p /opt/codo cd /opt/codo && git clone https://github.com/opendevops-cn/codo-cmdb.git cd codo-cmdb 2.4.2 修改相关配置 2.4.2.1 修改 settings.py 配置 # 导入环境变量文件，最开始准备的环境变量文件 source /opt/codo/env.sh # 修改配置 #后端数据库名称,建议不要修改，初始化data.sql已经指定了数据库名字，若需改请一块修改 CMDB_DB_DBNAME='codo_cmdb' # 任务系统的域名 sed -i.bak \"s#cookie_secret = .*#cookie_secret = '${cookie_secret}'#g\" settings.py # mysql配置 sed -i \"s#DEFAULT_DB_DBHOST = .*#DEFAULT_DB_DBHOST = os.getenv('DEFAULT_DB_DBHOST', '${DEFAULT_DB_DBHOST}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBPORT = .*#DEFAULT_DB_DBPORT = os.getenv('DEFAULT_DB_DBPORT', '${DEFAULT_DB_DBPORT}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBUSER = .*#DEFAULT_DB_DBUSER = os.getenv('DEFAULT_DB_DBUSER', '${DEFAULT_DB_DBUSER}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBPWD = .*#DEFAULT_DB_DBPWD = os.getenv('DEFAULT_DB_DBPWD', '${DEFAULT_DB_DBPWD}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBNAME = .*#DEFAULT_DB_DBNAME = os.getenv('DEFAULT_DB_DBNAME', '${CMDB_DB_DBNAME}')#g\" settings.py # 只读MySQL配置 sed -i \"s#READONLY_DB_DBHOST = .*#READONLY_DB_DBHOST = os.getenv('READONLY_DB_DBHOST', '${READONLY_DB_DBHOST}')#g\" settings.py sed -i \"s#READONLY_DB_DBPORT = .*#READONLY_DB_DBPORT = os.getenv('READONLY_DB_DBPORT', '${READONLY_DB_DBPORT}')#g\" settings.py sed -i \"s#READONLY_DB_DBUSER = .*#READONLY_DB_DBUSER = os.getenv('READONLY_DB_DBUSER', '${READONLY_DB_DBUSER}')#g\" settings.py sed -i \"s#READONLY_DB_DBPWD = .*#READONLY_DB_DBPWD = os.getenv('READONLY_DB_DBPWD', '${READONLY_DB_DBPWD}')#g\" settings.py sed -i \"s#READONLY_DB_DBNAME = .*#READONLY_DB_DBNAME = os.getenv('READONLY_DB_DBNAME', '${CMDB_DB_DBNAME}')#g\" settings.py # redis配置 sed -i \"s#DEFAULT_REDIS_HOST = .*#DEFAULT_REDIS_HOST = os.getenv('DEFAULT_REDIS_HOST', '${DEFAULT_REDIS_HOST}')#g\" settings.py sed -i \"s#DEFAULT_REDIS_PORT = .*#DEFAULT_REDIS_PORT = os.getenv('DEFAULT_REDIS_PORT', '${DEFAULT_REDIS_PORT}')#g\" settings.py sed -i \"s#DEFAULT_REDIS_PASSWORD = .*#DEFAULT_REDIS_PASSWORD = os.getenv('DEFAULT_REDIS_PASSWORD', '${DEFAULT_REDIS_PASSWORD}')#g\" settings.py # 这里如果配置codo-task的数据库地址，则将数据同步到作业配置 TASK_DB_DBNAME='codo_task' sed -i \"s#CODO_TASK_DB_HOST = .*#CODO_TASK_DB_HOST = os.getenv('CODO_TASK_DB_HOST', '${DEFAULT_DB_DBHOST}')#g\" settings.py sed -i \"s#CODO_TASK_DB_PORT = .*#CODO_TASK_DB_PORT = os.getenv('CODO_TASK_DB_PORT', '${DEFAULT_DB_DBPORT}')#g\" settings.py sed -i \"s#CODO_TASK_DB_USER = .*#CODO_TASK_DB_USER = os.getenv('CODO_TASK_DB_USER', '${DEFAULT_DB_DBUSER}')#g\" settings.py sed -i \"s#CODO_TASK_DB_PWD = .*#CODO_TASK_DB_PWD = os.getenv('CODO_TASK_DB_PWD', '${DEFAULT_DB_DBPWD}')#g\" settings.py sed -i \"s#CODO_TASK_DB_DBNAME = .*#CODO_TASK_DB_DBNAME = os.getenv('CODO_TASK_DB_DBNAME', '${TASK_DB_DBNAME}')#g\" settings.py 2.4.2.2 AWS事件和WebTerminnal配置 首先将webterminal部署上去 docker run -itd -p 8081:80 webterminal/webterminallte 修改settings.py文件 # Aws Events 事件邮件通知人 AWS_EVENT_TO_EMAIL = '1111@qq.com,2222@gmail.com' # Web Terminal 地址，请填写你部署的webterminal地址 # 注意这里是填写你上面docker run的机器外网IP WEB_TERMINAL = 'http://1.1.1.1:8081' 2.4.2.3 修改Dockerfile 使用自动构建的镜像，默认使用最新版本，这一步的目的是把修改后的配置覆盖进去 cat >Dockerfile 2.4.2.4 构建镜像、启动 docker build . -t codo_cmdb docker-compose up -d 2.4.2.5 创建数据库 mysql -h127.0.0.1 -P3307 -uroot -p${MYSQL_PASSWORD} -e 'create database `codo_cmdb` default character set utf8mb4 collate utf8mb4_unicode_ci;' 2.4.2.6 初始化表结构 docker exec -ti codo-cmdb_codo_cmdb_1 /usr/local/bin/python3 /var/www/codo-cmdb/db_sync.py 2.4.2.7 重启 docker-compose restart 2.4.2.8 验证服务启动 服务日志：/var/log/supervisor/cmdb.log #主程序日志 定时日志：/var/log/supervisor/cmdb_cron.log #一些后端守护自动运行的日志 日志没有报错即为正确 tailf /var/log/supervisor/cmdb.log tailf /var/log/supervisor/cmdb_cron.log 到此，资产管理 codo-cmdb 部署完成！ 2.5 定时任务 codo-cron CODO项目定时任务模块，定时任务完全兼容crontab，支持到秒级 备注： Docker部署需要将你的脚本目录单独挂载出来，若不理解的同学参考：codo-cron本地部署方式 2.5.1 获取代码 echo -e \"\\033[32m [INFO]: codo_cron(定时任务) Start install. \\033[0m\" if ! which wget &>/dev/null; then yum install -y wget >/dev/null 2>&1;fi if ! which git &>/dev/null; then yum install -y git >/dev/null 2>&1;fi [ ! -d /opt/codo/ ] && mkdir -p /opt/codo cd /opt/codo && git clone https://github.com/opendevops-cn/codo-cron.git cd codo-cron 2.5.2 修改相关配置 2.5.2.1 修改 settings.py 配置 同样，这里codo-cron也支持取env环境变量，建议还是修改下默认配置 # 导入环境变量文件，最开始准备的环境变量文件 source /opt/codo/env.sh # 后端数据库名称,建议不要修改，初始化data.sql已经指定了数据库名字，若需改请一块修改 CRON_DB_DBNAME='codo_cron' sed -i.bak \"s#cookie_secret = .*#cookie_secret = '${cookie_secret}'#g\" settings.py # mysql配置 sed -i \"s#DEFAULT_DB_DBHOST = .*#DEFAULT_DB_DBHOST = os.getenv('DEFAULT_DB_DBHOST', '${DEFAULT_DB_DBHOST}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBPORT = .*#DEFAULT_DB_DBPORT = os.getenv('DEFAULT_DB_DBPORT', '${DEFAULT_DB_DBPORT}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBUSER = .*#DEFAULT_DB_DBUSER = os.getenv('DEFAULT_DB_DBUSER', '${DEFAULT_DB_DBUSER}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBPWD = .*#DEFAULT_DB_DBPWD = os.getenv('DEFAULT_DB_DBPWD', '${DEFAULT_DB_DBPWD}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBNAME = .*#DEFAULT_DB_DBNAME = os.getenv('DEFAULT_DB_DBNAME', '${CRON_DB_DBNAME}')#g\" settings.py # 只读MySQL配置 sed -i \"s#READONLY_DB_DBHOST = .*#READONLY_DB_DBHOST = os.getenv('READONLY_DB_DBHOST', '${READONLY_DB_DBHOST}')#g\" settings.py sed -i \"s#READONLY_DB_DBPORT = .*#READONLY_DB_DBPORT = os.getenv('READONLY_DB_DBPORT', '${READONLY_DB_DBPORT}')#g\" settings.py sed -i \"s#READONLY_DB_DBUSER = .*#READONLY_DB_DBUSER = os.getenv('READONLY_DB_DBUSER', '${READONLY_DB_DBUSER}')#g\" settings.py sed -i \"s#READONLY_DB_DBPWD = .*#READONLY_DB_DBPWD = os.getenv('READONLY_DB_DBPWD', '${READONLY_DB_DBPWD}')#g\" settings.py sed -i \"s#READONLY_DB_DBNAME = .*#READONLY_DB_DBNAME = os.getenv('READONLY_DB_DBNAME', '${CRON_DB_DBNAME}')#g\" settings.py 2.5.2.2 修改Dockerfile 使用自动构建的镜像，默认使用最新版本，这一步的目的是把修改后的配置覆盖进去 cat >Dockerfile 2.5.2.3 构建镜像、启动 docker build . -t codo_cron_image docker-compose up -d 2.5.2.4 创建数据库 mysql -h127.0.0.1 -P3307 -uroot -p${MYSQL_PASSWORD} -e 'create database `codo_cron` default character set utf8mb4 collate utf8mb4_unicode_ci;' 2.5.2.5 初始化表结构 docker exec -ti codo-cron_codo_cron_1 /usr/local/bin/python3 /var/www/codo-cron/db_sync.py 2.5.2.6 重启 docker-compose restart 2.5.2.7 验证服务启动 日志没有报错即为正确 tailf /var/log/supervisor/cron.log 到此，定时任务系统部署完成！ 2.6 任务系统 codo-task CODO任务系统，负责整个系统中任务调度，此功能是必须要安装的 2.6.1 获取代码 echo -e \"\\033[32m [INFO]: codo-task(任务系统) Start install. \\033[0m\" if ! which wget &>/dev/null; then yum install -y wget >/dev/null 2>&1;fi if ! which git &>/dev/null; then yum install -y git >/dev/null 2>&1;fi [ ! -d /opt/codo/ ] && mkdir -p /opt/codo cd /opt/codo && git clone https://github.com/opendevops-cn/codo-task.git cd codo-task 2.6.2 修改相关配置 2.6.2.1 修改 settings.py 配置 ⚠️ 如果mq默认端口不是 5672 ，需要修改 DEFAULT_MQ_PORT # 导入环境变量文件，最开始准备的环境变量文件 source /opt/codo/env.sh # 修改配置 TASK_DB_DBNAME='codo_task' # 任务系统的域名 sed -i.bak \"s#cookie_secret = .*#cookie_secret = '${cookie_secret}'#g\" settings.py # mysql配置 sed -i \"s#DEFAULT_DB_DBHOST = .*#DEFAULT_DB_DBHOST = os.getenv('DEFAULT_DB_DBHOST', '${DEFAULT_DB_DBHOST}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBPORT = .*#DEFAULT_DB_DBPORT = os.getenv('DEFAULT_DB_DBPORT', '${DEFAULT_DB_DBPORT}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBUSER = .*#DEFAULT_DB_DBUSER = os.getenv('DEFAULT_DB_DBUSER', '${DEFAULT_DB_DBUSER}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBPWD = .*#DEFAULT_DB_DBPWD = os.getenv('DEFAULT_DB_DBPWD', '${DEFAULT_DB_DBPWD}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBNAME = .*#DEFAULT_DB_DBNAME = os.getenv('DEFAULT_DB_DBNAME', '${TASK_DB_DBNAME}')#g\" settings.py # 只读MySQL配置 sed -i \"s#READONLY_DB_DBHOST = .*#READONLY_DB_DBHOST = os.getenv('READONLY_DB_DBHOST', '${READONLY_DB_DBHOST}')#g\" settings.py sed -i \"s#READONLY_DB_DBPORT = .*#READONLY_DB_DBPORT = os.getenv('READONLY_DB_DBPORT', '${READONLY_DB_DBPORT}')#g\" settings.py sed -i \"s#READONLY_DB_DBUSER = .*#READONLY_DB_DBUSER = os.getenv('READONLY_DB_DBUSER', '${READONLY_DB_DBUSER}')#g\" settings.py sed -i \"s#READONLY_DB_DBPWD = .*#READONLY_DB_DBPWD = os.getenv('READONLY_DB_DBPWD', '${READONLY_DB_DBPWD}')#g\" settings.py sed -i \"s#READONLY_DB_DBNAME = .*#READONLY_DB_DBNAME = os.getenv('READONLY_DB_DBNAME', '${TASK_DB_DBNAME}')#g\" settings.py # redis配置 sed -i \"s#DEFAULT_REDIS_HOST = .*#DEFAULT_REDIS_HOST = os.getenv('DEFAULT_REDIS_HOST', '${DEFAULT_REDIS_HOST}')#g\" settings.py sed -i \"s#DEFAULT_REDIS_PORT = .*#DEFAULT_REDIS_PORT = os.getenv('DEFAULT_REDIS_PORT', '${DEFAULT_REDIS_PORT}')#g\" settings.py sed -i \"s#DEFAULT_REDIS_PASSWORD = .*#DEFAULT_REDIS_PASSWORD = os.getenv('DEFAULT_REDIS_PASSWORD', '${DEFAULT_REDIS_PASSWORD}')#g\" settings.py # MQ配置 sed -i \"s#DEFAULT_MQ_ADDR = .*#DEFAULT_MQ_ADDR = os.getenv('DEFAULT_MQ_ADDR', '${DEFAULT_MQ_ADDR}')#g\" settings.py sed -i \"s#DEFAULT_MQ_USER = .*#DEFAULT_MQ_USER = os.getenv('DEFAULT_MQ_USER', '${DEFAULT_MQ_USER}')#g\" settings.py sed -i \"s#DEFAULT_MQ_PWD = .*#DEFAULT_MQ_PWD = os.getenv('DEFAULT_MQ_PWD', '${DEFAULT_MQ_PWD}')#g\" settings.py 2.6.2.2 修改Dockerfile 使用自动构建的镜像，默认使用最新版本，这一步的目的是把修改后的配置覆盖进去 cat >Dockerfile 2.6.2.3 构建镜像、启动 docker build . -t codo_task_image docker-compose up -d 2.6.2.4 创建数据库 mysql -h127.0.0.1 -P3307 -uroot -p${MYSQL_PASSWORD} -e 'create database `codo_task` default character set utf8mb4 collate utf8mb4_unicode_ci;' 2.6.2.5 初始化表结构 docker exec -ti codo-task_codo_task_1 /usr/local/bin/python3 /var/www/codo-task/db_sync.py 2.6.2.6 重启 docker-compose restart 2.6.2.7 验证服务启动 日志没有报错即为正确 tailf /var/log/supervisor/task_scheduler.log tailf /var/log/supervisor/exec_task.log 到此，任务系统 codo-task 部署完成！ 2.7 运维工具 codo-tools CODO运维工具支持：告警管理、项目管理、事件管理、加密解密、随机密码、提醒管理等 2.7.1 获取代码 if ! which wget &>/dev/null; then yum install -y wget >/dev/null 2>&1;fi if ! which git &>/dev/null; then yum install -y git >/dev/null 2>&1;fi [ ! -d /opt/codo/ ] && mkdir -p /opt/codo cd /opt/codo && git clone https://github.com/opendevops-cn/codo-tools.git && cd codo-tools 2.7.2 修改相关配置 2.7.2.1 修改settings.py 配置 # 导入环境变量文件，最开始准备的环境变量文件 source /opt/codo/env.sh sed -i.bak \"s#cookie_secret = .*#cookie_secret = '${cookie_secret}'#g\" settings.py # mysql配置信息 ##我们项目支持取env环境变量，但是还是建议修改下。 DEFAULT_DB_DBNAME='codo_tools' sed -i \"s#DEFAULT_DB_DBHOST = .*#DEFAULT_DB_DBHOST = os.getenv('DEFAULT_DB_DBHOST', '${DEFAULT_DB_DBHOST}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBPORT = .*#DEFAULT_DB_DBPORT = os.getenv('DEFAULT_DB_DBPORT', '${DEFAULT_DB_DBPORT}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBUSER = .*#DEFAULT_DB_DBUSER = os.getenv('DEFAULT_DB_DBUSER', '${DEFAULT_DB_DBUSER}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBPWD = .*#DEFAULT_DB_DBPWD = os.getenv('DEFAULT_DB_DBPWD', '${DEFAULT_DB_DBPWD}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBNAME = .*#DEFAULT_DB_DBNAME = os.getenv('DEFAULT_DB_DBNAME', '${DEFAULT_DB_DBNAME}')#g\" settings.py # redis配置 sed -i \"s#DEFAULT_REDIS_HOST = .*#DEFAULT_REDIS_HOST = os.getenv('DEFAULT_REDIS_HOST', '${DEFAULT_REDIS_HOST}')#g\" settings.py sed -i \"s#DEFAULT_REDIS_PORT = .*#DEFAULT_REDIS_PORT = os.getenv('DEFAULT_REDIS_PORT', '${DEFAULT_REDIS_PORT}')#g\" settings.py sed -i \"s#DEFAULT_REDIS_PASSWORD = .*#DEFAULT_REDIS_PASSWORD = os.getenv('DEFAULT_REDIS_PASSWORD', '${DEFAULT_REDIS_PASSWORD}')#g\" settings.py 2.7.2.2 修改Dockerfile 使用自动构建的镜像，默认使用最新版本，这一步的目的是把修改后的配置覆盖进去 cat >Dockerfile 2.7.2.3 构建镜像、启动 docker build . -t codo_tools docker-compose up -d 2.7.2.4 创建数据库 mysql -h127.0.0.1 -P3307 -uroot -p${MYSQL_PASSWORD} -e 'create database `codo_tools` default character set utf8mb4 collate utf8mb4_unicode_ci;' 2.7.2.5 初始化表结构 docker exec -ti codo-tools_codo_tools_1 /usr/local/bin/python3 /var/www/codo-tools/db_sync.py 2.7.2.6 重启 docker-compose restart 2.7.2.7 测试 服务日志 /var/log/supervisor/tools.log 定时提醒日志 /var/log/supervisor/cron_jobs.log 日志没有报错即为正确 tailf /var/log/supervisor/tools.log tailf /var/log/supervisor/cron_jobs.log 到此，运维工具系统 codo-tools 部署完成！ 2.7 配置中心 kerrigan 2.7.1 获取代码 cd /opt/codo && git clone https://github.com/opendevops-cn/kerrigan.git && cd kerrigan 2.7.2 修改相关配置 2.7.2.1 修改 settings.py 配置 # 导入环境变量文件，最开始准备的环境变量文件 source /opt/codo/env.sh # 修改管理后端域名 sed -i.bak \"s#cookie_secret = .*#cookie_secret = '${cookie_secret}'#g\" settings.py # mysql配置信息 ##我们项目支持取env环境变量，但是还是建议修改下。 DEFAULT_DB_DBNAME='codo_kerrigan' sed -i \"s#DEFAULT_DB_DBHOST = .*#DEFAULT_DB_DBHOST = os.getenv('DEFAULT_DB_DBHOST', '${DEFAULT_DB_DBHOST}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBPORT = .*#DEFAULT_DB_DBPORT = os.getenv('DEFAULT_DB_DBPORT', '${DEFAULT_DB_DBPORT}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBUSER = .*#DEFAULT_DB_DBUSER = os.getenv('DEFAULT_DB_DBUSER', '${DEFAULT_DB_DBUSER}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBPWD = .*#DEFAULT_DB_DBPWD = os.getenv('DEFAULT_DB_DBPWD', '${DEFAULT_DB_DBPWD}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBNAME = .*#DEFAULT_DB_DBNAME = os.getenv('DEFAULT_DB_DBNAME', '${DEFAULT_DB_DBNAME}')#g\" settings.py # 只读MySQL配置，若是单台也直接写成Master地址即可 sed -i \"s#READONLY_DB_DBHOST = .*#READONLY_DB_DBHOST = os.getenv('READONLY_DB_DBHOST', '${READONLY_DB_DBHOST}')#g\" settings.py sed -i \"s#READONLY_DB_DBPORT = .*#READONLY_DB_DBPORT = os.getenv('READONLY_DB_DBPORT', '${READONLY_DB_DBPORT}')#g\" settings.py sed -i \"s#READONLY_DB_DBUSER = .*#READONLY_DB_DBUSER = os.getenv('READONLY_DB_DBUSER', '${READONLY_DB_DBUSER}')#g\" settings.py sed -i \"s#READONLY_DB_DBPWD = .*#READONLY_DB_DBPWD = os.getenv('READONLY_DB_DBPWD', '${READONLY_DB_DBPWD}')#g\" settings.py sed -i \"s#READONLY_DB_DBNAME = .*#READONLY_DB_DBNAME = os.getenv('READONLY_DB_DBNAME', '${DEFAULT_DB_DBNAME}')#g\" settings.py 2.7.2.2 修改Dockerfile 使用自动构建的镜像，默认使用最新版本，这一步的目的是把修改后的配置覆盖进去 cat >Dockerfile 2.7.2.3 构建镜像、启动 docker build . -t kerrigan_image docker-compose up -d 2.7.2.4 创建数据库 mysql -h127.0.0.1 -P3307 -uroot -p${MYSQL_PASSWORD} -e 'create database `codo_kerrigan` default character set utf8mb4 collate utf8mb4_unicode_ci;' 2.7.2.5 初始化表结构 docker exec -ti kerrigan_codo-kerrigan_1 /usr/local/bin/python3 /var/www/kerrigan/db_sync.py 2.7.2.6 重启 docker-compose restart 2.7.2.7 验证服务启动 日志没有报错即为正确 tailf /var/log/supervisor/kerrigan.log 到此，配置中心 kerrigan 部署完成！ 2.8 域名管理 codo-dns CODO域名管理模块，管理BIND 支持智能解析，多域名，多主。 2.8.1 获取代码 cd /opt/codo && git clone https://github.com/opendevops-cn/codo-dns.git cd codo-dns 2.8.2 修改相关配置 2.8.2.1 修改 settings.py 配置 # 导入环境变量文件，最开始准备的环境变量文件 source /opt/codo/env.sh # 后端数据库名称 CRON_DB_DBNAME='codo_dns' sed -i.bak \"s#cookie_secret = .*#cookie_secret = '${cookie_secret}'#g\" settings.py # mysql配置 sed -i \"s#DEFAULT_DB_DBHOST = .*#DEFAULT_DB_DBHOST = os.getenv('DEFAULT_DB_DBHOST', '${DEFAULT_DB_DBHOST}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBPORT = .*#DEFAULT_DB_DBPORT = os.getenv('DEFAULT_DB_DBPORT', '${DEFAULT_DB_DBPORT}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBUSER = .*#DEFAULT_DB_DBUSER = os.getenv('DEFAULT_DB_DBUSER', '${DEFAULT_DB_DBUSER}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBPWD = .*#DEFAULT_DB_DBPWD = os.getenv('DEFAULT_DB_DBPWD', '${DEFAULT_DB_DBPWD}')#g\" settings.py sed -i \"s#DEFAULT_DB_DBNAME = .*#DEFAULT_DB_DBNAME = os.getenv('DEFAULT_DB_DBNAME', '${CRON_DB_DBNAME}')#g\" settings.py # 只读MySQL配置 sed -i \"s#READONLY_DB_DBHOST = .*#READONLY_DB_DBHOST = os.getenv('READONLY_DB_DBHOST', '${READONLY_DB_DBHOST}')#g\" settings.py sed -i \"s#READONLY_DB_DBPORT = .*#READONLY_DB_DBPORT = os.getenv('READONLY_DB_DBPORT', '${READONLY_DB_DBPORT}')#g\" settings.py sed -i \"s#READONLY_DB_DBUSER = .*#READONLY_DB_DBUSER = os.getenv('READONLY_DB_DBUSER', '${READONLY_DB_DBUSER}')#g\" settings.py sed -i \"s#READONLY_DB_DBPWD = .*#READONLY_DB_DBPWD = os.getenv('READONLY_DB_DBPWD', '${READONLY_DB_DBPWD}')#g\" settings.py sed -i \"s#READONLY_DB_DBNAME = .*#READONLY_DB_DBNAME = os.getenv('READONLY_DB_DBNAME', '${CRON_DB_DBNAME}')#g\" settings.py 2.8.2.2 修改Dockerfile 使用自动构建的镜像，默认使用最新版本，这一步的目的是把修改后的配置覆盖进去 cat >Dockerfile 2.8.2.3 构建镜像、启动 docker build . -t codo_dns_image docker-compose up -d 2.8.2.4 创建数据库 mysql -h127.0.0.1 -P3307 -uroot -p${MYSQL_PASSWORD} -e 'create database `codo_dns` default character set utf8mb4 collate utf8mb4_unicode_ci;' 2.8.2.5 初始化表结构 docker exec -ti codo-dns_codo-dns_1 /usr/local/bin/python3 /var/www/codo-dns/db_sync.py 2.8.2.6 重启 docker-compose restart 2.8.2.7 验证服务启动 日志没有报错即为正确 tailf /var/log/supervisor/codo_dns.log 到此，域名管理 codo-dns 部署完成！ 2.9 API网关 api-gateway（部署容易出问题的地方) ⚠️⚠️⚠️重点部分，请仔细阅读 由于此项目是模块化、微服务化，因此需要在借助API网关，需要在API网关注册，此步骤是必须的。 注意事项 开始之前，你需要确认以下2个事情 DNS服务是否正常，域名能否正常解析 微服务的模块部署是否正常，进行检测 检查DNS思路 1. 确保你的dnsmasql服务是启动的，服务没有报错 2. 确保/etc/dnsmasqhosts文件有解析的IP 3. 确保你网关的这台机器/etc/resolv.conf DNS执行你刚部署的dnsmasq服务IP 4. 确保你网关所在的机器都能正常ping通所有的服务，比如：ping cmdb2.opendevops.cn 5. 确保你的防火墙规则是清空的`iptables -F` 6. 确保你的SELINUX是关闭的`setenforce 0` 服务健康检测 # 进行所有服务进行检测，返回200则正常 curl -I -X GET -m 10 -o /dev/null -s -w %{http_code} http://mg.opendevops.cn:8010/are_you_ok/ && echo curl -I -X GET -m 10 -o /dev/null -s -w %{http_code} http://task.opendevops.cn:8020/are_you_ok/ && echo curl -I -X GET -m 10 -o /dev/null -s -w %{http_code} http://cmdb2.opendevops.cn:8050/are_you_ok/ && echo curl -I -X GET -m 10 -o /dev/null -s -w %{http_code} http://kerrigan.opendevops.cn:8030/are_you_ok/ && echo curl -I -X GET -m 10 -o /dev/null -s -w %{http_code} http://cron.opendevops.cn:9900/are_you_ok/ && echo curl -I -X GET -m 10 -o /dev/null -s -w %{http_code} http://tools.opendevops.cn:8040/are_you_ok/ && echo curl -I -X GET -m 10 -o /dev/null -s -w %{http_code} http://dns.opendevops.cn:8060/are_you_ok/ && echo curl -I -X GET -m 10 -o /dev/null -s -w %{http_code} http://0.0.0.0:80 && echo 2.9.1 获取代码 cd /opt/codo/ && git clone https://github.com/ss1917/api-gateway.git && cd /opt/codo/api-gateway 2.9.2 修改相关配置 主要修改nginx.conf配置信息和config.lua配置，具体参考API网关块：API网关修改配置 2.9.2.1 全局nginx配置 这里主要修改resolver 内部DNS服务器地址 conf/nginx.conf ==一定要修改== user root; worker_processes auto; worker_rlimit_nofile 51200; error_log logs/error.log; events { use epoll; worker_connections 51024; } http { # 设置默认lua搜索路径 lua_package_path '$prefix/lua/?.lua;/blah/?.lua;;'; lua_code_cache on; # 线上环境设置为on, off时可以热加载lua文件 lua_shared_dict user_info 1m; lua_shared_dict my_limit_conn_store 100m; # 100M可以放1.6M个键值对 include mime.types; # 代理静态文件 client_header_buffer_size 64k; large_client_header_buffers 4 64k; init_by_lua_file lua/init_by_lua.lua; # nginx启动时就会执行 include ./conf.d/*.conf; # lua生成upstream resolver 10.10.10.12; # 内部DNS服务器地址 一定要修改 对应起来 } 2.9.2.2 网关配置 修改 conf/conf.d/gw.conf server { listen 80; server_name gw.opendevops.cn; lua_need_request_body on; # 开启获取body数据记录日志 location / { ### ws 支持 proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; ### proxy_redirect off; proxy_read_timeout 600; ### 获取真实IP proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; access_by_lua_file lua/access_check.lua; set $my_upstream $my_upstream; proxy_pass http://$my_upstream; ### 跨域 add_header Access-Control-Allow-Methods *; add_header Access-Control-Max-Age 3600; add_header Access-Control-Allow-Credentials true; add_header Access-Control-Allow-Origin $http_origin; add_header Access-Control-Allow-Headers $http_access_control_request_headers; if ($request_method = OPTIONS) { return 204; } } location ~ .*\\.(sh|bash|py|sql)$ { return 403; } location ~* ^/(image.*|admin.*|manage.*)/ { return 403; } } 2.9.2.3 注册API网关 请仔细阅读下面需要修改配置的地方 lua/configs.lua ==这个配置基本上都要修改，请务必仔细== json = require(\"cjson\") -- redis配置，一定要修改，并且和codo-admin保持一致，admin会把权限写进去提供网关使用 redis_config = { host = '10.10.10.12', port = 6379, auth_pwd = 'cWCVKJ7ZHUK12mVbivUf', db = 8, alive_time = 3600 * 24 * 7, channel = 'gw' } -- 注意：这里的token_secret必须要和codo-admin里面的token_secret保持一致 token_secret = \"pXFb4i%*834gfdh96(3df&%18iodGq4ODQyMzc4lz7yI6ImF1dG\" logs_file = '/var/log/gw.log' --刷新权限到redis接口 rewrite_cache_url = 'http://mg.opendevops.cn:8010/v2/accounts/verify/' -- 注意：rewrite_cache_token要和codo-admin里面的secret_key = '8b888a62-3edb-4920-b446-697a472b4001'保持一致 rewrite_cache_token = '8b888a62-3edb-4920-b446-697a472b4001' --并发限流配置 limit_conf = { rate = 10, --限制ip每分钟只能调用n*60次接口 burst = 10, --桶容量,用于平滑处理,最大接收请求次数 } --upstream匹配规则,API网关域名 gw_domain_name = 'gw.opendevops.cn' --下面的转发一定要修改，根据自己实际数据修改 rewrite_conf = { [gw_domain_name] = { rewrite_urls = { { uri = \"/dns\", rewrite_upstream = \"dns.opendevops.cn:8060\" }, { uri = \"/cmdb2\", rewrite_upstream = \"cmdb2.opendevops.cn:8050\" }, { uri = \"/tools\", rewrite_upstream = \"tools.opendevops.cn:8040\" }, { uri = \"/kerrigan\", rewrite_upstream = \"kerrigan.opendevops.cn:8030\" }, { uri = \"/cmdb\", rewrite_upstream = \"cmdb.opendevops.cn:8002\" }, { uri = \"/k8s\", rewrite_upstream = \"k8s.opendevops.cn:8001\" }, { uri = \"/task\", rewrite_upstream = \"task.opendevops.cn:8020\" }, { uri = \"/cron\", rewrite_upstream = \"cron.opendevops.cn:9900\" }, { uri = \"/mg\", rewrite_upstream = \"mg.opendevops.cn:8010\" }, { uri = \"/accounts\", rewrite_upstream = \"mg.opendevops.cn:8010\" }, } } } 2.9.2.4 修改Dockerfile 使用自动构建的镜像，默认使用最新版本，这一步的目的是把修改后的配置覆盖进去 cat >Dockerfile 2.9.2.5 构建镜像、启动 docker build . -t gateway_image docker-compose up -d 2.9.2.6 测试 状态码返回200即为正确 curl -I -X GET -m 10 -o /dev/null -s -w %{http_code} http://gw.opendevops.cn:8888/api/accounts/are_you_ok/ && echo 提醒:openresty服务器DNS必须指向--->最起初部署的DNS服务器地址,另外若你本机ping以上随便一个域名都不通的话，那你要确认下你本机DNS指向你最初部署了DNS服务器了？ 修改vim /etc/resolv.conf 到此，API网关 部署完成！ 3.0 访问codo 这里访问的地址就是在2.2.1.3步骤中 compose 文件中对外暴露的端口，这里设置为81端口，然后利用nginx做反向代理 用户名 admin 密码 admin@opendevops 登陆后页面 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"生产环境开源项目/配置管理/nacos/nacos安装.html":{"url":"生产环境开源项目/配置管理/nacos/nacos安装.html","title":"nacos安装","keywords":"","body":"nacos安装 nacos官网 nacos github地址 nacos三种部署模式官方说明文档 nacos集群部署官方文档 1.单机模式 1.1 下载编译后压缩包 # 下载包 wget https://github.com/alibaba/nacos/releases/download/1.4.2/nacos-server-1.4.2.tar.gz # 解压缩至/usr/local tar xf nacos-server-1.4.2.tar.gz -C /usr/local 1.2 启动服务 sh bin/startup.sh -m standalone 2.集群部署 集群部署架构图 开源的时候推荐用户把所有服务列表放到一个vip下面，然后挂到一个域名下面 http://ip1:port/openAPI 直连ip模式，机器挂则需要修改ip才可以使用。 http://SLB:port/openAPI 挂载SLB模式(内网SLB，不可暴露到公网，以免带来安全风险)，直连SLB即可，下面挂server真实ip，可读性不好。 http://nacos.com:port/openAPI 域名 + SLB模式(内网SLB，不可暴露到公网，以免带来安全风险)，可读性好，而且换ip方便，推荐模式 2.0 实验环境 主机名 IP 配置 系统 内核 nacos01 10.0.0.33 2c4g CentOS7.9 3.10.0-1160.el7.x86_64 nacos02 10.0.0.34 2c4g CentOS7.9 3.10.0-1160.el7.x86_64 nacos03 10.0.0.35 2c4g CentOS7.9 3.10.0-1160.el7.x86_64 2.1 环境准备 请确保是在环境中安装使用: 64 bit OS Linux/Unix/Mac，推荐使用Linux系统。 64 bit JDK 1.8+；下载.配置。 Maven 3.2.x+；下载.配置。 3个或3个以上Nacos节点才能构成集群。 2.1.1 安装jdk1.8+ jdk官网下载地址 jdk历史版本下载地址 下载jdk1.8二进制包并解压缩至 /usr/local # 安装jdk1.8.0_211 cat >/etc/profile.d/jdk8.sh 2.1.2 安装maven3.2.x+ maven官网下载地址 maven3.x版本官方下载地址 maven历史版本官方下载地址 ⚠️只有采用从github下载源码包自行编译的方式才需要安装maven ⚠️maven版本需要3.2.5以上，否则后续有坑 下载maven3.2.5 wget https://archive.apache.org/dist/maven/maven-3/3.2.5/binaries/apache-maven-3.2.5-bin.tar.gz 解压缩至 /usr/local 并修改名称 tar xf apache-maven-3.2.5-bin.tar.gz -C /usr/local/ && mv /usr/local/apache-maven-3.2.5/ /usr/local/maven-3.2.5 配置maven仓库为阿里云仓库，编辑 conf/settings.xml 文件，找到 mirrors 标签并加入以下内容 alimaven aliyun maven http://maven.aliyun.com/nexus/content/groups/public/ central 导出环境变量 echo 'export PATH=$PATH:/usr/local/maven-3.2.5/bin' > /etc/profile.d/maven.sh & source /etc/profile 验证 $ mvn -v Apache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-15T01:29:23+08:00) Maven home: /usr/local/maven-3.2.5 Java version: 1.8.0_162, vendor: Oracle Corporation Java home: /usr/local/jdk1.8.0_162/jre Default locale: en_US, platform encoding: UTF-8 OS name: \"linux\", version: \"3.10.0-1160.el7.x86_64\", arch: \"amd64\", family: \"unix\" 2.2 下载安装包 2.2.1 从github上下载源码方式 # 下载包 wget https://github.com/alibaba/nacos/archive/refs/tags/1.4.2.tar.gz # 解压缩至/usr/local tar xf nacos-1.4.2.tar.gz -C /usr/local # 进入目录编译安装 cd /usr/local/nacos-1.4.2 mvn -Prelease-nacos clean install -U 编译从未成功过，各种报错，直接放弃！ 2.2.2 下载编译后压缩包 # 下载包 wget https://github.com/alibaba/nacos/releases/download/1.4.2/nacos-server-1.4.2.tar.gz # 解压缩至/usr/local tar xf nacos-server-1.4.2.tar.gz -C /usr/local 2.3 配置集群配置文件 在nacos的解压目录 conf 下，编辑配置文件 cluster.conf ，请每行配置成 ip:port （请配置3个或3个以上节点） ⚠️nacos集群模式必须指定IP，不支持域名，否则会报错！ cat > cluster.conf 2.4 确定数据源 2.4.1 使用内置数据源 使用内置数据源无需进行任何配置 2.4.2 使用外置数据源 这里提前安装好了mysql5.6，官方要求数据库版本 5.6.5+ 导入sql，在nacos解压目录下有 conf/nacos-mysql.sql # 创建数据库 mysql -uroot -e \"CREATE DATABASE nacos_config DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci\" # 创建用户并授权 mysql -uroot -e \"grant all on nacos_config.* to 'nacos'@'%' identified by 'nacos'\" mysql -uroot -e \"flush privileges\" # 导入sql mysql -uroot -D nacos_config 2.4.3 配置 application.properties 需要修改的项 # nacos监听的端口 server.port=8848 # 如果使用外置数据源mysql，需要打开以下注释 spring.datasource.platform=mysql # mysql的相关配置，官方文档中的db.url.0一行有坑，必须写以下格式的 db.num=1 db.url.0=jdbc:mysql://10.0.0.33:3306/nacos_config?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true db.user.0=nacos db.password.0=nacos 2.5 启动服务 集群模式 使用内置数据源 sh bin/startup.sh -p embedded 使用外置数据源 sh bin/startup.sh 2.6 访问管理后台 浏览器访问 IP:端口/nacos 默认用户名密码都是 nacos 登陆后首页面 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"生产环境开源项目/代码质量检测平台/SonarQube/SonarQube.html":{"url":"生产环境开源项目/代码质量检测平台/SonarQube/SonarQube.html","title":"SonarQube","keywords":"","body":"SonarQube sonarqube github地址 sonarqube 官网 sonarqube 7.9(LTS版) 官方文档 sonarqube dockerhub地址 1.标准安装 2.docker安装 3.dokcer-compose安装 官方安装文档 3.1 设置系统参数 由于 SonarQube 使用嵌入式 Elasticsearch，请确保您的 Docker 主机配置符合Elasticsearch 生产模式要求和文件描述符配置。 需要执行以下命令 sysctl -w vm.max_map_count=524288 sysctl -w fs.file-max=131072 ulimit -n 131072 ulimit -u 8192 如果系统的文件描述符配置不符合要求就会有如下报错 docker logs -f sonarqube_sonarqube_1 报错如下 ERROR: [1] bootstrap checks failed. You must address the points described in the following [1] lines before starting Elasticsearch. bootstrap check failure [1] of [1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] ERROR: Elasticsearch did not exit normally - check the logs at /opt/sonarqube/logs/sonarqube.log 根据提示查看 /opt/sonarqube/logs/sonarqube.log 日志 2021.09.06 08:31:11 INFO app[][o.s.a.AppFileSystem] Cleaning or creating temp directory /opt/sonarqube/temp 2021.09.06 08:31:11 INFO app[][o.s.a.es.EsSettings] Elasticsearch listening on [HTTP: 127.0.0.1:9001, TCP: 127.0.0.1:37275] 2021.09.06 08:31:11 INFO app[][o.s.a.ProcessLauncherImpl] Launch process[[key='es', ipcIndex=1, logFilenamePrefix=es]] from [/opt/sonarqube/elasticsearch]: /opt/sonarqube/elasticsearch/bin/elasticsearch 2021.09.06 08:31:11 INFO app[][o.s.a.SchedulerImpl] Waiting for Elasticsearch to be up and running 2021.09.06 08:31:16 WARN app[][o.s.a.p.AbstractManagedProcess] Process exited with exit value [es]: 78 2021.09.06 08:31:16 INFO app[][o.s.a.SchedulerImpl] Process[es] is stopped 2021.09.06 08:31:16 INFO app[][o.s.a.SchedulerImpl] SonarQube is stopped 3.2 编辑 docker-compose yml文件 这里选择安装7.9LTS版 cat > docker-compose.yml 3.3 启动 docker-compose up -d 会启动 sonarqube_sonarqube_1 和 sonarqube_db_1 两个容器 $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 67e5a65c824c sonarqube:7.9.6-community \"./bin/run.sh\" 19 hours ago Up 52 minutes 0.0.0.0:9000->9000/tcp sonarqube_sonarqube_1 65a4c46204e5 postgres:12 \"docker-entrypoint.s…\" 19 hours ago Up 52 minutes 5432/tcp sonarqube_db_1 启动成功后访问 IP:9000 SonarQube汉化 SonarQube汉化插件github地址 SonarQube gitlab插件github地址 Administration -> Marketplace -> Plugins 下搜索 chinese ，选择 Chinese Pack LOCALIZATION 安装，安装显示 install Pending后重启服务器即可 汉化效果 SonarQube插件安装 SonarQube插件如果在应用市场中没有，则需要单独下载插件(jar包)放置于 $SONARQUBE_HOME/extensions/plugins 目录下，然后重启服务即可 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"生产环境开源项目/nexus/nexus安装.html":{"url":"生产环境开源项目/nexus/nexus安装.html","title":"安装","keywords":"","body":"nexus安装 nexus官网 nexus3官方文档 nexus3最新版官方下载地址 nexus3历史版本下载地址 nexus3安装系统要求 nexus3 docker hub 1.系统环境 系统 配置 硬盘 java版本 CentOS Linux release 7.6.1810 (Core) 8c16g 100g java version \"1.8.0_261\" 2.安装 2.1 二进制包安装 2.1.1 下载二进制包 wget https://download.sonatype.com/nexus/3/nexus-3.37.3-02-unix.tar.gz 2.1.2 解压缩包 # 解压缩后是 nexus-3.37.3-02 sonatype-work 2个目录 tar xf nexus-3.37.3-02-unix.tar.gz 2.1.3 配置nexus etc/nexus-default.properties # 配置nexus监听端口与地址，默认为8081和0.0.0.0 bin/nexus.rc # 配置nexus运行用户 2.1.4 启动nexus ⚠️运行nexus的用户必须能登陆系统，不能已系统用户运行nexus，否则会报错如下 $ ./nexus run This account is currently not available. 以普通用户运行nexus 方式一 前台运行 ./nexus run 方式二 后台运行 ./nexus start 2.1.5 登陆nexus admin 用户初始密码在 sonatype-work/nexus3/admin.password! 点击 next ，接下里就是设置 admin 用户密码以及是否允许匿名用户登陆 使用systemd管理nexus 编辑配置文件 /etc/systemd/system/nexus.service export NEXUS_BIN_PATH=\"/opt/nexus-3.37.3-02/bin/nexus\" cat > /etc/systemd/system/nexus.service 启动nexus systemctl daemon-reload systemctl enable nexus.service && systemctl start nexus.service 使用supervisor管理nexus export NEXUS_BIN_PATH=\"/opt/nexus-3.37.3-02/bin/nexus\" cat > /etc/supervisor/config.d/nexus.ini 2.2 docker安装 创建目录 export NEXUS_DIR_PATH=\"/some/dir/nexus-data\" [ -d ${NEXUS_DIR_PATH} ] || mkdir ${NEXUS_DIR_PATH} && chown -R 200 ${NEXUS_DIR_PATH} 启动容器 docker run -d \\ -p 8081:8081 \\ --name nexus \\ -h nexus \\ --restart=always \\ -v ${NEXUS_DIR_PATH}:/nexus-data sonatype/nexus3 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"生产环境开源项目/nexus/nexus重启报错.html":{"url":"生产环境开源项目/nexus/nexus重启报错.html","title":"nexus重启报错","keywords":"","body":"nexus重启报错 1.背景说明 公司生产nexus是二进制包方式运行的，并且生产mvn私服和npm私服都是基于nexus的，由于需要做nexus整体迁移，所以在迁移前压缩了一下nexus数据目录 sonatype-work ，但是因为磁盘空间不足压缩失败了(整体数据大小为近1T)，然后就删除了压缩的tar包恢复了部分磁盘空间，但是后续一个前端项目打包的时候出现了问题，即执行 yarn install 的时候卡住不动了，实际上这个时候nexus虽然进程还在但是实际上已经不能正确提供响应了 所以手动执行了 kill -9 命令杀掉了nexus进程(但是实际上不应该使用kill命令，应该使用 nexus stop 命令) 结果就是在启动nexus的时候就无法启动了 2.报错信息 查看 sonatype-work/nexus3/log/nexus.log 看到报错日志如下 看不懂。。。百度了半天 2022-01-06 22:46:51,273+0800 WARN [FelixStartLevel] *SYSTEM Sisu - Problem removing: org.eclipse.sisu.inject.LazyBeanEntry@1acb1a03 from: org.sonatype.nexus.extender.NexusLifecycleManager@4e6de86f via: org.sonatype.nexus.extender.NexusLifecycleManager$BundleContextMediator@6805c95 com.google.inject.ProvisionException: Unable to provision, see the following errors: 1) null returned by binding at org.eclipse.sisu.wire.LocatorWiring but the 5th parameter of com.sonatype.nexus.licensing.ext.internal.NexusLicenseManager.(NexusLicenseManager.java:57) is not @Nullable while locating org.sonatype.nexus.common.app.ApplicationDirectories for the 5th parameter of com.sonatype.nexus.licensing.ext.internal.NexusLicenseManager.(NexusLicenseManager.java:57) at / (via modules: org.sonatype.nexus.extender.modules.NexusBundleModule -> org.eclipse.sisu.space.SpaceModule) while locating com.sonatype.nexus.licensing.ext.internal.NexusLicenseManager while locating java.lang.Object annotated with * at org.eclipse.sisu.wire.LocatorWiring while locating com.sonatype.nexus.licensing.ext.LicenseManager for the 1st parameter of com.sonatype.nexus.licensing.ext.internal.NexusLicenseInstaller.(NexusLicenseInstaller.java:51) at / (via modules: org.sonatype.nexus.extender.modules.NexusBundleModule -> org.eclipse.sisu.space.SpaceModule) while locating com.sonatype.nexus.licensing.ext.internal.NexusLicenseInstaller while locating java.lang.Object annotated with * 2) null returned by binding at org.eclipse.sisu.wire.LocatorWiring but the 2nd parameter of com.sonatype.nexus.licensing.ext.internal.NexusLicenseInstaller.(NexusLicenseInstaller.java:51) is not @Nullable while locating org.sonatype.nexus.common.app.ApplicationLicense for the 2nd parameter of com.sonatype.nexus.licensing.ext.internal.NexusLicenseInstaller.(NexusLicenseInstaller.java:51) at / (via modules: org.sonatype.nexus.extender.modules.NexusBundleModule -> org.eclipse.sisu.space.SpaceModule) while locating com.sonatype.nexus.licensing.ext.internal.NexusLicenseInstaller while locating java.lang.Object annotated with * 2 errors at com.google.inject.internal.InjectorImpl$2.get(InjectorImpl.java:1028) at org.eclipse.sisu.inject.LazyBeanEntry.getValue(LazyBeanEntry.java:81) at org.sonatype.nexus.extender.NexusLifecycleManager.reindex(NexusLifecycleManager.java:172) at org.sonatype.nexus.extender.NexusLifecycleManager.sync(NexusLifecycleManager.java:153) at org.sonatype.nexus.extender.NexusLifecycleManager$BundleContextMediator.remove(NexusLifecycleManager.java:265) at org.sonatype.nexus.extender.NexusLifecycleManager$BundleContextMediator.remove(NexusLifecycleManager.java:1) at org.eclipse.sisu.inject.WatchedBeans.remove(WatchedBeans.java:101) at org.eclipse.sisu.inject.InjectorBindings.unsubscribe(InjectorBindings.java:96) at org.eclipse.sisu.inject.DefaultBeanLocator.remove(DefaultBeanLocator.java:127) at org.eclipse.sisu.launch.SisuTracker.removePublisher(SisuTracker.java:246) at org.eclipse.sisu.launch.SisuTracker.purgeBundles(SisuTracker.java:163) at org.sonatype.nexus.extender.NexusBundleTracker.close(NexusBundleTracker.java:88) at org.eclipse.sisu.launch.SisuExtender.stop(SisuExtender.java:56) at org.sonatype.nexus.extender.NexusBundleExtender.doStop(NexusBundleExtender.java:76) at org.sonatype.nexus.extender.NexusContextListener.contextDestroyed(NexusContextListener.java:266) at org.sonatype.nexus.extender.NexusBundleExtender.stop(NexusBundleExtender.java:56) at org.apache.felix.framework.util.SecureAction.stopActivator(SecureAction.java:719) at org.apache.felix.framework.Felix.stopBundle(Felix.java:2636) at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1391) at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:308) at java.lang.Thread.run(Thread.java:748) 2022-01-06 22:46:51,276+0800 INFO [FelixStartLevel] *SYSTEM org.sonatype.nexus.extender.NexusContextListener - Uptime: 7 seconds and 295 milliseconds (nexus-oss-edition/3.20.1.01) 2022-01-06 22:46:51,276+0800 INFO [FelixStartLevel] *SYSTEM org.sonatype.nexus.extender.NexusLifecycleManager - Shutting down 2022-01-06 22:46:51,276+0800 INFO [FelixStartLevel] *SYSTEM org.sonatype.nexus.extender.NexusLifecycleManager - Stop KERNEL 2022-01-06 22:46:51,282+0800 INFO [FelixStartLevel] *SYSTEM org.sonatype.nexus.bootstrap.jetty.JettyServer - Stopping 2022-01-06 22:46:51,282+0800 INFO [FelixStartLevel] *SYSTEM org.sonatype.nexus.bootstrap.jetty.JettyServer - Stopping: Server@69dfb304{STARTING}[9.4.18.v20190429] 2022-01-06 22:46:51,282+0800 ERROR [jetty-main-1] *SYSTEM org.sonatype.nexus.bootstrap.osgi.BootstrapListener - Failed to initialize java.lang.InterruptedException: null at java.lang.Object.wait(Native Method) at org.osgi.util.tracker.ServiceTracker.waitForService(ServiceTracker.java:502) at org.sonatype.nexus.bootstrap.osgi.BootstrapListener.contextInitialized(BootstrapListener.java:136) at org.eclipse.jetty.server.handler.ContextHandler.callContextInitialized(ContextHandler.java:957) at org.eclipse.jetty.servlet.ServletContextHandler.callContextInitialized(ServletContextHandler.java:553) at org.eclipse.jetty.server.handler.ContextHandler.startContext(ContextHandler.java:922) at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:365) at org.eclipse.jetty.webapp.WebAppContext.startWebapp(WebAppContext.java:1497) at org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1459) at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:852) at org.eclipse.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:278) at org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:545) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:167) at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:110) at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:113) at com.codahale.metrics.jetty9.InstrumentedHandler.doStart(InstrumentedHandler.java:101) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:167) at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:119) at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:113) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:167) at org.eclipse.jetty.server.Server.start(Server.java:418) at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:110) at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:113) at org.eclipse.jetty.server.Server.doStart(Server.java:382) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) at org.sonatype.nexus.bootstrap.jetty.JettyServer$JettyMainThread.run(JettyServer.java:274) 3.解决方法 删除 sonatype-work/nexus3/db/component 和 sonatype-work/nexus3/db/config 这2个目录下大小为0的以wal结尾的文件重新启动就可以了，具体原因未知 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"生产环境开源项目/kooder/kooder.html":{"url":"生产环境开源项目/kooder/kooder.html","title":"安装使用","keywords":"","body":"kooder kooder gitee地址 kooder github地址 kooder api官方文档 1.kooder是什么 Kooder 是一个开源的代码搜索工具，目标是为包括 Gitee/GitLab/Gitea 在内的代码托管系统提供自动的源码、仓库和 Issue 的搜索服务。 2.kooder架构 Kooder 服务包含两个模块，分别是 gateway 和 indexer（默认配置下 indexer 被集成到 gateway 中）。 其中 gateway 用来接受来自 HTTP 的索引任务， 对任务进行检查后存放到队列中； 同时 gateway 还接受搜索的请求，并返回搜索结果给客户端。而 indexer 进程负责监控队列中的索引任务， 并将这些要新增、删除和修改索引的任务更新到索引库中。 模块说明 core 核心对象和公共类 gateway 用来接收来自 HTTP 的索引和搜索的请求 indexer 构建、更新和删除索引的服务 数据流图 3.安装 官方支持 源码安装 、docker 安装，这里选择docker安装 3.1 安装docker和docker-compose 安装docker # 阿里云yum源 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum -y install docker-ce systemctl start docker && systemctl enable docker # 配置阿里云镜像加速地址 cat > /etc/docker/daemon.json 安装docker-compose docker-compose github 安装太慢 可以通过国内源加速安装 export COMPOSE_VERSION=2.6.1 curl -L https://get.daocloud.io/docker/compose/releases/download/v${COMPOSE_VERSION}/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose 3.2 安装kooder 克隆仓库 git clone https://gitee.com/koode/kooder.git 安装 ⚠️docker-compose不指定配置文件默认是 docker-compose.yaml，如果指定可以通过 -f 选项，但是不能写成 docker-compose up -d -f docker-compose.yaml ，否则会报错 unknown shorthand flag: 'f' in -f docker-compose -f docker-compose.yaml up -d 4.浏览器访问 浏览器访问 ip:8080，初始界面如下 5.配置kooder 配置文件就是克隆下的仓库中的 kooder.properties 文件默认内容如下 # Gitee Search # Gitee Search Gateway configurations http.url = http://192.168.1.25:8080 http.bind = http.port = 8080 http.log = on http.webroot = gateway/src/main/webapp http.startup.tasks = indexer,gitlab file.index.path = d://file.txt file.index.vender = # gitlab setting gitlab.url = http://192.168.1.25:10080/ gitlab.personal_access_token = Z66e7sxoH18twrkyYzoG gitlab.secret_token = gsearch gitlab.connect_timeout = 2000 gitlab.read_timeout = 10000 # gitee setting gitee.url = http://giteehost/ gitee.personal_access_token = bb319595dc98bb8fbdcf3fc442c25893 # Git git.username = admin@test.com git.password = bb319595dc98bb8fbdcf3fc442c25893 # git.ssh.key = ./data/ssh_key # git.ssh.keypass = # queue.provider = embed queue.redis.host = 127.0.0.1 queue.redis.port = 6379 queue.redis.database = 1 queue.redis.key = gsearch-queue # queue.embed.url = http://127.0.0.1:8080/queue/fetch queue.embed.path = ./data/queue queue.embed.batch_size = 10000 # storage.type = disk storage.disk.path = ./data/lucene storage.disk.use_compound_file = false storage.disk.max_buffered_docs = -1 storage.disk.ram_buffer_size_mb = 16 # storage.repositories.path = ./data/repositories storage.repositories.max_size_in_gigabyte = 200 indexer.no_task_interval = 1000 indexer.batch_fetch_count = 10 indexer.tasks_per_thread = 2 现在我们要对接gitlab，因此需要修改如下内容 # gitlab setting gitlab.url = http://10.0.0.100 gitlab.personal_access_token = Z66e7sxoH18twrkyYzoG gitlab.secret_token = gsearch gitlab.connect_timeout = 2000 gitlab.read_timeout = 10000 # 配置root或者管理员权限的用户名和密码 # Git git.username = root git.password = bb319595dc98bb8fbdcf3fc442c25893 gitlab.personal_access_token 中的token在gitlab中新建一个token即可，给到的权限为 api 、read_user 、read_api 、read_repository 配置完成后重启 docker-compose -f docker-compose.yaml restart 6.搜索效果 仓库搜索效果 代码搜索效果 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/日志/日志查询.html":{"url":"linux/日志/日志查询.html","title":"日志查询","keywords":"","body":"日志查询 一、线上的一次查询 nginx 单条日志如下 120.55.28.65 - 172.18.1.33:8000 - 115.61.104.224 - 100.116.167.56 - - [04/Jun/2019:16:31:27 +0800] \"GET /realmachine/recordNew.htm?uniqueId= HTTP/1.1\" 200 [0.041] [0.041] 36 \"https://www.testin.cn/realmachine/recorddetail.htm?taskid=5ba0a70b899d1b1eece6a6ae&from=remote&uniqueId=\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\" 查询过程 ⚠️sed查询的时候，指定的开始时间和结束时间必须存在，精确到秒，否则查询结果为空 1.截取固定时间点日志 sed -n '/04\\/Jun\\/2019:16:15:00/,/04\\/Jun\\/2019:16:16:00/p' /data/nginx/logs/www.testin.cn_access.log >/root/test.txt 2.截取固定时间点有504的报错日志 awk '{if($15==504) print $0}' test.txt #统计状态码 上述日志中第15列是状态码 awk '{a[$15]++}END{for(i in a) print a[i],i}' test.txt |sort -nr 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/日志/Linux 日志切割神器 Logrotate 原理和配置详解.html":{"url":"linux/日志/Linux 日志切割神器 Logrotate 原理和配置详解.html","title":"日志切割 logrotate","keywords":"","body":"[toc] Linux 日志切割神器 Logrotate 原理和配置详解 本文严重抄袭至互联网并稍作修改 写在开头 对于 Linux 系统安全来说，日志文件是极其重要的工具。不知为何，我发现很多运维同学的服务器上都运行着一些诸如每天切分 Nginx日志之类的 CRON 脚本，大家似乎遗忘了 Logrotate，争相发明自己的轮子，这真是让人沮丧啊！就好比明明身边躺着现成的性感美女，大家却忙着自娱自乐，罪过！ logrotate 程序是一个日志文件管理工具。用于分割日志文件，删除旧的日志文件，并创建新的日志文件，起到“转储”作用。可以节省磁盘空间。下面就对 logrotate 日志轮转操作做一梳理记录。 原文未说明示例是哪个linux发行版，这里以CentOS7.6为例 一、logrotate介绍 1.1 logrotate配置文件 Linux系统默认安装 logrotate 工具，它默认的配置文件在： /etc/logrotate.conf /etc/logrotate.d/ logrotate.conf 才是主要的配置文件，logrotate.d 是一个目录，该目录里的所有文件都会被主动的读入 /etc/logrotate.conf 中执行。 另外，如果 /etc/logrotate.d/ 里面的文件中没有设定一些细节，则会以 /etc/logrotate.conf 这个文件的设定来作为默认值。 Logrotate是基于CRON来运行的，其脚本是 /etc/cron.daily/logrotate，日志轮转是系统自动完成的。实际运行时，Logrotate会调用配置文件 /etc/logrotate.conf。可以在 /etc/logrotate.d 目录里放置自定义好的配置文件，用来覆盖Logrotate的缺省值。 /etc/cron.daily/logrotate 文件内容如下 $ cat /etc/cron.daily/logrotate #!/bin/sh /usr/sbin/logrotate -s /var/lib/logrotate/logrotate.status /etc/logrotate.conf EXITVALUE=$? if [ $EXITVALUE != 0 ]; then /usr/bin/logger -t logrotate \"ALERT exited abnormally with [$EXITVALUE]\" fi exit 0 如果等不及cron自动执行日志轮转，想手动强制切割日志，需要加-f参数；不过正式执行前最好通过Debug选项来验证一下（-d参数），这对调试也很重要： /usr/sbin/logrotate -f /etc/logrotate.d/nginx /usr/sbin/logrotate -d -f /etc/logrotate.d/nginx 1.2 logrotate 命令格式 logrotate [OPTION...] -d， --debug ：debug模式，测试配置文件是否有错误。 -f， --force ：强制转储文件。 -m， --mail=command ：压缩日志后，发送日志到指定邮箱。 -s， --state=statefile ：使用指定的状态文件。 -v， --verbose ：显示转储过程。 1.3 logrotate手动操作示例 根据日志切割设置进行操作，并显示详细信息： /usr/sbin/logrotate -v /etc/logrotate.conf /usr/sbin/logrotate -v /etc/logrotate.d/php 根据日志切割设置进行执行，并显示详细信息，但是不进行具体操作，debug模式 /usr/sbin/logrotate -d /etc/logrotate.conf /usr/sbin/logrotate -d /etc/logrotate.d/nginx 可在 /var/lib/logrotate/logrotate.status 查看各log文件的具体执行情况 二、切割介绍 2.1 切割说明 比如以系统日志 /var/log/message 做切割来简单说明下： 第一次执行完rotate(轮转)之后，原本的messages会变成messages.1，而且会制造一个空的messages给系统来储存日志； 第二次执行之后，messages.1会变成messages.2，而messages会变成messages.1，又造成一个空的messages来储存日志！ 如果仅设定保留三个日志（即轮转3次）的话，那么执行第三次时，则 messages.3这个档案就会被删除，并由后面的较新的保存日志所取代！也就是会保存最新的几个日志。 日志究竟轮换几次，这个是根据配置文件中的 rotate 参数来判定的。 看下 logrotate.conf 配置： # see \"man logrotate\" for details # rotate log files weekly(默认每一周执行一次rotate轮转工作) weekly # keep 4 weeks worth of backlogs(保留多少个日志文件(轮转几次).默认保留四个.就是指定日志文件删除之前轮转的次数，0 指没有备份) rotate 4 # create new (empty) log files after rotating old ones(自动创建新的日志文件，新的日志文件具有和原来的文件相同的权限；因为日志被改名，因此要创建一个新的来继续存储之前的日志) create # use date as a suffix of the rotated file(这个参数很重要！就是切割后的日志文件以当前日期为格式结尾，如xxx.log-20131216这样，如果注释掉，切割出来是按数字递增，即前面说的 xxx.log-1这种格式) dateext # uncomment this if you want your log files compressed(是否通过gzip压缩转储以后的日志文件，如xxx.log-20131216.gz ；如果不需要压缩，注释掉就行) #compress # RPM packages drop log rotation information into this directory(将 /etc/logrotate.d/ 目录中的所有文件都加载进来) include /etc/logrotate.d # no packages own wtmp and btmp -- we'll rotate them here() /var/log/wtmp { # 仅针对 /var/log/wtmp 所设定的参数 monthly # 每月一次切割，取代默认的一周 create 0664 root utmp # 文件大小超过 1M 后才会切割 minsize 1M # 文件大小超过 1M 后才会切割 rotate 1 # 只保留一个日志 } # 这个 wtmp 可记录用户登录系统及系统重启的时间，因为有 minsize 的参数，因此不见得每个月一定会执行一次喔.要看文件大小。 /var/log/btmp { missingok monthly create 0600 root utmp rotate 1 } # system-specific logs may be also be configured here. 由这个文件的设定可以知道 /etc/logrotate.d 其实就是由 /etc/logrotate.conf 所规划出来的目录，虽然可以将所有的配置都写入 /etc/logrotate.conf ，但是这样一来这个文件就实在是太复杂了，尤其是当使用很多的服务在系统上面时， 每个服务都要去修改 /etc/logrotate.conf 的设定也似乎不太合理了。 所以，如果独立出来一个目录，那么每个要切割日志的服务， 就可以独自成为一个文件，并且放置到 /etc/logrotate.d/ 当中。 其他重要参数说明： 参数 含义 compress 通过gzip 压缩转储以后的日志 nocompress 不做gzip压缩处理 copytruncate 用于还在打开中的日志文件，把当前日志备份并截断；是先拷贝再清空的方式，拷贝和清空之间有一个时间差，可能会丢失部分日志数据。 nocopytruncate 备份日志文件不过不截断 create mode owner group 轮转时指定创建新文件的属性，如create 0777 nobody nobody nocreate 不建立新的日志文件 delaycompress 和compress 一起使用时，转储的日志文件到下一次转储时才压缩 nodelaycompress 覆盖 delaycompress 选项，转储同时压缩。 missingok 如果日志丢失，不报错继续滚动下一个日志 errors address 专储时的错误信息发送到指定的 Email 地址 ifempty 即使日志文件为空文件也做轮转，这个是logrotate的缺省选项。 notifempty 当日志文件为空时，不进行轮转 mail address 把转储的日志文件发送到指定的 E-mail 地址 nomail 转储时不发送日志文件 olddir directory 转储后的日志文件放入指定的目录，必须和当前日志文件在同一个文件系统 noolddir 转储后的日志文件和当前日志文件放在同一个目录下 sharedscripts 运行postrotate脚本，作用是在所有日志都轮转后统一执行一次脚本。如果没有配置这个，那么每个日志轮转后都会执行一次脚本 prerotate 在logrotate转储之前需要执行的指令，例如修改文件的属性等动作；必须独立成行 postrotate 在logrotate转储之后需要执行的指令，例如重新启动 (kill -HUP) 某个服务！必须独立成行 daily 指定转储周期为每天 weekly 指定转储周期为每周 monthly 指定转储周期为每月 rotate count 指定日志文件删除之前转储的次数，0 指没有备份，5 指保留5 个备份 dateext 使用当期日期作为命名格式 dateformat .%s 配合dateext使用，紧跟在下一行出现，定义文件切割后的文件名，必须配合dateext使用，只支持 %Y %m %d %s 这四个参数 size(或minsize) log-size 当日志文件到达指定的大小时才转储，log-size能指定bytes(缺省)及KB (sizek)或MB(sizem).当日志文件 >= log-size 的时候就转储。 以下为合法格式：（其他格式的单位大小写没有试过）size = 5 或 size 5 （>= 5 个字节就转储）size = 100k 或 size 100ksize = 100M 或 size 100M 2.2 切割示例 2.2.1 小示例：下面一个切割nginx日志的配置 $ cat /etc/logrotate.d/nginx /usr/local/nginx/logs/*.log { daily rotate 7 missingok notifempty dateext sharedscripts postrotate if [ -f /usr/local/nginx/logs/nginx.pid ]; then kill -USR1 `cat /usr/local/nginx/logs/nginx.pid` fi endscript } 2.2.2 分享一例曾经使用过的nginx日志切割处理脚本 1）logrotate日志分割配置 $ cat /etc/logrotate.d/nginx /data/nginx_logs/*.access_log { nocompress daily copytruncate create ifempty olddir /data/nginx_logs/days rotate 0 } 2）日志分割脚本 $ cat /usr/local/sbin/logrotate-nginx.sh #!/bin/bash # 创建转储日志压缩存放目录 mkdir -p /data/nginx_logs/days # 手工对nginx日志进行切割转换 /usr/sbin/logrotate -vf /etc/logrotate.d/nginx # 当前时间 time=$(date -d \"yesterday\" +\"%Y-%m-%d\") # 进入转储日志存放目录 cd /data/nginx_logs/days # 对目录中的转储日志文件的文件名进行统一转换 for i in $(ls ./ | grep \"^\\(.*\\)\\.[[:digit:]]$\") do mv ${i} ./$(echo ${i}|sed -n 's/^\\(.*\\)\\.\\([[:digit:]]\\)$/\\1/p')-$(echo $time) done # 对转储的日志文件进行压缩存放，并删除原有转储的日志文件，只保存压缩后的日志文件。以节约存储空间 for i in $(ls ./ | grep \"^\\(.*\\)\\-\\([[:digit:]-]\\+\\)$\") do tar jcvf ${i}.bz2 ./${i} rm -rf ./${i} done # 只保留最近7天的压缩转储日志文件 find /data/nginx_logs/days/* -name \"*.bz2\" -mtime 7 -type f -exec rm -rf {} \\; 3）crontab定时执行 # logrotate 0 0 * * * /bin/bash -x /usr/local/sbin/logrotate-nginx.sh > /dev/null 2> 手动执行脚本，测试下看看： $ /bin/bash -x /usr/local/sbin/logrotate-nginx.sh $ cd /data/nginx_logs/days $ ls huantest.access_log-2017-01-18.bz2 2.2.3 php脚本切割一例 $ cat /etc/logrotate.d/php /Data/logs/php/*log { daily rotate 365 missingok notifempty compress dateext sharedscripts postrotate if [ -f /Data/app/php5.6.26/var/run/php-fpm.pid ]; then kill -USR1 `cat /Data/app/php5.6.26/var/run/php-fpm.pid` fi endscript postrotate /bin/chmod 644 /Data/logs/php/*gz endscript } $ ll /Data/app/php5.6.26/var/run/php-fpm.pid -rw-r--r-- 1 root root 4 Dec 28 17:03 /Data/app/php5.6.26/var/run/php-fpm.pid $ cd /Data/logs/php $ ll total 25676 -rw-r--r-- 1 root root 0 Jun 1 2016 error.log -rw-r--r-- 1 nobody nobody 182 Aug 30 2015 error.log-20150830.gz -rw-r--r-- 1 nobody nobody 371 Sep 1 2015 error.log-20150901.gz -rw-r--r-- 1 nobody nobody 315 Sep 7 2015 error.log-20150907.gz ......... ......... 2.2.4 nginx日志切割一例 $ cat /etc/logrotate.d/nginx /Data/logs/nginx/*log { daily rotate 365 missingok notifempty compress dateext sharedscripts postrotate /etc/init.d/nginx reload endscript } $ ll /Data/logs/nginx/www.huanqiu.com/ .......... -rw-r--r-- 1 root root 1652 Jan 1 00:00 error.log-20170101.gz -rw-r--r-- 1 root root 1289 Jan 2 00:00 error.log-20170102.gz -rw-r--r-- 1 root root 1633 Jan 3 00:00 error.log-20170103.gz -rw-r--r-- 1 root root 3239 Jan 4 00:00 error.log-20170104.gz 2.2.5 系统日志切割一例 $ cat /etc/logrotate.d/syslog /var/log/cron /var/log/maillog /var/log/messages /var/log/secure /var/log/spooler { sharedscripts postrotate /bin/kill -HUP `cat /var/run/syslogd.pid 2> /dev/null` 2> /dev/null || true endscript } $ ll /var/log/messages* -rw------- 1 root root 34248975 Jan 19 18:42 /var/log/messages -rw------- 1 root root 51772994 Dec 25 03:11 /var/log/messages-20161225 -rw------- 1 root root 51800210 Jan 1 03:05 /var/log/messages-20170101 -rw------- 1 root root 51981366 Jan 8 03:36 /var/log/messages-20170108 -rw------- 1 root root 51843025 Jan 15 03:40 /var/log/messages-20170115 $ ll /var/log/cron* -rw------- 1 root root 2155681 Jan 19 18:43 /var/log/cron -rw------- 1 root root 2932618 Dec 25 03:11 /var/log/cron-20161225 -rw------- 1 root root 2939305 Jan 1 03:06 /var/log/cron-20170101 -rw------- 1 root root 2951820 Jan 8 03:37 /var/log/cron-20170108 -rw------- 1 root root 3203992 Jan 15 03:41 /var/log/cron-20170115 $ ll /var/log/secure* -rw------- 1 root root 275343 Jan 19 18:36 /var/log/secure -rw------- 1 root root 2111936 Dec 25 03:06 /var/log/secure-20161225 -rw------- 1 root root 2772744 Jan 1 02:57 /var/log/secure-20170101 -rw------- 1 root root 1115543 Jan 8 03:26 /var/log/secure-20170108 -rw------- 1 root root 731599 Jan 15 03:40 /var/log/secure-20170115 $ ll /var/log/spooler* -rw------- 1 root root 0 Jan 15 03:41 /var/log/spooler -rw------- 1 root root 0 Dec 18 03:21 /var/log/spooler-20161225 -rw------- 1 root root 0 Dec 25 03:11 /var/log/spooler-20170101 -rw------- 1 root root 0 Jan 1 03:06 /var/log/spooler-20170108 -rw------- 1 root root 0 Jan 8 03:37 /var/log/spooler-20170115 2.2.6 tomcat日志切割一例 $ cat /etc/logrotate.d/tomcat /Data/app/tomcat-7-huanqiu/logs/catalina.out { rotate 14 daily copytruncate compress notifempty missingok } $ ll /Data/app/tomcat-7-huanqiu/logs/catalina.* -rw-r--r--. 1 root root 0 Jan 19 19:11 /Data/app/tomcat-7-huanqiu/logs/catalina.out -rw-r--r--. 1 root root 95668 Jan 19 19:11 /Data/app/tomcat-7-huanqiu/logs/catalina.out.1.gz 2.2.7 早期用过的nginx日志处理一例 $ cat /letv/sh/cut_nginx_log.sh #!/bin/bash # 你的日志文件存放目录 logs_path=\"/letv/logs/\" # 日志文件的名字，多个需要空格隔开 logs_names=(error access pv_access) dates=`date -d \"yesterday\" +\"%Y%m%d\"` mkdir -p ${logs_path}$dates/ num=${#logs_names[@]} for((i=0;i /dev/null 2>$1 三、logrotate默认自动切割生效时间 3.1 logrotate切割时间说明 logrotate是基于CRON来运行的，其脚本是 /etc/cron.daily/logrotate，实际运行时，logrotate会调用配置文件 /etc/logrotate.conf。 $ cat /etc/cron.daily/logrotate #!/bin/sh /usr/sbin/logrotate -s /var/lib/logrotate/logrotate.status /etc/logrotate.conf EXITVALUE=$? if [ $EXITVALUE != 0 ]; then /usr/bin/logger -t logrotate \"ALERT exited abnormally with [$EXITVALUE]\" fi exit 0 logrotate是基于CRON运行的，所以这个时间是由CRON控制的，具体可以查询CRON的配置文件 /etc/anacrontab（老版本的文件是 /etc/crontab） $ cat /etc/anacrontab # /etc/anacrontab: configuration file for anacron # See anacron(8) and anacrontab(5) for details. SHELL=/bin/sh PATH=/sbin:/bin:/usr/sbin:/usr/bin MAILTO=root # the maximal random delay added to the base delay of the jobs(随机的延迟时间，表示最大45分钟) RANDOM_DELAY=45 # the jobs will be started during the following hours only(脚本运行的时间 凌晨3点至晚10点) START_HOURS_RANGE=3-22 #period in days delay in minutes job-identifier command 1 5 cron.daily nice run-parts /etc/cron.daily 7 25 cron.weekly nice run-parts /etc/cron.weekly @monthly 45 cron.monthly nice run-parts /etc/cron.monthly ######### 字段说明 ######### period in days # 每天切割的时间点 delay in minutes # 延迟的分钟数 job-identifier # 标识符 command # 执行的命令 3.2 修改logrotate切割时间 通过默认 /etc/anacrontab 文件配置，会发现logrotate自动切割日志文件的默认时间是凌晨3点多。现在需要将切割时间调整到每天的晚上12点，即每天切割的日志是前一天的0-24点之间的内容。需要计划任务中指定执行切割的时间，并且加上 -f 选项强制切割。 # 1.备份原有文件 cp /etc/anacrontab{，.bak} # 2.重新编辑文件 $ cat /etc/logrotate.d/nstc_nohup.out /data/nstc/nohup.out { rotate 30 dateext daily copytruncate compress notifempty missingok } $ cat /etc/logrotate.d/syslog /var/log/cron /var/log/maillog /var/log/messages /var/log/secure /var/log/history { sharedscripts compress rotate 30 daily dateext postrotate /bin/kill -HUP `cat /var/run/syslogd.pid 2> /dev/null` 2> /dev/null || true endscript } # 3.结合crontab进行自定义的定时轮转操作 $ crontab -l # log logrotate 59 23 * * * /usr/sbin/logrotate -f /etc/logrotate.d/syslog >/dev/null 2>&1 59 23 * * * /usr/sbin/logrotate -f /etc/logrotate.d/nstc_nohup.out >/dev/null 2>&1 # 4.验证 $ ll /data/nstc/nohup.out* -rw------- 1 app app 33218 1月 25 09:43 /data/nstc/nohup.out -rw------- 1 app app 67678 1月 25 23:59 /data/nstc/nohup.out-20180125.gz 3.3 脚本实现日志切割 说明 除了利用自带的logrotate工具实现日志切割之外，还可以编写python脚本或shell脚本以实现日志切割。下面就简单列出几个实例说明下： 3.3.1 Python脚本实现日志切割 示例1：对jumpserver日志进行切割 # 1.编辑脚本内容 $ cat log_rotate.py #!/usr/bin/env python import datetime，os，sys，shutil log_path = '/opt/jumpserver/logs/' log_file = 'jumpserver.log' yesterday = (datetime.datetime.now() - datetime.timedelta(days = 1)) try: os.makedirs(log_path + yesterday.strftime('%Y') + os.sep + \\ yesterday.strftime('%m')) except OSError，e: print print e sys.exit() shutil.move(log_path + log_file，log_path \\ + yesterday.strftime('%Y') + os.sep \\ + yesterday.strftime('%m') + os.sep \\ + log_file + '_' + yesterday.strftime('%Y%m%d') + '.log') os.popen(\"sudo /opt/jumpserver/service.sh restart\") # 2.手动执行这个脚本： $ python log_rotate.py # 3.查看日志切割后的效果： $ ls /opt/jumpserver/logs/ 2017 jumpserver.log $ ls /opt/jumpserver/logs/2017/ 09 $ ls /opt/jumpserver/logs/2017/09/ jumpserver.log_20170916.log # 4.然后做每日的定时切割任务： $ crontab -l 30 1 * * * /usr/bin/python /mnt/log_rotate.py > /dev/null 2>&1 示例2：对nginx日志进行切割 $ cat log_rotate.py #!/usr/bin/env python import datetime，os，sys，shutil log_path = '/app/nginx/logs/' log_file = 'www_access.log' yesterday = (datetime.datetime.now() - datetime.timedelta(days = 1)) try: os.makedirs(log_path + yesterday.strftime('%Y') + os.sep + \\ yesterday.strftime('%m')) except OSError，e: print print e sys.exit() shutil.move(log_path + log_file，log_path \\ + yesterday.strftime('%Y') + os.sep \\ + yesterday.strftime('%m') + os.sep \\ + log_file + '_' + yesterday.strftime('%Y%m%d') + '.log') os.popen(\"sudo kill -USR1 `cat /app/nginx/logs/nginx.pid`\") 3.3.2 shell脚本实现日志切割 $ cat /app/script/log_rotate.sh #!/bin/sh function rotate() { logs_path=$1 echo Rotating Log: $1 cp ${logs_path} ${logs_path}.$(date -d \"yesterday\" +\"%Y%m%d\") > ${logs_path} rm -f ${logs_path}.$(date -d \"7 days ago\" +\"%Y%m%d\") } for i in $* do rotate $i done -------------------------------------------------------------------------------------------------------------- 每天定时切割日志的任务制定（比如对python的一个业务/data/log/xcspam/下的日志进行切割，0K的日志不进行切割）： $ crontab -e # xcspam 日志切割 30 0 * * * find /data/log/xcspam/ -size +0 -name '*.log' | xargs /app/script/log_rotate.sh 手动执行切割： $ find /data/log/xcspam/ -size +0 -name '*.log' | xargs /app/script/log_rotate.sh 切割后的日志效果： $ ls /data/log/xcspam/ xcspam_error.log xcspam_error.log-20170926 -------------------------------------------------------------------------------------------------------------- 比如对maridb日志进行切割 $ crontab -e # xcspam 日志切割 30 0 * * * find /var/log/mariadb/ -size +0 -name '*.log' | xargs /app/script/log_rotate.sh $ find /var/log/mariadb/ -size +0 -name '*.log' | xargs /app/script/log_rotate.sh $ ll /var/log/mariadb/ 总用量 8 -rw-r-----. 1 mysql mysql 0 9月 17 20:31 mariadb.log -rw-r-----. 1 root root 4532 9月 17 20:31 mariadb.log.20170916 -------------------------------------------------------------------------------------------------------------- 3.3.3 日志压缩脚本 $ ls /var/log/fss/nginx/ nginx.20190506.log nginx.20190507.log nginx.20190508.log $ cat /root/log_clean.sh #!/usr/bin/sh #根据系统/服务/日志保留天数三个参数压缩日志 #usage: sh clearlog.sh sysname appname keepdays sysName=$1 appName=$2 keepDay=$3 logDir=/var/log/${sysName}/${appName} logFile=${appName}.*[0-9][0-9].log cd ${logDir} find ./ -name \"${logFile}\" -mtime -${keepDay} -exec gzip {} \\; $ sh /root/log_clean.sh fss nginx 3 $ ls /var/log/fss/nginx/ nginx.20190506.log.gz nginx.20190507.log.gz nginx.20190508.log.gz 还可以针对日志保留策略，调整成日志清理脚本。 推荐用的Nginx日志轮转方法 [部署在nginx的日志目录下] #!/bin/bash yesterday=`date -d \"-1 days\" +'%Y%m%d'` cd `dirname $0` basedir=`pwd` logdir=\"${basedir}/bak\" bindir=\"${basedir%/*}/sbin\" mkdir -p ${logdir} for log in `ls *.log 2>/dev/null` do mv ${log} ${logdir}/${log}.${yesterday}.bak # gzip ${logdir}/${log}.${yesterday} done ${bindir}/nginx -s reload cd ${logdir} find . -type f -name \"*.bak\" -mtime +7 | xargs rm -f 四、尝试解决 logrotate 无法自动轮询日志的办法 现象说明： 使用logrotate轮询nginx日志，配置好之后，发现nginx日志连续两天没被切割，这是为什么呢？？ 然后开始检查日志切割的配置文件是否有问题，检查后确定配置文件一切正常。 于是怀疑是logrotate预定的cron没执行，查看了cron的日志，发现有一条 Dec 7 04:02:01 www crond[18959]: (root) CMD (run-parts /etc/cron.daily) 这样的日志，证明cron在04:02分时已经执行 /etc/cron.daily 目录下的程序。 接着查看 /etc/cron.daily/logrotate（这是logrotate自动轮转的脚本）的内容： $ cat /etc/cron.daily/logrotate #!/bin/sh /usr/sbin/logrotate -s /var/lib/logrotate/logrotate.status /etc/logrotate.conf EXITVALUE=$? if [ $EXITVALUE != 0 ]; then /usr/bin/logger -t logrotate \"ALERT exited abnormally with [$EXITVALUE]\" fi exit 0 没有发现异常，配置好的日志轮转操作都是由这个脚本完成的，一切运行正常，脚本应该就没问题。 直接执行命令： /usr/sbin/logrotate /etc/logrotate.conf 这些系统日志是正常轮询了，但nginx日志却还是没轮询。 接着强行启动记录文件维护操作，纵使logrotate指令认为没有需要，应该有可能是logroate认为nginx日志太小，不进行轮询。 故需要强制轮询，即在 /etc/cron.daily/logrotate 脚本中将 -t 参数替换成 -f 参数 $ cat /etc/cron.daily/logrotate #!/bin/sh /usr/sbin/logrotate -s /var/lib/logrotate/logrotate.status /etc/logrotate.conf EXITVALUE=$? if [ $EXITVALUE != 0 ]; then # 原先配置参数为 -t，现在修改为 -f # /usr/bin/logger -t logrotate \"ALERT exited abnormally with /usr/bin/logger -f logrotate \"ALERT exited abnormally with [$EXITVALUE]\" fi exit 0 最后重启下cron服务： systemctl restart crond.service 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/big-data/Ambari/CentOS7中用Ambari快速搭建大数据平台.html":{"url":"linux/big-data/Ambari/CentOS7中用Ambari快速搭建大数据平台.html","title":"Ambari平台搭建","keywords":"","body":"[toc] CentOS7中用Ambari快速搭建大数据平台 公司的产品是基于大数据平台的，近期要做公司产品私有化部署，因此学习一下 Ambari 本文参考地址 原文链接 一、Ambari简介 Ambari官网 Ambari github地址 Ambari是什么 Ambari 是创建、管理、监视 Hadoop 的集群的软件。这里的 Hadoop 是广义，指的是 Hadoop 整个生态圈（例如 Hive，Hbase，Sqoop，Zookeeper ，Spark ，Flink ，Flume ，Oozie 等），而并不仅是特指 Hadoop。用一句话来说，Ambari 就是为了让 Hadoop 以及相关的大数据软件更容易使用的一个工具。 Ambari 现在所支持的平台组件也越来越多，例如流行的 Spark，Storm 等计算框架，以及资源调度平台 YARN 等，我们都能轻松地通过 Ambari 来进行部署，为想构建大数据平台的初学者提供了很大的便捷。 Ambari 自身也是一个分布式架构的软件，主要由两部分组成：Ambari Server 和 Ambari Agent。简单来说，用户通过 Ambari Server 通知 Ambari Agent 安装对应的软件；Agent 会定时地发送各个机器每个软件模块的状态给 Ambari Server，最终这些状态信息会呈现在 Ambari 的 GUI，方便用户了解到集群的各种状态，并进行相应的维护。 Ambari 基于Web的工具，支持Apache Hadoop集群的创建、管理和监控。 HDP 包含了hadoop生态系统的所有软件项目，比如HBase、Zookeeper、Hive 、Pig等等。 HDP-UTILS 工具类库。 HDP-GPL LZO压缩库软件包存储在单独的HDP-GPL存储库中。 Ambari安装说明 Ambari类似于Ansible，并没有明确的 master/slave 之分 二、环境准备 2.0 安装纯净系统后执行的脚本 #!/usr/bin/env bash # # 修改系统yum源为aliyun并添加epel源 mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak [ ! -e /etc/yum.repos.d/CentOS-Base.repo ] && curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo && sed -i -e '/mirrors.cloud.aliyuncs.com/d' -e '/mirrors.aliyuncs.com/d' /etc/yum.repos.d/CentOS-Base.repo [ ! -e /etc/yum.repos.d/epel.repo ] && curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo yum makecache yum -y install tar wget net-tools git vim tree lrzsz htop iftop iotop psmisc python36 python3-devel zlib zlib-devel gcc gcc-c++ conntrack-tools jq socat bash-completion telnet nload strace tcpdump lsof sysstat # 关闭防火墙、selinux、NetworkManager systemctl disable firewalld NetworkManager sed -i '7s/enforcing/disabled/' /etc/selinux/config # 同步时间计划任务 由于后续要用到ntpserver，因此ntpdate选择不执行 #sed -i '/*\\/10 \\* \\* \\* \\* \\/usr\\/sbin\\/ntpdate ntp2\\.aliyun\\.com &>\\/dev\\/null/d' /var/spool/cron/root #echo \"*/10 * * * * /usr/sbin/ntpdate ntp2.aliyun.com &>/dev/null\" >>/var/spool/cron/root # 历史命令显示时间 sed -i '/HISTFILESIZE=2000/d' /etc/bashrc sed -i '/HISTSIZE=2000/d' /etc/bashrc sed -i '/HISTTIMEFORMAT=\"%Y-%m-%d %H:%M:%S \"/d' /etc/bashrc sed -i '/export HISTTIMEFORMAT/d' /etc/bashrc cat >>/etc/bashrc>/etc/security/limits.conf~/.pip/pip.conf 2.1 实验环境 角色 IP地址 主机名 ambari版本 硬件配置 系统 内核 master 10.0.0.136 ambari-server01.test.com 2.6.2.2 2c8g CentOS7.8 3.10.0-1127.el7.x86_64 slave 10.0.0.137 ambari-agent01.test.com 2.6.2.2 2c8g CentOS7.8 3.10.0-1127.el7.x86_64 slave 10.0.0.138 ambari-agent02.test.com 2.6.2.2 2c8g CentOS7.8 3.10.0-1127.el7.x86_64 2.2 配置ssh免密 ⚠️如无特殊说明，以下操作均在 ambari-server01 节点 2.2.1 编辑环境变量文件 环境变量文件中IP、主机名、子网网段可自行修改 [ -d /opt/ambari/script ] || mkdir -p /opt/ambari/script && cd /opt/ambari/script cat > env.sh 2.2.2 生成密钥对 ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa &>/dev/null 2.2.3 编辑expect自动化交互脚本 这里机器用户名是root，密码是国际标准通用密码1，ssh端口22 ⚠️执行路径在 /opt/ambari/script cat > ssh.exp 2.2.4 编辑shell脚本循环执行expect脚本 # 编辑脚本 source env.sh cat > ssh.sh 2.3 每个节点配置hosts信息 source /opt/ambari/script/env.sh for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} \"cat >> /etc/hosts 2.4 禁用防火墙和selinux 这一步已在2.0安装纯净系统后执行的脚本中执行过了 # 禁用防火墙 systemctl stop firewalld && systemctl disable firewalld # 禁用selinux 临时修改 setenforce 0 # 禁用selinux 永久修改，重启服务器后生效 sed -i '7s/enforcing/disabled/' /etc/selinux/config 2.5 安装JDK8 2.5.1 安装说明 ⚠️⚠️⚠️集群中每一个节点都需要安装jdk ⚠️jdk需要登陆到 oracle 官网才可以下载，这里下载的是版本是 jdk-8u251 jdk安装包下载至 /opt/ambari/pkg jdk官方下载地址 mkdir -p /opt/ambari/pkg 2.5.2 编写jdk安装脚本 cat >/opt/ambari/script/jdk8_install.sh /etc/profile.d/jdk8.sh 2.5.3 拷贝jdk安装包和jdk安装脚本到其余节点 source /opt/ambari/script/env.sh export NODE_IPS=(${NODE_IPS[1]} ${NODE_IPS[2]}) for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" scp ${NODE_NAMES}:/opt/ambari/script/jdk8_install.sh ${node_ip}:/opt/ambari/script scp ${NODE_NAMES}:/opt/ambari/pkg/jdk-8u251-linux-x64.tar.gz ${node_ip}:/opt/ambari/pkg done 2.5.4 所有节点执行jdk安装脚本 source /opt/ambari/script/env.sh export NODE_IPS=(${NODE_IPS[1]} ${NODE_IPS[2]}) for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} 'source /opt/ambari/script/jdk8_install.sh' done 2.6 配置时间同步 时间同步可选 ntp 和 chrony，这里选择 chrony ntp官网 chrony官网 ⚠️sed中有变量替换，需要使用双引号 2.6.1 各节点安装chrony source /opt/ambari/script/env.sh for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} yum -y install chrony done 2.6.2 cdh master节点修改服务器地址为阿里云 sed -i.bak '3,6d' /etc/chrony.conf && sed -i -e '3cserver ntp1.aliyun.com iburst' -e \"/^#allow/callow ${NODE_SUBNET}\" /etc/chrony.conf 2.6.3 alave节点修改同步服务器为master节点 # 这里选择另外两个节点执行 export NODE_IPS=(${NODE_IPS[1]} ${NODE_IPS[2]}) for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} sed -i.bak '3,6d' /etc/chrony.conf && sed -i \"3cserver ${NODE_NAMES[0]} iburst\" /etc/chrony.conf done 2.6.4 启动chronyd服务并设置开机自启 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} 'systemctl enable chronyd && systemctl start chronyd' done 2.6.5 检查端口，chronyd监听udp323端口 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} netstat -nupl|grep chronyd done 2.6.6 检查同步 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} chronyc sources done 2.7 安装HTTP服务器 httpd官网 说明 这里安装 HTTP 主要作用在为后期安装 ambari 制作本地源时候用的 CentOS7.8默认的httpd版本是2.4.6 yum -y install httpd systemctl enable httpd && systemctl start httpd 2.8 安装mysql ⚠️⚠️⚠️ 二进制安装的mysql启动脚本 /etc/init.d/mysql 和 安装目录/mysql/bin/mysqld_safe 这两个文件中都是默认在 /usr/local/mysql，如果安装目录不在 /usr/local/ 下，需要修改这两个文件中的路径，即把 /usr/local 替换为mysql安装目录 sed -i 's#/usr/local#你的mysql安装目录#g' /etc/init.d/mysql /你的mysql安装目录/mysql/bin/mysqld_safe 2.8.1 下载二进制包 mysql官方下载地址 mysql5.7.30 md5值 611be3b18a30498b705db773293ad341 wget https://downloads.mysql.com/archives/get/p/23/file/mysql-5.7.30-linux-glibc2.12-x86_64.tar.gz 2.8.2 解压缩mysql二进制包到/usr/local tar xf mysql-5.7.30-linux-glibc2.12-x86_64.tar.gz -C /usr/local 2.8.3 修改名称、做软连接 mv /usr/local/mysql-5.7.30-linux-glibc2.12-x86_64 /usr/local/mysql-5.7.30 && ln -s /usr/local/mysql-5.7.30 /usr/local/mysql 2.8.4 创建mysql用户 useradd -M -s /bin/nologin mysql 2.8.5 编辑主配置文件，myql-5.7.30二进制包默认没有mysql配置文件 ⚠️如果指定了mysql的socket文件位置，则必须添加[client]标签并同时指定socket文件位置，否则客户端会默认从 /tmp下找socket文件 # 备份/etc/my.cnf mv /etc/my.cnf /etc/my.cnf.old # 以下配置为最精简版，可根据实际情况进行相应设置 cat >> /etc/my.cnf 2.8.6 创建socker文件目录、目录文件授权 ⚠️⚠️⚠️如果mysql配置文件中指定了socket文件目录，则这个目录的权限必须是mysql，否则mysql会启动失败 mkdir -p /var/lib/mysql chown -R mysql.mysql /usr/local/mysql* /var/lib/mysql chown mysql.mysql /etc/my.cnf 2.8.7 拷贝启动脚本 cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysqld 2.8.8 初始化mysql ⚠️⚠️⚠️mysql-5.7.22初始化没有提示！！！ /usr/local/mysql/bin/mysqld --initialize-insecure --user=mysql --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data 参数说明 参数 说明 --user 指定mysql用户 --basedir 指定mysql安装目录 --datadir 指定mysql数据目录 --initialize-insecure 不生成随机密码 2.8.9 添加mysql命令环境变量 # 导出mysql命令环境变量 echo \"export PATH=/usr/local/mysql/bin:$PATH\" > /etc/profile.d/mysql.sh # 使配置生效 source /etc/profile 2.8.10 配置systemd管理mysql cat >> /etc/systemd/system/mysqld.service 2.8.11 启动mysql、检查启动 # 重新加载systemd系统服务 systemctl daemon-reload # 启动mysql并加入开机自启 systemctl start mysqld && systemctl enable mysqld # 查看mysql端口 $ netstat -ntpl | grep 3306 tcp6 0 0 :::3306 :::* LISTEN 31349/mysqld 2.8.12 创建ambari数据库及授权 mysql -uroot -e \"create database ambari character set utf8mb4\" mysql -uroot -e \"grant all on ambari.* to ambari@'localhost' identified by 'ambari'\" mysql -uroot -e \"grant all on ambari.* to ambari@'%' identified by 'ambari'\" mysql -uroot -e \"flush privileges\" 三、安装ambari 3.1 制作ambari本地源 各包大小 HDP-2.6.5.0-centos7-rpm.tar.gz 6.8G ambari-2.6.2.2-centos7.tar.gz 1.7G HDP-UTILS-1.1.0.22-centos7.tar.gz 87M 下载 ambari 、HDP-UTILS、 HDP 包 官方下载地址，可以从这里下载不同版本的包 mkdir /opt/ambari/yum && cd /opt/ambari/yum wget http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.6.2.2/ambari-2.6.2.2-centos7.tar.gz wget http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.5.0/HDP-2.6.5.0-centos7-rpm.tar.gz wget http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7/HDP-UTILS-1.1.0.22-centos7.tar.gz 将3个压缩包解压至 /var/www/html/ambari mkdir /var/www/html/ambari tar xf ambari-2.6.2.2-centos7.tar.gz -C /var/www/html/ambari tar xf HDP-UTILS-1.1.0.22-centos7.tar.gz -C /var/www/html/ambari tar xf HDP-2.6.5.0-centos7-rpm.tar.gz -C /var/www/html/ambari 3.2 安装制作本地yum源工具 yum -y install yum-utils createrepo 3.3 创建ambari、HDP、HDP-UTILS的repo仓库 ⚠️如果下载的版本不同，则需要修改 baseurl和gpgcheck中的url路径 3.3.1 创建ambari的repo仓库 source /opt/ambari/script/env.sh cat > /etc/yum.repos.d/ambari.repo 3.3.2 创建HDP、HDP-UTILS的repo仓库 cat > /etc/yum.repos.d/hdp.repo 3.3.3 生成本地缓存 yum clean all && yum makecache 3.3.4 通过本地源安装ambari ⚠️执行此命令会安装 ambari2.6.2.2 和 pg9.2.24 yum -y install ambari-server 四、配置ambari 4.1 下载mysql驱动 mysql驱动官方下载地址 由于使用的是mysql5.7，因此必须下载mysql5.7驱动，且必须放于 /usr/share/java 下 否则后续会有如下报错，原因是 ambari 默认的 mysql jdbc 驱动不支持 5.6 以上版本 Configuring ambari database... WARNING: Before starting Ambari Server, you must copy the MySQL JDBC driver JAR file to /usr/share/java and set property \"server.jdbc.driver.path=[path/to/custom_jdbc_driver]\" in ambari.properties. Press to continue. ERROR: Before starting Ambari Server, you must copy the MySQL JDBC driver JAR file to /usr/share/java and set property \"server.jdbc.driver.path=[path/to/custom_jdbc_driver]\" in ambari.properties. ERROR: Exiting with exit code -1. REASON: Before starting Ambari Server, you must copy the MySQL JDBC driver JAR file to /usr/share/java and set property \"server.jdbc.driver.path=[path/to/custom_jdbc_driver]\" in ambari.properties. # 创建/usr/share/java目录 [ -d /usr/share/java ] || mkdir /usr/share/java && cd /usr/share/java # 下载mysql驱动 wget https://downloads.mysql.com/archives/get/p/3/file/mysql-connector-java-5.1.46.tar.gz # 解压缩包 tar xf mysql-connector-java-5.1.46.tar.gz # 移动mysql驱动到/usr/share/java，且名称必须为 mysql-connector-java.jar mv mysql-connector-java-5.1.46/mysql-connector-java-5.1.46.jar /usr/share/java/mysql-connector-java.jar 4.2 启动配置程序 ambari-server setup 4.2.1 提示是否自定义设置 输入：y Customize user account for ambari-server daemon [y/n] (n)? 4.2.2 设置ambari-server 账号 输入：ambari Enter user account for ambari-server daemon (root): 4.2.3 设置JDK 输入：3 Checking JDK... [1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8 [2] Oracle JDK 1.7 + Java Cryptography Extension (JCE) Policy Files 7 [3] Custom JDK ============================================================================== Enter choice (1): 4.2.4 设置JDK家目录 输入：/usr/local/jdk1.8.0_251 jdk的家目录路径是 /usr/local/jdk1.8.0_251 WARNING: JDK must be installed on all hosts and JAVA_HOME must be valid on all hosts. WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts. Path to JAVA_HOME: 4.2.5 是否允许Ambari服务器下载和安装GPL许可的LZO包 输入：y Enable Ambari Server to download and install GPL Licensed LZO packages [y/n] (n)? 4.2.6 数据库配置 输入：y Enter advanced database configuration [y/n] (n)? 4.2.7 选择数据库类型 输入：3 输入3选择mysql Configuring database... ============================================================================== Choose one of the following options: [1] - PostgreSQL (Embedded) [2] - Oracle [3] - MySQL / MariaDB [4] - PostgreSQL [5] - Microsoft SQL Server (Tech Preview) [6] - SQL Anywhere [7] - BDB ============================================================================== Enter choice (1): 4.2.8 数据库信息填写 Enter choice (1): 3 Hostname (localhost): #默认即可 Port (3306): #默认3306 Database name (ambari): #这里数据库名称为ambari Username (ambari): #数据库用户名是ambari Enter Database Password (bigdata): #用户ambari的密码是ambari Re-enter password: #确认密码 Configuring ambari database... 4.2.9 继续配置远程数据库连接属性 输入：y ⚠️需要导入 /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql到ambari数据库中 WARNING: Before starting Ambari Server, you must run the following DDL against the database to create the schema: /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql Proceed with configuring remote database connection properties [y/n] (y)? 提示如下即为成功 Extracting system views... ambari-admin-2.6.2.2.1.jar ........... Adjusting ambari-server permissions and ownership... Ambari Server 'setup' completed successfully. 4.2.10 导入数据库 mysql -uroot -D ambari -e \"source /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql\" 4.3 启动ambari ambari-server start 提示如下即为成功 Using python /usr/bin/python Starting ambari-server Ambari Server running with administrator privileges. Organizing resource files at /var/lib/ambari-server/resources... Ambari database consistency check started... Server PID at: /var/run/ambari-server/ambari-server.pid Server out at: /var/log/ambari-server/ambari-server.out Server log at: /var/log/ambari-server/ambari-server.log Waiting for server start........................ Server started listening on 8080 DB configs consistency check: no errors and warnings were found. Ambari Server 'start' completed successfully. 4.4 登陆ambari 浏览器访问 http://IP:8080 登陆后首界面 到此，ambari安装成功！！！ 五、使用Ambari界面安装大数据组件 5.1 启动安装向导 选择 Launch Install Wizard 输入集群名称 5.2 选择集群版本、配置集群本地源 这里选择 HDP-2.6.5.0 版本 保留 redhat7 一处，删除其他 设置HDP安装源，修改为本地源 原先默认源 HDP-2.6 http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.5.0 HDP-2.6-GPL http://public-repo-1.hortonworks.com/HDP-GPL/centos7/2.x/updates/2.6.5.0 HDP-UTILS-1.1.0.22 http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7 HDP-2.6、HDP-2.6-GPL、HDP-UTILS-1.1.0.22 依次修改为如下 http://10.0.0.136/ambari/ambari/centos7/2.6.2.2-1 http://10.0.0.136/ambari/HDP/centos7/2.6.5.0-292 http://10.0.0.136/ambari/HDP-UTILS/centos7/1.1.0.22 修改完成后点击下一步 5.3 配置安装选项 5.3.1 修改配置文件 需要修改配置文件中https为http，否则后续会有类似如下报错 ERROR 2020-09-15 01:34:45,716 NetUtil.py:96-EOF occurred in vic lation of protocol (_ssl.c:618) ERROR 2020-09-15 01:34:45,716 NetUtil.py:97-SSLError:Failed to connect. Please check openssl Library versions. Refer to:https://bugzilla.redhat.com/show_bug.cgi?id=1022468 for more details. 1.修改 /etc/python/cert-verification.cfg 中 verify=platform_default 为 verify=disable sed -i.bak '/^verify/cverify=disable' /etc/python/cert-verification.cfg 2.修改 /etc/ambari-agent/conf/ambari-agent.ini 中 [security] 标签后加入以下两行内容 ⚠️这一步必须在 Confirm Hosts 进行安装后才会有相应的文件，也就是说进入到 Confirm Hosts 这一步中注册主机稍等一会才可以进行修改文件操作(目前了解是这样，不知道有没有更好的方法) 上图界面稍等几十秒然后再进行以下操作 ssl_verify_cert=0 force_https_protocol=PROTOCOL_TLSv1_2 sed -i.bak -e '/^credential_shell_cmd/assl_verify_cert=0\\nforce_https_protocol=PROTOCOL_TLSv1_2' /etc/ambari-agent/conf/ambari-agent.ini 以上两步修改完成后需要重启 ambari-agent ambari-agent restart 5.3.2 配置集群节点信息 输入集群节点FQDN式主机名、ambari-server的私钥、ambari-server的用户名和ssh端口 各节点 hosts 信息 10.0.0.136 ambari-server01.test.com 10.0.0.137 ambari-agent01.test.com 10.0.0.138 ambari-agent02.test.com ⚠️⚠️⚠️Target Hosts 处一定不要写IP地址，一定要写成FQDN式的主机名 集群节点 Target Hosts 处要写成如下 ambari-server01.test.com ambari-agent01.test.com ambari-agent02.test.com 否则就会报错如下，非常坑 ERROR 2020-09-14 17:07:52,075 main.py:246 - Ambari agent machine hostname (ambari01.hadoop) does not match expected ambari server hostname (10.0.0.136). Aborting registration. Please check hostname, hostname -f and /etc/hosts file to confirm your hostname is setup correctly 注册成功后显示如下 ⚠️5.3.3为遇到的问及记录 5.3.3 ambari安装到 Confirm Hosts遇到的报错 5.3.3.1 报错1 找不到 ambari server 重要日志 解决方法 集群节点 Target Hosts 处主机信息要写成如下FQDN式 ambari-server01.test.com ambari-agent01.test.com ambari-agent02.test.com 5.3.3.2 报错2 SSLError 5.2步骤中修改了HDP源为本地 HDP-2.6、HDP-2.6-GPL、HDP-UTILS-1.1.0.22 依次修改为如下 http://10.0.0.136/ambari/ambari/centos7/2.6.2.2-1 http://10.0.0.136/ambari/HDP/centos7/2.6.5.0-292 http://10.0.0.136/ambari/HDP-UTILS/centos7/1.1.0.22 解决方法 ⚠️以下操作需要在所有节点修改 1.修改 /etc/python/cert-verification.cfg 中 verify=platform_default 为 verify=disable sed -i.bak '/^verify/cverify=disable' /etc/python/cert-verification.cfg 2.修改 /etc/ambari-agent/conf/ambari-agent.ini 中 [security] 标签后加入以下两行内容 ssl_verify_cert=0 force_https_protocol=PROTOCOL_TLSv1_2 sed -i.bak -e '/^credential_shell_cmd/assl_verify_cert=0\\nforce_https_protocol=PROTOCOL_TLSv1_2' /etc/ambari-agent/conf/ambari-agent.ini 重启 ambari-agent ambari-agent restart 5.4 选择需要安装的组件 根据实际情况选择要安装的组件 5.5 分配管理端服务 根据实际情况选择各组件安装的节点 5.6 定制服务 5.6.1 HDFS 只保留一个路径 5.6.2 YARN 只保留一个路径 5.6.3 HIVE 指定mysql驱动位置，否则hive连接数据库会失败 ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar 输出如下即为成功 Using python /usr/bin/python Setup ambari-server Copying /usr/share/java/mysql-connector-java.jar to /var/lib/ambari-server/resources If you are updating existing jdbc driver jar for mysql with mysql-connector-java.jar. Please remove the old driver jar, from all hosts. Restarting services that need the driver, will automatically copy the new jar to the hosts. JDBC driver was successfully initialized. Ambari Server 'setup' completed successfully. 创建hive数据库及授权 mysql -uroot -e \"create database hive character set utf8mb4\" mysql -uroot -e \"grant all on hive.* to hivei@'localhost' identified by 'hive'\" mysql -uroot -e \"grant all on hive.* to hive@'%' identified by 'hive'\" mysql -uroot -e \"flush privileges\" 选择 Existing MySQL / MariaDB Database ，输入hive数据库密码，修改 Database URL 连接地址，测试连接成功即可 5.6.4 Ambari Metrics 需要设置grafana管理员用户的密码 5.6.5 Atlas ⚠️⚠️⚠️最好不要选择这个Atlas，网上查了半天也不知道标红的这两处该怎么写以及怎么查找 5.6.6 SmartSense 需要输入密码，默认admin即可 5.7 确认集群信息并部署 5.8 完成安装 完成安装 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/big-data/Ambari/ambari下载.html":{"url":"linux/big-data/Ambari/ambari下载.html","title":"Ambari下载","keywords":"","body":"[toc] ambari下载 一、令人懵逼的网站 ambari官网 https://supportmatrix.hortonworks.com/ 不知道这是什么网站 进入上面这个网站， Products 下 Sofware Download 点击 HDP 后会跳转到 另外一个网站 www.cloudera.com 支持中文的网站才是好网站 点击 Legacy HDP release 选择要下载的版本 二、下载hdp ⚠️从HDP3.1.5开始，下载会提示必须是HDP客户才能访问下载，无解，所以只能下载3.1.4 限制访问 您必须是HDP客户才能访问这些下载。如果您认为自己应该可以使用这些软件，请与支持人员或您的客户服务代表联系。 选择 3.1.4 版本，然后点击 Installation 选择 Apache Ambari Installation 先选择 Obtaining Public Repositories 然后再选择 HDP Stack Repositories 选择 HDP 3.1.4 Repositories 这里可根据系统类型选择不同的下载 可以下载 HDP3.1.4 和 HDP-UTILS1.1.0.22 # 下载HDP wget http://public-repo-1.hortonworks.com/HDP/centos7/3.x/updates/3.1.4.0/HDP-3.1.4.0-centos7-rpm.tar.gz # 下载HDP-UTILS wget http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7/HDP-UTILS-1.1.0.22-centos7.tar.gz # 下载HDP-GPL wget http://public-repo-1.hortonworks.com/HDP-GPL/centos7/3.x/updates/3.1.4.0/HDP-GPL-3.1.4.0-centos7-gpl.tar.gz 三、下载Ambari ⚠️从Ambari2.7.5.0开始，下载会提示必须是HDP客户才能访问下载，无解，所以只能下载2.7.4.0 选择 Installation 选择 Apache Ambari Installation 选择 Obtaining Public Repositories 下的 Ambari Repositories 这里可根据系统类型选择不同的下载 wget http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.7.4.0/ambari-2.7.4.0-centos7.tar.gz 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/big-data/MapReduce/腾讯云 MapReduce 集群 core 节点下线过程.html":{"url":"linux/big-data/MapReduce/腾讯云 MapReduce 集群 core 节点下线过程.html","title":"腾讯云 MapReduce 集群 core 节点下线过程","keywords":"","body":"腾讯云 MapReduce 集群 core 节点下线过程 [toc] 背景 官方版 当 EMR 的 core 节点需进行升级，对老的 core 节点进行下线操作来替换新的 core 节点 上线。 实际情况 之前的傻逼领导和傻逼运维不根据实际业务的数据量大小就瞎🐔8⃣️开一堆配置特别高的机器(数量50多台，总核心1000+、总内存5T+、总磁盘800T+，这还不算之前退的机器)，然后总体利用率连一半都不到，40%左右，😠，这简直是艹泥🐎啊，现在傻逼老板让减配(他的理念就是既不想花钱又想要用高的配置)，然后不懂技术的\"技术总监\"就天天催着下边的人赶紧把机器退了，👍 目标 集群在下线 Core 节点后，数据仍保证完整可靠性，同时集群服务仍正常运行 下线步骤 一、hadoop fsck / 运行健康检查，确认hdfs健康状态为healthy，如果有单副本情况存在，务必调整为多副本。 二、如果数据量较大，务必先调优，否则下线数据迁移异常缓慢。 参考文档： 加快副本复制速度文档 三、开始下线节点 参考文档： 大数据EMR-core节点下线操作 四、申请白名单，控制台缩容节点 第一步 加快副本复制速度 ⚠️一定要做加快副本复制速度操作，否则复制会特别慢(第一次操作由于没有做此步骤，导致后续复制速度特别慢，经和腾讯云沟通30T数据(2台机器)在不加速的情况下需要大概2周以上！) 1.1 在 emr 控制台的 hdfs-site.xml 里增加如下 5 个参数，下发 NameNode 节点 dfs.namenode.replication.max-streams 20 dfs.namenode.replication.max-streams-hard-limit 40 dfs.namenode.replication.work.multiplier.per.iteration 10 dfs.datanode.balance.max.concurrent.moves 30 dfs.datanode.balance.bandwidthPerSec 52428800 参数说明 参数 说明 默认值 参考值 dfs.namenode.replication.work.multip lier.per.iteration 决 定 了 可 以 从 很 多 under replication blocks 中选出多少个 block 准备进行复制。如果该参数配 置 得 太 小 ， 则 dfs.namenode.replication.max-str eams 配置得再大没有用；可以选出的 block 数与集群 live 的 datadnode 成正比。 2 10 dfs.namenode.replication.max-streams 单个 DataNode 最大同时恢复的块数 量，可以间接控制 DataNode 恢复数据 块 的 带 来 的 网 络 等 压 力。 需 要 与 dfs.namenode.replication.work.mu ltiplier.per.iteration 配置项配合 使用； 2 20 dfs.namenode.replication.max-streams -hard-limit balance/退服性能参数，最高优先级 复制流的数量的硬限制 4 40 dfs.datanode.balance.max.concurrent. moves DataNode 上同时用于 Balancer 待移 动 block 的最大线程个数 5 30 dfs.datanode.balance.bandwidthPerSec 指定 DataNode 用于 Balancer 的带宽 10485760 (10mb) 52428800 (50mb) 在 基本信息 -> 实例信息 中点击 组件信息 在 HDFS 处点击下拉框，选择 配置管理 选择 hdfs-site.xml ，然后点击 修改配置 选择 新增配置项，然后把上述5个值依次添加并保存 1.2 重启 NameNode 角色选择 NameNode，重启方式选择 安全重启模式，重启NameNode 到此，加速副本复制速度操作完成！ 常见问题 1.单副本问题 问题描述 HDFS web UI 页面出现如下状况:下线进度停止 blocks with no live replicas 栏出现大量块 原因 下线节点存在 单副本的情况，hdfs 拒绝继续执行下线以防数据丢失 解决方案 老外解决方案原版链接 执行命令 hadoop version 查看集群 hadoop 版本 Hadoop 2.7 及以下版本 master 运行如下脚本设置单副本块为多副本 cd /data/emr/hdfs/logs su hadoop -c \"hdfs dfsadmin -metasave metasave-report.txt\" cat /data/emr/hdfs/logs/metasave-report.txt | grep \"l: 1\" | cut -d':' -f1 >> ./single_replica for hdfsfile in cat /data/emr/hdfs/logs/single_replica; do su hadoop -c \"hadoop fs -setrep 3 $hdfsfile\"; done Hadoop 2.8 及以上版本 master 运行如下脚本设置多副本 hadoop fsck / -files -blocks -replicaDetails |grep -C 1 Live_repl=1 |grep OK |awk '{print $1}' >/tmp/single_replica for hdfsfile in `cat /tmp/single_replica`; do su hadoop -c \"hadoop fs -setrep 3 $hdfsfile\"; done 运行完，如下即表示问题修复： 2.DataNode 线程不足问题 数据迁移过程中 DataNode 有如下信息 Threads quota is exceeded 或者 dataxceiver error 说明 DataNode 线程不足 将以下参数添加到 hdfs-site.xml 中并重启 HDFS 集群 dfs.datanode.max.xcievers =16384 第二步、DataNode下线 core节点就是DataNode 2.1 Active Namenode 节点操作 Active NameNode 在hdfs web UI界面查看，生产中我们的 Active NameNode是master节点 编辑 /usr/local/service/hadoop/etc/hadoop/hdfsexcludedhosts 填写想要下线的节点 IP ，ip 数量建议(1-2 个)，一行一个IP vim /usr/local/service/hadoop/etc/hadoop/hdfsexcludedhosts ⚠️获取 active 节点状态，执行命令 hdfs haadmin -getServiceState 查看或者控制台服务监控查看，其中 在hdfs web UI 界面查看(一般为nn1或者nn2，即NameNode1 NameNode2) 命令执行如下，nn1节点一定要为active状态(这里要看你的实际设置，生产中我们的2个master节点就是NameNode节点)，否则集群无法正常提供服务，会导致大数据任务执行失败 $ su hadoop -c \"hdfs haadmin -getServiceState nn1\" SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/service/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/service/tez/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] active nn1节点是 active 状态，nn2节点就为 standby 状态，反之nn1节点是standby 状态，nn2节点就为 active 状态 $ su hadoop -c \"hdfs haadmin -getServiceState nn2\" SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/service/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/service/tez/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] standby 2.2 Active NameNode 节点执行 hadoop dfsadmin -refreshNodes 执行成功后最后会提示 Refresh nodes successful for ，其中的两个IP为两个master(NameNode节点)的IP $ su hadoop -c \"hadoop dfsadmin -refreshNodes\" DEPRECATED: Use of this script to execute hdfs command is deprecated. Instead use the hdfs command for it. SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/service/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/service/tez/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] Refresh nodes successful for /10.0.100.17:4007 Refresh nodes successful for /10.0.100.11:4007 2.3 hdfs web UI 界面验证 打开 HDFS 原生 web UI，在 Datanodes 页面可以看到想要下线的节点状态变为 Decommission In Progress， 说明这些节点的数据块正在被复制到其他节点，等待所有数据块被复制完成，这些节点状态就会变为 Decommissioned 在最下边可以查看数据复制进度 所有数据块被复制完成后，要下线节点的状态就由 Decommissioned in Progress 变成 Decommissioned 2.4 在 emr 控制台停止如上 2 个节点的 DataNode 服务 ⚠️一定要等2.3步骤中执行完成，即想要下线的节点状态变为 Decommission 才可以继续后续操作 在集群服务HDFS选项中，找到要下线的 DataNode 节点，选中，然后点击 暂停 分别选中两个要下线的 DataNode 节点，依次暂停 暂停中的 DataNode，模式为 维护模式 两个 DataNode 节点暂停后，在HDFS web UI 中就可以看到两个节点的状态变为了Dead, Decommissioned 2.5 编辑配置文件，删除要下线机器的IP 第一个文件 在2.1步骤中，我们编辑了Active NameNode节点的 /usr/local/service/hadoop/etc/hadoop/hdfsexcludedhosts 填写想要下线的节点 IP ，现在把这个文件中的ip删除 第二个文件 /usr/local/service/hadoop/etc/hadoop/hdfshosts (两个 NameNode 节点都删除下线 ip) 2.6 两个 NameNode 节点都执行 hadoop dfsadmin -refreshNodes $ su hadoop -c \"hadoop dfsadmin -refreshNodes\" DEPRECATED: Use of this script to execute hdfs command is deprecated. Instead use the hdfs command for it. SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/service/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/service/tez/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] Refresh nodes successful for /10.0.100.17:4007 Refresh nodes successful for /10.0.100.11:4007 执行完成后，在HDFS中要下线的NameNode就没了，之前在emr控制台中暂停的2个NameNode状态如下 HDFS 原生 web UI 将不再存在以上两节点，至此 DataNode 下线完毕！ 第三步、NodeManager 下线操作 DataNode 下线完后操作 3.1 两个主备 ResourceManager 节点操作 生产中我们的 ResourceManager 服务运行在2个 NameNode 节点上 编辑 /usr/local/service/hadoop/etc/hadoop/yarnexcludedhosts 填写想要下线的节点 IP vim /usr/local/service/hadoop/etc/hadoop/yarnexcludedhosts 3.2 两个主备ResourceManager 节点都执行 yarn rmadmin -refreshNodes $ su hadoop -c \"yarn rmadmin -refreshNodes\" SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/service/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/service/tez/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 3.3 打开 YARN 原生 web UI 直到 Decommisioned Nodes 处出现下线的节点，说明这些节点已完成任务或本身无任务状态，这些节点状态就会变为 Decommissioned 在 Decommisioned Nodes(退役节点) 处点击下方的对应的数字 搜索要下线节点的IP，如果出现要下线的节点IP则说明这个节点目前已经处于退役状态 3.4 在 emr 控制台停掉如上 2 个节点的 NodeManager 服务 在集群服务中点击 YARN 角色处选择 NodeManager，找到要下线的节点，选择并 暂停 暂停完成后，节点维护状态就会变为 维护模式 3.5 两个主备 ResourceManager 节点编辑两个文件，删除要下线的2个节点的 ip /usr/local/service/hadoop/etc/hadoop/yarnexcludedhosts /usr/local/service/hadoop/etc/hadoop/yarnhosts vim /usr/local/service/hadoop/etc/hadoop/yarnexcludedhosts vim /usr/local/service/hadoop/etc/hadoop/yarnhosts 3.6 两个主备 ResourceManager 节点重新执行 yarn rmadmin -refreshNodes $ su hadoop -c \"yarn rmadmin -refreshNodes\" SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/service/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/service/tez/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] YARN 原生 web UI 将不再存在以上两节点(腾讯云给的文档中说此步骤操作后，YARN 原生 web UI中不存在以上两个节点，但是实际上是存在的，经与腾讯云沟通后对方说存在也不影响)，至此 Nodemanager 下线完毕 ⚠️对于 NodeManager 下线，emr-V1.3.1 版本需重启两个 ResourceManager 后，WEB 页面才剔除节点， 但实际影响不大。在执行完步骤 3.6 后，下线节点的 NodeManager 实际已从集群中移除，任务不会再分配到该下线的 NodeManmager 节点 第四步、RegionServer 下线操作 ⚠️若存在 HBASE，请将 DataNode 下线完成后操作 4.1 登录 EMR 控制台，将下线节点的 RegionServer 进入维护模式 HBASE 中分为 HMaster 和 RegionServer 两个角色 在集群列表中选择 HBASE 角色选择 RegionServer 选择要下线的 RegionServer 节点，设置状态为 暂停 点击暂停后稍等一会，节点的状态就变为了暂停中，维护状态就变为了 维护模式 在HBASE web UI 中可以看到 RegionServer 的状态变为了 Dead 4.2 登录到对应下线机器，执行如下命令 su hadoop -c \"/usr/local/service/hbase/bin/graceful_stop.sh IP\" su hadoop -c \"/usr/local/service/hbase/bin/graceful_stop.sh 10.0.100.76 su hadoop -c \"/usr/local/service/hbase/bin/graceful_stop.sh 10.0.100.89 下线的节点1 $ su hadoop -c \"/usr/local/service/hbase/bin/graceful_stop.sh 10.0.100.76\" 2021-04-10T15:41:57 Disabling load balancer SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/service/hbase/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/service/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/service/tez/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 2021-04-10T15:42:06 Previous balancer state was true 2021-04-10T15:42:06 Unloading 10.0.100.76 region(s) SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/service/hbase/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/service/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/service/tez/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] RuntimeError: Server 10.0.100.76:6002 not online stripServer at /usr/local/service/hbase/bin/region_mover.rb:194 unloadRegions at /usr/local/service/hbase/bin/region_mover.rb:305 (root) at /usr/local/service/hbase/bin/region_mover.rb:488 2021-04-10T15:42:14 Unloaded 10.0.100.76 region(s) 2021-04-10T15:42:14 Stopping regionserver on 10.0.100.76 no regionserver to stop because no pid file /data/emr/hbase/pid/hbase-hadoop-regionserver.pid 2021-04-10T15:42:14 Restoring balancer state to true 下线的节点2 $ su hadoop -c \"/usr/local/service/hbase/bin/graceful_stop.sh 10.0.100.89\" 2021-04-10T15:46:25 Disabling load balancer SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/service/hbase/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/service/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/service/tez/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 2021-04-10T15:46:35 Previous balancer state was true 2021-04-10T15:46:35 Unloading 10.0.100.89 region(s) SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/service/hbase/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/service/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/service/tez/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] RuntimeError: Server 10.0.100.89:6002 not online stripServer at /usr/local/service/hbase/bin/region_mover.rb:194 unloadRegions at /usr/local/service/hbase/bin/region_mover.rb:305 (root) at /usr/local/service/hbase/bin/region_mover.rb:488 2021-04-10T15:46:43 Unloaded 10.0.100.89 region(s) 2021-04-10T15:46:43 Stopping regionserver on 10.0.100.89 no regionserver to stop because no pid file /data/emr/hbase/pid/hbase-hadoop-regionserver.pid 2021-04-10T15:46:43 Restoring balancer state to true ⚠️报错可以忽略 HBASE 原生web UI 查看下线节点 region 为0即代表 RegionServer 下线完毕 4.3 控制台下线 core节点操作 在控制台 集群资源 -> 资源管理 选择 Core，找到要下线的节点，点击 缩容 即可 勾选 已阅读并同意，然后点击 下一步 点击 开始销毁 至此，core节点下线就完成了！ 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux命令/vim命令.html":{"url":"linux/linux命令/vim命令.html","title":"vim命令","keywords":"","body":"[toc] vim命令 1.vim编辑器 vi Visual Interface 可视化界面 vim vi增强版 2.vim模式及模式转换 vim模式 命令模式，刚打开一个文件就是命令模式 编辑模式，可以编辑内容 末行模式，可进行搜索、替换、切换文件等操作 vim模式转换 命令模式-->编辑模式 i: 在当前光标所在行的字符前面，转换为编辑模式 I: 在当前光标所在行的行首，转换为编辑模式 a: 在当前光标所在行的字符后面，转换为编辑模式 A: 在当前光标所在行的行尾，转换为编辑模式 o: 在当前光标所在行的下方，新建一行 O: 在当前光标所在行的上方，新建一行 编辑模式-->命令模式 ESC键 编辑模式-->末行模式 先esc 然后： ： ？ / 3.vim打开文件方式 vim +# 打开文件，并定位于第#行 vim + 打开文件，定位至最后一行 vim + /正则表达式 打开文件，定位至第一次被正则表达式匹配到的行的行首 4.vim关闭文件方式 4.1 末行模式关闭文件 :q 退出 :wq 退出并保存 :q! 不保存退出 :w 保存 :wq! 强行保存退出 4.2 编辑模式下退出 ZZ 保存并退出 ZQ 不保存退出 5.移动光标 5.1 逐字符移动(也可以使用上下左右箭头) h 向左 j 向下 k 向上 l 向右 例：5l 向右移动5个字符 5.2 以单词为单位移动 w 移至下一个词的词首 e 跳至当前或下一个单词的词尾 end b 跳至当前或前一个单词的词首 back 5.3 行内跳转 0 绝对行首 ^ 行首的第一个非空字符 $ 绝对行尾 5.4 行间跳转 5.4.1 编辑模式跳转 nG、ngg 跳转至n行 G 最后一行 gg 第一行第一个字符 5.4.2 末行模式跳转 :n 跳转至n行 :$ 跳转至最后一行 6.翻屏 Ctrl+f 向下翻一屏 Ctrl+b 向上翻一屏 Ctrl+d 向下翻半屏 Ctrl+u 向上翻半屏 7.删除 d x 删除当前光标所在处的单个字符 X 删除当前光标所在处前面的单个字符 nx 删除光标所在处及向后的n个字符 dd 删除当前光标所在行 ndd 删除包括当前光标所在行的n行 dw 删除光标所在处到下一个词的词首 de 删除光标所在处到当前词的词尾 db 删除光标所在处到上一个单词的词首 8.复制 yy yy 复制当前光标所在行 nyy 复制当前光标所在行及后n行 9.粘贴 p p 粘贴至光标所在行的下一行 P 粘贴至光标所在行的上一行 10.修改 c cc 删除光标所在行并进入编辑模式 C 删除光标所在处到本行结尾并进入编辑模式 11.替换 r r 编辑模式直接按r替换，一次只能替换一个 R 替换模式，可以替换多个 12.撤销 u u 撤销前一次的编辑操作，连续u命令可以撤销此前的n次编辑操作 nu 直接撤销最近n次编辑操作 Ctrl+r 撤销撤销操作 13.重复前一次操作 编辑模式按“.”键，会重复前一次的操作，比替换、编辑、删除等 14.可视化操作 v 按字符选取 V 按矩形选取 ctrl+v 批量操作，先选中要操作的区域，然后大写I编辑，esc，回车即可完成编辑 15.查找(末行模式) / 从上而下 ？ 从下而上 16.查找并替换(末行模式) s %表示全文 g表示全部替换 示例 1,3s/A/B 1,$s/A/B %s/A/B %s/A/B/g 17.vim编辑多个文件 vim file1 file2 file3 ... :next 切换至下一个文件 :prev 切换至上一个文件 :last 切换至最后一个文件 :first 切换至第一个文件 18.高级话题 18.1显示或取消行号 :set nu :set nonu 18.2查找到的文本高亮显示或取消高亮显示 :set hls :set nohls 19.vim配置文件 /etc/vimrc ~/.vimrc 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux命令/vim进阶.html":{"url":"linux/linux命令/vim进阶.html","title":"vim进阶用法","keywords":"","body":"[toc] vim进阶 vim进阶一 使用F1键执行文件 vim是一个类似于Vi的著名的功能强大、高度可定制的文本编辑器 我们Linux运维经常在Linux中使用到Vim编辑器，当使用Vim写shell脚本或者python脚本的时候，想要运行测试时候怎么办？Esc➡：➡wq，到shell终端执行脚本 上述情况很复杂！！ 下面设置vim配置文件，让vim编辑器在不退出就能执行脚本 1.创建并编辑当前用户的vim配置文件 vim ~/.vimrc 添加如下代码： \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"Programming makes the world better \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" map :call CompileRunGcc() func! CompileRunGcc() exec \"w\" if &filetype == 'c' exec '!g++ % -o % 2.编辑脚本，并进行测试 //编辑测试脚本 [root@web01 ~]# cat test.sh # !/usr/bin/env bash echo \"test\" //测试，vim编辑脚本，末行模式按F1 //再次按回车，会回到脚本当中,这样就可以不退出脚本，直接执行脚本进行测试了！！！ vim进阶二 linux脚本自动添加脚本头 编辑当前用户vim配置文件 #vim ~/.vimrc 或者定义全局也行 #vim /etc/vimrc 在最下方添加如下代码： CentOS7 function HappyPython() call setline(1, \"#!/usr/bin/env python\") call append(1, \"#-*- coding:utf8 -*-\") normal G normal o endf autocmd bufnewfile *.py call HappyPython() function HappyShell() call setline(1, \"#!/usr/bin/env bash\") call append(1, \"export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin\") normal G normal o endf autocmd bufnewfile *.sh call HappyShell() CentOS6 function HappyPython() call setline(1, \"#!/usr/bin/env python\") call append(1, \"#-*- coding:utf8 -*-\") normal G normal o endf autocmd bufnewfile *.py call HappyPython() function HappyShell() call setline(1, \"#!/usr/bin/env bash\") call append(1, \"export PATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin\") normal G normal o endf autocmd bufnewfile *.sh call HappyShell() 保存退出后，我们试试开始使用vim编辑.py文件和.sh文件 就会发现py文件会自动添加了python脚本头！ sh文件自动添加了shell脚本头！ vim进阶三 vim编辑python脚本时Tab补全 使用Linux写python脚本的时候，初期最痛苦的是什么？当然是各种库的不熟悉，知道了库，里面的方法还要挨个看，挨个记。 所以这时候，很多小伙伴使用了ipython，最强大的功能是什么呢？小伙伴们都知道，可以自动填充缩进，最重要的一点当然是可以补全啦！ 在这里不得不提，vim的强大，可以定制化，支持python补全。 下面进行设置，如何能够让vim编写python脚本可以tab补全： 第一步、安装git [root@web01 ~]# yum -y install git 第二步、创建当前用户隐藏vim目录，并进入到这个目录 [root@web01 ~]# mkdir ~/.vim ; cd ~/.vim 第三步、使用git克隆下插件包（也可提前下载好，copy进来） [root@web01 .vim]# git clone https://github.com/rkulla/pydiction.git [root@web01 .vim]# cp -r ~/.vim/pydiction/after/ ~/.vim 第四步、编辑vim配置文件，并添加如下内容 [root@web01 .vim]# vim ~/.vimrc filetype plugin on let g:pydiction_location = '/root/.vim/pydiction/complete-dict' let g:pydiction_menu_height = 10 ⚠️⚠️⚠️此处有坑请注意！！ 代码第二行，使用find命令查找一下自己的complete-dict文件路径，一定要写对！！ 第五步、编辑python脚本，进行测试 编辑python脚本，然后输入库名，然后tab补齐即可 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux命令/date命令.html":{"url":"linux/linux命令/date命令.html","title":"date命令","keywords":"","body":"[toc] date命令 1.命令说明 date命令根据给定格式显示日期或设置系统日期时间，print or set the system date and time centos7中date命令所在路径 [root@aliyun ~]# which date /usr/bin/date 2.命令格式 date [OPTION]…[+FORMAT] date [选项] [格式] 3.常用选项 3.1 -d 根据描述显示指定日期 //查看当前系统日期 [root@test1 ~]# date Mon Aug 20 21:15:32 CST 2018 //设置时间为一天前 [root@test1 ~]# date -d \"-1 day\" Sun Aug 19 21:15:34 CST 2018 3.2 -s 手动设置时间 //手动设置时间 [root@test1 ~]# date -s '2022-2-22 22:22:22' Tue Feb 22 22:22:22 CST 2022 //查看当前时间 [root@test1 ~]# date Tue Feb 22 22:22:22 CST 2022 4.常用输出 4.1 +%F 输出日期 [root@test ~]# date +%F 2018-08-29 4.2 +%T 输出时间 [root@test1 ~]# date +%T 10:08:38 4.3 +%j 输出当前天是一年中的第几天 [root@test1 ~]# date +%j 251 4.4 +%w 输出星期 ⚠️0表示周日 [root@test1 ~]# date +%w 1 4.5 +%s 1970-01-01 00:00:00 开始到现在经过的秒数 [root@test1 ~]# date +%s 1535508552 5.其他输出 5.1年份相关 5.1.1 +%Y 输出年份(4位数) [root@test1 ~]# date +%Y 2018 5.1.2 +%y 输出年份(00-99表示) [root@test1 ~]# date +%y 18 5.2月份相关 5.2.1 +%m 输出月份(0-12表示) [root@test1 ~]# date +%m 08 5.2.2 +%b 月份英文缩写 [root@test1 ~]# date +%b Aug 5.2.3 +%B 月份英文全写 [root@test1 ~]# date +%B August 5.3日期相关 5.3.1 +%w 输出星期(0代表周日) [root@test1 ~]# date +%w 3 5.3.2 +%c 输出日期(与date命令输出稍微有差别) [root@test1 ~]# date +%c Wed 29 Aug 2018 10:11:12 AM CST [root@test1 ~]# date Wed Aug 29 10:11:12 CST 2018 5.3.3 +%d 输出日期(1-31表示) [root@test1 ~]# date +%d 29 5.3.4 +%D 输出日期(月/日/年) [root@test1 ~]# date +%D 08/29/18 5.4星期相关 5.4.1 +%a 输出星期(英文缩写) [root@test1 ~]# date +%a Wed 5.4.2 +%A 输出星期(英文全称) [root@test1 ~]# date +%A Wednesday 5.4.3 +%W 输出星期(数字表示) [root@test1 ~]# date +%w 3 5.5小时相关 5.5.1 +%H、+%k 输出小时(00-23表示) [root@test1 ~]# date +%H 10 [root@test1 ~]# date +%k 10 5.5.2 +%l 输出小时(01-12表示) [root@test1 ~]# date +%l 10 5.6分钟相关 5.6.1 +%M 输出分钟(00-59表示) [root@test1 ~]# date +%M 30 5.7秒数相关 5.7.1 +%S 输出秒数 [root@test1 ~]# date +%S 28 5.7.2 +%N 输出纳秒 纳秒nanoseconds (000000000..999999999) [root@test1 ~]# date +%N 121213066 5.8时区相关 5.8.1 +%Z 输出时区 CST表示中部标准时间 [root@test1 ~]# date +%Z CST 5.9其他相关 5.9.1 +%P、+%p 输出AM或者PM [root@test1 ~]# date +%p PM [root@test1 ~]# date +%P pm 5.9.2 +%r、+%X 输出时间(含时分秒，小时以12小时AM/PM来表示) [root@test1 ~]# date +%r 10:40:15 AM [root@test1 ~]# date +%X 10:40:25 AM 5.9.3 +%x 以月/日/年输出日期 [root@test1 ~]# date +%x 08/29/2018 5.9.4 +%n 输出时显示新的一行 //注意有两行 [root@test1 ~]# date +%n [root@test1 ~]# 5.9.5 +%t 输出时插入tab //有一个空行 [root@test1 ~]# date +%t [root@test1 ~]# 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux命令/xargs命令.html":{"url":"linux/linux命令/xargs命令.html","title":"xargs命令","keywords":"","body":"[toc] xargs命令 1.命令说明 xargs命令是给其他命令传递参数的一个过滤器，也是组合多个命令的一个工具。它擅长将标准输入数据转换成命令行参数，xargs能够处理管道或者stdin并将其转换成特定命令的命令参数。xargs也可以将单行或多行文本输入转换为其他格式，例如多行变单行，单行变多行。xargs的默认命令是echo，空格是默认定界符。这意味着通过管道传递给xargs的输入将会包含换行和空白，不过通过xargs的处理，换行和空白将被空格取代。xargs是构建单行命令的重要组件之一 2.命令选项 2.1 -d 指定分隔符 [root@test1 test]# echo 123@123@123|xargs 123@123@123 [root@test1 test]# echo 123@123@123|xargs -d@ 123 123 123 2.2 -n 指定参数输出列数 [root@test1 test]# echo 123 123 123 |xargs -n1 123 123 123 [root@test1 test]# echo 123 123 123 |xargs -n2 123 123 123 [root@test1 test]# echo 123 123 123 |xargs -n3 123 123 123 2.3 -p 询问是否执行命令 使用该选项之后xargs并不会马上执行其后面的命令，而是输出即将要执行的完整的命令(包括命令以及传递给命令的命令行参数)，询问是否执行，输入 y 才继续执行，否则不执行 [root@test1 test]# echo 123 | xargs -p sed 's#1#9#' sed s#1#9# 123 ?... 2.4 -E 指定一个字符串 当xargs解析出多个命令行参数的时候，如果搜索到-E指定的命令行参数，则只会将-E指定的命令行参数之前的参数(不包括-E指定的这个参数)传递给xargs后面的命令 ⚠️注意：-E只有在xargs不指定-d的时候有效，如果指定了-d则不起作用，而不管-d指定的是什么字符，空格也不行 [root@test1 test]# echo '11 22 33' | xargs -E '33' echo 11 22 指定-d选项，-E选项就失效了 [root@test1 test]# echo '11 22 33' | xargs -d ' ' -E '33' echo 11 22 33 2.5 -i 使管道前命令结果成为后续操作命令的参数 2.6 -0(数字0) 识别find结束标记 1.当前路径下的文件，文件名中包含空格 [root@test1 test]# ll total 0 -rw-r--r-- 1 root root 0 Aug 21 05:16 abc 01.jpg -rw-r--r-- 1 root root 0 Aug 21 05:16 abc 02.jpg -rw-r--r-- 1 root root 0 Aug 21 05:16 abc 03.jpg 2.find命令查找当前路径下这三个文件，会报错，因为find会认为含有空格的文件为两个文件 3.解决方法 方式一 让find命令给每个文件名的结束处加上一个结束标记 find . -type f -name \"*.jpg\" -print0|xargs -0 ls -l -print0 加上结束标记 xargs0 识别结束标记 方式二 find . -type f -name \"*.jpg\" |xargs -i ls -l {} 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux命令/rpm命令.html":{"url":"linux/linux命令/rpm命令.html","title":"rpm命令","keywords":"","body":"[toc] rpm命令 1.命令说明 rpm命令是RPM软件包的管理工具。 rpm原本是Red Hat Linux发行版专门用来管理Linux各项套件的程序，由于它遵循GPL规则且功能强大方便，因而广受欢迎。逐渐受到其他发行版的采用。 RPM套件管理方式的出现，让Linux易于安装，升级，间接提升了Linux的适用度。 2.命令格式 rpm 选项 参数 3.选项 3.1安装参数 -i 安装软件包 -v 显示详细信息 -h 显示安装进度 --nodeps 不验证软件包的依赖性 --force 强制安装，即使覆盖其他包的文件也安装 3.2卸载参数 -e 卸载软件包 3.3升级参数 -U 升级软件包 3.4查询参数 -a 查询所有 -q 查询已安装软件包 -c 仅列出配置文件 -l 列出包中文件信息，需配合-q参数 -f 查询指定文件属于哪个软件包，需配合-q参数 -p 查询未安装软件包信息 -i 查询软件包详细信息 -R 查询软件包依赖性 4.rpm软件包信息 rpm 包名字结构： glibc-2.17-196.el7_4.2.x86_64 glibc -2 .17 -196 -el7 x86 64 软件名 主版本号 次版本号 修订号 RHEL7 CPU架构平台 支持系统位数 5.rpm包中文件的提取 模拟cat命令被删除再到恢复 1.查找cat命令属于哪个文件 [root@test1 ~]# which cat /bin/cat 2.删除cat命令文件 [root@test1 ~]# rm -rf /bin/cat 3.rpm -qf /bin/cat [root@test1 ~]# rpm -qf /bin/cat coreutils-8.4-46.el6.x86_64 4.挂载光盘 [root@test1 ~]# mount /dev/sr0 /mnt mount: block device /dev/sr0 is write-protected, mounting read-only 5.恢复文件 [root@test1 ~]# rpm2cpio /mnt/Packages/coreutils-8.4-46.el6.x86_64.rpm |cpio -idv ./bin/cat ./bin/cat 25240 blocks rpm2cpio 将rpm包转换为cpio格式 cpio 是一种标准工具，它用于创建软件档案文件和从文件档案中提取文件 -i：copy-in模式 还原 -d：还原时自动新建目录 -v：显示还原过程 6.将当前目录恢复的cat文件移动到/bin/即可 [root@test1 ~]# cp bin/cat /bin 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux命令/echo命令.html":{"url":"linux/linux命令/echo命令.html","title":"echo命令","keywords":"","body":"[toc] echo命令 1.命令说明 echo命令用于在shell中打印shell变量的值，或者直接输出指定的字符串 2.命令格式 echo [选项] [参数] 3.常用选项 3.1 -n 不输出换行 [root@aliyun ~]# echo -n hehe hehe[root@aliyun ~]# 3.2 -e 使转移字符生效 \\n 换行且光标移至行首 \\c 最后不加上换行符号 \\t 插入tab \\e 转义 \\b 删除前一个字符 \\v 输出垂直制表符，与\\f输出结果相同 \\a 发出警告声 \\r 光标移至行首，但不换行 3.2.1 \\n 换行 [root@exercise1 ~]# echo -e 'hehe\\nhehe' hehe hehe 3.2.2 \\t 输出制表符 [root@exercise1 ~]# echo -e 'hehe\\thehe' hehe hehe 3.2.3 \\c 不换行 [root@exercise1 ~]# echo -e 'hehehehe\\c' hehehehe[root@exercise1 ~]# 3.2.4 \\v 垂直制表符 [root@exercise1 ~]# echo -e 'hehe\\vhehe' hehe hehe 3.2.5 \\e 转义 等同于\\033 \\e写法 \\033写法 4.bash里面的颜色 4.1设置前景颜色 写法 含义 [30m 将字符的显示颜色改为黑色 [31m 将字符的显示颜色改为红色 [32m 将字符的显示颜色改为绿色 [33m 将字符的显示颜色改为淡黄色 [34m 将字符的显示颜色改为蓝色 [35m 将字符的显示颜色改为紫色 [36m 将字符的显示颜色改为天蓝色 [37m 将字符的显示颜色改为灰色 4.2设置背景颜色 写法 含义 [40m 将背景色设置为黑色 [41m 将背景色设置为红色 [42m 将背景色设置为绿色 [43m 将背景色设置为淡黄色 [44m 将背景色设置为蓝色 [45m 将背景色设置为紫色 [46m 将背景色设置为淡蓝色 [47m 将背景色设置为灰色 windows终端下的效果 mac终端下的效果 4.3其他设置 编码 颜色/动作 0 重新设置属性到缺省设置 1 设置粗体 2 设置一半亮度（模拟彩色显示器的颜色） 4 设置下划线（模拟彩色显示器的颜色） 5 设置闪烁 7 设置反向图象 22 设置一般密度 24 关闭下划线 25 关闭闪烁 27 关闭反向图象 写法示例 5;31m 5表示设置闪烁 31m表示设置字体颜色为红色 [root@exercise1 ~]# echo -e \"\\033[5;31m呵呵\\033[0m\" 使用多个颜色设置的时候，使用分号分隔即可 \\033[31;47;1mhello world\\033[0m 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux命令/find命令.html":{"url":"linux/linux命令/find命令.html","title":"find命令","keywords":"","body":"[toc] find命令 1.命令说明 find命令用来在指定目录下查找文件。 任何位于参数之前的字符串都将被视为欲查找的目录名。如果使用该命令时，不设置任何参数，则find命令将在当前目录下查找子目录与文件。并且将查找到的子目录和文件全部进行显示 2.命令格式 find 查找范围 选项 操作 查找范围：默认当前目录 操作：默认输出到终端 3.命令选项 3.1 -name 按照文件名查找 [root@exercise1 ~]# pwd /root [root@exercise1 ~]# find -name *.log ./install.log 3.2 -size 按照文件大小查找 符号 + 大于 - 小于 不加符号是等于 文件大小单元 b ——>块（512字节） c ——>字节 w ——>字（2字节） k ——> 千字节 M ——>兆字节 G ——> G字节 //查找当前目录大小为110K的文件 [root@exercise1 tmp]# ll -h total 144K -rw-r--r-- 1 root root 11K Aug 9 13:35 1.1 -rw-r--r-- 1 root root 15K Aug 9 13:35 2.2 -rw-r--r-- 1 root root 110K Aug 9 13:35 3.3 -rw-r--r-- 1 root root 580 Aug 9 13:39 ip_date.txt [root@exercise1 tmp]# find . -size 110k ./3.3 //查找当前目录小于100K的文件 [root@exercise1 tmp]# ll -h total 144K -rw-r--r-- 1 root root 11K Aug 9 13:35 1.1 -rw-r--r-- 1 root root 15K Aug 9 13:35 2.2 -rw-r--r-- 1 root root 110K Aug 9 13:35 3.3 -rw-r--r-- 1 root root 812 Aug 9 13:41 ip_date.txt [root@exercise1 tmp]# find . -size -100k . ./1.1 ./ip_date.txt ./2.2 //查找当前目录大于100K的文件 [root@exercise1 tmp]# ll -h total 144K -rw-r--r-- 1 root root 11K Aug 9 13:35 1.1 -rw-r--r-- 1 root root 15K Aug 9 13:35 2.2 -rw-r--r-- 1 root root 110K Aug 9 13:35 3.3 -rw-r--r-- 1 root root 928 Aug 9 13:42 ip_date.txt [root@exercise1 tmp]# find . -size +100k ./3.3 //查找当前目录大于10k小于100k的文件 [root@exercise1 tmp]# ll -h total 144K -rw-r--r-- 1 root root 11K Aug 9 13:35 1.1 -rw-r--r-- 1 root root 15K Aug 9 13:35 2.2 -rw-r--r-- 1 root root 110K Aug 9 13:35 3.3 -rw-r--r-- 1 root root 1.1K Aug 9 13:43 ip_date.txt [root@exercise1 tmp]# find . -size +10k -size -100k ./1.1 ./2.2 3.3 -user 按照文件所有者查找 //查找文件所有者为gun的文件 [root@exercise1 ~]# find / -user gun find: `/proc/5507/task/5507/fd/5': No such file or directory find: `/proc/5507/task/5507/fdinfo/5': No such file or directory find: `/proc/5507/fd/5': No such file or directory find: `/proc/5507/fdinfo/5': No such file or directory /var/spool/mail/gun /home/gun /home/gun/.bash_logout /home/gun/.bash_profile /home/gun/.bashrc //查找文件的时候会有报错 上面的例子中，5507就是运行find命令时，find命令的PID，只在运行期间出现，等find命令运行完成之后，就会消失，这并不是一个实际错误 //解决方法，将错误输出重定向 [root@exercise1 ~]# find / -user gun 2>/dev/null /var/spool/mail/gun /home/gun /home/gun/.bash_logout /home/gun/.bash_profile /home/gun/.bashrc 3.4 -perm 按照文件权限查找 3.4.1 mode 表示精确匹配 //查找权限为755的文件或目录 [root@exercise1 test]# ll total 12 drwxr-xr-x 2 root root 4096 Aug 9 15:22 dir1 drwxrwxrwx 2 root root 4096 Aug 9 15:22 dir2 drwxr-xrw- 2 root root 4096 Aug 9 15:23 dir3 -rw-r--r-- 1 root root 0 Aug 9 15:38 file [root@exercise1 test]# find . -perm 755 . ./dir1 3.4.2 -mode 表示权限每一位至少匹配 //示例：find . -perm -111 表示所有者，所属组，其他人都至少有执行权限 [root@exercise1 test]# ll total 12 drwxr-xr-x 2 root root 4096 Aug 9 2018 dir1 drwxrwxrwx 2 root root 4096 Aug 9 2018 dir2 drwxr-xrw- 2 root root 4096 Aug 9 2018 dir3 -rwxr--r-- 1 root root 0 Aug 9 2018 file [root@exercise1 test]# find . -perm -111 . ./dir1 ./dir2 3.4.3 +mode 表示权限只要有一位匹配即可 //示例：find . -perm +111 表示所有者，所属组，其他人任意一个有执行权限就可以 [root@exercise1 test]# ll total 12 drwxr-xr-x 2 root root 4096 Aug 9 2018 dir1 drwxrwxrwx 2 root root 4096 Aug 9 2018 dir2 drwxr-xrw- 2 root root 4096 Aug 9 2018 dir3 -rwxr--r-- 1 root root 0 Aug 9 2018 file [root@exercise1 test]# find . -perm +111 . ./dir3 ./dir1 ./file ./dir2 3.5 -type 按照文件类型查找 文件类型 f 普通文件 d 目录 l 链接文件 c 字符设备 b 块设备 s 套接字文件 p 管道 //示例，查找当前目录下的文件 [root@exercise1 test]# ll total 16 drwxr-xr-x 2 root root 4096 Aug 9 2018 dir1 drwxrwxrwx 2 root root 4096 Aug 9 2018 dir2 drwxr-xrw- 2 root root 4096 Aug 9 2018 dir3 -rwxr--r-- 1 root root 0 Aug 9 2018 file -rw-r--r-- 1 root root 178 Aug 7 14:19 hehe [root@exercise1 test]# find . -type f ./hehe ./file 3.6 按照时间戳查找 +n 表示最近一次修改是在n天之前(常用，用于删除n天前的日志) -n 表示最近一次修改是在n天之内 -atime access 访问时间，指文件被访问的时间 -ctime change 改变时间，指文件属性被改变 -mtime modify 修改时间，指文件内容被修改 //示例，查找/tmp下文件修改时间是在3天之前的 [root@exercise1 ~]# find /tmp/ -mtime +3 /tmp/systemd-private-81d53e50debf4c9db3563ac49d9d3d6a-php-fpm.service-8iRh7w /tmp/systemd-private-81d53e50debf4c9db3563ac49d9d3d6a-php-fpm.service-8iRh7w/tmp /tmp/hsperfdata_root /tmp/.XIM-unix /tmp/.font-unix /tmp/supervisord.pid /tmp/mysql.sock /tmp/.X11-unix /tmp/.ICE-unix /tmp/supervisor.sock /tmp/systemd-private-81d53e50debf4c9db3563ac49d9d3d6a-ntpd.service-lCUH17 /tmp/systemd-private-81d53e50debf4c9db3563ac49d9d3d6a-ntpd.service-lCUH17/tmp /tmp/.Test-unix 3.7 -name 按照文件名查找 //查找/tmp中以.txt结尾的文件 [root@exercise1 test]# find /tmp -type f -name *.txt /tmp/test/3.txt /tmp/test/1.txt /tmp/test/2.txt /tmp/test/6/3.txt /tmp/test/6/1.txt /tmp/test/6/5.txt /tmp/test/6/2.txt /tmp/test/6/4.txt /tmp/test/6/6.txt /tmp/ip_date.txt 3.8 -regex 基于正则表达式匹配文件 [root@exercise1 test]# ls 1.pdf 1.py 1.txt [root@exercise1 test]# find . -regex \".*\\(\\.txt\\|\\.pdf\\)$\" ./1.pdf ./1.txt 3.9 -maxdepth 向下最大深度限制为n(n代笔数字) //当前目录 [root@exercise1 test]# tree . . ├── a │ └── aa │ └── aaa ├── b │ └── bb │ └── bbb └── c └── cc 8 directories, 0 files //指定最大查找深度为2 [root@exercise1 test]# find . -maxdepth 2 . ./b ./b/bb ./c ./c/cc ./a ./a/aa 3.10 -mindepth 搜索出深度距离当前目录至少n个子目录的所有文件 [root@exercise1 test]# tree . . ├── a │ └── aa │ └── aaa ├── b │ └── bb │ └── bbb └── c └── cc 8 directories, 0 files [root@exercise1 test]# find . -mindepth 2 ./b/bb ./b/bb/bbb ./b/bb/bbb/b.txt ./c/cc ./c/cc/c.txt ./a/aa ./a/aa/aaa ./a/aa/aaa/a.txt [root@exercise1 test]# find . -mindepth 1 ./b ./b/bb ./b/bb/bbb ./b/bb/bbb/b.txt ./c ./c/cc ./c/cc/c.txt ./a ./a/aa ./a/aa/aaa ./a/aa/aaa/a.txt [root@tencent test]# 3.11 find逻辑运算符 符号 作用 -a 与 -o 或 -not或者! 非 4.操作 4.1 -exec 执行命令 {}表示前边匹配的内容 \\;是固定格式 查找/tmp以.txt结尾的文件并删除 [root@exercise1 ~]# ls /opt 1.txt 2.txt 3.txt [root@exercise1 ~]# find /opt -type f -name *.txt -exec rm -rf {} \\; [root@exercise1 ~]# ls /opt 4.2 -ok 功能与-exec相同，执行命令前会提示是否执行 查找/tmp以.txt结尾的文件并删除 [root@exercise1 ~]# find /opt -type f -name *.txt -ok rm -rf {} \\; ? find动作处理 动作 含义 -print 打印查找到的内容(默认) -ls 以长格式显示的方式打印查找到的内容 -delete 删除查找到的文件(仅能删除空目录) -ok 后面跟自定义 shell 命令(会提示是否操作) -exec 后面跟自定义 shell 命令(标准写法 -exec \\;) 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux命令/计划任务总结.html":{"url":"linux/linux命令/计划任务总结.html","title":"计划任务命令","keywords":"","body":"[toc] 计划任务总结 1.crontab命令 1.1命令说明 crontab命令的功能是在指定的时间间隔执行定义好的命令或脚本 1.2命令格式 crontab 选项 参数 1.3选项 -e 编辑计划任务 相当于 vim /var/spool/cron/root -l 查看计划任务 -r 清除计划任务 慎用！！！ -u user 指定要设定计划任务的用户 1.4格式 字段含义 minute hour day month week command 顺序：分 时 日 月 周 字段说明 字段 含义 minute 表示分钟，可以是从0到59之间的任何整数 hour 表示小时，可以是从0到23之间的任何整数 day 表示日期，可以是从1到31之间的任何整数 month 表示月份，可以是从1到12之间的任何整数 week 表示星期几，可以是从0到7之间的任何整数，这里的0或7代表星期日 command 要执行的命令，可以是系统命令，也可以是自己编写的脚本文件 在以上各个字段中，还可以使用以下特殊字符 符号 含义 星号（*） 代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作 逗号（,） 可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9” 中杠（-） 可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6” 正斜线（/） 可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10，如果用在minute字段，表示每十分钟执行一次 1.4.1使用示例 每分钟执行ntpdate命令同步时间 * * * * * /usr/sbin/ntpdate ntp2.aliyun.com >/dev/null 2>&1 每小时的第3,5分钟执行 3,5 * * * * command 上午的8点到11点的第3和第15分执行 3，15 8-11 * * * command 每隔两天的上午8点到11点的第3和第15分钟执行 3,15 8-11 */2 * * command 每天21点30分执行 30 21 * * * command 每周六、日上午10点10分执行 10 10 * * 6,7 command 每天12点执行 00 12 * * * command 1.5配置文件 1.5.1 /etc/crontab 主配置文件 SHELL=/bin/bash PATH=/sbin:/bin:/usr/sbin:/usr/bin MAILTO=root HOME=/ # For details see man 4 crontabs # Example of job definition: # .---------------- minute (0 - 59) # | .------------- hour (0 - 23) # | | .---------- day of month (1 - 31) # | | | .------- month (1 - 12) OR jan,feb,mar,apr ... # | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat # | | | | | # * * * * * user-name command to be executed //配置文件说明 SHELL=/bin/bash #SHELL变量指定了系统要使用哪个shell，这里是bash PATH=/sbin:/bin:/usr/sbin:/usr/bin #PATH变量指定了系统执行命令的路径 MAILTO=root #MAILTO变量指定了crond的任务执行信息将通过电子邮件发送给root用户，如果MAILTO变量的值为空，则表示不发送任务执行信息给用户 HOME=/ #HOME变量指定了在执行命令或者脚本时使用的主目录 1.5.2 /etc/cron.deny 默认情况下普通用户可以使用crontab命令，此文件中的用户不能使用crontab命令编辑计划任务 [root@localhost ~]# cat /etc/cron.deny u1 写入到/etc/cron.deny文件中的用户将不能使用crontab命令编辑计划任务 [u1@localhost ~]$ crontab -e You (u1) are not allowed to use this program (crontab) See crontab(1) for more information 1.5.3 /etc/cron.allow 此文件中的用户可以使用crontab命令编辑计划任务，此文件优先级比/etc/cron.deny高，两个文件同时存在，以allow文件为主 [root@localhost ~]# cat /etc/cron.allow u1 1.5.4 /var/spool/cron 用户的计划任务文件目录 [root@localhost cron]# pwd /var/spool/cron [root@localhost cron]# ls root u1 2.at命令 2.1命令说明 at命令只是想要让特定任务运行一次 ctrl+d退出at命令编辑 2.2命令格式 at 选项 时间 2.3选项 -V 显示版本号 [root@localhost tmp]# at -V at version 3.1.10 -f file 读取文件，at不一定非要通过交互式输入来指定操作，也可以读取脚本文件 [root@localhost ~]# cat a.sh #!/bin/bash # touch /tmp/a.txt [root@localhost ~]# at -f /root/a.sh 15:30 #15点30分执行/root/a.sh job 8 at 2018-08-11 15:30 -l 列出所有的指定，也可以使用atq命令 [root@localhost ~]# at -l 8 2018-08-11 15:30 a root [root@localhost ~]# atq 9 2018-08-12 15:31 a root -d 删除任务 2.4使用示例 时间 写法示例 说明 Minute 命令 at now + 5 minutes 任务在5分钟后运行 Hour 命令 at now + 1 hour 任务在1小时后运行 Days 命令 at now + 3 days 任务在3天后运行 Weeks 命令 at now + 2 weeks 任务在两周后运行 Fixed 命令 at midnigt 任务在午夜运行 Fixed 命令 at 10:30 pm 任务在晚上10点30分运行 Fixed 命令 at 23:59 12/31/2018 任务在2018年12月31号23点59分运行 2.5at一次性计划任务文件 2.5.1 /var/spool/at at一次性计划任务文件 [root@localhost at]# pwd /var/spool/at [root@localhost at]# ls a00009018621a3 a0000b01861c98 a0000c01861f2c spool a00009018621a3 a0000b01861c98 a0000c01861f2c就是at一次性计划任务文件 只能看到具体命令，无法查看具体执行时间 2.5.2 deny,allow文件，作用与crond一样，allow文件优先于deny文件 /etc/at.deny /etc/at.allow 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux命令/归档压缩命令.html":{"url":"linux/linux命令/归档压缩命令.html","title":"归档压缩命令","keywords":"","body":"[toc] 归档压缩命令 1.归档命令tar 1.1命令说明 用来压缩和解压文件，tar本身不具有压缩功能，他是调用压缩功能实现的 1.2命令格式 tar [option] 归档后的文件名 要归档的文件 1.3选项 1.3.1压缩选项(都必须配合-f选项) ⚠️压缩选项前边的-可以不加 -f 使用归档文件 -c 建立一个压缩文件 -h 不压缩链接文件，压缩链接文件源文件 不加-h选项，打包链接文件解压后会造成断链 加-h选项后，打包链接文件并解压就没有问题了 -z 打包后调用gzip压缩 -j 打包后调用bzip2压缩 -r/-u 向压缩文件末尾追加文件，更新压缩文件 --exclude 指定不打包文件 -P 不显示从成员名中删除 不加-P的效果 可以看到，加上-P选项之后不提示tar: Removing leading `/' from member names -p 保持源文件属性不变 1.3.2解压缩选项 -x 解压缩文件，可以使用xf直接解压缩，系统会自动解压，也可以使用-z、-j选项解压缩文件 -C 将压缩归档文件解压到哪个位置 1.3.3查看压缩选项 -t 查看压缩文件中的文件 2.压缩命令gzip 2.1命令说明 用来压缩文件 2.2命令格式 gzip [option] file 2.3选项 2.3.1压缩选项 -c 保留源文件，需要结合重定向符号 不加-c选项，压缩文件后源文件没有被保留 gzip -c 源文件 > /xx/xx.gz -n n表示数字，用于指定压缩比，范围是1-9，压缩比越大压缩时间越长，默认是6 2.3.2解压缩选项 -d 解压缩 2.3.3查看压缩选项 使用zcat命令查看.gz压缩文件的内容 2.3.4其他选项 -v 显示详细信息 -t 检测压缩文件正确性 -V 显示gzip版本信息 3.压缩命令bzip2 3.1命令说明 bzip2命令用来解压缩文件，压缩比比bzip大 3.2命令格式 gzip [option] file 3.3选项 3.3.1压缩选项 -k 保留源文件 不加-k选项，压缩文件后源文件没有被保留 bzip2 -k 源文件，压缩后保留源文件 -n n表示数字，用于指定压缩比，范围是1-9，压缩比越大压缩时间越长，默认6 3.3.2解压缩选项 -d 解压缩 3.3.3查看压缩选项 使用bzcat查看.bz2压缩文件 3.3.4其他选项 -v 显示详细信息 -t 检测压缩文件正确性 -V 显示bzip2版本信息 总结 类型 解压方法 *.tar 用tar -xvf解压 *.gz 用gzip -d或gunzip解压 *.tar.gz 和 *.tgz 用tar -xzf解压 *.xz 用tar -jxvf解压 *.bz2 用bzip2 -d或bunzip2解压 *.tar.bz2 用tar -xjf解压 *.Z 用uncompress解压 *.tar.Z 用tar -xZf解压 *.rar 用unrar -e解压 *.zip 用unzip解压 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux命令/磁盘相关命令.html":{"url":"linux/linux命令/磁盘相关命令.html","title":"磁盘相关命令","keywords":"","body":"[toc] 磁盘相关命令 1.fdisk 1.1命令说明 fdisk是一个磁盘操作工具，主要操作2T以下的磁盘 1.2命令格式 fdisk 选项 设备名 1.3选项 -u 磁盘分区的时候以扇区为单位，默认是柱面 -l 查看磁盘信息 -c 关闭dos兼容模式 ⚠️不加-c选项就会提示不支持dos兼容模式 1.4分区说明 1.4.1 frisk 设备名 分区界面后的选项 进入分区界面后按m键会提示帮助信息，红色字体为常用选项 Command (m for help): m Command action a toggle a bootable flag #切换一个可启动的标志 b edit bsd disklabel #编辑bsd磁盘标签 c toggle the dos compatibility flag #切换dos兼容标志 d delete a partition #删除分区 l list known partition types #已知的分区表类型 m print this menu #显示帮助菜单 n add a new partition #创建一个新的分区 o create a new empty DOS partition table #创建一个新的空DOS分区表 p print the partition table #打印分区表 q quit without saving changes #不保存退出 s create a new empty Sun disklabel #创建一个新的空sun磁盘标签 t change a partition's system id #改变分区的系统id u change display/entry units #改变显示输入单元 v verify the partition table #验证分区表 w write table to disk and exit #保存退出 x extra functionality (experts only) #额外的功能 1.4.2创建一个主分区 第一步、fdisl+设备名进入分区界面，按n会提示创建主分区还是扩展分区 第二步、按p，然后指定主分区号为1 第三步、起始扇区默认即可，然后大小指定10M 第四步、查看创建的主分区 1.4.3创建扩展分区和逻辑分区 逻辑分区依赖于扩展分区，逻辑分区编号从5开始，扩展分区只能有一个 第一步、按n创建分区后按e选择扩展分区，并指定分区编号为2 第二步、起始扇区大小默认即可，将磁盘剩余的空间全部给扩展分区 第三步、按n，然后按l创建逻辑分区，并指定大小为50M 第四步、查看刚创建的扩展分区和逻辑分区 2.parted命令 2.1命令说明 parted命令主要用于对2T以上的磁盘进行分区操作，支持MBR分区表（只能有4个主分区），支持GPT分区表（主分区可以有多个），parted对磁盘的修改是实时生效的！ 2.2命令格式 parted 设备名 2.3常用命令 print 显示分区信息 mktable/mklabel 创建磁盘分区表 mkpart 创建分区 rm 删除分区 q 退出不保存 2.4parted交互式创建分区 第一步、指定分区表类型 mklable gpt 第二步、创建分区 mkpart primary 0 10 创建一个10M的主分区，分区名称任意 第三步、查看创建的分区 2.5parted非交互式创建分区 第一步、指定分区表类型 parted /dev/sdc mklabel gpt 第二步、创建主分区 parted /dev/sdc mkpart 1 100% 第三步、查看分区 parted /dev/sdc p 3.增加交换分区 3.1增加swap 使用dd命令创建一个文件 dd if=/dev/zero of=/tmp/file bs=1M count=10 3.2把创建的文件变为swap mkswap 文件名 3.3激活swap swapon 文件名 3.4挂载swap ⚠️⚠️⚠️一定要用追加>> echo \"/tmp/swap1 swap swap defaults 0 0\" >>/etc/fstab /tmp/swap1 swap swap defaults 0 0 设备名 挂载点 文件系统 参数选项 是否备份 是否开机检测 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux命令/Linux权限总结.html":{"url":"linux/linux命令/Linux权限总结.html","title":"Linux权限总结","keywords":"","body":"[toc] linux权限总结 一、linux普通权限 1.1 文件权限查看 [root@exercise1 ~]# ll -rw-r--r--. 1 root root 7570 Aug 4 13:15 install.log.syslog drwxr-xr-x 9 root root 4096 Aug 8 12:07 test 1.2 用户分组权限概念 -rw-r--r-- 权限 含义 第1位 - 表示文件类型，-表示普通文件，d表示目录 第2-4位 rw- 表示文件所有者权限 第5-7位 r-- 表示文件所属组权限 第8-10位 r-- 表示文件其他人权限 1.3 权限位含义 权限位 含义 r 读 w 写 x 执行 1.4 权限位数字表示法 读+写+执行=4+2+1=7 权限位 数字 含义 r 4 读 w 2 写 x 1 执行 1.5 权限对文件及目录含义 1.5.1 文件rwx权限 权限位 含义 r 读取文件内容 w 修改文件内容 需要r权限配合只有w权限的时候 强制保存退出会导致源文件内容丢失 x 表示是否执行 需要r权限配合 1.5.2 目录rwx权限 权限位 含义 r 查看目录内容 相当于ls 需要x权限配合 w 是否能删除目录内容 是否能在目录中创建文件 重命名 目录中的文件 x 是否能进入到目录 是否能查看目录中文件的属性 二、linux特殊权限 2.1 suid 用户在运行命令的时候相当于root用户 设置方法 u+s 示例说明 1.普通用户 www 无法使用 less 命令查看系统日志 /var/log/messages 2.给 /usr/bin/less 设置 suid 设置 suid 后文件权限所有者处就变为 rws ，多了一个 s 权限，并且文件底色变成了红色 使用命令 stat 查看文件属性，此时文件权限位是 4755 3.为 /usr/bin/less 设置 suid 后 www 用户就可以查看系统日志了 2.2 sgid 用户若对此目录具有r与x的权限时，该用户能够进入此目录 用户在此目录下的有效用户组将会变成该目录的用户组 若用户在此目录下具有w的权限，则用户创建新文件的用户组与此目录的用户组相同 设置方法 g+s 示例说明 1.普通用户 www 对 /tmp 目录有 777 权限，在没有设置 /tmp 的 sgid 时，www 用户在此创建的文件和目录属组是本身，即 www 2.为 /tmp 目录设置 sgid 后，www 用户在 /tmp 下创建的文件和目录属组就是 root 2.3 sbit sticky粘滞位 当一个用户对某目录是具有用户组或其他人的身份，并具有w权限(即具有写入的权限时)，这表明该用户可以对该目录下任何人新建的目录或文件进行删除、移动、重命名等操作。不过，如果该目录具有SBIT权限时，则仅有文件属主和root才能删除、移动、重命名此文件，普通用户无法删除该目录下不属于自己的文件 设置方法 o+t 示例说明 1.在没有设置 sbit 时，普通用户 www 可以删除 /tmp 下属主属组不是自己的文件和目录 2.设置 sbit 后，www 用户只能删除文件所有者是自己的文件 设置 sbit 后，文件权限其他人处变为了 rwt 使用命令 stat 查看 /tmp 权限，此时为 1777 此时， www 用户无法删除文件所有者不是自己的文件 三、linux隐藏权限 3.1 权限位 a (append) 只能追加和查看，其他操作都无法执行 i (immutable) 不可变，只能查看，其他操作都无法执行 3.2 设置隐藏权限命令 chattr +增加 -取消 为文件添加隐藏权限 a 后,可以看到，文件只能被追加和查看，其他操作无法执行 为文件添加隐藏权限 i 后，可以看到，文件只能被查看，其他操作无法执行 3.3 查看隐藏权限 lsattr 四、FACL Filesystem Acess FACL是一种权限分配之外的普遍方式，例如，默认情况下你需要确认3个权限组，owner、group、other，而使用FACL，利用文件扩展属性保存额外的访问控制权限，你可以增加权限给其他用户或组，而不单只是简单的other或者是拥有者不存在的组别，可以允许指定的用户拥有写权限而不再是让他们整个组拥有写权限 4.1 FACL格式 [u|g]：[用户名|组名]：权限 文件 例如 u:hehe:rwx file 对于文件file，用户hehe有rwx权限 4.2 设置FACL setfacl 选项 含义 -m 设置FACL权限 -x 取消FACL权限 -R 递归设置，-R需要写在-m选项前边 -b 删除全部FACL权限 设置FACL示例 /test 目录权限为 750 ，其他人没有任何权限，但是现在想让用户 www 拥有 rw 权限 1.没有设置FACL之前，www 用户无法进入 /test 目录，无法查看 /test 目录内容 2.为 www 用户设置 /test 的FACL setfacl -m u:www:r-x /test 3.验证，设置FACL之后，只有 www 这一个用户对 /test 目录拥有 rx 权限，其他普通用户没有权限 取消FACL示例 取消FACL，-x选项，与设置FACL不同，取消的时候格式中不用再加权限 4.3 查看FACL getfacl 没有设置FACL前 设置FACL后 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux命令/Linux用户登录记录日志和相关查看命令.html":{"url":"linux/linux命令/Linux用户登录记录日志和相关查看命令.html","title":"Linux用户登录记录日志和相关查看命令","keywords":"","body":"[toc] Linux用户登录记录日志和相关查看命令总结 1.Linux用户登录信息放在三个文件中 utmp、wtmp、btmp文件 1.1/var/run/utmp 记录当前正在登录系统的用户信息，默认由who和w记录当前登录用户的信息，uptime记录系统启动时间； who命令和w命令及uptime命令输出 //who命令 [root@tencent ~]# who root pts/0 2018-11-08 17:17 (12.66.1.11) //w命令 [root@tencent ~]# w 21:02:03 up 18 days, 23:04, 1 user, load average: 0.00, 0.02, 0.07 USER TTY FROM LOGIN@ IDLE JCPU PCPU WHAT root pts/0 12.66.1.11 17:17 3.00s 0.32s 0.32s -bash //uptime命令 [root@tencent ~]# uptime 21:02:46 up 18 days, 23:04, 1 user, load average: 0.00, 0.01, 0.06 1.2/var/log/wtmp 记录当前正在登录和历史登录系统的用户信息，默认由last命令查看； last命令输出 [root@tencent ~]# last|head root pts/0 123.66.144.124 Fri Nov 8 17:17 still logged in root pts/0 122.1.22.20 Thu Nov 7 20:17 - 22:55 (02:37) root pts/0 122.1.22.16 Tue Nov 5 16:31 - 22:02 (05:31) root pts/0 122.1.22.16 Tue Nov 5 11:05 - 11:18 (00:12) root pts/0 122.1.67.92 Mon Nov 4 17:10 - 22:57 (05:47) root pts/0 123.66.18.69 Wed Oct 30 08:04 - 11:58 (03:53) root pts/1 12.66.17.23 Tue Oct 29 20:33 - 22:56 (02:22) root pts/0 12.66.17.23 Tue Oct 29 19:41 - 22:56 (03:14) root pts/1 12.66.16.12 Mon Oct 28 20:19 - 00:54 (04:35) root pts/0 12.66.16.12 Mon Oct 28 15:22 - 20:20 (04:57) 1.3/var/log/btmp 记录失败的登录尝试信息，默认由lastb命令查看 lastb命令输出 [root@tencent ~]# lastb btmp begins Fri Nov 1 03:49:01 2019 这三个文件都是二进制数据文件，并且三个文件结构完全相同，是由/usr/include/bits/utmp.h文件定义了这三个文件的结构体。 默认情况下文件的日志信息会通过logrotate日志管理工具定期清理。logrotate的配置文件是/etc/logrotate.conf，此处是logrotate的缺省设置，通常不需要对它进行修改。日志文件的轮循压缩等设置存放在独立的配置文件中，它（们）放在/etc/logrotate.d/目录下，它会覆盖缺省设置。 如果不想记录相关信息，则可以直接将相关文件删除即可。如果系统不存在该文件，则需要在此路径touch一个文件就可以继续记录相关信息了。 此外： 如果想禁用who命令，则只需要将utmp的可读权限去掉就行，这样非root用户就不能用此命令了；如果是btmp文件，手工创建的话注意权限必须为600，否则不能正确写入信息。 2.相关命令 下面介绍查看这三个日志文件的命令，分别是lastlog、last、lastb、ac、who、w、users、utmpdump 其中last、lastb、who、utmpdump可以通过指定参数而查看三个中的任意一个文件 2.1 lastlog 列出所有用户最近登录的信息，或者指定用户的最近登录信息。lastlog引用的是/var/log/lastlog文件中的信息，包括login-name、port、last login time [root@tencent ~]# lastlog Username Port From Latest root pts/1 111.55.66.123 Fri Nov 8 21:17:12 +0800 2018 bin **Never logged in** daemon **Never logged in** adm **Never logged in** lp **Never logged in** sync **Never logged in** shutdown **Never logged in** halt **Never logged in** mail **Never logged in** operator **Never logged in** games **Never logged in** ftp **Never logged in** nobody **Never logged in** 2.2 last 列出当前和曾经登入系统的用户信息，它默认读取的是/var/log/wtmp文件的信息。 输出的内容包括：用户名、终端位置、登录源信息、开始时间、结束时间、持续时间。注意最后一行输出的是wtmp文件起始记录的时间。 当然也可以通过last -f参数指定读取文件，可以是/var/log/btmp、/var/run/utmp [root@tencent ~]# last|head root pts/1 12.66.1.12 Fri Nov 8 21:17 still logged in root pts/0 23.6.55.12 Fri Nov 8 17:17 still logged in root pts/0 23.6.55.12 Thu Nov 7 20:17 - 22:55 (02:37) root pts/0 23.6.55.12 Tue Nov 5 16:31 - 22:02 (05:31) root pts/0 23.6.55.12 Tue Nov 5 11:05 - 11:18 (00:12) root pts/0 23.6.55.12 Mon Nov 4 17:10 - 22:57 (05:47) root pts/0 23.6.55.12 Wed Oct 30 08:04 - 11:58 (03:53) root pts/1 23.6.55.124 Tue Oct 29 20:33 - 22:56 (02:22) root pts/0 23.6.55.12 Tue Oct 29 19:41 - 22:56 (03:14) root pts/1 23.6.55.122 Mon Oct 28 20:19 - 00:54 (04:35) 2.3 lastb 列出失败尝试的登录信息，和last命令功能完全相同，只不过它默认读取的是/var/log/btmp文件的信息 当然也可以通过last -f参数指定读取文件，可以是/var/log/btmp、/var/run/utmp [root@tencent ~]# lastb btmp begins Fri Nov 1 03:49:01 2019 2.4 ac 输出所有用户总的连接时间，默认单位是小时。由于ac是基于wtmp统计的，所以修改或者删除wtmp文件都会使ac的结果受影响。(Suse默认没有该命令) 安装ac命令 yum -y install psacct [root@tencent ~]# ac total 559.07 2.5 who 查看当前登入系统的用户信息 语法who [OPTION]... [ FILE | ARG1 ARG2 ]。 who命令强大的一点是，它既可以读取utmp文件也可以读取wtmp文件，默认没有指定FILE参数时，who查询的是utmp的内容。当然可以指定FILE参数，比如who -aH /var/log/wtmp,则此时查看的是wtmp文件 [root@tencent ~]# who root pts/0 2018-11-08 17:17 (23.66.1.2) root pts/1 2018-11-08 21:17 (23.66.1.2) [root@tencent ~]# who -rH NAME LINE TIME IDLE PID COMMENT run-level 3 2018-10-20 21:58 2.6 w 查看当前登入系统的用户信息及用户当前的进程（而who命令只能看用户不能看进程） 该命令能查看的信息包括字系统当前时间，系统运行时间，登陆系统用户总数及系统1、5、10分钟内的平均负载信息。后面的信息是用户，终端，登录源，login time，idle time，JCPU，PCPU，当前执行的进程等 w的信息来自两个文件：用户登录信息来自/var/run/utmp，进程信息来自/proc [root@tencent ~]# w 21:35:24 up 18 days, 23:37, 2 users, load average: 0.02, 0.08, 0.07 USER TTY FROM LOGIN@ IDLE JCPU PCPU WHAT root pts/0 23.66.1.2 17:17 3:48 0.33s 0.33s -bash root pts/1 23.66.1.2 21:17 4.00s 0.04s 0.00s w 2.7 users 显示当前正在登入系统的用户名 语法是users [OPTION]... [FILE] 如果未指定FILE参数则默认读取的是/var/run/utmp，当然也可以指定通用相关文件/var/log/wtmp，此时输出的就不是当前用户了 [root@tencent ~]# users root root 2.8 utmpdump utmpdump用于转储二进制日志文件到文本格式的文件以便查看，同时也可以修改二进制文件！！ 包括/var/run/utmp、/var/log/wtmp、/var/log/btmp 语法为：utmpdump [options] [filename] 修改文件实际就可以抹除系统记录，所以一定要设置好权限，防止非法入侵 例子：修改utmp或wtmp。由于这些都是二进制日志文件，你不能像编辑文件一样来编辑它们。取而代之是，你可以将其内容输出成为文本格式，并修改文本输出内容，然后将修改后的内容导入回二进制日志中。如下： //查看文件信息，是一个二进制文件，不能直接查看，因此需要导出文件信息到一个普通文件中 [root@tencent ~]# file /var/log/wtmp /var/log/wtmp: Hitachi SH big-endian COFF object, not stripped //导出文件信息到hehe文件中，这样就能查看文件内容了 [root@tencent ~]# utmpdump /var/log/wtmp > hehe Utmp dump of /var/log/wtmp 查看文件内容 [root@tencent ~]# tail -5 hehe [8] [31245] [ ] [ ] [pts/0 ] [ ] [0.0.0.0 ] [二 11月 05 22:02:12 2018 ] [7] [16733] [ts/0] [root ] [pts/0 ] [23.66.1.2 ] [23.66.1.20 ] [四 11月 07 20:17:25 2018 ] [8] [16727] [ ] [ ] [pts/0 ] [ ] [0.0.0.0 ] [四 11月 07 22:55:15 2018 ] [7] [32715] [ts/0] [root ] [pts/0 ] [23.66.1.2 ] [23.66.1.2 ] [五 11月 08 17:17:09 2018 ] [7] [02148] [ts/1] [root ] [pts/1 ] [23.66.1.2 ] [23.66.1.24 ] [五 11月 08 21:17:12 2018 ] //还可以将导出的二进制文件信息导回源文件 1.导出二进制文件/var/log/wtmp文件内容到一个文件中 [root@tencent ~]# utmpdump /var/log/wtmp > hehe 2.备份/var/log/wtmp [root@tencent ~]# cp /var/log/wtmp{,.bak} 3.查看两个文件的行数 [root@tencent ~]# wc -l /var/log/wtmp.bak ./hehe 35 /var/log/wtmp 384 ./hehe 4.清空备份的/var/log/wtmp [root@tencent ~]# > /var/log/wtmp.bak [root@tencent ~]# wc -l /var/log/wtmp.bak 0 /var/log/wtmp.bak 5.将导出的文件再导回到/var/log/wtmp.bak [root@tencent ~]# utmpdump -r hehe > /var/log/wtmp.bak Utmp undump of hehe [root@tencent ~]# wc -l /var/log/wtmp.bak 5 /var/log/wtmp.bak 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux命令/Linux用户组管理.html":{"url":"linux/linux命令/Linux用户组管理.html","title":"Linux用户组管理","keywords":"","body":"[toc] Linux用户组管理 1.Linux用户管理 1.1useradd命令创建用户过程 1、不带任何参数使用添加用户时，首先读取/etc/login.defs /etc/default/useradd 预先定义的规则 2、根据设置的规则添加用户，同时会向/etc/passwd /etc/group文件添加新建的用户和组，但/etc/shadow /etc/gshadow也会同步生成记录 3、同时系统会根据/etc/login.defs /etc/default/useradd文件中配置的信息建立用户的家目录，并复制/etc/skel中所有隐藏的环境配置文件到新用户的家目录中，以完成对用户环境的初始化设置 1.2用户配置文件 1.2.1 /etc/passwd 作用 存储用户信息文件，每一行表示一个用户信息，有多少行就表示有多少个用户 格式 root : x : 0 : 0 : root : /root : /bin/bash 格式含义(此文件由7个字段的数据组成，字段之间用“：”分隔) 1用户名：2密码：3用户标识号UID：4组标识号GID：5个人资料：6主目录：7命令解释器 1.2.2 /etc/shadow 作用 存储用户密码信息文件 格式 u1 : !! : 17749 : 0 : 99999 : 7 : : : 格式含义(此文件由9个字段的数据组成，字段之间用“：”分隔) 1用户名 2 密码 ！！表示没有密码 3 最近改动密码的日期 4 密码不可被更动的天数 5 密码需要重新变更的天数 6 密码需要变更期限前的警告期限 7 密码过期的宽限时间 8帐号失效日期 9 保留 1.2.3 /etc/skel(目录) 作用 创建用户相关的目录，此目录用来存放新用户需要的所有基础环境变量文件 [root@exercise1 skel]# pwd /etc/skel [root@exercise1 skel]# ls -a . .. .bash_logout .bash_profile .bashrc [root@exercise1 skel]# ll -a total 28 drwxr-xr-x. 2 root root 4096 Aug 4 13:09 . drwxr-xr-x. 103 root root 12288 Aug 8 06:29 .. -rw-r--r--. 1 root root 18 Mar 23 2017 .bash_logout -rw-r--r--. 1 root root 176 Mar 23 2017 .bash_profile -rw-r--r--. 1 root root 124 Mar 23 2017 .bashrc 与/etc/skel有关的问题 1.用户u1登陆系统后提示符如下 -bash-4.1$ 2. 原因 用户家目录下的相关环境配置文件被删除 3.解决方法 复制/etc/skel下的.bash*到用户家目录 cp /etc/skel/.bash* ~ 1.3组配置文件 1.3.1 /etc/group 作用 存储组相关信息 格式 g1: x : 500 : 格式含义(此文件由4个字段的数据组成，字段之间用“：”分隔) 1组名 2组密码 3组管理员 4用户组成员 1.3.2 /etc/gshadow 作用 存储组密码信息 格式 hehe : ! : : 格式含义(此文件由4个字段的数据组成，字段之间用“：”分隔) 1组名 2组密码 3组管理员 4用户组成员 1.4Linux用户分类 1.4.1管理员用户 默认是root用户，它的UID 和GID均为0，系统安装完成后自动生成的，默认通过它就可以登录系统，拥有最高的管理权限 1.4.2普通用户 由系统管理员root创建的，创建完成后可以登录系统，但默认无法创建、修改和删除任何管理员下的文件；UID从500-65535 1.4.3系统用户(或虚拟用户) 安装系统后默认生成的用户，大多数不能登录系统，但它们是系统正常运行不可缺少的，它们的存在主要是为了方便系统管理，满足相应的系统进程对文件所属用户的要求；UID从 1-499 1.5用户相关命令 1.5.1 useradd 创建用户 语法格式 useradd 选项 用户名 选项 -n 不创建以用户名为名的组 -c 创建用户时，添加个人信息 -u 用户ID值，这个值必须是唯一的 -s 用户登录后使用的shell -g 指定用户对应的组，对应的组必须在系统中存在 -M 不创建用户家目录 1.5.2 usermod 修改用户 语法格式 usermod 选项 用户名 选项(usermod只有-l选项与useradd不同) -c 修改用户的个人信息，同useradd 的-c功能 -g 修改用户对应的用户组，同 useradd的-d功能 -s 修改用户登录后使用的shell名称，同useradd的-s功能 -u 修改用户的uid ，同useradd 的-u功能 -l 修改用户的名称 -usermod -l 新用户名称 旧用户名称 1.5.3 userdel 删除用户 语法格式 userdel 选项 用户名 选项 -f 强制删除用户 -r 删除用户的同时，删除与用户相关的所有文件(包含邮箱信息/var/spool/mail/) ⚠️ 当使用-r 也无法彻底清空用户内容时，把这两个配置文件中与要删除的用户相关的信息，注释或删除掉。 /etc/passwd /etc/group 1.5.4 passwd 修改用户密码 命令格式 passwd 选项 用户名 选项 --stdin 非交互式设置密码 [root@exercise2 local]# echo 123abc|passwd --stdin caonima Changing password for user caonima. passwd: all authentication tokens updated successfully. -d 删除密码 [root@exercise2 ~]# passwd -d hehe Removing password for user hehe. passwd: Success -l 锁定密码 [root@exercise2 ~]# passwd -l hehe Locking password for user hehe. passwd: Success 锁定用户hehe的密码后，其他用户就无法登陆hehe用户，会提示错误的密码 [haha@exercise2 ~]$ su - hehe Password: su: incorrect password -u 解锁密码 [root@exercise2 ~]# passwd -u hehe Unlocking password for user hehe. passwd: Success -S 显示账户状态信息 账户信息包含7个字段 第1个字段是用户的登录名 第2个字段指示用户账号口令是锁定(L)、无口令(NP)还是有可用口令(P) 第3个字段给出最后一次口令修改的 日期 接下来4个字段是最小有效期，最大有效期，警告字段和口令的休止期，这些时期用天标志 [root@exercise2 ~]# passwd -S hehe hehe PS 2018-08-06 0 1 1 -1 (Password set, SHA512 crypt.) -w,--warndays WARN_DAYS(长格式) 设置口令需要修改前发出警告的天数 WARN_DAYS选项是口令过期前的天数。据到期日这些天数时， 用户将被警告其口令即将过期 -x, --maxdays MAX_DAYS 设置口令有效的最大天数 MAX_DAYS天数后，口令需要修改 -i, --inactive INACTIVE 此选项用于在口令过期几天后禁用该账户 用户账号的口令过期INACTIVE指定的天数后，该用户将无法再登录此账号 -n, --mindays MIN_DAYS 设置口令修改的最小天数间隔为MIN_DAYS 此字段设为0表示用于可以随时修改其口令 -e, --expire 使一个账户的口令立即过期。这实际上强迫用户在下次登录时修改密码 WARNING: Your password has expired. You must change your password now and login again! Changing password for user hehe. Changing password for hehe. (current) UNIX password: 1.5.5 su 切换用户 ⚠️不加- 直接切换到root家目录，环境变量没有改变,此方式不安全 [root@exercise1 ~]# su hehe [hehe@exercise1 root]$ pwd /root [hehe@exercise1 root]$ ls ls: cannot open directory .: Permission denied [hehe@exercise1 root]$ cd / [hehe@exercise1 /]$ ls app bin caonima dev hehe lib lost+found misc net opt proc sbin server sys tmp var backup boot cgroup etc home lib64 media mnt oldboy package root selinux srv test usr [hehe@exercise1 /]$ ⚠️加- 切换到自己的家目录，环境变量改变 [root@exercise1 ~]# su - hehe [hehe@exercise1 ~]$ pwd /home/hehe 1.5.6 sudo 用户授权 作用 通过配置文件来限制用户的权限 ，可以让普通用户在执行指定的命令或程序时，拥有超级用户的权限 sudo工作过程 1.当用户执行sudo时，系统会主动寻找/etc/sudoers文件，判断该用户是否有执行sudo的权限 2.确认用户具有可执行sudo的权限后，让用户输入用户自己的密码确认 3.若密码输入成功，则开始执行sudo后续的命令 4.root执行sudo时不需要输入密码(因为sudoers文件中有配置root ALL=(ALL) ALL这样一条规则) 选项 -l 显示用户拥有的sudo权限 -k 清空密码，密码有效时间默认5分钟 授权示例 示例1：给普通用户u1提权,让普通用户可以查看root用户的家目录；普通用户可以使用useradd命令，创建新用户 范例分析步骤： 1） useradd u1 2） visudo=vi打开/etc/sudoers文件 或 vim /etc/sudoers 注：visudo会检查内部语法，避免用户输入错误信息，所以我们一般使用visudo，编辑此文件要用root权限 3) 编辑文件的第98行，编辑完成后，wq! 强制保存退出(vim 编辑/etc/sudoers 文件权限默认440) root ALL=(ALL) ALL u1 ALL=(ALL) /bin/ls,/usr/sbin/useradd u1 ALL=(ALL) /bin/*,!/bin/ls,!/usr/sbin/useradd 排除/bin/ls u1 ALL=(ALL) NOPASSWD:/bin/ls 执行sudo命令不需要输入密码 4）使用u1 用户登录测试 sudo useradd u11 //可成功创建用户，证明提权成功 sudo ls /root //可查看root的家，证明提权成功 5） sudo -l //-l 参数是列出当前用户可执行的命令，但只有在sudoers文件里的用户才能使用该选项 1.5.7 用户查询命令 1.5.7.1 w 显示目前登入系统的用户信息 执行这项指令可得知目前登入系统的用户有哪些人，以及他们正在执行的程序 [root@exercise1 ~]# w 09:09:47 up 6:31, 7 users, load average: 0.07, 0.06, 0.01 USER TTY FROM LOGIN@ IDLE JCPU PCPU WHAT root pts/0 192.168.1.6 05:33 2:35m 0.37s 0.37s -bash root pts/1 192.168.1.6 05:43 3:07m 0.04s 0.02s bash root pts/2 192.168.1.6 05:53 3:07m 0.06s 0.05s bash root pts/3 192.168.1.6 05:55 3:13m 0.02s 0.01s bash root pts/4 192.168.1.6 06:01 3:07m 0.01s 0.01s -bash root pts/5 192.168.1.6 07:46 0.00s 0.14s 0.01s w root pts/6 192.168.1.6 08:04 1:04m 0.04s 0.03s -bash 1.5.7.2 id 查看用户UID、GID [root@exercise1 ~]# id root uid=0(root) gid=0(root) groups=0(root) 1.5.7.3 last 显示用户登录情况 [root@exercise1 ~]# last u1 pts/7 192.168.1.6 Wed Aug 8 08:05 - 08:35 (00:29) root pts/7 192.168.1.6 Wed Aug 8 08:05 - 08:05 (00:00) root pts/6 192.168.1.6 Wed Aug 8 08:04 still logged in root pts/5 192.168.1.6 Wed Aug 8 07:46 still logged in root pts/4 192.168.1.6 Wed Aug 8 06:01 still logged in root pts/3 192.168.1.6 Wed Aug 8 05:55 still logged in root pts/2 192.168.1.6 Wed Aug 8 05:53 still logged in root pts/1 192.168.1.6 Wed Aug 8 05:43 still logged in root pts/0 192.168.1.6 Wed Aug 8 05:33 still logged in user pts/3 192.168.1.6 Wed Aug 8 03:34 - 05:00 (01:26) user pts/2 192.168.1.6 Wed Aug 8 03:33 - 05:00 (01:27) u1 pts/6 192.168.1.6 Wed Aug 8 03:25 - 03:25 (00:00) 1.5.7.4 lastlog 显示linux中所有用户最近一次远程登录的信息 [root@exercise1 ~]# lastlog Username Port From Latest root pts/7 192.168.1.6 Wed Aug 8 08:05:24 +0800 2018 bin **Never logged in** daemon **Never logged in** adm **Never logged in** lp **Never logged in** sync **Never logged in** 1.6组相关命令 1.6.1 groupadd 创建组 语法格式 groupadd 选项 用户组 选项 -g gid 指定用户组的GID，GID唯一不能为负数，如果不指定GID从500开始 -f 新增一个组，强制覆盖一个已存在的组，GID、组成员不会改变 1.6.2 gpasswd 将已存在的用户加入到组中 语法格式 gpasswd 选项 用户名 组名 选项 -a：添加一个用户到组,可以追加到组 -M：添加多个用户到组，覆盖之前的组成员 -d：从组删除用户 ⚠️ 1. -a只能添加一个用户到组中，批量添加用户到组用-M选项 2. -M选项再次执行添加用户到组会覆盖之前的用户 1.6.3 groupmod 修改组信息 语法格式 groupmod 选项 组名 选项 -n 修改组名 -g 修改GID 1.6.4 groupdel 删除组 语法格式 groupdel 组名 1.6.5 groups 查看用户属于哪些组 语法格式 groups 用户名 [root@exercise1 ~]# groups root root : root 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux命令/awk/1.awk文本处理、语法、工作原理.html":{"url":"linux/linux命令/awk/1.awk文本处理、语法、工作原理.html","title":"1.awk文本处理、语法、工作原理","keywords":"","body":"[toc] awk文本处理、语法、工作原理 一、awk文本处理 1.逐行扫描文件(或流), 从第一行到最后一行 2.寻找匹配的特定模式的行,在行上进行操作 3.如果没有指定处理动作,则把匹配的行显示到标准输出 4.如果没有指定模式，则所有被操作的行都被处理 二、awk语法格式及重要选项 2.1语法格式 awk option 'pattern {action}' file awk 选项 ‘匹配模式 {动作}’ 文件 pattern 匹配模式 action 处理动作，针对符合匹配模式的数据进行的处理动作，如果没有pattern，只有action，会对所有文本执行action的处理动作，如果没有action，只有pattern，会打印出符合匹配模式的行 2.2两个重要选项 2.2.1 -F 指定字段分隔符 示例：取出IP地址 -F指定分隔符是连续的空格冒号(centos6.9) #centos6.9 ifconfig eth0 |awk -F'[ :]+' 'NR==2{print $4}' #centos7.5 ifconfig eth0|awk -F'[ ]+' 'NR==2{print $3}' 2.2.2 -v 创建或修改awk中的变量 #创建一个变量并输出，只在awk中生效，且awk中输出变量不需要加$符号 [root@test1 ~]# awk -v n1=10 -v n2=20 'BEGIN{print n1,n2}' 10 20 一个-v选项输出多个变量，注意-v后边必须与大括号紧挨 awk -v{a=10,b=20} 'BEGIN{print a,b}' 10 20 三 、awk工作原理 版本一 1.awk将文件中的每一行作为输入, 并将每一行赋给内部变量$0, 以换行符结束 2.awk开始进行字段分解，每个字段存储在已编号的变量中，从$1开始[默认空格分割] 3.awk默认字段分隔符是由内部FS变量来确定, 可以使用-F修订 4.awk行处理时使用了print函数打印分割后的字段 5.awk在打印后的字段加上空格，因为$1,$3 之间有一个逗号。逗号被映射至OFS内部变量中，称为输出字段分隔符， OFS默认为空格. 6.awk输出之后，将从文件中获取另一行，并将其存储在$0中，覆盖原来的内容，然后将新的字符串分隔成字段并进行处理。该过程将持续到所有行处理完毕. 版本二 1.先执行命令行的参数 2.如果有BEGIN{}，就执行其中的内容 此时awk还没有开始读取文件内容 3.BEGIN{}执行完后读取文件内容 ①.判断是否满足条件 ②.符合，执行命令 ③.不符合，读取下一行 4.文件内容读取完成后，最后执行END{}中的内容 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux命令/awk/2.awk BEGIN模式、END模式.html":{"url":"linux/linux命令/awk/2.awk BEGIN模式、END模式.html","title":"2.awk BEGIN模式、END模式","keywords":"","body":"[toc] awk BEGIN模式、END模式 一、BEGIN模式 BEGIN模式在awk程序执行后，但尚未执行处理动作之前需要做的工作（定义变量） 作用一 修改标题 //文件a.txt内容如下 101,abc,CEO 102,def,CTO 103,qaz,COO //默认打印 [root@7-test1 ~]# awk '{print}' a.txt 101,abc,CEO 102,def,CTO 103,qaz,COO //打印标题，注意ID、name必须用双引号 [root@7-test1 ~]# awk 'BEGIN{print \"ID\",\"name\",\"position\"}{print}' a.txt ID name position 101,abc,CEO 102,def,CTO 103,qaz,COO 作用二 做运算 示例1:计算1到100的和 [root@7-test1 ~]# awk 'BEGIN{for(i=1;i 示例2:普通计算 [root@7-test1 ~]# awk 'BEGIN{print 3+3}' 6 [root@7-test1 ~]# awk 'BEGIN{print 3*3}' 9 [root@7-test1 ~]# awk 'BEGIN{print 3-3}' 0 [root@7-test1 ~]# awk 'BEGIN{print 3**3}' 27 [root@7-test1 ~]# awk 'BEGIN{print 3%3}' 0 [root@7-test1 ~]# awk 'BEGIN{print 3/3}' 1 作用三 修改awk内置变量 record 字段 separator 分隔符 变量名 对应单词 含义 FS field separator 字段分隔符,默认空格或tab OFS output field separator 输出分隔符,默认空格 RS record separator 记录分隔符,默认换行符\\n ORS output record separator 输出记录分隔符 FS 指定字段分隔符 //passwd.txt文件内容如下 [root@7-test1 ~]# cat passwd.txt root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/bin:/sbin/nologin daemon:x:2:2:daemon:/sbin:/sbin/nologin 示例1：以冒号为分隔符，打印文件第一列 //-F写法 [root@7-test1 ~]# awk -F: '{print $1}' passwd.txt root bin daemon //BEGIN {FS}写法 [root@7-test1 ~]# awk 'BEGIN{FS=\":\"}{print $1}' passwd.txt root bin daemon OFS 指定输出分隔符 //文件内容如下 [root@7-test1 ~]# cat a.txt 1 2 3 示例1：打印文件第一列和第三列，以冒号为分隔符 //-v OFS写法 [root@7-test1 ~]# awk -v OFS=: '{print $1,$3}' a.txt 1:3 //BEGIN {OFS}写法 [root@7-test1 ~]# awk 'BEGIN{OFS=\":\"}{print $1,$3}' a.txt 1:3 ⚠️⚠️⚠️注意：！！！ -v OFS写法的一个坑：当print $0的时候，-v OFS会不生效 //正确输出应该为1:2:3 [root@7-test1 ~]# awk -v OFS=: '{print $0}' a.txt 1 2 3 解决方法 在print$0前加一个$1=$1 [root@7-test1 ~]# awk -v OFS=: '{$1=$1;print $0}' a.txt 1:2:3 RS 指定记录分隔符，即指定以什么为分隔符，默认为\\n //文件内容 文件默认以换行为记录分隔符 [root@7-test1 ~]# cat passwd.txt root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/bin:/sbin/nologin 示例1：将文件的记录分隔符指定为/，即指定文件以/为换行符 //-v RS写法 [root@7-test1 ~]# awk -v RS=/ '{print $0}' passwd.txt root:x:0:0:root: root: bin bash bin:x:1:1:bin: bin: sbin nologin //BEGIN {RS}写法 [root@7-test1 ~]# awk 'BEGIN{RS=\"/\"}{print $0}' passwd.txt root:x:0:0:root: root: bin bash bin:x:1:1:bin: bin: sbin nologin ORS 指定输出记录分隔符 //文件内容 [root@7-test1 ~]# cat a.txt 1 2 3 示例1：将文件的输出记录分隔符指定为/ //-v ORS写法 [root@7-test1 ~]# awk -v ORS=\"A\" '{print $0}' a.txt 1A2A3A //BEGIN {ORS}写法 [root@7-test1 ~]# awk 'BEGIN{ORS=\"A\"}{print $0}' a.txt 1A2A3A 扩展-awk命令行参数 ARGC是命令行参数数量 ARGV是将命令行参数存到数组，元素由ARGC指定，数组下标从0开始 ARGC [root@test1 ~]# awk 'BEGIN{print ARGC}' 1 [root@test1 ~]# awk 'BEGIN{print ARGC}' 1 2 3 ARGV [root@test1 ~]# awk 'BEGIN{print ARGV[0]}' 1 2 awk [root@test1 ~]# awk 'BEGIN{print ARGV[1]}' 1 2 1 [root@test1 ~]# awk 'BEGIN{print ARGV[2]}' 1 2 2 二、END模式 END模式在awk读取完文件之后执行，主要作用为显示计算结果 作用 显示计算结果 示例1:统计/etc/passwd文件中前10行非登陆shell的个数 //文件内容 [root@7-test1 awk]# head -10 /etc/passwd root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/bin:/sbin/nologin daemon:x:2:2:daemon:/sbin:/sbin/nologin adm:x:3:4:adm:/var/adm:/sbin/nologin lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin sync:x:5:0:sync:/sbin:/bin/sync shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown halt:x:7:0:halt:/sbin:/sbin/halt mail:x:8:12:mail:/var/spool/mail:/sbin/nologin operator:x:11:0:operator:/root:/sbin/nologin //统计文件中非登陆shell的个数，即统计/sbin/nologin的个数 [root@7-test1 awk]# head -10 /etc/passwd|awk '/nologin/{i++}END{print i}' 6 示例2:统计/etc/services中空行数量 [root@7-test1 awk]# awk '/^$/{i++}END{print i}' /etc/services 17 示例3:统计nginx访问日志中访问量前10的IP [root@7-test1 ~]# awk '{a[$1]++}END{for(i in a) print a[i],i}' /var/log/nginx/access.log|sort -nr|head 275 109.98.109.101 262 146.196.97.242 210 47.52.155.218 152 142.234.200.81 75 72.211.58.142 10 120.25.208.128 7 42.236.10.84 6 42.236.10.92 3 5.188.210.12 3 42.236.10.75 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux命令/awk/3.awk内置变量.html":{"url":"linux/linux命令/awk/3.awk内置变量.html","title":"3.awk内置变量","keywords":"","body":"[toc] awk内置变量 $0 表示匹配的整行 //文件内容 [root@7-test1 ~]# cat passwd.txt root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/bin:/sbin/nologin 示例： [root@7-test1 ~]# awk '{print $0}' passwd.txt root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/bin:/sbin/nologin NR 总的行号 //文件a.txt和b.txt的内容如下 文件a.txt 101,abc,CEO 102,def,CTO 103,qaz,COO 文件b.txt 666,ABC,CEO 777,DEF,CTO 888,QAZ,COO //打印两个文件，NR统计总的行号 [root@7-test1 ~]# awk '{print NR,$0}' a.txt b.txt 1 101,abc,CEO 2 102,def,CTO 3 103,qaz,COO 4 666,ABC,CEO 5 777,DEF,CTO 6 888,QAZ,COO FNR 当前文件的行号 文件a.txt 101,abc,CEO 102,def,CTO 103,qaz,COO 文件b.txt 666,ABC,CEO 777,DEF,CTO 888,QAZ,COO //打印两个文件，FNR只统计每个文件的行号 [root@7-test1 ~]# awk '{print FNR,$0}' a.txt b.txt 1 101,abc,CEO 2 102,def,CTO 3 103,qaz,COO 1 666,ABC,CEO 2 777,DEF,CTO 3 888,QAZ,COO NF 文件最后一列 //打印/etc/passwd文件最后一列 [root@7-test1 ~]# awk -F: '{print $NF}' /etc/passwd|head /bin/bash /sbin/nologin /sbin/nologin /sbin/nologin /sbin/nologin /bin/sync /sbin/shutdown /sbin/halt /sbin/nologin /sbin/nologin 分隔符 record 字段 separator 分隔符 变量名 对应单词 含义 FS field separator 字段分隔符,默认空格或tab OFS output field separator 输出分隔符,默认空格 RS record separator 记录分隔符,默认换行符\\n ORS output record separator 输出记录分隔符 FS 指定字段分隔符 //passwd.txt文件内容如下 [root@7-test1 ~]# cat passwd.txt root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/bin:/sbin/nologin daemon:x:2:2:daemon:/sbin:/sbin/nologin 示例1：以冒号为分隔符，打印文件第一列 //-F写法 [root@7-test1 ~]# awk -F: '{print $1}' passwd.txt root bin daemon //BEGIN {FS}写法 [root@7-test1 ~]# awk 'BEGIN{FS=\":\"}{print $1}' passwd.txt root bin daemon OFS 指定输出分隔符 //文件内容如下 [root@7-test1 ~]# cat a.txt 1 2 3 示例1：打印文件第一列和第三列，以冒号为分隔符 //-v OFS写法 [root@7-test1 ~]# awk -v OFS=: '{print $1,$3}' a.txt 1:3 //BEGIN {OFS}写法 [root@7-test1 ~]# awk 'BEGIN{OFS=\":\"}{print $1,$3}' a.txt 1:3 ⚠️⚠️⚠️注意：！！！ -v OFS写法的一个坑：当print $0的时候，-v OFS会不生效 //正确输出应该为1:2:3 [root@7-test1 ~]# awk -v OFS=: '{print $0}' a.txt 1 2 3 解决方法 在print$0前加一个$1=$1 [root@7-test1 ~]# awk -v OFS=: '{$1=$1;print $0}' a.txt 1:2:3 RS 指定记录分隔符，即指定以什么为分隔符，默认为\\n //文件内容 文件默认以换行为记录分隔符 [root@7-test1 ~]# cat passwd.txt root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/bin:/sbin/nologin 示例1：将文件的记录分隔符指定为/，即指定文件以/为换行符 //-v RS写法 [root@7-test1 ~]# awk -v RS=/ '{print $0}' passwd.txt root:x:0:0:root: root: bin bash bin:x:1:1:bin: bin: sbin nologin //BEGIN {RS}写法 [root@7-test1 ~]# awk 'BEGIN{RS=\"/\"}{print $0}' passwd.txt root:x:0:0:root: root: bin bash bin:x:1:1:bin: bin: sbin nologin ORS 指定输出记录分隔符 //文件内容 [root@7-test1 ~]# cat a.txt 1 2 3 示例1：将文件的输出记录分隔符指定为/ //-v ORS写法 [root@7-test1 ~]# awk -v ORS=\"A\" '{print $0}' a.txt 1A2A3A //BEGIN {ORS}写法 [root@7-test1 ~]# awk 'BEGIN{ORS=\"A\"}{print $0}' a.txt 1A2A3A 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux命令/awk/4.awk匹配模式.html":{"url":"linux/linux命令/awk/4.awk匹配模式.html","title":"4.awk匹配模式","keywords":"","body":"[toc] awk匹配模式 一、正则表达式 1.1匹配记录(整行) //匹配/etc/passwd文件中以root开头的 [root@7-test1 ~]# awk '/^root/' /etc/passwd root:x:0:0:root:/root:/bin/bash [root@7-test1 ~]# awk '$0 ~/^root/' /etc/passwd root:x:0:0:root:/root:/bin/bash 1.2匹配字段 匹配操作符(~|!~) //匹配/etc/passwd文件中第一列以root开头的 [root@7-test1 ~]# awk '$1~/^root/' /etc/passwd root:x:0:0:root:/root:/bin/bash //匹配/etc/passwd文件中最后一列不是以bash结尾的 [root@7-test1 ~]# awk '$NF !~ /bash$/' /etc/passwd|head -5 bin:x:1:1:bin:/bin:/sbin/nologin daemon:x:2:2:daemon:/sbin:/sbin/nologin adm:x:3:4:adm:/var/adm:/sbin/nologin lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin sync:x:5:0:sync:/sbin:/bin/sync ^和~用法示例 ^ 在awk中表示以...开头的列，在sed、grep中表示以...开头的行 ^用法示例 示例：打印文件中以1开头的行 //文件内容 [root@7-test1 awk]# cat t.txt 1 101,hehe,CEO 2 102,haha,CTO 3 103,xixi,COO 4 105,yy,CFO 5 106,xx,CIO 6 110,jjxx,COCO [root@7-test1 awk]# awk '/^1/{print}' t.txt 1 101,hehe,CEO ～用法示例 //文件内容 [root@7-test1 awk]# cat t1.txt zhang san 1234567 :550:100:175 li si 88883336 :155:90:201 wang wu 66666666 :250:60:50 zhao liu 67676798 :150:80:75 hao qi 11112222 :250:100:175 zhu ba 000098763 :50:95:135 //显示所有ID号码最后一位是2或7的人的全名 [root@7-test1 awk]# awk '$3~/[27]$/{print $1,$2}' t1.txt zhang san hao qi //显示文件中第三列以1开头的行 [root@7-test1 awk]# awk '$3~/^1/{print}' t1.txt zhang san hao qi //显示zhang的姓氏和ID号码 [root@7-test1 awk]# awk '$1~/zhang/{print $1,$3}' t1.txt zhang 1234567 //显示zhang的捐款，每个值都以$开头，如$250$100$175 方法一： [root@7-test1 awk]# awk -vFS='[ :]+' -vOFS=$ '$1~/zhang/{print \"$\"$4,$5,$NF}' t1.txt $550$100$175 方法二： awk替换函数gsub gsub 全局替换 awk内置命令(函数) 使用方法 gsub(//,\"\",$n) n表示数字 gsub(/找谁/,\"替换为什么\",替换哪一列) [root@7-test1 awk]# awk '$1~/zhang/{gsub(/:/,\"$\",$NF);print $1,$NF}' t1.txt zhang $550$100$175 二、条件表达式 2.1关系运算符 运算符 含义 小于 小于等于 == 等于 != 不等于 > 大于 >= 大于等于 ~ 模糊匹配 !~ 模糊匹配取反 2.2示例 示例1:打印第一列大于3的行 //文件内容 [root@7-test1 awk]# cat t.txt 1 101,hehe,CEO 2 102,haha,CTO 3 103,xixi,COO 4 105,yy,CFO 5 106,xx,CIO 6 110,jjxx,COCO [root@7-test1 awk]# awk '{if($1>3)print}' t.txt 4 105,yy,CFO 5 106,xx,CIO 6 110,jjxx,COCO 示例2:显示磁盘使用率大于10%的磁盘分区和挂载点 [root@7-test1 awk]# df -h Filesystem Size Used Avail Use% Mounted on /dev/mapper/centos-root 17G 2.4G 15G 14% / devtmpfs 224M 0 224M 0% /dev tmpfs 236M 0 236M 0% /dev/shm tmpfs 236M 9.6M 226M 5% /run tmpfs 236M 0 236M 0% /sys/fs/cgroup /dev/sda1 1014M 132M 883M 14% /boot tmpfs 48M 0 48M 0% /run/user/0 [root@7-test1 awk]# df -h|awk -F'[% ]+' 'NR>1 && $5>10{print $1,$5\"%\",$NF}' /dev/mapper/centos-root 14% / /dev/sda1 14% /boot 示例3:计算内存使用率 centOS6 //centOS6查看内存使用率 [root@test1 ~]# free -m total used free shared buffers cached Mem: 474 171 303 0 30 44 -/+ buffers/cache: 95 378 Swap: 1023 0 1023 //centOS6计算内存使用率 [root@test1 ~]# free -m|awk 'NR==3{print $3/($3+$4)*100\"%\"}' 20.296% centOS7 //centOS7查看内存使用率 [root@7-test1 awk]# free -m total used free shared buff/cache available Mem: 470 90 243 5 136 330 Swap: 2047 0 2047 //centOS7计算内存使用率 [root@7-test1 awk]# free -m|awk 'NR==2{print ($2-$NF)/$2*100\"%\"}' 30% 示例4:同时计算内存使用率和空闲率 centOS6 //centOS6计算内存使用率和空闲率 [root@test1 ~]# free -m|awk 'NR==3{print $3/($3+$4)*100\"%\",$NF/($3+$NF)*100\"%\"}' 20.4641% 79.5359% centOS7 //centOS7计算内存使用率和空闲率 [root@7-test1 ~]# free -m|awk 'NR==2{print ($2-$NF)/$2*100\"%\",$NF/$2*100\"%\"}' 30% 70% 2.3awk的一个坑 ⚠️⚠️⚠️awk会把数字当成字符处理 //查看磁盘使用率 [root@test1 ~]# df -h Filesystem Size Used Avail Use% Mounted on /dev/sda3 19G 2.4G 16G 14% / tmpfs 238M 0 238M 0% /dev/shm /dev/sda1 190M 35M 146M 19% /boot //打印第二行第5列小于9的列，结果不正确 [root@test1 ~]# df -h|awk 'NR>1 && $51 && $5+01 && $5 三、运算表达式 示例1:打印/etc/passwd第三列与10相乘大于5000的内容 [root@test1 ~]# awk -F: '$3 * 10 > 5000' /etc/passwd polkitd:x:999:998:User for polkitd:/:/sbin/nologin chrony:x:998:996::/var/lib/chrony:/sbin/nologin mysql:x:1000:1000::/home/mysql:/sbin/nologin 示例2：打印/etc/passwd第三列与10相乘大于500的内容，并打印第一列和第三列 [root@test1 ~]# awk -F: 'BEGIN{OFS=\"--\"} { if($3*10>5000) {print $1,$3} } END {print \"打印ok\"}' /etc/passwd polkitd--999 chrony--998 mysql--1000 打印ok 示例3:加、减、乘、除、取模、幂运算 //文件test.txt内容如下 [root@test1 ~]# cat test.txt a 1 2 3 4 5 b 1 2 3 4 5 a 5 6 7 8 9 [root@test1 ~]# awk '/a/{print $2+10}' test.txt 11 15 [root@test1 ~]# awk '/a/{print $2-10}' test.txt -9 -5 [root@test1 ~]# awk '/a/{print $2*10}' test.txt 10 50 [root@test1 ~]# awk '/a/{print $2/10}' test.txt 0.1 0.5 [root@test1 ~]# awk '/a/{print $2%10}' test.txt 1 5 [root@test1 ~]# awk '/a/{print $2**10}' test.txt 1 9765625 四、逻辑操作符和复合模式 4.1逻辑操作符 符号 含义 && 逻辑与 **\\ \\ ** 逻辑或 ！ 逻辑非 示例1:打印/etc/passwd文件中用户名为root并且打印uid小于15的行 //文件中第三列是UID [root@test1 ~]# awk -F: '$1~/root/ && $3 示例2:打印用户名为root或第三列大于500的行 [root@test1 ~]# awk -F: '$1~/root/ || $3>=500' /etc/passwd root:x:0:0:root:/root:/bin/bash polkitd:x:999:998:User for polkitd:/:/sbin/nologin chrony:x:998:996::/var/lib/chrony:/sbin/nologin mysql:x:1000:1000::/home/mysql:/bin/false 4.2三元运算符 三元运算符表达式 三元运算符的形式1：[ 结果 = 条件 ? 结果1：结果2 ] 三元运算符的形式2：[ 条件 ？ 表达式1 ： 表达式2 ] 示例： //文件t.txt内容如下 [root@test1 ~]# cat t.txt student1 98 student2 99 student3 93 student4 78 student5 85 根据学生成绩判断学生成绩是否为优秀，这里规定分数大于90是优秀，低于90非优秀 //非三元运算写法 [root@test1 ~]# awk '{if($2>90){print $1,\"优秀\"}else{print $1,\"不优秀\"}}' t.txt student1 优秀 student2 优秀 student3 优秀 student4 不优秀 student5 不优秀 //三元运算写法一 结果 = 条件 ? 结果1：结果2 [root@test1 ~]# awk '{res=$2>90?\"优秀\":\"不优秀\";print $1,res}' t.txt student1 优秀 student2 优秀 student3 优秀 student4 不优秀 student5 不优秀 #上述awk写法中的print也可以单独写在一个大括号中 awk '{res=$2>90?\"优秀\":\"不优秀\"}{print $1,res}' t.txt #可以使用三元运算符统计个数 //三元运算写法二 条件 ？ 表达式1 ： 表达式2 [root@test1 ~]# awk '{$2>90?a++:b++}END{print a,b}' t.txt 3 2 a表示符合条件的，b表示不符合条件的，大于90分的有3个，低于90分的有2个 至今没懂的一道题🦙🦙🦙 文件内容 [root@test1 ~]# cat lessons.txt 634751 预排 568688 预排 386760 删除 619373 预排 428491 预排 487563 完成 603342 完成 436339 完成 结果： 删除 386760 完成 487563,603342,436339 预排 634751,568688,619373,428491 [root@test1 ~]# awk '{a[$2]=a[$2]\" \"$1}END{for(i in a) print i,a[i]}' lessons.txt 删除 386760 完成 487563 603342 436339 预排 634751 568688 619373 428491 [root@test1 ~]# awk '{a[$2]=a[$2]\",\"$1}END{for(i in a) print i,a[i]}' lessons.txt 删除 ,386760 完成 ,487563,603342,436339 预排 ,634751,568688,619373,428491 方法一 [root@test1 ~]# awk '{a[$2]=a[$2]?a[$2]\",\"$1:a[$2]$1}END{for(i in a) print i,a[i]}' lessons.txt 删除 386760 完成 487563,603342,436339 预排 634751,568688,619373,428491 方法二 [root@test1 ~]# awk '{if(a[$2]){a[$2]=a[$2]\",\"$1}else{a[$2]=a[$2]$1}}END{for(i in a) print i,a[i]}' lessons.txt 删除 386760 完成 487563,603342,436339 预排 634751,568688,619373,428491 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux命令/awk/5.awk条件.html":{"url":"linux/linux命令/awk/5.awk条件.html","title":"5.awk条件","keywords":"","body":"[toc] awk条件 一、if 1.1语法格式 {if(表达式) {语句1;语句2;...}} 1.2示例 示例1:打印/etc/passwd文件中的root用户 [root@test1 ~]# awk -F: '{if ($3==0) {print $1 \" is adminisitrator\"}}' /etc/passwd root is adminisitrator 示例2:统计/etc/passwd中系统用户数 [root@test1 ~]# awk -F: '{ if($3>0 && $3 示例3:统计/etc/passwd中普通用户数量 [root@test1 ~]# awk -F: '{if ($3>1000) {i++}} END{print i}' /etc/passwd 2 示例4:写出一个shell脚本，把当前目录下的文本文件number.txt里面数字大于100的求和并输出，并打印所在行行号及内容，最后还要输出每一行的总和 //文件内容 [root@test1 ~]# cat number.txt 100 98 205 303 1 99 66 33 [root@test1 ~]# awk '{if($1>100){sum+=$1;print NR,$0}}{i+=$1}END{print sum,i}' number.txt 3 205 4 303 508 905 二、if...else 2.1语法格式 {if(表达式)｛语句;语句;... ｝else{语句;语句;...}} 2.2示例 示例1:if else语句简单使用 //文件内容 [root@test1 ~]# cat test.txt 1 2 3 4 5 6 [root@test1 ~]# awk '{if ($3==3) {print $1} else {print $NF}}' test.txt 1 [root@test1 ~]# awk '{if ($3==6) {print $1} else {print $NF}}' test.txt 6 示例2:打印/etc/passwd中非管理员个数和管理员个数 [root@test1 ~]# awk -F: '{if ($3==0) {a++} else {b++}} END{print \"管理员用户个数：\"a;print \"其他用户个数：\"b}' /etc/passwd 管理员用户个数：1 其他用户个数：24 三、if...else if...else 3.1语法格式 {if(表达式 1)｛语句;语句；... ｝else if(表达式 2)｛语句;语句；... ｝else｛语句;语句；... }} 3.2示例 统计/etc/passwd文件中用户的种类 #/etc/passwd文件中第三列是用户UID，centos7中普通用户UID大于1000 awk -F: '{if($3==0){i++} else if($3>0 && $30 && $3=1000){k++}} END{print \"管理员个数为:\"i;print \"系统用户个数为:\"j;print \"普通用户个数为:\"k}' /etc/passwd 管理员个数为:1 系统用户个数为:25 普通用户个数为:1 四、switch-case #示例 [root@test1 ~]# awk 'BEGIN{a=1;b=2;c=3; switch(a){case 1:print a;break;case 2:print b;break; case 3:print c;break;}}' 1 #格式化后的整体结构如下 BEGIN{ a=1; b=2; c=3; switch(a){ case 1: print a;break; case 2: print b;break; case 3: print c;break; } } 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux命令/awk/6.awk循环.html":{"url":"linux/linux命令/awk/6.awk循环.html","title":"6.awk循环","keywords":"","body":"[toc] awk循环 1.for循环 1.1 c格式 1.1.1 c格式语法 for(初始值;终止值;步长值){语句} 1.1.2示例 //示例1 [root@test1 ~]# awk 'BEGIN{for(i=1;i 1.2 列表格式 1.2.1 列表格式语法 for(变量 in 数组){语句} 1.2.2示例 [root@test1 ~]# awk 'BEGIN{a[0]=10;a[1]=11;for(i in a) print i,a[i]}' 0 10 1 11 2.while循环 2.1语法格式 while(条件){语句} 2.2示例 语句示例 //打印1-5 [root@test1 ~]# awk 'BEGIN{i=1;while(i awk文件示例 //用awk获取文件中第三列到倒数第二列字段 [root@test1 ~]# cat > awk.awk 3.do while循环 3.1语法 do {语句} while(条件) 3.2示例 示例：计算1-100的和 [root@test1 ~]# awk 'BEGIN{do{sum+=i;i++}while(i 循环中的关键字 break：当 break 语句用于 while 或 for 语句时，导致退出程序循环 continue：当 continue 语句用于 while 或 for 语句时，使程序循环移动到下一个迭代 next：能能够导致读入下一个输入行，并返回到脚本的顶部。这可以避免对当前输入行执行其他的操作过程 exit：语句使主输入循环退出并将控制转移到END,如果END存在的话。如果没有定义END规则，或在END中应用exit语句，则终止脚本的执行 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux命令/awk/7.awk数组.html":{"url":"linux/linux命令/awk/7.awk数组.html","title":"7.awk数组","keywords":"","body":"[toc] awk数组 1.awk数组格式 数组名[索引]=值 2.创建awk数组 //创建一个数组，数组名是a，数组的索引是\"aaa\"和\"bbb\" [root@test1 ~]# awk 'BEGIN{a[\"aaa\"]=\"www.aaa.com\";a[\"bbb\"]=\"www.bbb.com\"; print a[\"aaa\"] \"\\n\" a[\"bbb\"]}' www.aaa.com www.bbb.com 3.删除数组元素 //使用delete 数组名[索引] 删除数组元素 [root@test1 ~]# awk 'BEGIN{a[\"aaa\"]=\"www.aaa.com\";a[\"bbb\"]=\"www.bbb.com\";delete a[\"aaa\"]; print a[\"aaa\"] \"\\n\" a[\"bbb\"]}' www.bbb.com 4.按照索引遍历 //以下示例了以统计的字段为数组的索引 [root@test1 ~]# awk -F: '{a[++i]=$1} END{print a[1]}' /etc/passwd root [root@test1 ~]# awk -F: '{a[i++]=$1} END{print a[1]}' /etc/passwd bin [root@test1 ~]# head -2 /etc/passwd root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/bin:/sbin/nologin 5.awk数组示例 5.1简单示例 5.1.1统计/etc/passwd中各种类型shell的数量 [root@test1 ~]# awk -F: '{a[$NF]++} END{for(i in a){print i,a[i]}}' /etc/passwd /bin/sync 1 /bin/bash 2 /sbin/nologin 22 /sbin/halt 1 /sbin/shutdown 1 5.1.2 80端口访问状态统计 [root@test1 ~]# ss -an|awk '/:80/{a[$2]++} END{for(i in a){print i,a[i]}}' LISTEN 3 5.2结合日志文件的示例 nginx访问日志格式如下 82.113.63.230 - - [25/Jan/2019:15:15:56 +0800] \"GET / HTTP/1.1\" 301 169 \"-\" \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\" \"-\" 5.2.1统计2019年1月25日当天的PV量 [root@test1 ~]# awk '/25\\/Jan\\/2019/{i++}END{print i}' /var/log/nginx/access.log 90 5.2.2统计15-19点的PV量 [root@test1 ~]# awk '$4>=\"[25/Jan/2019:15:00:00\" && $4 5.2.3统计2019年1月25日当天内访问最多的10个IP [root@test1 ~]# awk '/25\\/Jan\\/2019/ {a[$1]++} END {for(i in a){print i,a[i]}}' /var/log/nginx/access.log |sort -rn|head 5.2.4统计2019年1月25日15-19点访问次数最多的10个IP [root@test1 ~]# awk '$4>=\"[25/Jan/2019:15:00:00\" && $4 5.2.5统计2019年1月25日访问次数大于100的IP [root@test1 ~]# awk '/25\\/Jan\\/2019/ {a[$1]++} END {for(i in a){if(a[i]>10){print i,a[i]}}}' /var/log/nginx/access.log 5.2.6统计2019年1月25日访问最多的10个页面($requests top 10) //nginx访问日志中第7列是访问url [root@test1 ~]# awk '/25\\/Jan\\/2019/ {a[$7]++} END {for(i in a){print a[i],i}}' /var/log/nginx/access.log |sort -rn|head 5.2.7统计每个IP访问状态码的数量 [root@test1 ~]# awk '{a[$1 \" \" $9]++} END {for(i in a){print a[i],i}}' /var/log/nginx/access.log|sort -rn|head 10 101.227.27.188 301 9 113.193.226.3 301 3 59.36.132.240 301 3 182.254.52.17 301 2 80.82.70.187 301 2 64.246.161.190 302 2 14.18.182.223 302 2 14.18.182.223 301 2 124.161.103.251 301 1 83.97.20.33 301 5.2.8统计访问状态码为404及出现的次数($status) //统计某一天的 [root@test1 ~]# awk '{if($9==\"404\") a[$9]++} END {for(i in a){print i,a[i]}}' /var/log/nginx/access.log //统计某一天某一时刻的 [root@test1 ~]# awk '$4>=\"[25/Jan/2019:15:00:00\" && $4 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux命令/awk/8.awk函数.html":{"url":"linux/linux命令/awk/8.awk函数.html","title":"8.awk函数","keywords":"","body":"[toc] awk函数 1.awk内置函数 1.1 gsub 1.1.1语法 gsub(/要替换的内容/,'替换后的内容',替换哪一列) 1.1.2示例 //将/etc/passwd文件中root一行中的冒号替换为加号 [root@test1 ~]# awk '$1~/^root/{gsub(/:/,\"+\",$NF);print $0}' /etc/passwd root+x+0+0+root+/root+/bin/bash 1.2 substr 1.2.1语法 substr(某一列,从第几个字符开始,截取几个字符结束) 1.2.2示例 //简单使用示例 [root@test1 ~]# echo 'abcdefg'|awk '{print substr($1,3,3)}' cde 1.3 split 1.3.1语法 split(某一列,数组名,/要替换的内容/) 把某一列通过正则表达式切割然后放到数组中 1.3.2示例 [root@test1 ~]# echo GET /a/b/c/d/e.jpg|awk '{split($2,a,/\\./);print a[1]}' /a/b/c/d/e [root@test1 ~]# echo GET /a/b/c/d/e.jpg|awk '{split($2,a,/\\./);print a[2]}' jpg 1.4 system 1.3.1含义 awk调用shell命令 1.3.2示例 //⚠️命令外必须用双引号包裹 awk 'BEGIN{system(\"ls\")}' 2.awk自定义函数 //编辑awk文件function.awk cat >function.awk 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux命令/grep命令.html":{"url":"linux/linux命令/grep命令.html","title":"grep命令","keywords":"","body":"[toc] grep命令 1.正则表达式 1.1基本正则表达式 1.2扩展正则表达式 1.3特殊字字符类 [:alnum:] [a-zA-Z0-9]匹配人一个字母或数字字符 [:alpha:] 匹配任意一个字母（包括大小写） [:digit:] 匹配任意一个数字 [:lower:] 匹配小写字母 [:upper:] 匹配大写字母 [:punct:] 匹配标点符号 [:blank:] 匹配空格与制表符 [:space:] 匹配一个包括换行符、回车等在内的所有空白字符 [:graph:] 匹配任意可看的见的且可打印的字符 [:print:] 匹配任何一个可以打印的字符 [:xdigit:] 匹配任意一个16进制数（0-9，a-f，A-F） 2.grep命令 2.1命令说明 Linux系统中grep命令是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。grep全称是Global Regular Expression Print，表示全局正则表达式版本，它的使用权限是所有用户 2.2命令格式 grep [options] file 2.3选项 -E 如果加这个选项，那么后面的匹配模式就是扩展的正则表达式，也就是 grep -E = egrep -i 比较字符时忽略大小写区别 -l 过滤的时候只显示文件名 找出包含sshd的文件 -w 把表达式作为词来查找，相当于正则中的\"\\\"(...表示你自定义的规则) -x 被匹配到的内容，正好是整个行，相当于正则\"^...$\" -v 取反 -c count，统计，统计匹配结果的行数，不是匹配结果的次数，是行数 -m 只匹配规定的行数，之后的内容就不再匹配了 -n 在输出的结果里显示行号，这里的行号是该行内容在原文件中的行号，而不是在输出结果中行号 -o 只显示匹配内容，grep 默认是显示满足匹配条件的一行，加上这个参数就只显示匹配结果，比如我们要匹配一个 ip 地址，就只需要结果，而不需要该行的内容 -R 递归匹配。如果要在一个目录中多个文件或目录匹配内容，则需要这个参数 -B 输出满足条件行的前几行，比如 grep -B 3 \"aa\" file 表示在 file 中输出有 aa 的行，同时还要输出 aa 的前 3 行 -A 这个与-B 类似，输出满足条件行的后几行 -C 这个相当于同时用-B -A，也就是前后都输出 -P 支持perl正则 2.4grep扩展用法 2.4.1扩展用法1 grep后向引用 示例：将文本中连续相同的数字打印出来 ()表示匹配的内容,当前为匹配数字,\\1为后向引用, echo '1222233333222444455556666669999'|egrep -o '([0-9])\\1+' 命令后边如果写+就会无法截取1,因为1只有一个 此时就需要在后边再单独匹配数字1 echo '1222233333222444455556666669999'|egrep -o '([0-9])\\1+|[0-9]' 另外两种写法 echo '1222233333222444455556666669999'|egrep -o '([0-9])\\1*' echo '1222233333222444455556666669999'|egrep -o '(.)\\1*' 2.4.2扩展用法2 grep -P与零宽断言匹配 零宽断言:perl语言正则匹配 核心:截取特定字符串左边或右边的内容 截取string右边的内容 lookahead (?=string) 截取string左边的内容 lookbehind (? 零宽断言截取字符串 示例1:取出文件中:右边的内容 //文件内容如下,现在要截取出:右边的数字 [root@exercise2 ~]# cat a.txt id:01 id:02 id:03 id:04 id:05 id:06 id:07 id:08 id:09 id:10 666 test666 方法: grep -Po \"(? 示例2:截取IP地址 [root@7-test1 ~]# ip a s eth0 2: eth0: mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:93:32:b9 brd ff:ff:ff:ff:ff:ff inet 10.0.0.200/24 brd 10.0.0.255 scope global noprefixroute eth0 valid_lft forever preferred_lft forever inet6 fe80::19ee:50ab:53fc:55f9/64 scope link noprefixroute valid_lft forever preferred_lft forever ()中有小于号,表明要截取字符串右边的内容 [root@7-test1 ~]# ip a s eth0 | grep -Po '(? 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux命令/sed命令.html":{"url":"linux/linux命令/sed命令.html","title":"sed命令","keywords":"","body":"[toc] sed命令 1.sed命令工作流程 1、将文件的第一行读入到自己的缓存空间，删除换行符 2、匹配，看一下该行是否为要编辑的行，如果是，执行编辑命令，不是，执行1 3、执行编辑命令 4、加上换行符输出到屏幕 5、判断是否为文件尾，如果是，退出，不是，再重复1-4 默认情况下，sed缓存空间内的行都会输出到屏幕，除非使用-n选项 默认情况下，sed将修改的行输出到屏幕，并没有修改源文件，使用-i选项修改源文件 2.命令格式 sed 选项 地址1,地址2 命令 标记 文件名 3.选项 选项 含义 -n 拟制输出，不输出未修改的行，强制输出用命令p -i 修改源文件，需要强制备份源文件 -i.bak即可 -r 让sed支持扩展正则表达式，默认支持标准正则表达式 -f 指定文件，将sed命令写到文件中，然后执行sed -f 文件名 要修改的文件，但是从未成功执行过！！！ -e 允许一条命令执行多个sed子命令 sed -e ‘s#a#b#g’ -e ‘s#A#B#g’ 3.1定位、匹配 3.1.1使用行号 定位1-5行 1,5 定位到最后一行 $ 指定起始匹配行和步长 1~5(从第一行开始，每隔5行匹配) 定位奇数行 1～2 定位偶数行 0～2 定位某行之后的n行 1,+3 3.1.2使用正则表达式 /正则表达式/ 4.命令 命令 含义 a\\ 在当前行后添加一行或多行 c\\ 用新文本修改（替换）当前行中的文本 d 删除行 i\\ 在当前行之前插入文本 h 把模式空间里的内容复制到暂存缓存区 H 把模式空间里的内容追加到暂存缓存区 g 取出暂存缓冲区里的内容，将其复制到模式空间，覆盖该处原有内容 G 取出暂存缓冲区里的内容，将其复制到模式空间，追加在原有内容后面 l 列出非打印字符 p 打印行 n 读入下一输入行，并从下一条命令而不是第一条命令开始处理 q 结束或退出sed r 从文件中读取输入行 ! 对所选行之外的所有行应用命令 s 用一个字符串替换另一个 4.1 p 打印 文件内容 [root@test1 ~]# cat hehe 123456 111111 abcdef (abc)def 2 222222 例：打印第1行 [root@test1 ~]# sed -n '1p' hehe 123456 文件内容 [root@test1 test]# cat hehe.txt 1 101,hehe,CEO 2 102,haha,CTO 3 103,xixi,COO 4 104,yy,CFO 5 105,xx,CIO 6 110,jjxx,COCO 例：打印从101开始到105结束的行 [root@test1 test]# sed -n '/101/,/105/p' hehe.txt 1 101,hehe,CEO 2 102,haha,CTO 3 103,xixi,COO 4 104,yy,CFO 5 105,xx,CIO 例：打印从101开始到第3行 [root@test1 test]# sed -n '/101/,3p' hehe.txt 1 101,hehe,CEO 2 102,haha,CTO 3 103,xixi,COO 4.2 d 删除 命令d用于删除输入行 sed 先将输入行从文件复制到模式缓存区，然后对该行执行sed命令，最后将模式缓存区的内容显示在屏幕上 如果发出的是命令d，当前模式缓存区的输入行会被删除，不被显示 文件内容 [root@test1 ~]# cat hehe 123456 111111 abcdef (abc)def 2 222222 例：将有1的行删除 [root@test1 ~]# sed '/1/d' hehe abcdef (abc)def 2 222222 例：将文件的1到3行删除 [root@test1 ~]# sed '1,3d' hehe (abc)def 2 222222 文件内容 [root@test1 test]# cat 111.txt 101,aaa,CEO 102,bbb,CTO 103,ccc,COO 104,ddd,CFO 105,eee,CIO 110,fff,COCO 例：将文件的第1行到第5行删除 [root@test1 test]# sed '1,5d' 111.txt 110,fff,COCO 例：将文件从ddd到文件结尾删除 [root@test1 test]# sed '/ddd/,$d' 111.txt 101,aaa,CEO 102,bbb,CTO 103,ccc,COO 例：删除不包含fff的行 [root@test1 test]# sed '/fff/!d' 111.txt 110,fff,COCO sed删除指定内容后的行 文件内容 $ cat haha aaa bbb ccc 123 例：删除bbb后的所有内容(包含bbb) $ sed '/bbb/,$d' haha aaa 但是上述写法把包含bbb的行也删除了，如果想要把bbb保留下来，需要这么写，其中N b是固定的字符，其余字符可以替换为任意字母，除了sed中的p和d，既除了a是任意字母外，其余均为固定写法 $ sed '/bbb/{p;:a;N;$!ba;d}' haha aaa bbb https://www.soinside.com/question/niV938HWFE7ZhkP4Cn4ygR 4.3 s 替换 文件内容 [root@test1 ~]# cat hehe 123456 111111 abcdef (abc)def 2 222222 例：将文件中的1全部替换为? [root@test1 ~]# sed 's/1/?/g' hehe ?23456 ?????? abcdef (abc)def 2 222222 例：将abcdef替换为abchehe, \\1表示匹配的abc [root@test1 ~]# cat hehe 123456 111111 abcdef (abc)def 2 222222 [root@test1 ~]# sed -r 's/(abc)def/\\1hehe/' hehe \\1表示前边匹配的abc 123456 111111 abchehe (abc)def 2 222222 4.4 逗号 指定行的范围 [root@test1 ~]# cat hehe 123456 111111 abcdef (abc)def 2 222222 111 例：匹配111111到222222之间的行 [root@test1 ~]# sed -n '/111111/,/222222/p' hehe 111111 abcdef (abc)def 222222 例：匹配从第5行开始，到第一个以2开头之间的行 [root@test1 ~]# sed -n '5,/2/p' hehe 111111 abcdef (abc)def 2 例：匹配111111所在行到以2开头的行 [root@test1 ~]# sed -n '/111111/,/2/p' hehe 111111 abcdef (abc)def 2 例：匹配从111111开始到第一个以2开头的行，然后将以1开头的行替换为hehe [root@test1 ~]# sed -n '/111111/,/2/s/^1/hehe/p' hehe hehe11111 4.5 e 多重编辑 -e命令是编辑命令，用于sed执行多个编辑任务的情况下 在下一行开始编辑前，所有的编辑动作将应用到模式缓存区的行上 [root@test1 ~]# cat hehe 123456 111111 abcdef (abc)def 2 222222 例：将第一行删除，并且替换abc为ABC [root@test1 ~]# sed -e '1d' -e 's#abc#ABC#g' hehe 111111 ABCdef (ABC)def 2 222222 4.6 a 追加 文件内容 [root@test1 ~]# cat hehe 123456 111111 abcdef (abc)def 2 222222 111 例：在以2开头的行后追加HEHE [root@test1 ~]# sed '/^2/aHEHE' hehe 123456 111111 abcdef (abc)def 2 HEHE 222222 HEHE 111 4.7 i 插入 i命令是插入命令，类似于a命令，但不是在当前行后增加文本，而是在当前行前面插入新的文本，即刚读入缓存区模式的行 [root@test1 ~]# cat hehe 123456 111111 abcdef (abc)def 2 222222 111 例：在以2开头的行前边插入HEHE [root@test1 ~]# sed '/^2/iHEHE' hehe 123456 111111 abcdef (abc)def HEHE 2 HEHE 222222 111 4.8 c 修改 c命令是修改命令,sed 使用该命令将已有的文本修改成新的文本,旧文本被覆盖 文件内容 [root@test1 ~]# cat hehe 123456 111111 abcdef (abc)def 2 222222 111 例：将文件中abcdef替换为ABCDEF [root@test1 ~]# sed '/abcdef/cABCDEF' hehe 123456 111111 ABCDEF (abc)def 2 222222 111 4.9 n 获取下一行 n命令表示下一条命令，sed 使用该命令获取输入文件的下一行，并将其读入到模式缓冲区中，任何 sed 命令都将应用到匹配行，即紧接着的下一行上 文件内容 [root@test1 ~]# cat hehe 123456 111111 abcdef (abc)def 2 222222 111 例：匹配111111一行，n表示获取下一行，即abcdef,并且替换e为E [root@test1 ~]# sed '/111111/{n;s/e/E/;}' hehe 123456 111111 abcdEf (abc)def 2 222222 111 4.10 y 转换 y命令表示转换，该命令与 tr 命令相似，字符按照一对一的方式从左到右进行转换 例如 y/abc/ABC/，会把小写字母转换成大写字母，a-->A,b-->B,c-->C，与s不同的是，y会全部替换，而s需要在最后加g 文件内容 [root@test1 ~]# cat hehe 123456 111111 abcdef (abc)def 2 222222 111 例：将1-3行的1转换为Q [root@test1 ~]# sed '1,3y/1/Q/' hehe Q23456 QQQQQQ abcdef (abc)def 2 222222 111 5.实际使用示例 5.1示例1 使用sed命令将/etc/passwd文件中红用户名和登陆shell调换位置 思路：将配置文件分组，用户名为第一组，最后的登陆shell分为一组，然后中间的内容分为一组，因此 hehe 是第一组 501:501::/home/hehe: 是第二组 /bin/bash 是第三组 [root@test1 ~]# tail -1 /etc/passwd hehe:x:500:500::/home/hehe:/bin/bash [root@test1 ~]# tail -1 /etc/passwd |sed -r 's#(.*)(:x.*:)(/.*)#\\3\\2\\1#g' /bin/bash:x:500:500::/home/hehe:hehe 5.2示例2 用sed命令取出ip地址 [root@test1 ~]# ifconfig eth0|sed -nr '2s#(.*:)(.*)(B.*)#\\2#gp' 192.168.1.8 5.3示例3 将以下文件进行批量需改（用一条命令完成） 修改前1.sh 2.sh 3.sh 修改后1_test.sh 2_test.sh 3_test.sh ls *.sh|sed -nr 's#((.*)\\.sh)#mv \\1 \\2_test.sh#gp' |bash 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/容器/docker/1.docker简介.html":{"url":"linux/容器/docker/1.docker简介.html","title":"1.docker简介","keywords":"","body":"[toc] docker简介 一、什么是Docker Docker是基于go语言实现的开源容器项目，诞生于2013年年初，最初发起者是dotCloud公司，2013年年底改为Docker Inc docker官网 1.1什么是容器 容器就是在隔离的环境运行的一个进程，如果进程停止，容器就会销毁，隔离环境拥有自己的系统文件、ip地址、主机名 1.2docker容器 docker是通过内核虚拟化技术 namespaces:提供容器资源隔离 cgroups:资源限制 docker通过操作系统层的虚拟化实现隔离，不需要类似虚拟机额外的操作系统开销，提高了资源利用率 优点： 1.创建分布式应用程序，快速分发和部署 2.迁移性好，通过容器来打包应用、解耦应用和运行平台 3.速度快，实现服务秒启动，占用系统资源少 缺点： 1.docker本质是一个进程，如果占用资源过多，会触发 oom机制 oom==out of memory 容器的启动 环境隔离 共用宿主机的内核：启动进程 二、Docker核心概念 2.1三大核心概念 2.1.1 镜像 Docker镜像是创建Docker容器的基础，镜像本身只读，一个镜像包含一个基本的操作系统环境，里面仅安装了一个应用程序，比如安装了apache应用程序，就可以称之为apache镜像 2.1.2 容器 Docker容器是从镜像创建的应用运行实例，它可以启动、开始、停止、删除，这些容器彼此相互隔离、互不可见 可以把容器看作是一个简易版linux系统环境(包括root用户权限、进程空间、用户空间、网络空间)以及运行在其中的应用程序打包而成的盒子 2.1.3 仓库 Docker仓库是集中存放镜像文件的场所 仓库注册服务器存放多个仓库，每个仓库集中存放某一类镜像，包括多个镜像文件，通过不同标签来进行区分 Docker仓库分为公开仓库和私有仓库 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/容器/docker/2.docker安装.html":{"url":"linux/容器/docker/2.docker安装.html","title":"2.docker安装","keywords":"","body":"[toc] 快速安装 # 阿里云yum源 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum -y install docker-ce systemctl start docker && systemctl enable docker # 配置阿里云镜像加速地址 cat > /etc/docker/daemon.json CentOS7安装docker 1.安装docker最新版 1.1 下载yum源 # 阿里云yum源 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 1.2 安装docker最新版 yum -y install docker-ce 1.3 启动docker systemctl start docker && systemctl enable docker 1.4 查看docker版本 $ docker version Client: Version: 18.09.1 API version: 1.39 Go version: go1.10.6 Git commit: 4c52b90 Built: Wed Jan 9 19:35:01 2019 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 18.09.1 API version: 1.39 (minimum version 1.12) Go version: go1.10.6 Git commit: 4c52b90 Built: Wed Jan 9 19:06:30 2019 OS/Arch: linux/amd64 Experimental: false 1.5 配置docker镜像加速 # 配置阿里云镜像加速地址 cat > /etc/docker/daemon.json 1.6 创建并运行第一个容器 $ docker run -d -p 80:80 nginx Unable to find image 'nginx:latest' locally latest: Pulling from library/nginx 6ae821421a7d: Pull complete da4474e5966c: Pull complete eb2aec2b9c9f: Pull complete Digest: sha256:dd2d0ac3fff2f007d99e033b64854be0941e19a2ad51f174d9240dda20d9f534 Status: Downloaded newer image for nginx:latest 6cd3a11bda37336b8d6f0ba3e266e697945f72d520bbd0225a6e93818c8d581d 2.安装docker指定版本 2.1 下载yum源 # 阿里云yum源 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 2.2 查看可用版本 $ yum list docker-ce --showduplicates | sort -r 已加载插件：fastestmirror, langpacks 可安装的软件包 * webtatic: uk.repo.webtatic.com * updates: mirrors.aliyun.com Loading mirror speeds from cached hostfile * extras: mirrors.aliyun.com docker-ce.x86_64 3:19.03.3-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.2-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.1-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.0-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.9-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.8-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.7-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.6-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.5-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.4-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.3-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.2-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.1-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.0-3.el7 docker-ce-stable docker-ce.x86_64 18.06.3.ce-3.el7 docker-ce-stable docker-ce.x86_64 18.06.2.ce-3.el7 docker-ce-stable docker-ce.x86_64 18.06.1.ce-3.el7 docker-ce-stable docker-ce.x86_64 18.06.0.ce-3.el7 docker-ce-stable docker-ce.x86_64 18.03.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 18.03.0.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.12.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.12.0.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.09.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.09.0.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.06.2.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.06.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.06.0.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.03.3.ce-1.el7 docker-ce-stable docker-ce.x86_64 17.03.2.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.03.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.03.0.ce-1.el7.centos docker-ce-stable 2.3 安装docker指定版本 yum -y install docker-ce-18.03.1.ce docker-ce-cli-18.01.1.ce 2.4 查看版本 $ docker -v Docker version 18.03.1-ce, build 9ee9f40 2.5 配置docker镜像加速 # 配置阿里云镜像加速地址 cat > /etc/docker/daemon.json 3.二进制安装docker 系统环境 内核版本 内存 docker版本 3.10.0-1062.el7.x86_64 1c2G 18.09.9 docker二进制包下载地址 3.1 下载docker二进制包 export DOCKER_VERSION=18.09.9 wget https://download.docker.com/linux/static/stable/x86_64/docker-${DOCKER_VERSION}.tgz 3.2 解压缩包 tar xf docker-DOCKER_VERSION.tgz -C /usr/local/ 3.3 导出环境变量 ⚠️如果后续想要使用systemd管理docker，最好把docker二进制包中的所有文件拷贝到/usr/bin，否则会管理失败 cp /usr/local/docker/* /usr/bin 3.4 使用systemd管理docker Control Docker with systemd 官方文档关于使用systemd管理docker的说明 如果你是使用二进制方式安装的 docker，那么你也许需要整合 docker 到 systemd 中去。为了完成这个任务，你需要安装两个单元文件（service 和 socket）到 /etc/systemd/system 中去 Manually create the systemd unit files When installing the binary without a package, you may want to integrate Docker with systemd. For this, install the two unit files (service and socket) from the github repository to /etc/systemd/system. ⚠️需要下载的是docker.service.rpm和docker.socket这两个文件，需要把docker.service.rpm重命名为docker.service，然后再移动到/etc/systemd/system下 wget https://github.com/moby/moby/raw/branch/branch/master/contrib/init/systemd/docker.service.rpm wget https://github.com/moby/moby/raw/branch/branch/master/contrib/init/systemd/docker.socket 这里我们直接向文件写入内容 docker.service cat > /etc/systemd/system/docker.service docker.socket cat >/etc/systemd/system/docker.socket linux 中 /etc/systemd/system和/usr/lib/systemd/system 的区别 每一个 Unit（服务等） 都有一个配置文件，告诉 Systemd 怎么启动这个 Unit 。 Systemd 默认从目录/etc/systemd/system/读取配置文件。 但是，里面存放的大部分文件都是符号链接，指向目录/usr/lib/systemd/system/，真正的配置文件存放在那个目录。 systemctl enable命令用于在上面两个目录之间，建立符号链接关系。 sudo systemctl enable clamd@scan.service # 等同于 sudo ln -s '/usr/lib/systemd/system/clamd@scan.service' '/etc/systemd/system/multi-user.target.wants/clamd@scan.service' 3.5 重新加载服务并启动docker systemctl daemon-reload systemctl start docker && systemctl enable docker 3.6 验证docker版本 $ docker version Client: Docker Engine - Community Version: 18.09.9 API version: 1.39 Go version: go1.11.13 Git commit: 039a7df9ba Built: Wed Sep 4 16:50:02 2019 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 18.09.9 API version: 1.39 (minimum version 1.12) Go version: go1.11.13 Git commit: 039a7df9ba Built: Wed Sep 4 16:55:50 2019 OS/Arch: linux/amd64 Experimental: false ⚠️二进制安装的dcoker默认是没有命令补全的，需要从yum安装的机器上拷贝 /usr/share/bash-completion/completions下名为docker的文件 加载文件生效 source /usr/share/bash-completion/docker 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/容器/docker/3.docker容器操作.html":{"url":"linux/容器/docker/3.docker容器操作.html","title":"3.docker容器操作","keywords":"","body":"[toc] docker容器操作 1.docker运行容器 1.1docker后台运行容器 //docker运行一个容器 [root@docker01 ~]# docker run -d -p 80:80 nginx Unable to find image 'nginx:latest' locally latest: Pulling from library/nginx 6ae821421a7d: Pull complete da4474e5966c: Pull complete eb2aec2b9c9f: Pull complete Digest: sha256:dd2d0ac3fff2f007d99e033b64854be0941e19a2ad51f174d9240dda20d9f534 #参数说明 run 创建并运行一个容器 -d 后台运行 -p 端口映射 宿主机端口:容器端口 nginx docker镜像名称 //访问容器 访问宿主机IP:80端口 1.2docker交互式运行容器 //docker交互式运行容器 [root@docker1 ~]# docker run -it --name nginx nginx /bin/bash root@07c25f8aa98b:/# #参数shuoming -it 分配交互式终端 --name 指定容器的名字 /bin/bash 覆盖容器的初始命令 第一个nginx 容器名称 第二个nginx 镜像名称 2.docker停止容器 命令：docker stop 容器ID或容器名称 //查看容器，此时nginx容器正在运行 [root@docker1 ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e5008e3abc3c nginx \"nginx -g 'daemon of…\" 8 seconds ago Up 7 seconds 0.0.0.0:80->80/tcp elated_hermann //停止容器，停止容器可以加容器的ID或者容器名字 [root@docker1 ~]# docker stop e5008e3abc3c e5008e3abc3c //再次查看，可以看到容器已经停止 [root@docker1 ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e5008e3abc3c nginx \"nginx -g 'daemon of…\" About a minute ago Exited (0) 2 seconds ago elated_hermann 3.docker进入容器 3.1docker进入容器方法 目的 docker进入容器（为了调试、排错） 方法 docker exec -it 容器ID或容器名称 /bin/bash #会分配一个新的终端tty docker attach #使用同一个终端 nsenter 用的少 #需要安装util-linux 3.2示例 exec进入容器 //运行一个容器 [root@docker1 ~]# docker run -d nginx 47a36ef42fb743a4c28ac8d3c82cfa1947698c7c0a74a5ccb59815b959cb7a48 //查看容器 [root@docker1 ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 47a36ef42fb7 nginx \"nginx -g 'daemon of…\" 8 seconds ago Up 7 seconds 80/tcp zen_spence //根据容器ID进入容器 [root@docker1 ~]# docker exec -it 47a36ef42fb7 /bin/bash root@47a36ef42fb7:/# cat /etc/issue Debian GNU/Linux 9 \\n \\l //显示容器详细信息 [root@docker1 ~]# docker ps --no-trunc CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 47a36ef42fb743a4c28ac8d3c82cfa1947698c7c0a74a5ccb59815b959cb7a48 nginx \"nginx -g 'daemon off;'\" 5 minutes ago Up 5 minutes 80/tcp zen_spence 4.docker退出容器 4.1docker退出容器，容器不运行 方法 exit ctrl+d //启动一个容器 [root@docker1 ~]# docker run -it --name centos6.9 centos:6.9 /bin/bash //exit退出容器 [root@7076abed1c74 /]# exit exit //查看容器，容器此时已经停止 [root@docker1 ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7076abed1c74 centos:6.9 \"/bin/bash\" 11 seconds ago Exited (0) 1 second ago centos6.9 4.2临时退出容器,容器依然运行 方法 ctrl+p 然后 ctrl+q ⚠️⚠️⚠️docker attach进入容器使用的是同一个终端，再次打开一个终端,两个终端的操作是同时进行的 最好使用exec进入容器 //启动一个容器，然后临时退出 [root@docker1 ~]# docker run -it --name centos6.9 centos:6.9 /bin/bash [root@50492b58763a /]# [root@docker1 ~]# //查看容器，容器此时没有退出 [root@docker1 ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 50492b58763a centos:6.9 \"/bin/bash\" 3 minutes ago Up 3 minutes centos6.9 //再次进入刚才临时退出的容器，可以用attach或者exec [root@docker1 ~]# docker attach 50492b58763a [root@50492b58763a /]# [root@docker1 ~]# docker exec -it 50492b58763a /bin/bash [root@50492b58763a /]# 5.docker夯住容器 docker容器内的第一个进程必须一直处于前台运行的状态，必须夯住，否则这个容器就处于退出状态 docker容器只能执行一条初始命令 示例1 //启动一个nginx容器 [root@docker01 ~]# docker run -d -p 80:80 nginx //查看nginx容器详细信息 [root@docker1 ~]# docker ps --no-trunc CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 47a36ef42fb743a4c28ac8d3c82cfa1947698c7c0a74a5ccb59815b959cb7a48 nginx \"nginx -g 'daemon off;'\" 5 minutes ago Up 5 minutes 80/tcp zen_spence 可以看到nginx容器启动的命令\"nginx -g 'daemon off;'\" 表示nginx在前台运行 示例2 //后台启动centos容器 [root@docker1 ~]# docker run -d centos:6.9 e1fab2c0c3a5df84fa812d475b5f9ed27b31f928aad8874319671cd9ad792e20 //查看容器，可以看到，容器第一个进程没有夯住，已经退出 [root@docker1 ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e1fab2c0c3a5 centos:6.9 \"/bin/bash\" 4 seconds ago Exited (0) 4 seconds ago upbeat_dirac //要想夯住容器，必须加一个能夯住的命令，例如vi一个不存在的文件，tail -F 一个文件 #方式一 [root@docker1 ~]# docker run -d centos:6.9 vi 123.txt dcc90f30af539cf2c14748c22cd4815a26283d2cdd5bd8d4a0aece8d9c1a8e84 [root@docker1 ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dcc90f30af53 centos:6.9 \"vi 123.txt\" 2 seconds ago Up 2 seconds frosty_mirzakhani #方式二 [root@docker1 ~]# docker run -d centos:6.9 tail -F /var/log/messages ddc401b2bba8347b8b257f224e4c6a35798d9b6e08986f71201d3473d82a0ff0 [root@docker1 ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ddc401b2bba8 centos:6.9 \"tail -F /var/log/me…\" 3 seconds ago Up 2 seconds heuristic_gauss tail -F 如果文件不存在，也能夯住 -f 如果文件不存在，会退出 6.docker删除容器 6.1docker温柔删除容器 命令 docker rm 容器ID或者容器名称 //查看docker容器 [root@docker1 ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 9fcda2502aa5 centos:6.9 \"/bin/bash\" 13 seconds ago Exited (0) 8 seconds ago centos6.9 e5008e3abc3c nginx \"nginx -g 'daemon of…\" 6 minutes ago Exited (0) 4 minutes ago elated_hermann //删除容器，可以加容器的ID或者名称 [root@docker1 ~]# docker rm centos6.9 centos6.9 //再次查看容器，可以看到名称为centos6.9的容器已经删除 [root@docker1 ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e5008e3abc3c nginx \"nginx -g 'daemon of…\" 10 minutes ago Exited (0) 8 minutes ago elated_hermann 6.2docker强制删除容器 最好不要强制删除容器！！！ 命令 docker rm -f 容器ID或容器名称 //运行一个容器 [root@docker1 ~]# docker run -d nginx:latest e57f21e6917bc109fb13afbe7e9237b13a3741f0fd1b4971afaa31fdfdcaec6a //查看容器 [root@docker1 ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e57f21e6917b nginx:latest \"nginx -g 'daemon of…\" 9 seconds ago Up 7 seconds 80/tcp stoic_leakey e5008e3abc3c nginx \"nginx -g 'daemon of…\" 16 minutes ago Exited (0) 15 minutes ago elated_hermann //尝试删除正在运行的容器,会报错 [root@docker1 ~]# docker rm e57f21e6917b Error response from daemon: You cannot remove a running container e57f21e6917bc109fb13afbe7e9237b13a3741f0fd1b4971afaa31fdfdcaec6a. Stop the container before attempting removal or force remove //加-f参数强制删除 [root@docker1 ~]# docker rm -f e57f21e6917b e57f21e6917b //再次查看容器，可以看到容器已经被删除 [root@docker1 ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e5008e3abc3c nginx \"nginx -g 'daemon of…\" 19 minutes ago Exited (0) 17 minutes ago elated_hermann 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/容器/docker/4.1registry私有仓库.html":{"url":"linux/容器/docker/4.1registry私有仓库.html","title":"registry","keywords":"","body":"[toc] registry私有仓库 docker私有仓库registry只需要启动一个容器即可 试验环境 docker01 10.0.0.10 docker02 10.0.0.11 一、普通registry 1.1docker01启动私有仓库容器 [root@docker01 ~]# docker run -d -p 5000:5000 --restart=always --name registry -v \\ /opt/myregistry:/var/lib/registry registry 参数解释 -d 后台运行 -p 映射端口 --restart=always 重启docker服务时拉起容器 --name 名字 -v 挂载卷， /opt/myregistry:/var/lib/registry表示将宿主机的/opt/myregistry挂载到容器的/var/lib/registry 1.2docker02给镜像打标签 1.初始镜像 [root@docker02 ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE centos 6.9 adf829198a7f 4 months ago 195MB 2.给镜像打标签 语法 docker tag 镜像名称 标签名称 [root@docker02 ~]# docker tag centos:6.9 10.0.0.10:5000/centos:6.9 3.打完标签后的镜像，id相同，相当于硬链接 [root@docker02 ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE 10.0.0.10:5000/centos 6.9 adf829198a7f 4 months ago 195MB centos 6.9 adf829198a7f 4 months ago 195MB 1.3docker02推送镜像到私有仓库docker01 1.默认推送是采用https协议，因此第一次推送会报错 [root@docker02 ~]# docker push 10.0.0.10:5000/centos:6.9 The push refers to repository [10.0.0.10:5000/centos] Get https://10.0.0.10:5000/v2/: http: server gave HTTP response to HTTPS client 2.修改配置文件，使推送使用http协议 [root@docker02 ~]# vim /etc/docker/daemon.json { \"insecure-registries\": [\"10.0.0.10:5000\"] } 3.重启docker [root@docker02 ~]# systemctl restart docker 4.再次推送即可成功 [root@docker02 ~]# docker push 10.0.0.10:5000/centos:6.9 The push refers to repository [10.0.0.20:5000/centos] aaa5621d7c01: Pushed 6.9: digest: sha256:7e172600dff1903f186061ce5f5295664ec9942ca120e4e5b427ddf01bb2b35b size: 529 1.4docker 普通registry缺点 没有认证，任何人都可以推送镜像到私有仓库，不安全！！！ 1.docker02导入镜像或者下载镜像 [root@docker02 ~]# docker load -i nginx.tar.gz 2.给镜像打标签 [root@docker02 ~]# docker tag nginx:latest 10.0.0.10:5000/nginx:latest 3.推送任意一个镜像到私有仓库 [root@docker02 ~]# docker push 10.0.0.10:5000/nginx:latest 4.推送镜像存放路径 创建私有镜像仓库时指定的宿主机目录/opt/myregistry -v /opt/myregistry:/var/lib/registry registry [root@docker01 ~]# ls /opt/myregistry/docker/registry/v2/repositories/ centos/ centos6.9_ssh/ nginx/ 二、带basic认证的registry 2.1docker01初始环境准备 1.安装httpd-tools [root@docker01 ~]# yum -y install httpd-tools 2.创建存放密码的目录并设置密码 [root@docker01 ~]# mkdir -p /opt/registry-var/auth [root@docker01 ~]# htpasswd -Bbn test 123456 >> /opt/registry-var/auth/htpasswd 2.2docker01启动容器 [root@docker01 ~]# docker run -d -p 5000:5000 --name registry --restart=always -v /opt/registry-var/auth/:/auth/ -v \\ /opt/myregistry:/var/lib/registry -e \"REGISTRY_AUTH=htpasswd\" -e \\ \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd\\ registry 2.3docker02直接拉取镜像 1.直接拉取镜像会报错，因为没有认证 [root@docker02 ~]# docker pull 10.0.0.10:5000/nginx Using default tag: latest Error response from daemon: Get http://10.0.0.10:5000/v2/nginx/manifests/latest: no basic auth credentials 2.4登陆私有仓库，然后拉取镜像 1.登陆私有仓库 [root@docker02 ~]# docker login 10.0.0.10:5000 Username: test Password: WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded 2.拉取镜像 [root@docker02 ~]# docker pull 10.0.0.10:5000/nginx Using default tag: latest latest: Pulling from nginx Digest: sha256:278fefc722ffe1c36f6dd64052758258d441dcdb5e1bbbed0670485af2413c9f Status: Image is up to date for 10.0.0.10:5000/nginx:latest 3.上传镜像，先打标签，再上传 [root@docker02 ~]# docker tag centos:6.9 10.0.0.10:5000/my-centos [root@docker02 ~]# docker push 10.0.0.10:5000/my-centos registry镜像存储位置 registry镜像存储位置为 挂载目录/docker/registry/v2/repositories/镜像名称 目录挂载 /opt/myregistry:/var/lib/registry 镜像存储位置 /opt/myregistry/docker/registry/v2/repositories 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/容器/docker/4.2docker企业级私有仓库harbor.html":{"url":"linux/容器/docker/4.2docker企业级私有仓库harbor.html","title":"harbor","keywords":"","body":"[toc] docker企业级私有仓库harbor 1.harbor介绍 Habor是由VMWare公司开源的容器镜像仓库。事实上，Habor是在Docker Registry上进行了相应的企业级扩展，从而获得了更加广泛的应用，这些新的企业级特性包括：管理用户界面，基于角色的访问控制 ，AD/LDAP集成以及审计日志等，足以满足基本企业需求。 官方地址 github地址 2.harbor主要功能 基于角色访问控制（RBAC） 在企业中，通常有不同的开发团队负责不同的项目，镜像像代码一样，每个人角色不同需求也不同，因此就需要访问权限控制，根据角色分配相应的权限。 例如，开发人员需要对项目构建这就用到读写权限（push/pull），测试人员只需要读权限（pull），运维一般管理镜像仓库，具备权限分配能力，项目经理具有所有权限。 镜像复制 可以将仓库中的镜像同步到远程的Harbor，类似于MySQL主从同步功能。 LDAP Harbor支持LDAP认证，可以很轻易接入已有的LDAP。 镜像删除和空间回收 Harbor支持在Web删除镜像，回收无用的镜像，释放磁盘空间。 图形页面管理 用户很方面搜索镜像及项目管理。 审计 对仓库的所有操作都有记录。 REST API 完整的API，方便与外部集成。 3.harbor组件 组件 功能 harbor-adminserver 配置管理中心 harbor-db Mysql数据库 harbor-jobservice 负责镜像复制 harbor-log 记录操作日志 harbor-ui Web管理页面和API nginx 前端代理，负责前端页面和镜像上传/下载转发 redis 会话 registry 镜像存储 4.harbor部署环境要求 2核4G+40G硬盘 docker 17.06+ docker compose 1.18+ openssl 最新版 Hardware Resource Capacity Description CPU minimal 2CPU 4 CPU is preferred Mem minimal 4GB 8GB is preferred Disk minimal 40GB 160GB is preferred Software Software Version Description Docker engine version 17.06.0-ce+ or higher For installation instructions, please refer to: docker engine doc Docker Compose version 1.18.0 or higher For installation instructions, please refer to: docker compose doc Openssl latest is preferred Generate certificate and keys for Harbor Network ports Port Protocol Description 443 HTTPS Harbor portal and core API will accept requests on this port for https protocol, this port can change in config file(harbor入口和核心API将接受请求在此端口上的https协议，该端口可以在配置文件中更改) 4443 HTTPS Connections to the Docker Content Trust service for Harbor, only needed when Notary is enabled, This port can change in config file(连接到Docker内容信任服务的端口，只有在启用公证员时才需要，该端口可以在配置文件中更改) 80 HTTP Harbor portal and core API will accept requests on this port for http protocol(Harbor portal和core API将在这个端口上接受http协议请求) 5.安装docker compose harbor需要docker compose1.18+版本 docker compose安装地址 1.下载docker compose安装包 [root@docker02 ~]# curl -L \"https://github.com/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose 2.给docker compose赋予执行命令 [root@docker02 ~]# chmod +x /usr/local/bin/docker-compose 3.验证安装 [root@docker02 ~]# docker-compose -v docker-compose version 1.24.1, build 4667896b 6.安装harbor Harbor安装有3种方式： 在线安装：从Docker Hub下载Harbor相关镜像，因此安装软件包非常小 离线安装：安装包包含部署的相关镜像，因此安装包比较大 OVA安装程序：当用户具有vCenter环境时，使用此安装程序，在部署OVA后启动Harbor 1.下载安装包 [root@docker02 ~]# wget https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-offline-installer-v1.8.1.tgz 2.解压缩包到/usr/local [root@docker02 ~]# tar xf harbor-offline-installer-v1.8.1.tgz -C /usr/local/ 3.进入harbor目录，修改配置文件harbor.yml [root@docker02 harbor]# ls harbor.v1.8.1.tar.gz harbor.yml install.sh LICENSE prepare [root@docker02 harbor]# pwd /usr/local/harbor [root@docker02 harbor]# vim harbor.yml 5行 hostname: reg.mydomain.com //修改为IP地址或者域名 27行 harbor_admin_password: Harbor12345 //修改密码 4.准备配置文件，执行完成后会多出目录common 文件docker-compose.yml [root@docker02 harbor]# ./prepare [root@docker02 harbor]# ls common docker-compose.yml harbor.v1.8.1.tar.gz harbor.yml install.sh LICENSE prepare 5.安装并启动harbor [root@docker02 harbor]# ./install.sh 6.查看运行状态 [root@docker02 harbor]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d0d76fee6c95 goharbor/nginx-photon:v1.8.1 \"nginx -g 'daemon of…\" 4 minutes ago Up 4 minutes (healthy) 0.0.0.0:80->80/tcp nginx 6b40878a29a0 goharbor/harbor-jobservice:v1.8.1 \"/harbor/start.sh\" 4 minutes ago Up 4 minutes harbor-jobservice ad03c4b60343 goharbor/harbor-portal:v1.8.1 \"nginx -g 'daemon of…\" 4 minutes ago Up 4 minutes (healthy) 80/tcp harbor-portal 2fa3bbc7c1fb goharbor/harbor-core:v1.8.1 \"/harbor/start.sh\" 4 minutes ago Up 4 minutes (healthy) harbor-core cd9e038ab419 goharbor/harbor-registryctl:v1.8.1 \"/harbor/start.sh\" 4 minutes ago Up 4 minutes (healthy) registryctl 2738dc4ca7ab goharbor/redis-photon:v1.8.1 \"docker-entrypoint.s…\" 4 minutes ago Up 4 minutes 6379/tcp redis 457e7efc19c9 goharbor/registry-photon:v2.7.1-patch-2819-v1.8.1 \"/entrypoint.sh /etc…\" 4 minutes ago Up 4 minutes (healthy) 5000/tcp registry 37403c4806b4 goharbor/harbor-db:v1.8.1 \"/entrypoint.sh post…\" 4 minutes ago Up 4 minutes (healthy) 5432/tcp harbor-db 90ca1cc8b794 goharbor/harbor-log:v1.8.1 7.访问harbor 用户名 admin 密码 123456 宿主机80端口 登陆后首界面 到此，harbor http方式安装完成！！！ 8.https访问harbor 如果需要https访问harbor，需要做以下操作 harbor https官方文档 编辑harbor.yml，打开https项的注释，并且写上https证书的位置 https: # # https port for harbor, default is 443 port: 443 # # The path of cert and key files for nginx certificate: /etc/nginx/ssl_key/harbor/xxx.pem private_key: /etc/nginx/ssl_key/harbor/xxx.key 9.harbor使用 将镜像推送到harbor，需要注意的是，推送的格式如下 因此，需要先给镜像打标签 //查看centos镜像 docker images|grep centos centos latest 67fa590cfc1c 4 months ago 202MB centos 6.9 2199b8eb8390 9 months ago 195MB //给镜像按照格式打标签 docker image tag centos:latest harbor.pptfz.top/pptfz/pptfz-centos:latest 镜像名称 harbor地址 项目名 镜像标签名 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/容器/docker/5.1docker容器网络.html":{"url":"linux/容器/docker/5.1docker容器网络.html","title":"docker网络","keywords":"","body":"[toc] docker容器网络 docker容器网络示意图 一、docker网络相关信息 1.1docker网卡、网关信息 //宿主机查看docker默认网卡信息，安装完docker并启动后，会有docker0网卡，默认IP地址172.17.0.1 [root@docker1 ~]# ifconfig docker0: flags=4163 mtu 1500 inet 172.17.0.1 netmask 255.255.0.0 broadcast 172.17.255.255 inet6 fe80::42:61ff:fef9:e707 prefixlen 64 scopeid 0x20 ether 02:42:61:f9:e7:07 txqueuelen 0 (Ethernet) RX packets 50722 bytes 2466382 (2.3 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 82049 bytes 119024354 (113.5 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 //查看容器网关，网关默认为172.17.0.1 [root@ddc401b2bba8 /]# route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 172.17.0.1 0.0.0.0 UG 0 0 0 eth0 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth0 1.2启动一个busybox容器 1.宿主机docker1运行busybox容器 [root@docker1 ~]# docker run -it busybox Unable to find image 'busybox:latest' locally latest: Pulling from library/busybox 8e674ad76dce: Pull complete Digest: sha256:c94cf1b87ccb80f2e6414ef913c748b105060debda482058d2b8d0fce39f11b9 Status: Downloaded newer image for busybox:latest #查看网卡 / # ip a 1: lo: mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 32: eth0@if33: mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff inet 172.17.0.3/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever #查看网关 / # route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 172.17.0.1 0.0.0.0 UG 0 0 0 eth0 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth0 / # 2.busybox容器ping宿主机docker2 / # ping 10.0.0.61 3.宿主机docker2抓包查看 可以看到，涞源的IP是宿主机docker1的eth0网卡，说明宿主机进行了nat地址转换，将dockerIP 172.17.0.2转换为宿主机eth0IP [root@docker2 ~]# tcpdump -i eth0 icmp -nn tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 23:04:03.121544 IP 10.0.0.60 > 10.0.0.61: ICMP echo request, id 2816, seq 52, length 64 23:04:03.121576 IP 10.0.0.61 > 10.0.0.60: ICMP echo reply, id 2816, seq 52, length 64 1.3docker容器用到的内核转发和iptables规则 1.内核转发 #宿主机docker1查看内核转发，内核转发是docker开启的 [root@docker1 ~]# sysctl -a|grep -w net.ipv4.ip_forward net.ipv4.ip_forward = 1 2.iptables规则 #宿主机docker1查看iptables规则 [root@docker1 ~]# iptables -t nat -L -n Chain POSTROUTING (policy ACCEPT) target prot opt source destination MASQUERADE all -- 172.17.0.0/16 0.0.0.0/0 二、docker端口映射（允许外网访问容器） docker指定端口映射，docker会自动添加一条iptables规则来实现端口映射 -p 宿主机端口:容器端口 -p 宿主机IP:宿主机端口:容器端口 -p 宿主机IP::容器端口(随机端口) -p 宿主机端口:容器端口:udp -p 81:80 -p 443:443 可以指定多个-p 2.1方式一 -p 宿主机端口:容器端口 1.启动一个nginx容器并作端口映射 1.启动一个nginx容器，并将宿主机8080端口映射到容器80端口 [root@docker1 ~]# docker run -d -p 8080:80 nginx:latest ccef9a059f90886370577063f68bc4cab495f6497d394d16941f936b9fb8468a 2.宿主机查看iptables规则，可以看到iptables将宿主机8080端口映射到了容器的80端口 [root@docker1 ~]# iptables -t nat -L -n Chain DOCKER (2 references) target prot opt source destination RETURN all -- 0.0.0.0/0 0.0.0.0/0 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 to:172.17.0.4:80 2.宿主机80端口访问 3.宿主机8080端口访问 2.2方式二 -p 宿主机IP:宿主机端口:容器端口 1.宿主机eth0网卡添加一个辅助IP 1.添加一个辅助IP [root@docker1 ~]# ifconfig eth0:1 10.0.0.66/24 up 2.查看网卡eth0 [root@docker1 ~]# ifconfig |grep -A 2 eth0 eth0: flags=4163 mtu 1500 inet 10.0.0.60 netmask 255.255.255.0 broadcast 10.0.0.255 inet6 fe80::20c:29ff:fe65:ee41 prefixlen 64 scopeid 0x20 -- eth0:1: flags=4163 mtu 1500 inet 10.0.0.66 netmask 255.255.255.0 broadcast 10.0.0.255 ether 00:0c:29:65:ee:41 txqueuelen 1000 (Ethernet) 2.启动容器 //添加辅助IP后，宿主机的80端口就可以再次使用 [root@docker1 ~]# docker run -d -p 10.0.0.60:80:80 nginx:latest 80257a4c036183df9106d38992bf2762007bc5bc1550bed408cf4232eb1ae160 [root@docker1 ~]# docker run -d -p 10.0.0.66:80:80 nginx:latest ccabd14eea387c05ce7af46c002c8f9621aa1907642551aff79f7ca7e4bd25aa 3.查看端口监听 [root@docker1 ~]# netstat -ntpl Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 10.0.0.66:80 0.0.0.0:* LISTEN 18647/docker-proxy tcp 0 0 10.0.0.60:80 0.0.0.0:* LISTEN 2942/docker-proxy 4.查看iptables转发规则 //访问宿主机80端口，将请求转发至第一个容器80端口；访问宿主机辅助IP80端口，将请求转发至第二个容器80端口 [root@docker1 ~]# iptables -t nat -L -n Chain DOCKER (2 references) target prot opt source destination RETURN all -- 0.0.0.0/0 0.0.0.0/0 DNAT tcp -- 0.0.0.0/0 10.0.0.60 tcp dpt:80 to:172.17.0.4:80 DNAT tcp -- 0.0.0.0/0 10.0.0.66 tcp dpt:80 to:172.17.0.5:80 2.3方式三 -p 宿主机IP::容器端口(宿主机端口随机) 宿主机端口不写，默认随机启动一个端口 1.启动一个容器 1.启动一个容器 [root@docker1 ~]# docker run -d -p 10.0.0.60::80 nginx:latest fbf5d8973f9094bf18c0e0b9ad63fa3b350e1bd2afc94083824e081c20fdb0c8 2.查看启动的容器信息，可以看到，随机启动了一个32768端口 [root@docker1 ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES fbf5d8973f90 nginx:latest \"nginx -g 'daemon of…\" 11 seconds ago Up 10 seconds 10.0.0.60:32768->80/tcp elastic_ritchie 2.浏览器访问32768端口 2.4方式四 -p 宿主机端口:容器端口:udp 映射默认采用tcp方式，此方式为指定udp方式 例如，容器中运行了dns服务，需要指定为udp方式 2.5方式五 -p 81:80 -p 443:443 可以指定多个-p 容器中运行了多个服务，此时需要映射多个端口 三、docker端口随机映射 -P 与-p 宿主机IP::容器端口(宿主机端口随机)方式相同 1.启动一个容器 1.启动一个容器 [root@docker1 ~]# docker run -d -P nginx:latest c1ffac8baf576208bbf107a24fd83e832b4811ad270bf8006d636e92b6438c39 2.查看容器信息，可以看到将宿主机32769映射到docker80端口 [root@docker1 ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c1ffac8baf57 nginx:latest \"nginx -g 'daemon of…\" 33 seconds ago Up 32 seconds 0.0.0.0:32769->80/tcp clever_lichterman docker端口映射核心 通过iptables实现端口映射！！！ 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/容器/docker/5.2docker网络模式.html":{"url":"linux/容器/docker/5.2docker网络模式.html","title":"docker网络模式","keywords":"","body":"[toc] docker网络模式 指定容器网络类型 --net或者--network 1.docker网络模式 docker网络模式，共4种 模式 含义 bridge 桥接模式，默认 host 与宿主机共享网络 none 无网络 container 与容器共享网络 2.查看docker网络模式 2.1查看docker网络 查看docker网络，默认有3种 [root@docker01 ~]# docker network ls NETWORK ID NAME DRIVER SCOPE 752c74d78d18 bridge bridge local 970755d8e30b host host local f144b44069ab none null local 2.2第一种 bridge，桥接，默认模式 //启动一个容器test1 [root@docker01 ~]# docker run -d --name test1 busybox:latest vi 1 698a85c6cf33bc6a02903f0e6ac7f96e429247a17bef705467a9879c1d2de8b2 //查看容器网络信息 [root@docker01 ~]# docker inspect test1 \"Networks\": { \"bridge\": { //容器网络模式为bridge \"IPAMConfig\": null, \"Links\": null, \"Aliases\": null, \"NetworkID\": \"752c74d78d186a14b2c6e1659c54b50d3396a89fec4b34664c7991ddc50fac55\", \"EndpointID\": \"\", \"Gateway\": \"\", \"IPAddress\": \"\", \"IPPrefixLen\": 0, \"IPv6Gateway\": \"\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"MacAddress\": \"\", \"DriverOpts\": null } 2.3第二种 host，与宿主机共享网络 1.查看宿主机网络，eth0 IP为10.0.0.20 [root@docker01 ~]# ip a s eth0 2: eth0: mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:49:fe:8f brd ff:ff:ff:ff:ff:ff inet 10.0.0.20/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe49:fe8f/64 scope link valid_lft forever preferred_lft forever 2.启动一个容器test2，设置网络模式为host [root@docker01 ~]# docker run -it --name test2 --net host busybox:latest / # 3.查看容器网络，可以看到，启动的容器网卡与宿主机一致 / # ip a s eth0 2: eth0: mtu 1500 qdisc pfifo_fast qlen 1000 link/ether 00:0c:29:49:fe:8f brd ff:ff:ff:ff:ff:ff inet 10.0.0.20/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe49:fe8f/64 scope link valid_lft forever preferred_lft forever 2.4第三种 none，无网络模式 1.启动一个容器test3，网络模式指定为none [root@docker01 ~]# docker run -it --name test3 --net none busybox:latest 2.查看IP，只有lo网卡 / # ip a 1: lo: mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 3.无法上网 / # ping baidu.com ping: bad address 'baidu.com' 4.没有路由信息 / # route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 2.5第四种 container，共享容器网络 container:要共享的容器ID或名称 1.先启动一个容器test5，容器IP为172.17.0.4，可以上网 [root@docker01 ~]# docker run -it --name test5 busybox:latest / # ip a 1: lo: mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 42: eth0@if43: mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:04 brd ff:ff:ff:ff:ff:ff inet 172.17.0.4/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever / # ping baidu.com PING baidu.com (220.181.38.148): 56 data bytes 64 bytes from 220.181.38.148: seq=0 ttl=127 time=9.747 ms ^C --- baidu.com ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 9.747/9.747/9.747 ms 2.再次启动一个容器test6，指定网络共享容器test5 [root@docker01 ~]# docker run -it --name test6 --net container:test5 busybox:latest 3.查看IP，可以看到IP地址与共享的容器test5一样 / # ip a 1: lo: mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 46: eth0@if47: mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:04 brd ff:ff:ff:ff:ff:ff inet 172.17.0.4/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/容器/docker/6.docker数据卷.html":{"url":"linux/容器/docker/6.docker数据卷.html","title":"6.docker数据卷","keywords":"","body":"[toc] docker数据卷 1.docker挂载卷的方式 方式一：宿主机创建一个卷，然后挂载到容器某一个路径下，适合做持久化 此方式将容器中的数据拷贝到宿主机的卷中，此时容器中的目录内容是什么决定了宿主机中的目录内容 方式二：直接将宿主机的某一个目录挂载到容器某一个路径下 此方式将宿主机的目录拷贝到容器的目录中，此时宿主机中的目录内容是什么决定了容器中的目录内容 2.docker创建数据卷示例 2.1创建一个名为docker-volume数据卷 [root@docker1 ~]# docker volume create docker-volume docker-volume 2.2查看创建的数据卷 [root@docker1 ~]# docker volume ls DRIVER VOLUME NAME local docker-volume 2.3查看数据卷具体信息，存放的位置等 默认存放于/var/lib/docker/volumes/docker-volume/_data [root@docker1 ~]# docker volume inspect docker-volume [ { \"CreatedAt\": \"2019-06-24T22:39:03+08:00\", \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/docker-volume/_data\", \"Name\": \"docker-volume\", \"Options\": {}, \"Scope\": \"local\" } ] 2.4启动一个nginx容器，并将刚才创建的数据卷挂载到容器的/usr/share/nginx/html [root@docker1 ~]# docker run -d -p 80:80 -v docker-volume:/usr/share/nginx/html nginx:latest 36d6db627b83638ac9a03025c3f7b7b7fd0688ae4a8d3bf75b422ade4016c0a2 #参数说明 -d 后台运行 -p 端口映射 -v 卷名称:要挂载到容器的路径 2.5浏览器访问刚启动的容器 2.6将nginx容器的默认显示界面重写 [root@docker1 ~]# echo hehe > /var/lib/docker/volumes/docker-volume/_data/index.html 2.7再次访问容器，可以看到，内容已经变化 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/容器/docker/7.docker手动构建镜像.html":{"url":"linux/容器/docker/7.docker手动构建镜像.html","title":"7.docker手动构建镜像","keywords":"","body":"[toc] docker手动构建镜像 1.手动创建docker镜像步骤 第一步、手动启动一个容器，在容器中安装自定义服务 第二步、docker commit 把容器提交为镜像 第三步、测试镜像功能 2.ssh服务镜像制作示例 2.1启动一个镜像 1.启动一个centos6.9 [root@docker1 ~]# docker run -it -p 222:22 centos:6.9 /bin/bash [root@ab815c87fb9c /]# 2.修改镜像yum源 [root@ab815c87fb9c /]# curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo [root@ab815c87fb9c /]# curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-6.repo [root@ab815c87fb9c /]# yum clean all && yum makecache 2.2镜像中安装ssh服务并启动 1.安装ssh服务 [root@ab815c87fb9c /]# yum -y install openssh-server 2.启动ssh 第一次启动服务，目的是生成密钥对 [root@ab815c87fb9c ~]# service sshd start Generating SSH2 RSA host key: [ OK ] Generating SSH1 RSA host key: [ OK ] Generating SSH2 DSA host key: [ OK ] Starting sshd: [ OK ] 3.验证服务启动状况 [root@ab815c87fb9c ~]# service sshd status openssh-daemon (pid 163) is running... [root@ab815c87fb9c ~]# netstat -ntpl Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 163/sshd tcp 0 0 :::22 :::* LISTEN 163/sshd 2.3设置容器root密码 [root@ab815c87fb9c /]# echo 1|passwd --stdin root Changing password for user root. passwd: all authentication tokens updated successfully. 2.4终端访问刚创建的镜像 222端口 2.5提交镜像 1.临时退出镜像 ctrl+p 然后 ctrl+q 2.查看容器信息 [root@docker1 ~]# docker ps -al CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ab815c87fb9c centos:6.9 \"/bin/bash\" 24 minutes ago Up 24 minutes 0.0.0.0:222->22/tcp priceless_yonath 3.提交镜像 commit后可以加容器名称或容器ID [root@docker1 ~]# docker commit ab815c87fb9c centos6.9_ssh:v1.1 sha256:0fcaf66caf5a6bd48c18e0172827aa04a2bf9fcb5a8f849b20f58a116deac5b7 4.查看镜像 [root@docker1 ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE centos6.9_ssh v1.1 0fcaf66caf5a About a minute ago 504MB 2.6测试镜像 注意：启动的容器提供服务的同时必须夯住 1.启动容器，使用刚才提交的的镜像,这里使用 sshd -D 使容器能夯住 [root@docker1 ~]# docker run -d -p 223:22 centos6.9_ssh:v1.1 /usr/sbin/sshd -D ead64de773abd3c1d08647575d096789a5a28d64960f225f7a42ec9cf01211ee 2.查看容器 [root@docker1 ~]# docker ps -al CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ead64de773ab centos6.9_ssh:v1.1 \"/usr/sbin/sshd -D\" 51 seconds ago Up 50 seconds 0.0.0.0:223->22/tcp stupefied_liskov 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/容器/docker/8.dockerfile自动构建docker镜像.html":{"url":"linux/容器/docker/8.dockerfile自动构建docker镜像.html","title":"8.docker自动构建镜像","keywords":"","body":"[toc] dockerfile自动构建docker镜像 1.dockerfile 1.1 dockerfile说明 dockerfile定义 dockerfile类似ansible的剧本 dockerfile特点 1.更适合传输，文件体积小 2.实现更加定制化 dockerfile主要组成部分 基础镜像信息 FROM centos:6.9 制作镜像操作指令 RUN yum -y install httpd 容器启动时执行指令 CMD [\"/bin/bash\"] 1.2 dockerfile常用指令 指令 含义 FROM 指定基础镜像，基于哪个镜像 MAINTAINER 构建者信息(不是必须，只是对构建的镜像做一个说明) RUN 指定运行命令 ADD 将宿主机文件拷贝到容器中，会自动解压；可以拷贝远程主机文件 COPY 复制文件 WORKDIR 指定工作目录 VOLUME 设卷，挂载宿主机目录 EXPOSE 指定对外的端口 CMD 容器启动后要运行的命令，容易被替换 ENV 环境变量 ENTRYPOINT 容器启动后执行的命令（无法被替换，启动容器的时候指定的命令，会被当成参数） 1.2.1 dockerfile常用指令 FROM 用来指定基础镜像 # 指定基础镜像为centos7.8 FROM centos:7.8 ..... ⚠️FROM中有一个特殊的镜像 scratch 表明这是一个空白镜像 FROM scratch ...... 1.2.2 dockerfile常用指令 RUN 指定镜像中运行的命令 shell 格式：RUN RUN echo 'Hello, Docker!' > /usr/share/nginx/html/index.html exec 格式：RUN [\"可执行文件\", \"参数1\", \"参数2\"]，不常用 ⚠️RUN指令应该尽量写成一条，以减少镜像提及 # 每一个RUN都会生成一层镜像 RUN nginx -t RUN nginx -s reload # 应该写成一条 RUN nginx -t && nginx -s reload 1.2.3 dockerfile常用指令 ADD 将本机或远程文件拷贝到镜像中，如果是压缩文件则会自动解压 exec 格式用法：ADD [\"\",... \"\"]，特别适合路径中带有空格的情况。 shell 格式用法：ADD ... # 拷贝远程地址中的文件到镜像中的/opt下 ADD http://www.baidu.com/baidu.tar.gz /opt ⚠️需要注意的是对于从远程 URL 获取资源的情况，由于 ADD 指令不支持认证，如果从远程获取资源需要认证，则只能使用RUN wget 或 RUN curl 替代了 1.2.4 dockerfile常用指令 COPY 将本机文件拷贝到镜像中 exec 格式用法：COPY [\"\",... \"\"]，特别适合路径中带有空格的情况。 shell 格式用法：COPY ... # 将当前上下文中的test.file拷贝到镜像中的/opt下 COPY test.file /opt 1.2.5 dockerfile常用指令 WORKDIR 指定容器中RUN、CMD等命令的工作目录 # 在镜像中/opt下生成文件file WORKDIR /opt RUN echo 'test' > file 1.2.6 dockerfile常用指令 VOLUME VOLUME 指令用于暴露任何数据库存储文件，配置文件，或容器创建的文件和目录。强烈建议使用 VOLUME 来管理镜像中的可变部分和用户可以改变的部分。 # 对外暴露/data/mysql/conf下的所有内容 VOLUME /data/mysql/conf 1.2.7 dockerfile常用指令 EXPOSE 指定镜像对外开放的端口 # 指定镜像对外开放的端口 EXPOSE 80 1.2.8 dockerfile常用指令 ENV 指定环境变量，通过ENV定义的环境变量，可以被后面的所有指令中使用 # 指定myslq数据目录的环境变量 ENV mysql_data_path /data/mysql/data 1.2.9 dockerfile常用指令 CMD 容器启动以后，默认的执行命令 如果我们执行 docker run 没有指定任何的执行命令或者 Dockerfile 里面也没有指定 ENTRYPOINT，那么就会使用 CMD 指定的执行命令执行了。这也说明了 ENTRYPOINT 才是容器启动以后真正要执行的命令。 ⚠️一个 Dockerfile 如果有多个 CMD，只有最后一个生效，官网推荐采用这种方式。 CMD 总共有三种用法： CMD [\"executable\", \"param1\", \"param2\"] # exec 形式 CMD [\"param1\", \"param2\"] # 作为 ENTRYPOINT 的默认参数 CMD command param1 param2 # shell 形式 其中 shell 形式，就是没有中括号的形式，命令 command 默认是在/bin/sh -c下执行的，比如： FROM busybox CMD echo \"hello cmd shell\" CMD 示例1 docker run没有指定任何执行命令，会使用CMD指定的执行命令 # 编辑dockerfile cat > Dockerfile CMD 示例2 docker run没有指定任何执行命令，会使用CMD指定的执行命令，并且只有最后一个CMD命令生效 # 编辑dockerfile cat > Dockerfile CMD 示例3 docker run 指定了执行命令，则所有的CMD命令都不生效 # 编辑dockerfile cat > Dockerfile 1.2.10 dockerfile常用指令 ENTRYPOINT 容器启动后执行的命令（无法被替换，启动容器的时候指定的命令，会被当成参数） 根据官方定义来说 ENTRYPOINT 才是用于定义容器启动以后的执行程序的，允许将镜像当成命令本身来运行（用 CMD 提供默认选项），从名字也可以理解，是容器的入口。ENTRYPOINT 一共有两种用法： ENTRYPOINT [\"executable\", \"param1\", \"param2\"] (exec 形式) ENTRYPOINT command param1 param2 (shell 形式) 对应命令行 exec 模式，也就是带中括号的。和 CMD 的中括号形式是一致的，但是这里貌似是在shell的环境下执行的，与cmd有区别。如果 run 命令后面有执行命令，那么后面的全部都会作为 ENTRYPOINT 的参数。如果 run 后面没有额外的命令，但是定义了 CMD，那么 CMD 的全部内容就会作为 ENTRYPOINT 的参数，这同时是上面我们提到的 CMD 的第二种用法。所以说 ENTRYPOINT 不会被覆盖。当然如果要在 run 里面覆盖，也是有办法的，使用--entrypoint参数即可。 ENTRYPOINT 示例1 docker run没有指定任何执行命令 # 编辑dockerfile cat > Dockerfile ENTRYPOINT 示例2 docker run 指定执行命令 # 编辑dockerfile cat > Dockerfile ⚠️对于 shell 模式的，任何 docker run 和 CMD 的参数都无法被传入到 ENTRYPOINT 里 示例如下，我们可以发现entrypoint后加不加执行的命令都不会有任何输出。所以一般情况下对于 ENTRYPOINT 来说使用中括号的 exec 形式更好。 # 编辑Dockerfile cat > Dockerfile 一般会用 ENTRYPOINT 的中括号形式作为 Docker 容器启动以后的默认执行命令，里面放的是不变的部分，可变部分比如命令参数可以使用 CMD 的形式提供默认版本，也就是 run 里面没有任何参数时使用的默认参数。如果我们想用默认参数，就直接 run，否则想用其他参数，就 run 里面加上参数。 2.dockerfile构建镜像步骤 第一步、编写dockerfile 第二步、docker build构建镜像(构建镜像的时候，dockerfile中有多少个命令就提交多少个临时容器，最后再总提交一次并且每次提交的临时容器会被删除) 第三步、启动容器测试 3.dockerfile简单示例 3.1示例一：基础dockerfile---ssh服务镜像 3.1.1 新建目录，专门存放dockerfile mkdir /dockerfile/centos6.9_ssh 3.1.2 编辑Dockerfile $ cat > Dockerfile 3.1.3 构建镜像 # 基于dockerfile开始构建镜像 $ docker build -t centos6.9_ssh:v2.1 . 。。。 Successfully built 6a6d13135aaa Successfully tagged centos6.9_ssh:v2.1 # 说明 build 构建镜像 -t 指定镜像名称 . 指定上下文路径，而不是Dockerfile文件路径 3.1.4 测试镜像 # 启动一个容器测试镜像 $ docker run -d -p 3222:22 centos6.9_ssh:v2.1 a57c6406e001f4a295ec8c847627326403f83a44d72adf719d44f8f3f4463ebc # 查看镜像,可以看到，容器启动时没有指定默认命令，已从dockerfile中CMD读取初始运行名令 $ docker ps -al CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a57c6406e001 centos6.9_ssh:v2.1 \"/usr/sbin/sshd -D\" 29 seconds ago Up 28 seconds 0.0.0.0:3222->22/tcp elated_kirch 3.1.5 ssh连接测试 3.2 示例二：基础Dockerfile--多服务 3.2.1 新建目录，专门存放Dockerfile mkdir -p /dockerfile/centos6.9_ssh_http 3.2.2 编辑Dockerfile $ cd /dockerfile/centos6.9_ssh_http $ cat > Dockerfile 3.2.3 编辑脚本 cat >init.sh 3.2.4 构建镜像 # 基于Dockerfile构建镜像 $ docker build -t centos6.9_ssh_http:v2.1 . 。。。 Successfully built 1f4662cc114d Successfully tagged centos6.9_ssh_http:v2.1 # 查看镜像 $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE centos6.9_ssh_http v2.1 1f4662cc114d 7 minutes ago 310MB 3.2.5 启动容器 # 启动容器 $ docker run -d -p 5222:22 -p 88:80 centos6.9_ssh_http:v2.1 9880dd05d4cdc3849c4e62888d16592e6551ee3fb3fbae314ffd33c2744874c9 # 查看容器 $ docker ps -al CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES fa48d4052ff8 centos6.9_ssh_http:v2.1 \"/bin/bash /init.sh\" 4 seconds ago Up 3 seconds 0.0.0.0:5222->22/tcp, 0.0.0.0:88->80/tcp zen_payne 3.2.6 测试镜像 ssh镜像测试 apache镜像测试 $ curl -I 10.0.0.60:88 HTTP/1.1 403 Forbidden Date: Sun, 30 Jun 2019 15:37:34 GMT Server: Apache/2.2.15 (CentOS) Accept-Ranges: bytes Content-Length: 4961 Connection: close Content-Type: text/html; charset=UTF-8 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/容器/docker/9.修改docker镜像、容器默认存放位置.html":{"url":"linux/容器/docker/9.修改docker镜像、容器默认存放位置.html","title":"9.修改docker镜像、容器默认存放位置","keywords":"","body":"[toc] 修改docker镜像、容器默认存放位置 1.修改需求 因为Docker默认是存放在系统盘中/var/lib/docker的，当用的时间比较久后，产生的镜像及容器越来越多之后，可能会导致你的系统盘满了，这时我们需要将Docker的镜像及容器指向另外一个路径 2.修改步骤 2.1docker镜像、容器默认存放于/var/lib/docker [root@docker02 ~]# cd /var/lib/docker [root@docker02 docker]# ls builder buildkit containers image network overlay2 plugins runtimes swarm tmp trust volumes 2.2修改docker镜像、容器存放位置 1.编辑配置文件 /etc/docker/daemon.json 加入以下一行 \"graph\":\"/data/docker\" #目录可自行修改 2.新建目录 [root@docker02 ~]# mkdir -p /data/docker 3.修改后的配置文件 [root@docker02 ~]# cat /etc/docker/daemon.json { \"registry-mirrors\": [\"https://gqk8w9va.mirror.aliyuncs.com\"], \"graph\":\"/data/docker\" } 2.3重启docker [root@docker02 ~]# systemctl restart docker 2.4拷贝原镜像和容器到新目录 1.修改完docker镜像、容器存放位置后，原先的镜像和容器查看不存在 [root@docker02 ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE [root@docker02 ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2.拷贝原镜像和容器到新目录 [root@docker02 ~]# \\cp -rp /var/lib/docker/* /data/docker/ 3重启docker [root@docker02 ~]# systemctl restart docker 4.查看修改 [root@docker02 ~]# docker info|grep Docker Docker Root Dir: /data/docker 5.查看镜像和容器，已恢复 [root@docker02 ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE jumpserver/jms_all 1.4.8 e9274ba449e8 3 months ago 1.31GB [root@docker02 ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2789b0d4a200 jumpserver/jms_all:1.4.8 \"entrypoint.sh\" About an hour ago Up 49 seconds 0.0.0.0:80->80/tcp, 0.0.0.0:2222->2222/tcp jms_all 6.测试镜像和容器都无问题后可以删除/var/lib/docker [root@docker02 ~]# rm -rf /var/lib/docker 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/容器/docker/10.docker容器间互联 --link.html":{"url":"linux/容器/docker/10.docker容器间互联 --link.html","title":"10.docker容器间互联 --link","keywords":"","body":"[toc] docker容器间互联 --link(单方向) 1.先启动一个容器 1.启动一个nginx容器 [root@docker01 ~]# docker run -d --name nginx nginx 656aa889b827eaef68d99538c691a4872139f0c82ae72a7810fed64dbe2d6bac 2.查看启动的容器 [root@docker01 ~]# docker ps -al CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 656aa889b827 nginx \"nginx -g 'daemon of…\" 3 seconds ago Up 2 seconds 80/tcp nginx 2.再次启动一个容器，添加--link参数 1.再次启动一个busybox容器，--link后面为 要连接的容器名称:连接的容器的别名 [root@docker01 ~]# docker run -it --link nginx:nginx busybox:latest 2.因为要连接的容器的名称为nginx，因此ping nginx，可以ping通 / # ping nginx PING nginx (172.17.0.5): 56 data bytes 64 bytes from 172.17.0.5: seq=0 ttl=64 time=0.814 ms 64 bytes from 172.17.0.5: seq=1 ttl=64 time=0.212 ms ^C --- nginx ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.212/0.513/0.814 ms 3.查看hosts文件，可以看到有nginx的hosts解析 / # cat /etc/hosts 127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters 172.17.0.5 nginx 656aa889b827 172.17.0.6 4d6a7fb490e8 3.再次启动一个容器 1.再次启动一个容器，--link后边为 要连接的容器:容器别名 [root@docker01 ~]# docker run -it --link nginx:web01 busybox:latest 2.ping容器名称或者容器别名都可以ping通 / # ping nginx PING nginx (172.17.0.5): 56 data bytes 64 bytes from 172.17.0.5: seq=0 ttl=64 time=0.474 ms ^C --- nginx ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.474/0.474/0.474 ms / # ping web01 PING web01 (172.17.0.5): 56 data bytes 64 bytes from 172.17.0.5: seq=0 ttl=64 time=0.137 ms ^C --- web01 ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.137/0.137/0.137 ms 3.查看hosts文件，可以看到会把要连接的容器的别名、容器ID、容器名称都添加 / # cat /etc/hosts 127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters 172.17.0.5 web01 656aa889b827 nginx 172.17.0.6 c94f83726145 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/容器/docker/12.1docker跨主机通信之macvlan.html":{"url":"linux/容器/docker/12.1docker跨主机通信之macvlan.html","title":"macvlan","keywords":"","body":"[toc] docker跨主机通信之macvlan 1.macvlan说明 macvlan可以虚拟多个mac地址，相当于虚拟多个网卡 macvlan优点 与局域网其他主机处于同一个网段 macvlan缺点 每次启动容器都需要手动指定IP地址 2.macvlan跨主机通信示例 2.1实验环境 主机名 IP docker01 10.0.0.60 docker02 10.0.0.61 2.2docker01和docker02都创建macvlan网络 //在docker01和docker02上相同操作 [root@docker01 ~]# docker network create --driver macvlan --subnet 10.0.0.0/24 --gateway 10.0.0.254 -o parent=eth0 macvlan_1 #参数说明 --driver //指定网络类型 --subnet //指定网段 ---gateway //指定网关 -o parent=eth0 //指定基于哪块物理网卡 macvlan_1 //网络名称，可任意 2.3查看创建的网络 //可以看到创建的名称为macvlan_1的网络 [root@docker01 ~]# docker network ls NETWORK ID NAME DRIVER SCOPE f2228fe9ebe8 bridge bridge local 1c11b715a65c host host local b0ddd6fd07ed macvlan_1 macvlan local 74d6b08c35c5 none null 2.4创建使用macvlan网络的容器 1.docker01启动一个容器，IP指定为10.0.0.3 [root@docker01 ~]# docker run -it --network macvlan_1 --ip=10.0.0.3 centos:latest /bin/bash 2.docker01启动的容器ping docker02宿主机10.0.0.61和docker02启动的容器10.0.0.4 [root@33da263729a4 /]# ping -c 3 10.0.0.61 PING 10.0.0.61 (10.0.0.61) 56(84) bytes of data. 64 bytes from 10.0.0.61: icmp_seq=1 ttl=64 time=0.960 ms 64 bytes from 10.0.0.61: icmp_seq=2 ttl=64 time=0.358 ms 64 bytes from 10.0.0.61: icmp_seq=3 ttl=64 time=0.242 ms --- 10.0.0.61 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2001ms rtt min/avg/max/mdev = 0.242/0.520/0.960/0.314 ms [root@33da263729a4 /]# ping -c 3 10.0.0.4 PING 10.0.0.4 (10.0.0.4) 56(84) bytes of data. 64 bytes from 10.0.0.4: icmp_seq=1 ttl=64 time=0.714 ms 64 bytes from 10.0.0.4: icmp_seq=2 ttl=64 time=0.457 ms 64 bytes from 10.0.0.4: icmp_seq=3 ttl=64 time=0.275 ms --- 10.0.0.4 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2002ms rtt min/avg/max/mdev = 0.275/0.482/0.714/0.180 ms 3.docker02启动一个容器，IP指定为10.0.0.4 [root@docker02 ~]# docker run -it --network macvlan_1 --ip=10.0.0.4 centos:latest /bin/bash 4.docker02启动的容器ping docker01宿主机10.0.0.60和docker02启动的容器10.0.0.3 [root@93ecb0bf3aa9 /]# ping -c 3 10.0.0.60 PING 10.0.0.60 (10.0.0.60) 56(84) bytes of data. 64 bytes from 10.0.0.60: icmp_seq=1 ttl=64 time=0.687 ms 64 bytes from 10.0.0.60: icmp_seq=2 ttl=64 time=0.390 ms 64 bytes from 10.0.0.60: icmp_seq=3 ttl=64 time=0.242 ms --- 10.0.0.60 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2000ms rtt min/avg/max/mdev = 0.242/0.439/0.687/0.186 ms [root@93ecb0bf3aa9 /]# ping -c 3 10.0.0.3 PING 10.0.0.3 (10.0.0.3) 56(84) bytes of data. 64 bytes from 10.0.0.3: icmp_seq=1 ttl=64 time=0.542 ms 64 bytes from 10.0.0.3: icmp_seq=2 ttl=64 time=0.486 ms 64 bytes from 10.0.0.3: icmp_seq=3 ttl=64 time=0.345 ms --- 10.0.0.3 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2002ms rtt min/avg/max/mdev = 0.345/0.457/0.542/0.086 ms ⚠️⚠️⚠️如果ping不通，需要将网卡设置为混杂模式 ip link set eth0 promisc on 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/容器/docker/12.2docker跨主机通信之overlay.html":{"url":"linux/容器/docker/12.2docker跨主机通信之overlay.html","title":"overlay","keywords":"","body":"[toc] docker跨主机通信之overlay 1.overlay说明 overlay优点 会自动分配IP地址，需要consul数据库 网络类型为overlay的容器默认有两块网卡 eth0 用于跨主机间容器通信 eth1 用于连接外网 网关默认为172.18.0.1，是宿主机docker_gwbridge网卡的IP地址 overlay网络跨主机通信示意图 2.overlay跨主机通信示例 2.1实验环境 主机名 IP docker01 10.0.0.60 docker02 10.0.0.61 2.2docker01操作 2.2.1启动consul容器 [root@docker01 ~]# docker run -d -p 8500:8500 -h consul --name consul --restart=always progrium/consul -server -bootstrap 2.2.2查看容器，容器映射了好多端口 [root@docker01 ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 84c733c42a91 progrium/consul \"/bin/start -server …\" 24 seconds ago Up 23 seconds 53/tcp, 53/udp, 8300-8302/tcp, 8400/tcp, 8301-8302/udp, 0.0.0.0:8500->8500/tcp consul 2.2.3容器启动后可以访问一个web界面 10.0.0.60:8500 2.2.4修改docker配置文件/etc/docker/daemon.json 1.修改配置文件/etc/docker/daemon.json，加入以下三行 \"hosts\":[\"tcp://0.0.0.0:2376\",\"unix:///var/run/docker.sock\"], \"cluster-store\": \"consul://10.0.0.60:8500\", \"cluster-advertise\": \"10.0.0.60:2376\" #参数说明 \"hosts\":[\"tcp://0.0.0.0:2376\",\"unix:///var/run/docker.sock\"], //本地监听tcp2376端口，同时指定sock文件 \"cluster-store\": \"consul://10.0.0.60:8500\", //集群信息存放位置,这里为docker01 10.0.0.60 \"cluster-advertise\": \"10.0.0.60:2376\" //客户机自身IP地址,这里为docker01 10.0.0.60 2.修改docker服务启动文件，否则后续重启docker会报错 [root@docker01 ~]# vim /usr/lib/systemd/system/docker.service 修改 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock 修改为 ExecStart=/usr/bin/dockerd 3.重启docker [root@docker01 ~]# systemctl daemon-reload && systemctl restart docker 2.3docker02操作 2.3.1启动consul容器 [root@docker02 ~]# docker run -d -p 8500:8500 -h consul --name consul --restart=always progrium/consul -server -bootstrap 2.3.2查看容器，容器映射了好多端口 [root@docker02 ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a793cf0b4aed progrium/consul \"/bin/start -server …\" 37 seconds ago Up 35 seconds 53/tcp, 53/udp, 8300-8302/tcp, 8400/tcp, 8301-8302/udp, 0.0.0.0:8500->8500/tcp consul 2.3.3容器启动后可以访问一个web界面 10.0.0.61:8500 2.3.4修改docker配置文件/etc/docker/daemon.json 1.修改配置文件/etc/docker/daemon.json，加入以下三行 \"hosts\":[\"tcp://0.0.0.0:2376\",\"unix:///var/run/docker.sock\"], \"cluster-store\": \"consul://10.0.0.60:8500\", \"cluster-advertise\": \"10.0.0.60:2376\" #参数说明 \"hosts\":[\"tcp://0.0.0.0:2376\",\"unix:///var/run/docker.sock\"], //本地监听tcp2376端口，同时指定sock文件 \"cluster-store\": \"consul://10.0.0.60:8500\", //集群信息存放位置，这里为docker01 10.0.0.60 \"cluster-advertise\": \"10.0.0.61:2376\" //客户机自身IP地址，这里为docker02 10.0.0.61 2.修改docker服务启动文件，否则后续重启docker会报错 [root@docker02 ~]# vim /usr/lib/systemd/system/docker.service 修改 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock 修改为 ExecStart=/usr/bin/dockerd 3.重启docker [root@docker02 ~]# systemctl daemon-reload && systemctl restart docker 2.3.5最终效果 访问docker1 consul的web界面 KEY/VALUE-->docker-->nodes 正确情况为出现两个docker节点 到此，基础环境配置完成！！！ 2.4通信测试 2.4.1docker01创建overlay网络，会自动同步到docker02，因为docker01和docker02都在consul集群中 1.docker01创建overlay网络 [root@docker01 ~]# docker network create -d overlay --subnet 172.16.1.0/24 --gateway 172.16.1.254 overlay1 7a836393a60d6f87bd0053b2d75198a60960db8a98a34488d145eef1fef35711 2.查看创建的网络，名称为overlay，类型为global，与其余网络类型不一样 [root@docker01 ~]# docker network ls NETWORK ID NAME DRIVER SCOPE d25efee562f5 bridge bridge local 1c11b715a65c host host local b0ddd6fd07ed macvlan_1 macvlan local 74d6b08c35c5 none null local 7a836393a60d overlay1 overlay global 3.docker02查看网络，可以看到docker01创建的overlay网络自动同步到了docker02 [root@docker02 ~]# docker network ls NETWORK ID NAME DRIVER SCOPE 59e0e535367b bridge bridge local c464712c8392 host host local 7d9727dd0d2d macvlan_1 macvlan local be782cbc19a4 none null local 7a836393a60d overlay1 overlay global 2.4.2docker01启动容器overlay1 1.启动一个容器，指定网络为之前创建的overlay1 [root@docker01 ~]# docker run -it --net overlay1 --name overlay1 busybox:latest /bin/sh 2.查看IP地址，IP地址为172.16.1.1，因为之前创建overlay网络的时候指定了网段为172.16.1.0/24 / # ip a 1: lo: mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 46: eth0@if47: mtu 1450 qdisc noqueue link/ether 02:42:ac:10:01:01 brd ff:ff:ff:ff:ff:ff inet 172.16.1.1/24 brd 172.16.1.255 scope global eth0 valid_lft forever preferred_lft forever 49: eth1@if50: mtu 1500 qdisc noqueue link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff inet 172.18.0.2/16 brd 172.18.255.255 scope global eth1 valid_lft forever preferred_lft forever 2.4.3docker02启动容器overlay2 1.启动一个容器，指定网络为之前创建的overlay1 [root@docker02 ~]# docker run -it --net overlay1 --name overlay2 busybox:latest /bin/sh 2.查看IP地址，IP地址为172.16.1.2，因为之前创建overlay网络的时候指定了网段为172.16.1.0/24 / # ip a 1: lo: mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 12: eth0@if13: mtu 1450 qdisc noqueue link/ether 02:42:ac:10:01:02 brd ff:ff:ff:ff:ff:ff inet 172.16.1.2/24 brd 172.16.1.255 scope global eth0 valid_lft forever preferred_lft forever 15: eth1@if16: mtu 1500 qdisc noqueue link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff inet 172.18.0.2/16 brd 172.18.255.255 scope global eth1 valid_lft forever preferred_lft forever 2.4.4测试容器互通及连接外网 docker01测试 1.ping docker02启动的容器overlay2 / # ping -c 2 172.16.1.2 PING 172.16.1.2 (172.16.1.2): 56 data bytes 64 bytes from 172.16.1.2: seq=0 ttl=64 time=0.831 ms 64 bytes from 172.16.1.2: seq=1 ttl=64 time=0.437 ms --- 172.16.1.2 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.437/0.634/0.831 ms 2.也可以ping容器名称，容器名称存放于consul集群中 / # ping -c 2 overlay2 PING overlay2 (172.16.1.2): 56 data bytes 64 bytes from 172.16.1.2: seq=0 ttl=64 time=0.704 ms 64 bytes from 172.16.1.2: seq=1 ttl=64 time=0.453 ms --- overlay2 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.453/0.578/0.704 ms 3.连接外网 / # ping -c 2 www.baidu.com PING www.baidu.com (61.135.169.121): 56 data bytes 64 bytes from 61.135.169.121: seq=0 ttl=127 time=4.863 ms 64 bytes from 61.135.169.121: seq=1 ttl=127 time=5.121 ms --- www.baidu.com ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 4.863/4.992/5.121 ms docker02测试 1.ping docker01启动的容器overlay1 / # ping -c 2 172.16.1.1 PING 172.16.1.1 (172.16.1.1): 56 data bytes 64 bytes from 172.16.1.1: seq=0 ttl=64 time=0.755 ms 64 bytes from 172.16.1.1: seq=1 ttl=64 time=0.660 ms --- 172.16.1.1 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.660/0.707/0.755 ms 2.也可以ping容器名称，容器名称存放于consul集群中 / # ping -c 2 overlay1 PING overlay1 (172.16.1.1): 56 data bytes 64 bytes from 172.16.1.1: seq=0 ttl=64 time=0.900 ms 64 bytes from 172.16.1.1: seq=1 ttl=64 time=0.561 ms --- overlay1 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.561/0.730/0.900 ms 3.连接外网 / # ping -c 2 www.baidu.com PING www.baidu.com (61.135.169.125): 56 data bytes 64 bytes from 61.135.169.125: seq=0 ttl=127 time=5.063 ms 64 bytes from 61.135.169.125: seq=1 ttl=127 time=4.792 ms --- www.baidu.com ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 4.792/4.927/5.063 ms 到此，跨主机间容器通信完成！！！ 3.overlay网络类型的容器网卡问题 3.1启动一个容器后，可以看到网络类型为overlay的容器有两块网卡eth0、eth1 1.查看容器IP，发现有两块网卡 eth0为172.16.1.1，用于跨主机容器间通信 eth1为172.18.0.2，用于连接外网 / # ip a 1: lo: mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 46: eth0@if47: mtu 1450 qdisc noqueue link/ether 02:42:ac:10:01:01 brd ff:ff:ff:ff:ff:ff inet 172.16.1.1/24 brd 172.16.1.255 scope global eth0 valid_lft forever preferred_lft forever 49: eth1@if50: mtu 1500 qdisc noqueue link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff inet 172.18.0.2/16 brd 172.18.255.255 scope global eth1 valid_lft forever preferred_lft forever 2.查看容器网关，可以看到容器网关为172.18.0。1 / # route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 172.18.0.1 0.0.0.0 UG 0 0 0 eth1 172.16.1.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 172.18.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth1 3.查看宿主机网卡docker_gwbridge，可以看到docker_gwbridge的IP地址为172.18.0.1，是网络类型为overlay容器的网关地址 [root@docker01 ~]# ifconfig docker_gwbridge docker_gwbridge: flags=4163 mtu 1500 inet 172.18.0.1 netmask 255.255.0.0 broadcast 172.18.255.255 inet6 fe80::42:38ff:fea8:17e prefixlen 64 scopeid 0x20 ether 02:42:38:a8:01:7e txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 4.overlay网络类型网络命名空间 namespace实现网络环境隔离，网络命令空间有自己的IP地址 1.查看网络命名空间默认为空 [root@docker01 ~]# ip netns [root@docker01 ~]# 2.创建目录软连接，docker01和docker02相同操作 因为ip netns只能查看到/var/run/netns下面的网络命名空间，而docker默认是放在/var/run/docker/netns，因此需要做目录软连接 [root@docker01 ~]# ln -s /var/run/docker/netns/ /var/run/netns 3.查看网络命名空间，可以看到两个容器间有一个相同的网络命名空间 1-7a836393a6 (id: 1) #docker01 [root@docker01 ~]# ip netns 7ab4caaffca3 (id: 2) 1-7a836393a6 (id: 1) 7c88a1ff4a27 (id: 0) #docker02 [root@docker02 ~]# ip netns b7bfd18ee204 (id: 2) 1-7a836393a6 (id: 1) 5eb2da02b6de (id: 0) 4.进入到这个相同的网络命名空间(如果不进入网络命名空间则无法查看vxlan网卡) [root@docker02 ~]# ip netns exec 1-7a836393a6 /bin/bash #查看vxlan网卡，可以看到有br0、lo、veth0、vxlan0 如果启动多个网络类型为overlay的容器，则会有多个vethN,N代表数字 且br0的IP地址为172.16.1.254，即为创建overlay网络指定的网段的网关地址 [root@docker02 ~]# ifconfig br0: flags=4163 mtu 1450 inet 172.16.1.254 netmask 255.255.255.0 broadcast 172.16.1.255 ether 76:aa:f3:f4:77:17 txqueuelen 0 (Ethernet) RX packets 1 bytes 28 (28.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73 mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 loop txqueuelen 1000 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 veth0: flags=4163 mtu 1450 ether 76:aa:f3:f4:77:17 txqueuelen 0 (Ethernet) RX packets 7 bytes 574 (574.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 7 bytes 574 (574.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 vxlan0: flags=4163 mtu 1450 ether be:5c:12:ff:1d:38 txqueuelen 0 (Ethernet) RX packets 5 bytes 420 (420.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 5 bytes 420 (420.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 #查看br0 [root@docker02 ~]# brctl show bridge name bridge id STP enabled interfaces br0 8000.76aaf3f47717 no veth0 vxlan0 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/容器/docker/13.docker修改默认网段.html":{"url":"linux/容器/docker/13.docker修改默认网段.html","title":"13.docker修改默认网段","keywords":"","body":"docker修改默认网段 docker默认网段为 172.17.0.0/16 ，修改docker默认网段需要编辑 /etc/docker/daemon.json 文件，并增加如下配置，然后重启docker即可 { \"bip\":\"192.168.0.1/24\" } 修改完成后查看 docker0 网卡就会看到IP地址段已经发生变更，但是这样只能修改 docker0 网卡的IP段，在使用 docker-compose 安装某些软件的时候，例如 gitea ，会创建名为 br-xxx 桥接模式的网卡，这些网卡默认会从 172.17 开始递增(这一块原理没有搞明白) 如果我们的网络环境下有 172.17 、172.18 等子网，那则会发生冲突，解决方法有多种，1是 docker-compose 安装的时候指定创建的虚拟网卡网段，2是修改虚拟网卡的网段，同时还需要删除路由并新建路由，但是这样不灵活，每一次在新环境部署的时候都需要做相同的操作，比较繁琐 在github中找到了一个isseu，isseu中有说明到这个问题，比较好的操作方法是直接修改docker配置文件 /etc/docker/daemon.json ，然后新增如下配置，这样无论是 docker0 还是桥接网卡都会从配置的地址池中分配IP，从而不影响当前网络 { \"debug\" : true, \"default-address-pools\" : [ { \"base\" : \"10.51.0.1/16\", \"size\" : 24 } ] } 上面配置意思：后面服务再创建地址池使用 10.50.0.1/16 网段范围划分，每个子网掩码划分为 255.255.255.0。 参考链接 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/容器/docker/docker命令大全.html":{"url":"linux/容器/docker/docker命令大全.html","title":"docker命令大全","keywords":"","body":"[toc] docker命令大全 一、容器生命周期管理 1.1 run docker run ：创建一个新的容器并运行一个命令 语法 docker run [OPTIONS] IMAGE [COMMAND] [ARG...] OPTIONS说明： -a stdin: 指定标准输入输出内容类型，可选 STDIN/STDOUT/STDERR 三项； -d: 后台运行容器，并返回容器ID； -i: 以交互模式运行容器，通常与 -t 同时使用； -P: 随机端口映射，容器内部端口随机映射到主机的高端口 -p: 指定端口映射，格式为：主机(宿主)端口:容器端口 -t: 为容器重新分配一个伪输入终端，通常与 -i 同时使用； --name=\"nginx-lb\": 为容器指定一个名称； --dns 8.8.8.8: 指定容器使用的DNS服务器，默认和宿主一致； --dns-search example.com: 指定容器DNS搜索域名，默认和宿主一致； -h \"mars\": 指定容器的hostname； -e username=\"ritchie\": 设置环境变量； --env-file=[]: 从指定文件读入环境变量； --cpuset=\"0-2\" or --cpuset=\"0,1,2\": 绑定容器到指定CPU运行； -m :设置容器使用内存最大值； --net=\"bridge\": 指定容器的网络连接类型，支持 bridge/host/none/container: 四种类型； --link=[]: 添加链接到另一个容器； --expose=[]: 开放一个端口或一组端口； --volume , -v: 绑定一个卷 实例 使用docker镜像nginx:latest以后台模式启动一个容器,并将容器命名为mynginx。 $ docker run --name mynginx -d nginx:latest 使用镜像nginx:latest以后台模式启动一个容器,并将容器的80端口映射到主机随机端口。 $ docker run -P -d nginx:latest 使用镜像 nginx:latest，以后台模式启动一个容器,将容器的 80 端口映射到主机的 80 端口,主机的目录 /data 映射到容器的 /data。 $ docker run -p 80:80 -v /data:/data -d nginx:latest 绑定容器的 8080 端口，并将其映射到本地主机 127.0.0.1 的 80 端口上。 $ docker run -p 127.0.0.1:80:8080/tcp ubuntu bash 使用镜像nginx:latest以交互模式启动一个容器,在容器内执行/bin/bash命令。 $ docker run -it nginx:latest /bin/bash root@b8573233d675:/# 1.2 start/stop/restart docker start :启动一个或多个已经被停止的容器 docker stop :停止一个运行中的容器 docker restart :重启容器 语法 docker start [OPTIONS] CONTAINER [CONTAINER...] docker stop [OPTIONS] CONTAINER [CONTAINER...] docker restart [OPTIONS] CONTAINER [CONTAINER...] 实例 启动已被停止的容器myrunoob $ docker start myrunoob 停止运行中的容器myrunoob $ docker stop myrunoob 重启容器myrunoob $ docker restart myrunoob 1.3 kill docker kill :杀掉一个运行中的容器。 语法 docker kill [OPTIONS] CONTAINER [CONTAINER...] OPTIONS说明： -s :向容器发送一个信号 实例 杀掉运行中的容器mynginx $ docker kill -s KILL mynginx mynginx 1.4 rm docker rm ：删除一个或多个容器。 语法 docker rm [OPTIONS] CONTAINER [CONTAINER...] OPTIONS说明： -f :通过 SIGKILL 信号强制删除一个运行中的容器。 -l :移除容器间的网络连接，而非容器本身。 -v :删除与容器关联的卷。 实例 强制删除容器 db01、db02： $ docker rm -f db01 db02 移除容器 nginx01 对容器 db01 的连接，连接名 db： $ docker rm -l db 删除容器 nginx01, 并删除容器挂载的数据卷： $ docker rm -v nginx01 删除所有已经停止的容器： $ docker rm `docker ps -a -q` 1.5 pause/unpause docker pause :暂停容器中所有的进程。 docker unpause :恢复容器中所有的进程。 语法 docker pause [OPTIONS] CONTAINER [CONTAINER...] docker unpause [OPTIONS] CONTAINER [CONTAINER...] 实例 暂停数据库容器db01提供服务。 $ docker pause db01 恢复数据库容器db01提供服务。 $ docker unpause db01 1.6 create docker create ：创建一个新的容器但不启动它 用法同1.1run 语法 docker create [OPTIONS] IMAGE [COMMAND] [ARG...] 语法同1.1run 实例 使用docker镜像nginx:latest创建一个容器,并将容器命名为mynginx $ docker create --name mynginx nginx:latest 09b93464c2f75b7b69f83d56a9cfc23ceb50a48a9db7652ee4c27e3e2cb1961f 1.7 exec docker exec ：在运行的容器中执行命令 语法 docker exec [OPTIONS] CONTAINER COMMAND [ARG...] OPTIONS说明： -d :分离模式: 在后台运行 -i :即使没有附加也保持STDIN 打开 -t :分配一个伪终端 实例 在容器 mynginx 中以交互模式执行容器内 /root/runoob.sh 脚本: $ docker exec -it mynginx /bin/sh /root/runoob.sh http://www.runoob.com/ 在容器 mynginx 中开启一个交互模式的终端: docker exec -i -t mynginx /bin/bash root@b1a0703e41e7:/# 也可以通过 docker ps -a 命令查看已经在运行的容器，然后使用容器 ID 进入容器。 查看已经在运行的容器 ID： $ docker ps -a ... 9df70f9a0714 openjdk \"/usercode/script.sh…\" ... 第一列的 9df70f9a0714 就是容器 ID。 通过 exec 命令对指定的容器执行 bash: $ docker exec -it 9df70f9a0714 /bin/bash 二、容器操作 2.1 ps docker ps :列出容器 语法 docker ps [OPTIONS] OPTIONS说明： -a :显示所有的容器，包括未运行的。 -f :根据条件过滤显示的内容。 --format :指定返回值的模板文件。 -l :显示最近创建的容器。 -n :列出最近创建的n个容器。 --no-trunc :不截断输出。 -q :静默模式，只显示容器编号。 -s :显示总的文件大小。 实例 列出所有在运行的容器信息。 $ docker ps CONTAINER ID IMAGE COMMAND ... PORTS NAMES 09b93464c2f7 nginx:latest \"nginx -g 'daemon off\" ... 80/tcp, 443/tcp myrunoob 96f7f14e99ab mysql:5.6 \"docker-entrypoint.sh\" ... 0.0.0.0:3306->3306/tcp mymysql 输出详情介绍： CONTAINER ID: 容器 ID。 IMAGE: 使用的镜像。 COMMAND: 启动容器时运行的命令。 CREATED: 容器的创建时间。 STATUS: 容器状态。 状态有7种： created（已创建） restarting（重启中） running（运行中） removing（迁移中） paused（暂停） exited（停止） dead（死亡） PORTS: 容器的端口信息和使用的连接类型（tcp\\udp）。 NAMES: 自动分配的容器名称。 列出最近创建的3个容器信息。 docker ps -n 3 CONTAINER ID IMAGE COMMAND CREATED 09b93464c2f7 nginx:latest \"nginx -g 'daemon off\" 2 days ago ... b8573233d675 nginx:latest \"/bin/bash\" 2 days ago ... b1a0703e41e7 nginx:latest \"nginx -g 'daemon off\" 2 days ago ... 列出所有创建的容器ID。 docker ps -a -q 09b93464c2f7 b8573233d675 ... 2.2 inspect docker inspect : 获取容器/镜像的元数据。 语法 docker inspect [OPTIONS] NAME|ID [NAME|ID...] OPTIONS说明： -f :指定返回值的模板文件。 -s :显示总的文件大小。 --type :为指定类型返回JSON。 实例 获取镜像mysql:5.6的元信息。 $ docker inspect mysql:5.6 [ { \"Id\": \"sha256:2c0964ec182ae9a045f866bbc2553087f6e42bfc16074a74fb820af235f070ec\", \"RepoTags\": [ \"mysql:5.6\" ], \"RepoDigests\": [], \"Parent\": \"\", \"Comment\": \"\", \"Created\": \"2016-05-24T04:01:41.168371815Z\", \"Container\": \"e0924bc460ff97787f34610115e9363e6363b30b8efa406e28eb495ab199ca54\", \"ContainerConfig\": { \"Hostname\": \"b0cf605c7757\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": false, \"AttachStdout\": false, \"AttachStderr\": false, \"ExposedPorts\": { \"3306/tcp\": {} }, ... 获取正在运行的容器mymysql的 IP。 $ docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' mymysql 172.17.0.3 2.3 top docker top :查看容器中运行的进程信息，支持 ps 命令参数。 语法 docker top [OPTIONS] CONTAINER [ps OPTIONS] 容器运行时不一定有/bin/bash终端来交互执行top命令，而且容器还不一定有top命令，可以使用docker top来实现查看container中正在运行的进程。 实例 查看容器mymysql的进程信息。 $ docker top mymysql UID PID PPID C STIME TTY TIME CMD 999 40347 40331 18 00:58 ? 00:00:02 mysqld 查看所有运行容器的进程信息。 $ for i in `docker ps |grep Up|awk '{print $1}'`;do echo \\ && docker top $i; done 2.4 attach docker attach :连接到正在运行中的容器。 语法 docker attach [OPTIONS] CONTAINER 要attach上去的容器必须正在运行，可以同时连接上同一个container来共享屏幕（与screen命令的attach类似）。 官方文档中说attach后可以通过CTRL-C来detach，但实际上经过我的测试，如果container当前在运行bash，CTRL-C自然是当前行的输入，没有退出；如果container当前正在前台运行进程，如输出nginx的access.log日志，CTRL-C不仅会导致退出容器，而且还stop了。这不是我们想要的，detach的意思按理应该是脱离容器终端，但容器依然运行。好在attach是可以带上--sig-proxy=false来确保CTRL-D或CTRL-C不会关闭容器。 实例 容器mynginx将访问日志指到标准输出，连接到容器查看访问信息。 $ docker attach --sig-proxy=false mynginx 192.168.239.1 - - [10/Jul/2016:16:54:26 +0000] \"GET / HTTP/1.1\" 304 0 \"-\" \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36\" \"-\" --sig-proxy=false 保证ctrl+c或者ctrl+d不会关闭容器 2.5 events docker events : 从服务器获取实时事件 语法 docker events [OPTIONS] OPTIONS说明： -f ：根据条件过滤事件； --since ：从指定的时间戳后显示所有事件; --until ：流水时间显示到指定的时间为止； 实例 显示docker 2016年7月1日后的所有事件。 $ docker events --since=\"1467302400\" 2016-07-08T19:44:54.501277677+08:00 network connect 66f958fd13dc4314ad20034e576d5c5eba72e0849dcc38ad9e8436314a4149d4 (container=b8573233d675705df8c89796a2c2687cd8e36e03646457a15fb51022db440e64, name=bridge, type=bridge) 2016-07-08T19:44:54.723876221+08:00 container start b8573233d675705df8c89796a2c2687cd8e36e03646457a15fb51022db440e64 (image=nginx:latest, name=elegant_albattani) 2016-07-08T19:44:54.726110498+08:00 container resize b8573233d675705df8c89796a2c2687cd8e36e03646457a15fb51022db440e64 (height=39, image=nginx:latest, name=elegant_albattani, width=167) 2016-07-08T19:46:22.137250899+08:00 container die b8573233d675705df8c89796a2c2687cd8e36e03646457a15fb51022db440e64 (exitCode=0, image=nginx:latest, name=elegant_albattani) ... 显示docker 镜像为mysql:5.6 2016年7月1日后的相关事件。 $ docker events -f \"image\"=\"mysql:5.6\" --since=\"1467302400\" 2016-07-11T00:38:53.975174837+08:00 container start 96f7f14e99ab9d2f60943a50be23035eda1623782cc5f930411bbea407a2bb10 (image=mysql:5.6, name=mymysql) 2016-07-11T00:51:17.022572452+08:00 container kill 96f7f14e99ab9d2f60943a50be23035eda1623782cc5f930411bbea407a2bb10 (image=mysql:5.6, name=mymysql, signal=9) ... 如果指定的时间是到秒级的，需要将时间转成时间戳。如果时间为日期的话，可以直接使用，如--since=\"2016-07-01\"。 2.6 logs docker logs : 获取容器的日志 语法 docker logs [OPTIONS] CONTAINER OPTIONS说明： -f : 跟踪日志输出 --since :显示某个开始时间的所有日志 -t : 显示时间戳 --tail :仅列出最新N条容器日志 实例 跟踪查看容器mynginx的日志输出。 $ docker logs -f mynginx 192.168.239.1 - - [10/Jul/2016:16:53:33 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36\" \"-\" 2016/07/10 16:53:33 [error] 5#5: *1 open() \"/usr/share/nginx/html/favicon.ico\" failed (2: No such file or directory), client: 192.168.239.1, server: localhost, request: \"GET /favicon.ico HTTP/1.1\", host: \"192.168.239.130\", referrer: \"http://192.168.239.130/\" 192.168.239.1 - - [10/Jul/2016:16:53:33 +0000] \"GET /favicon.ico HTTP/1.1\" 404 571 \"http://192.168.239.130/\" \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36\" \"-\" 192.168.239.1 - - [10/Jul/2016:16:53:59 +0000] \"GET / HTTP/1.1\" 304 0 \"-\" \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36\" \"-\" ... 查看容器mynginx从2016年7月1日后的最新10条日志。 $ docker logs --since=\"2016-07-01\" --tail=10 mynginx 2.7 wait docker wait : 阻塞运行直到容器停止，然后打印出它的退出代码。 语法 docker wait [OPTIONS] CONTAINER [CONTAINER...] 实例 $ docker wait CONTAINER 2.8 export docker export :将文件系统作为一个tar归档文件导出到STDOUT。 语法 docker export [OPTIONS] CONTAINER OPTIONS说明： -o :将输入内容写到文件。 实例 将id为a404c6c174a2的容器按日期保存为tar文件。 $ docker export -o mysql-`date +%Y%m%d`.tar a404c6c174a2 $ ls mysql-`date +%Y%m%d`.tar mysql-20160711.tar 解压后是所有的文件系统目录 bin dev home lib64 mnt proc run srv tmp var boot etc lib media opt root sbin sys usr 2.9 port docker port :列出指定的容器的端口映射，或者查找将PRIVATE_PORT NAT到面向公众的端口。 语法 docker port [OPTIONS] CONTAINER [PRIVATE_PORT[/PROTO]] 实例 查看容器mynginx的端口映射情况。 $ docker port mymysql 3306/tcp -> 0.0.0.0:3306 三、容器rootfs命令 3.1 commit docker commit :从容器创建一个新的镜像。 语法 docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]] OPTIONS说明： -a :提交的镜像作者； -c :使用Dockerfile指令来创建镜像； -m :提交时的说明文字； -p :在commit时，将容器暂停。 实例 将容器a404c6c174a2 保存为新的镜像,并添加提交人信息和说明信息。 $ docker commit -a \"runoob.com\" -m \"my apache\" a404c6c174a2 mymysql:v1 sha256:37af1236adef1544e8886be23010b66577647a40bc02c0885a6600b33ee28057 $ docker images mymysql:v1 REPOSITORY TAG IMAGE ID CREATED SIZE mymysql v1 37af1236adef 15 seconds ago 329 MB 3.2 cp docker cp :用于容器与主机之间的数据拷贝。 语法 docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|- docker cp [OPTIONS] SRC_PATH|- CONTAINER:DEST_PATH OPTIONS说明： -L :保持源目标中的链接 实例 将主机/www/runoob目录拷贝到容器96f7f14e99ab的/www目录下。 $ docker cp /www/runoob 96f7f14e99ab:/www/ 将主机/www/runoob目录拷贝到容器96f7f14e99ab中，目录重命名为www。 $ docker cp /www/runoob 96f7f14e99ab:/www 将容器96f7f14e99ab的/www目录拷贝到主机的/tmp目录中。 $ docker cp 96f7f14e99ab:/www /tmp/ 3.3 diff docker diff : 检查容器里文件结构的更改。 语法 docker diff [OPTIONS] CONTAINER 实例 查看容器mymysql的文件结构更改。 $ docker diff mymysql A /logs A /mysql_data C /run C /run/mysqld A /run/mysqld/mysqld.pid A /run/mysqld/mysqld.sock C /tmp 四、镜像仓库 4.1 login docker login : 登陆到一个Docker镜像仓库，如果未指定镜像仓库地址，默认为官方仓库 Docker Hub docker logout : 登出一个Docker镜像仓库，如果未指定镜像仓库地址，默认为官方仓库 Docker Hub 语法 docker login [OPTIONS] [SERVER] docker logout [OPTIONS] [SERVER] OPTIONS说明： -u :登陆的用户名 -p :登陆的密码 实例 登陆到Docker Hub $ docker login -u 用户名 -p 密码 登出Docker Hub $ docker logout 4.2 pull docker pull : 从镜像仓库中拉取或者更新指定镜像 语法 docker pull [OPTIONS] NAME[:TAG|@DIGEST] OPTIONS说明： -a :拉取所有 tagged 镜像 --disable-content-trust :忽略镜像的校验,默认开启 实例 从Docker Hub下载java最新版镜像。 $ docker pull java 从Docker Hub下载REPOSITORY为java的所有镜像。 $ docker pull -a java 4.3 push docker push : 将本地的镜像上传到镜像仓库,要先登陆到镜像仓库 语法 docker push [OPTIONS] NAME[:TAG] OPTIONS说明： --disable-content-trust :忽略镜像的校验,默认开启 实例 上传本地镜像myapache:v1到镜像仓库中。 $ docker push myapache:v1 4.4 search docker search：从Docker Hub查找更多 语法 docker search [OPTIONS] TERM 选项说明： --automated：只列出自动构建类型的附加； --no-trunc：显示完整的包含描述； -s：列出收藏者数不小于指定值的附加。 实例 从Docker Hub查找所有预期名包含java，并且收藏数大于10的替代 $ docker search -s 10 java NAME DESCRIPTION STARS OFFICIAL AUTOMATED java Java is a concurrent, class-based... 1037 [OK] anapsix/alpine-java Oracle Java 8 (and 7) with GLIBC ... 115 [OK] develar/java 46 [OK] isuper/java-oracle This repository contains all java... 38 [OK] lwieske/java-8 Oracle Java 8 Container - Full + ... 27 [OK] nimmis/java-centos This is docker images of CentOS 7... 13 [OK] 参数说明： NAME: 镜像仓库源的名称 DESCRIPTION: 镜像的描述 OFFICIAL: 是否 docker 官方发布 stars: 类似 Github 里面的 star，表示点赞、喜欢的意思。 AUTOMATED: 自动构建。 五、本地镜像管理 5.1 images docker images : 列出本地镜像。 语法 docker images [OPTIONS] [REPOSITORY[:TAG]] OPTIONS说明： -a :列出本地所有的镜像（含中间映像层，默认情况下，过滤掉中间映像层）； --digests :显示镜像的摘要信息； -f :显示满足条件的镜像； --format :指定返回值的模板文件； --no-trunc :显示完整的镜像信息； -q :只显示镜像ID。 实例 查看本地镜像列表。 $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE mymysql v1 37af1236adef 5 minutes ago 329 MB ... 列出本地镜像中REPOSITORY为ubuntu的镜像列表。 $ docker images ubuntu REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu 14.04 90d5884b1ee0 9 weeks ago 188 MB ubuntu 15.10 4e3b13c8a266 3 months ago 136.3 MB 5.2 rmi docker rmi : 删除本地一个或多少镜像。 语法 docker rmi [OPTIONS] IMAGE [IMAGE...] OPTIONS说明： -f :强制删除； --no-prune :不移除该镜像的过程镜像，默认移除； 实例 强制删除本地镜像 runoob/ubuntu:v4。 $ docker rmi -f runoob/ubuntu:v4 Untagged: runoob/ubuntu:v4 Deleted: sha256:1c06aa18edee44230f93a90a7d88139235de12cd4c089d41eed8419b503072be Deleted: sha256:85feb446e89a28d58ee7d80ea5ce367eebb7cec70f0ec18aa4faa874cbd97c73 5.3 tag docker tag : 标记本地镜像，将其归入某一仓库。 语法 docker tag [OPTIONS] IMAGE[:TAG] [REGISTRYHOST/][USERNAME/]NAME[:TAG] 实例 将镜像ubuntu:15.10标记为 runoob/ubuntu:v3 镜像。 $ docker tag ubuntu:15.10 runoob/ubuntu:v3 $ docker images runoob/ubuntu:v3 REPOSITORY TAG IMAGE ID CREATED SIZE runoob/ubuntu v3 4e3b13c8a266 3 months ago 136.3 MB 5.4 build docker build 命令用于使用 Dockerfile 创建镜像。 语法 docker build [OPTIONS] PATH | URL | - OPTIONS说明： --build-arg=[] :设置镜像创建时的变量； --cpu-shares :设置 cpu 使用权重； --cpu-period :限制 CPU CFS周期； --cpu-quota :限制 CPU CFS配额； --cpuset-cpus :指定使用的CPU id； --cpuset-mems :指定使用的内存 id； --disable-content-trust :忽略校验，默认开启； -f :指定要使用的Dockerfile路径； --force-rm :设置镜像过程中删除中间容器； --isolation :使用容器隔离技术； --label=[] :设置镜像使用的元数据； -m :设置内存最大值； --memory-swap :设置Swap的最大值为内存+swap，\"-1\"表示不限swap； --no-cache :创建镜像的过程不使用缓存； --pull :尝试去更新镜像的新版本； --quiet, -q :安静模式，成功后只输出镜像 ID； --rm :设置镜像成功后删除中间容器； --shm-size :设置/dev/shm的大小，默认值是64M； --ulimit :Ulimit配置。 --tag, -t: 镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签。 --network: 默认 default。在构建期间设置RUN指令的网络模式 实例 使用当前目录的 Dockerfile 创建镜像，标签为 runoob/ubuntu:v1。 $ docker build -t runoob/ubuntu:v1 . 使用URL github.com/creack/docker-firefox 的 Dockerfile 创建镜像。 $ docker build github.com/creack/docker-firefox 也可以通过 -f Dockerfile 文件的位置： $ docker build -f /path/to/a/Dockerfile . 在 Docker 守护进程执行 Dockerfile 中的指令前，首先会对 Dockerfile 进行语法检查，有语法错误时会返回： $ docker build -t test/myapp . Sending build context to Docker daemon 2.048 kB Error response from daemon: Unknown instruction: RUNCMD 5.5 history docker history : 查看指定镜像的创建历史。 语法 docker history [OPTIONS] IMAGE OPTIONS说明： -H :以可读的格式打印镜像大小和日期，默认为true --no-trunc :显示完整的提交记录； -q :仅列出提交记录ID。 实例 查看本地镜像runoob/ubuntu:v3的创建历史。 $ docker history runoob/ubuntu:v3 IMAGE CREATED CREATED BY SIZE COMMENT 4e3b13c8a266 3 months ago /bin/sh -c #(nop) CMD [\"/bin/bash\"] 0 B 3 months ago /bin/sh -c sed -i 's/^#\\s*\\(deb.*universe\\)$/ 1.863 kB 3 months ago /bin/sh -c set -xe && echo '#!/bin/sh' > /u 701 B 3 months ago /bin/sh -c #(nop) ADD file:43cb048516c6b80f22 136.3 MB 5.6 save docker save : 将指定镜像保存成 tar 归档文件。 语法 docker save [OPTIONS] IMAGE [IMAGE...] OPTIONS 说明： -o :输出到的文件。 实例 将镜像 runoob/ubuntu:v3 生成 my_ubuntu_v3.tar 文档 $ docker save -o my_ubuntu_v3.tar runoob/ubuntu:v3 runoob@runoob:~$ ll my_ubuntu_v3.tar -rw------- 1 runoob runoob 142102016 Jul 11 01:37 my_ubuntu_v3.ta 5.7 load docker load : 导入使用 docker save 命令导出的镜像。 语法 docker load [OPTIONS] OPTIONS 说明： --input , -i : 指定导入的文件，代替 STDIN。 --quiet , -q : 精简输出信息。 实例 导入镜像： #查看镜像 $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE #导入镜像 $ docker load 5.8 import docker import : 从归档文件中创建镜像。 语法 docker import [OPTIONS] file|URL|- [REPOSITORY[:TAG]] OPTIONS说明： -c :应用docker 指令创建镜像； -m :提交时的说明文字； 实例 从镜像归档文件my_ubuntu_v3.tar创建镜像，命名为runoob/ubuntu:v4 $ docker import my_ubuntu_v3.tar runoob/ubuntu:v4 sha256:63ce4a6d6bc3fabb95dbd6c561404a309b7bdfc4e21c1d59fe9fe4299cbfea39 runoob@runoob:~$ docker images runoob/ubuntu:v4 REPOSITORY TAG IMAGE ID CREATED SIZE runoob/ubuntu v4 63ce4a6d6bc3 20 seconds ago 142.1 MB 六、info|version 6.1 info docker info : 显示 Docker 系统信息，包括镜像和容器数。。 语法 docker info [OPTIONS] 实例 查看docker系统信息。 $ docker info Containers: 12 Images: 41 Storage Driver: aufs Root Dir: /var/lib/docker/aufs Backing Filesystem: extfs Dirs: 66 Dirperm1 Supported: false Execution Driver: native-0.2 Logging Driver: json-file Kernel Version: 3.13.0-32-generic Operating System: Ubuntu 14.04.1 LTS CPUs: 1 Total Memory: 1.954 GiB Name: iZ23mtq8bs1Z ID: M5N4:K6WN:PUNC:73ZN:AONJ:AUHL:KSYH:2JPI:CH3K:O4MK:6OCX:5OYW 6.2 version ocker version :显示 Docker 版本信息。 语法 docker version [OPTIONS] OPTIONS说明： -f :指定返回值的模板文件。 实例 显示 Docker 版本信息。 $ docker version Client: Docker Engine - Community Version: 19.03.3 API version: 1.40 Go version: go1.12.10 Git commit: a872fc2f86 Built: Tue Oct 8 00:58:10 2019 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 19.03.3 API version: 1.40 (minimum version 1.12) Go version: go1.12.10 Git commit: a872fc2f86 Built: Tue Oct 8 00:56:46 2019 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.2.6 GitCommit: 894b81a4b802e4eb2a91d1ce216b8817763c29fb runc: Version: 1.0.0-rc8 GitCommit: 425e105d5a03fabd737a126ad93d62a9eeede87f docker-init: Version: 0.18.0 GitCommit: fec3683 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/容器/docker/docker-compose安装.html":{"url":"linux/容器/docker/docker-compose安装.html","title":"docker compose 安装","keywords":"","body":"[toc] docker-compose安装 1.docker-compose简介 docker compose github地址 docker compose 官方安装地址 官方安装文档 Compose中有两个重要的概念： 服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。 项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在docker-compose.yml文件中定义。 Compose的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。Compose 项目由 Python 编写，实现上调用了 Docker 服务提供的 API 来对容器进行管理。所以只要所操作的平台支持 Docker API，就可以在其上利用 Compose 来进行编排管理。 2.docker-compose安装 2.1 下载安装包 2.1.1 github下载 docker compose github地址 curl -L \"https://github.com/docker/compose/releases/download/2.2.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose 2.1.2 国内源下载 wget https://get.daocloud.io/#install-compose if [ $? -ne 0 ];then echo \"下载错误\" exit 1 fi VERSION=`awk -F'/' '/get.daocloud.io\\/docker\\/compose/{print $8}' index.html` curl -L https://get.daocloud.io/docker/compose/releases/download/${VERSION}/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose 2.2 给二进制文件添加可执行权限 chmod +x /usr/local/bin/docker-compose 2.3 完成安装，查看版本 $ docker-compose -v Docker Compose version v2.2.2 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/容器/containerd/containerd安装.html":{"url":"linux/容器/containerd/containerd安装.html","title":"containerd安装","keywords":"","body":"containerd安装 containerd官网 containerd github containerd官方架构图 总体来看 containerd 可以分为三个大块：Storage、Metadata 和 Runtime 1.下载安装包 由于 containerd 需要调用 runc，所以我们也需要先安装 runc，不过 containerd 提供了一个包含相关依赖的压缩包 cri-containerd-cni-${VERSION}.${OS}-${ARCH}.tar.gz ，直接下载这个包即可 export CONTAINERD_VERSION=1.5.9 wget https://github.com/containerd/containerd/releases/download/v${CONTAINERD_VERSION}/cri-containerd-cni-${CONTAINERD_VERSION}-linux-amd64.tar.gz 2.解压包 tar包解压缩后是3个目录 etc 、 opt 、 usr 解压前可使用 tar -tf 命令查看包内容 tar xf cri-containerd-cni-${CONTAINERD_VERSION}-linux-amd64.tar.gz -C / 3.创建containerd配置文件 containerd 的默认配置文件为 /etc/containerd/config.toml，我们可以通过 containerd config default > /etc/containerd/config.toml 命令生成一个默认的配置 mkdir -p /etc/containerd && containerd config default > /etc/containerd/config.toml 4.修改containerd配置文件 对于使用 systemd 作为 init system 的 Linux 的发行版，使用 systemd 作为容器的 cgroup driver 可以确保节点在资源紧张的情况更加稳定，所以推荐将 containerd 的 cgroup driver 配置为 systemd 修改前面生成的配置文件 /etc/containerd/config.toml，在 plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options 配置块下面将 SystemdCgroup 设置为 true 修改 SystemdCgroup = false 修改为 SystemdCgroup = true 5.配置containerd仓库加速 需要在 cri 配置块下面的 registry 配置块下面进行配置 registry.mirrors，在 [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors] 下边新增4行内容 [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"docker.io\"] endpoint = [\"https://bqr1dr1n.mirror.aliyuncs.com\"] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"k8s.gcr.io\"] endpoint = [\"https://registry.aliyuncs.com/k8sxio\"] registry.mirrors.\"xxx\": 表示需要配置 mirror 的镜像仓库，例如 registry.mirrors.\"docker.io\" 表示配置 docker.io 的 mirror。 endpoint: 表示提供 mirror 的镜像加速服务，比如我们可以注册一个阿里云的镜像服务来作为 docker.io 的 mirror。 修改 sandbox_image 为阿里云地址 修改 sandbox_image = \"k8s.gcr.io/pause:3.5\" 修改为 sandbox_image = \"registry.aliyuncs.com/k8sxio/pause:3.5\" 6.containerd存储配置说明 /etc/containerd/config.toml 配置中还有两个关于存储的配置路径 root = \"/var/lib/containerd\" state = \"/run/containerd\" 其中 root 是用来保存持久化数据，包括 Snapshots, Content, Metadata 以及各种插件的数据，每一个插件都有自己单独的目录，Containerd 本身不存储任何数据，它的所有功能都来自于已加载的插件。 而另外的 state 是用来保存运行时的临时数据的，包括 sockets、pid、挂载点、运行时状态以及不需要持久化的插件数据。 7.启动containerd 由于上面我们下载的 containerd 压缩包中包含一个 etc/systemd/system/containerd.service 的文件，这样我们就可以通过 systemd 来配置 containerd 作为守护进程运行了，内容如下所示 $ cat /etc/systemd/system/containerd.service # Copyright The containerd Authors. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. [Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target local-fs.target [Service] ExecStartPre=-/sbin/modprobe overlay ExecStart=/usr/local/bin/containerd Type=notify Delegate=yes KillMode=process Restart=always RestartSec=5 # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNPROC=infinity LimitCORE=infinity LimitNOFILE=infinity # Comment TasksMax if your systemd version does not supports it. # Only systemd 226 and above support this version. TasksMax=infinity OOMScoreAdjust=-999 [Install] WantedBy=multi-user.target 这里有两个重要的参数： Delegate: 这个选项设置为 yes 表示允许 containerd 以及运行时自己管理自己创建容器的 cgroups。如果不设置这个选项，systemd 就会将进程移到自己的 cgroups 中，从而导致 containerd 无法正确获取容器的资源使用情况。 KillMode: 这个选项用来处理 containerd 进程被杀死的方式。默认情况下，systemd 会在进程的 cgroup 中查找并杀死 containerd 的所有子进程。KillMode 字段可以设置的值如下。 control-group（默认值）：当前控制组里面的所有子进程，都会被杀掉 process：只杀主进程 mixed：主进程将收到 SIGTERM 信号，子进程收到 SIGKILL 信号 none：没有进程会被杀掉，只是执行服务的 stop 命令 我们需要将 KillMode 的值设置为 process，这样可以确保升级或重启 containerd 时不杀死现有的容器。 启动containerd systemctl daemon-reload && systemctl enable containerd && systemctl start containerd 8.验证 $ ctr version Client: Version: v1.5.9 Revision: 1407cab509ff0d96baa4f0eb6ff9980270e6e620 Go version: go1.16.12 Server: Version: v1.5.9 Revision: 1407cab509ff0d96baa4f0eb6ff9980270e6e620 UUID: 66736362-67e3-4dfd-933e-8852f0057173 9.卸载containerd 从 github 下载的 cri-containerd-cni-1.5.9-linux-amd64.tar.gz 压缩包解压缩后是 etc、 opt、 usr 3个目录，这3个目录内容如下 $ tree etc/ opt/ usr etc/ ├── cni │ └── net.d │ └── 10-containerd-net.conflist ├── crictl.yaml └── systemd └── system └── containerd.service opt/ ├── cni │ └── bin │ ├── bandwidth │ ├── bridge │ ├── dhcp │ ├── firewall │ ├── flannel │ ├── host-device │ ├── host-local │ ├── ipvlan │ ├── loopback │ ├── macvlan │ ├── portmap │ ├── ptp │ ├── sbr │ ├── static │ ├── tuning │ ├── vlan │ └── vrf └── containerd └── cluster ├── gce │ ├── cloud-init │ │ ├── master.yaml │ │ └── node.yaml │ ├── cni.template │ ├── configure.sh │ └── env └── version usr └── local ├── bin │ ├── containerd │ ├── containerd-shim │ ├── containerd-shim-runc-v1 │ ├── containerd-shim-runc-v2 │ ├── containerd-stress │ ├── crictl │ ├── critest │ ├── ctd-decoder │ └── ctr └── sbin └── runc 13 directories, 36 files ⚠️一般会将cri-containerd-cni-xxx-linux-amd64.tar.gz直接解压到根目录下，因此卸载continerd只需要删除相应文件即可 停止containerd服务 systemctl disable containerd && systemctl stop containerd && systemctl status containerd 删除目录、文件 rm -rf /etc/{cni,crictl.yaml,systemd/system/containerd.service} /opt/{cni,containerd} /usr/local/{sbin/runc,bin/{containerd,containerd-shim,containerd-shim-runc-v1,containerd-shim-runc-v2,containerd-stress,crictl,critest,ctd-decoder,ctr}} 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/容器/containerd/containerd使用.html":{"url":"linux/容器/containerd/containerd使用.html","title":"containerd使用","keywords":"","body":"containerd使用 1.查看帮助 输入 ctr 命令即可获得所有相关的操作命令使用方式 $ ctr NAME: ctr - __ _____/ /______ / ___/ __/ ___/ / /__/ /_/ / \\___/\\__/_/ containerd CLI USAGE: ctr [global options] command [command options] [arguments...] VERSION: v1.5.9 DESCRIPTION: ctr is an unsupported debug and administrative client for interacting with the containerd daemon. Because it is unsupported, the commands, options, and operations are not guaranteed to be backward compatible or stable from release to release of the containerd project. COMMANDS: plugins, plugin provides information about containerd plugins version print the client and server versions containers, c, container manage containers content manage content events, event display containerd events images, image, i manage images leases manage leases namespaces, namespace, ns manage namespaces pprof provide golang pprof outputs for containerd run run a container snapshots, snapshot manage snapshots tasks, t, task manage tasks install install a new package oci OCI tools shim interact with a shim directly help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --debug enable debug output in logs --address value, -a value address for containerd's GRPC server (default: \"/run/containerd/containerd.sock\") [$CONTAINERD_ADDRESS] --timeout value total timeout for ctr commands (default: 0s) --connect-timeout value timeout for connecting to containerd (default: 0s) --namespace value, -n value namespace to use with commands (default: \"default\") [$CONTAINERD_NAMESPACE] --help, -h show help --version, -v print the version 2.镜像操作 2.1 拉取镜像 ctr image pull containerd拉取镜像可以使用 ctr image pull 来完成，但是需要加上 docker.io Host 地址 示例1 # docker方法 docker pull gitea/gitea:1.16.0 # containerd方法 ctr image pull docker.io/gitea/gitea:1.16.0 示例2 # 使用docker拉取镜像最后会提示镜像完整地址，containerd拉取镜像的时候必须加上这个路径 $ docker pull nginx:alpine alpine: Pulling from library/nginx 59bf1c3509f3: Pull complete ... 40e5d2fe5bcd: Pull complete Digest: sha256:eb05700fe7baa6890b74278e39b66b2ed1326831f9ec3ed4bdc6361a4ac2f333 Status: Downloaded newer image for nginx:alpine docker.io/library/nginx:alpine # 使用containerd拉取 ctr image pull docker.io/library/nginx:alpine 2.2 列出本地镜像 ctr image ls $ ctr image ls REF TYPE DIGEST SIZE PLATFORMS LABELS docker.io/gitea/gitea:1.16.0 application/vnd.docker.distribution.manifest.list.v2+json sha256:67ccf27b427ec65fd7378d0999a3d94e9649f1953d2bb115864faa71ce7b9ec2 98.4 MiB linux/amd64,linux/arm64/v8 - docker.io/library/nginx:alpine application/vnd.docker.distribution.manifest.list.v2+json sha256:da9c94bec1da829ebd52431a84502ec471c8e548ffb2cedbf36260fd9bd1d4d3 9.7 MiB linux/386,linux/amd64,linux/arm/v6,linux/arm/v7,linux/arm64/v8,linux/ppc64le,linux/s390x - 使用 -q（--quiet） 选项可以只打印镜像名称 $ ctr image ls -q docker.io/gitea/gitea:1.16.0 docker.io/library/nginx:alpine 2.3 检测本地镜像 ctr image check 主要查看其中的 STATUS，complete 表示镜像是完整可用的状态。 $ ctr image check REF TYPE DIGEST STATUS SIZE UNPACKED docker.io/gitea/gitea:1.16.0 application/vnd.docker.distribution.manifest.list.v2+json sha256:67ccf27b427ec65fd7378d0999a3d94e9649f1953d2bb115864faa71ce7b9ec2 complete (9/9) 98.4 MiB/98.4 MiB true docker.io/library/nginx:alpine application/vnd.docker.distribution.manifest.list.v2+json sha256:da9c94bec1da829ebd52431a84502ec471c8e548ffb2cedbf36260fd9bd1d4d3 complete (7/7) 9.7 MiB/9.7 MiB true 2.4 给镜像重新打标签 ctr image tag # 打标签 $ ctr image tag docker.io/library/nginx:alpine abc.com/def/nginx:alpine abc.com/def/nginx:alpine # 查看 $ ctr image ls -q abc.com/def/nginx:alpine docker.io/gitea/gitea:1.16.0 docker.io/library/nginx:alpine 2.5 删除镜像 ctr image rm 加上 --sync 选项可以同步删除镜像和所有相关的资源 # 查看镜像 $ ctr image ls -q abc.com/def/nginx:alpine docker.io/gitea/gitea:1.16.0 docker.io/library/nginx:alpine # 删除镜像 $ ctr image rm abc.com/def/nginx:alpine abc.com/def/nginx:alpine # 再次查看镜像 $ ctr image ls -q docker.io/gitea/gitea:1.16.0 docker.io/library/nginx:alpine 2.6 将镜像挂载到主机目录 ctr image mount # 挂载 ctr image mount docker.io/library/nginx:alpine /mnt $ ls /mnt bin dev docker-entrypoint.d docker-entrypoint.sh etc home lib media mnt opt proc root run sbin srv sys tmp usr var $ tree -L 1 /mnt /mnt ├── bin ├── dev ├── docker-entrypoint.d ├── docker-entrypoint.sh ├── etc ├── home ├── lib ├── media ├── mnt ├── opt ├── proc ├── root ├── run ├── sbin ├── srv ├── sys ├── tmp ├── usr └── var 18 directories, 1 file 2.7 将镜像从主机目录上卸载 ctr image unmount ctr image unmount /mnt 2.8 导出镜像 ctr image export ctr image export --all-platforms nginx.tar.gz docker.io/library/nginx:alpine 执行命令报错 $ ctr image export --all-platforms nginx.tar.gz docker.io/library/nginx:alpine ctr: content digest sha256:4bb6d6fd8bff453bece3b2dd033897ba8b4023bc686a57c50fae1b0668ae18f1: not found 解决方法是在pull镜像的时候加上 --all-platforms 选项 ctr i pull --all-platforms docker.io/library/nginx:alpine 2.9 导入镜像 ctr image import # 查看镜像 $ ctr image ls -q docker.io/gitea/gitea:1.16.0 # 导入镜像 $ ctr image import nginx.tar.gz unpacking docker.io/library/nginx:alpine (sha256:da9c94bec1da829ebd52431a84502ec471c8e548ffb2cedbf36260fd9bd1d4d3)...done # 再次查看镜像 $ ctr image ls -q docker.io/gitea/gitea:1.16.0 docker.io/library/nginx:alpine 3.容器操作 3.1 查看容器操作帮助 ctr container $ ctr container NAME: ctr containers - manage containers USAGE: ctr containers command [command options] [arguments...] COMMANDS: create create container delete, del, rm delete one or more existing containers info get info about a container list, ls list containers label set and clear labels for a container checkpoint checkpoint a container restore restore a container from checkpoint OPTIONS: --help, -h show help 3.2 创建容器 ctr container create ctr container create docker.io/library/nginx:alpine nginx 3.3 列出容器 ctr container ls $ ctr container ls CONTAINER IMAGE RUNTIME nginx docker.io/library/nginx:alpine io.containerd.runc.v2 # 加-q选项精简显示 $ ctr container ls -q nginx 3.4 查看容器详细配置 ctr container info $ ctr container info nginx { \"ID\": \"nginx\", \"Labels\": { \"io.containerd.image.config.stop-signal\": \"SIGQUIT\", \"maintainer\": \"NGINX Docker Maintainers \\u003cdocker-maint@nginx.com\\u003e\" }, \"Image\": \"docker.io/library/nginx:alpine\", \"Runtime\": { \"Name\": \"io.containerd.runc.v2\", \"Options\": { \"type_url\": \"containerd.runc.v1.Options\" } }, \"SnapshotKey\": \"nginx\", \"Snapshotter\": \"overlayfs\", \"CreatedAt\": \"2022-02-03T14:06:26.311050811Z\", \"UpdatedAt\": \"2022-02-03T14:06:26.311050811Z\", \"Extensions\": null, \"Spec\": { \"ociVersion\": \"1.0.2-dev\", \"process\": { \"user\": { \"uid\": 0, \"gid\": 0, ...... 3.5 删除容器 ctr container rm 除了使用 rm 子命令之外也可以使用 delete 或者 del 删除容器 ctr container rm nginx 4.任务 上面我们通过 container create 命令创建的容器，并没有处于运行状态，只是一个静态的容器。一个 container 对象只是包含了运行一个容器所需的资源及相关配置数据，表示 namespaces、rootfs 和容器的配置都已经初始化成功了，只是用户进程还没有启动。 一个容器真正运行起来是由 Task 任务实现的，Task 可以为容器设置网卡，还可以配置工具来对容器进行监控等。 4.1 查看任务操作帮助 ctr task $ ctr task NAME: ctr tasks - manage tasks USAGE: ctr tasks command [command options] [arguments...] COMMANDS: attach attach to the IO of a running container checkpoint checkpoint a container delete, rm delete one or more tasks exec execute additional processes in an existing container list, ls list tasks kill signal a container (default: SIGTERM) pause pause an existing container ps list processes for container resume resume a paused container start start a container that has been created metrics, metric get a single data point of metrics for a task with the built-in Linux runtime OPTIONS: --help, -h show help 4.2 启动容器 ctr task start $ ctr task start -d nginx /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/ /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh 4.2.1 启动容器的一个报错 启动容器报错 $ ctr task start -d nginx ctr: failed to create shim: OCI runtime create failed: unable to retrieve OCI runtime error (open /run/containerd/io.containerd.runtime.v2.task/default/nginx/log.json: no such file or directory): runc did not terminate successfully: exit status 127: unknown github中有提到containerd1.5.7版本在centos7中无法运行runc，经过测试，containerd1.5.5版本后会涉及这个问题 通过运行runc命令排查，发现缺少依赖：seccomp_api_get $ runc runc: symbol lookup error: runc: undefined symbol: seccomp_api_get 但是centos7.9系统已经安装 libseccomp-devel了，再次检查发现，seccomp_api_get 对 libseccomp 版本有要求，至少 libseccomp v2.4 版本才提供有 seccomp_api_get，centos7本身源只有这个包 libseccomp-devel-2.3.1-4.el7.x86_64 rpmfind.net 和 pkgs.org 这2个网站中 libseccomp 版本都是2.3，最终在 cbs.centos.org 中找到了2.4版本的 libseccomp ，安装2.4版本后问题解决 4.3 查看启动的容器 ctr task ls $ ctr task ls TASK PID STATUS nginx 6684 RUNNING 4.4 进入容器 ctr task exec ⚠️这里需要注意必须要指定 --exec-id 参数，这个 id 可以随便写，只要唯一就行 ctr task exec --exec-id 0 -t nginx /bin/sh 4.5 暂停容器 ctr task pause # 暂停容器 ctr task pause nginx # 容器暂停后状态就变为了PAUSED $ ctr task ls TASK PID STATUS nginx 6684 PAUSED 4.5 恢复容器 ctr task resume # 恢复容器 ctr task resume nginx # 查看 $ ctr task ls TASK PID STATUS nginx 6684 RUNNING 4.6 停止容器 ctr task kill ⚠️ctr 没有 stop 容器的功能，只能暂停或者杀死容器 # kill容器 ctr task kill nginx # 查看，容器的状态就变为了STOPPED $ ctr task ls TASK PID STATUS nginx 6684 STOPPED 4.7 删除容器 ctr task rm ctr task rm nginx 4.8 获取容器的 cgroup 相关信息 ctr task metrics $ ctr task ls TASK PID STATUS nginx 6961 RUNNING [root@k8s-test ~]# ctr task metrics nginx ID TIMESTAMP nginx 2022-02-05 11:30:00.893668953 +0000 UTC METRIC VALUE memory.usage_in_bytes 1388544 memory.limit_in_bytes 9223372036854771712 memory.stat.cache 12288 cpuacct.usage 49227598 cpuacct.usage_percpu [49227598] pids.current 2 pids.limit 0 4.9 查看容器中所有进程在宿主机中的 PID ctr task ps 其中第一个 PID 6961 就是我们容器中的1号进程 $ ctr task ps nginx PID INFO 6961 - 6991 - 5.命名空间 5.1 查看命名空间 ctr ns ls $ ctr ns ls NAME LABELS default 5.2 创建命名空间 ctr ns create # 创建命名空间test ctr ns create test # 查看命名空间 $ ctr ns ls NAME LABELS default test 5.3 删除命名空间 ctr ns rm # 删除命名空间 $ctr ns rm test test # 查看命名空间 $ ctr ns ls NAME LABELS default 5.4 指定命名空间 ctr -n # 默认命名空间default下的镜像 $ ctr -n default image ls REF TYPE DIGEST SIZE PLATFORMS LABELS docker.io/library/nginx:alpine application/vnd.docker.distribution.manifest.list.v2+json sha256:da9c94bec1da829ebd52431a84502ec471c8e548ffb2cedbf36260fd9bd1d4d3 9.7 MiB linux/386,linux/amd64,linux/arm/v6,linux/arm/v7,linux/arm64/v8,linux/ppc64le,linux/s390x - # 命名空间test下的镜像，这里在pull镜像的时候没有指定命名空间，因此为空 $ ctr -n test image ls REF TYPE DIGEST SIZE PLATFORMS LABELS pull镜像时通过 -n 选项指定命名空间 # pull镜像时指定命名空间 ctr -n test image pull docker.io/library/nginx:alpine # 指定命名空间查看镜像 $ ctr -n test image ls REF TYPE DIGEST SIZE PLATFORMS LABELS docker.io/library/nginx:alpine application/vnd.docker.distribution.manifest.list.v2+json sha256:da9c94bec1da829ebd52431a84502ec471c8e548ffb2cedbf36260fd9bd1d4d3 9.7 MiB linux/386,linux/amd64,linux/arm/v6,linux/arm/v7,linux/arm64/v8,linux/ppc64le,linux/s390x - 我们知道 Docker 其实也是默认调用的 containerd，事实上 Docker 使用的 containerd 下面的命名空间默认是 moby，而不是 default，所以假如我们有用 docker 启动容器，那么我们也可以通过 ctr -n moby 来定位下面的容器 ctr -n moby container ls 同样 Kubernetes 下使用的 containerd 默认命名空间是 k8s.io，所以我们可以使用 ctr -n k8s.io 来查看 Kubernetes 下面创建的容器 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/容器/containerd/containerd 高级命令行工具 nerdctl.html":{"url":"linux/容器/containerd/containerd 高级命令行工具 nerdctl.html","title":"containerd 高级命令行工具 nerdctl","keywords":"","body":"containerd 高级命令行工具 nerdctl nerrdctl github地址 1.nerdctl简介 安装containerd后是使用 ctr 操作管理 containerd 镜像容器，但是大家都习惯了使用 docker cli，ctr 使用起来可能还是不太顺手，为了能够让大家更好的转到 containerd 上面来，社区提供了一个新的命令行工具：nerdctl。nerdctl 是一个与 docker cli 风格兼容的 containerd 客户端工具，而且直接兼容 docker compose 的语法的，这就大大提高了直接将 containerd 作为本地开发、测试或者单机容器部署使用的效率。 2.nerdctl安装 2.1 未安装containerd ⚠️如果没有安装 containerd，则可以下载 nerdctl-full-\\-linux-amd64.tar.gz 包进行安装，并且nerdctl-full这个包是与containerd版本一一对应的，即containerd发布一个版本后，nerdctl-full也随即发布一个版本，虽然2者版本号不一致，但是是互相对应的 2.1.1 下载包 export NERDCTL_VERSION=0.16.1 wget https://github.com/containerd/nerdctl/releases/download/v${NERDCTL_VERSION}/nerdctl-full-${NERDCTL_VERSION}-linux-amd64.tar.gz 2.1.2 查看包内容 # 查看包内容，解压缩后是bin lib libexec share 4个目录 $ tar -tf nerdctl-full-${NERDCTL_VERSION}-linux-amd64.tar.gz bin/ bin/buildctl bin/buildkitd bin/containerd bin/containerd-fuse-overlayfs-grpc bin/containerd-rootless-setuptool.sh bin/containerd-rootless.sh bin/containerd-shim-runc-v2 bin/containerd-stargz-grpc bin/ctd-decoder bin/ctr bin/ctr-enc bin/ctr-remote bin/fuse-overlayfs bin/ipfs bin/nerdctl bin/rootlessctl bin/rootlesskit bin/runc bin/slirp4netns lib/ lib/systemd/ lib/systemd/system/ lib/systemd/system/buildkit.service lib/systemd/system/containerd.service lib/systemd/system/stargz-snapshotter.service libexec/ libexec/cni/ libexec/cni/bandwidth libexec/cni/bridge libexec/cni/dhcp libexec/cni/firewall libexec/cni/host-device libexec/cni/host-local libexec/cni/ipvlan libexec/cni/isolation libexec/cni/loopback libexec/cni/macvlan libexec/cni/portmap libexec/cni/ptp libexec/cni/sbr libexec/cni/static libexec/cni/tuning libexec/cni/vlan libexec/cni/vrf share/ share/doc/ share/doc/nerdctl/ share/doc/nerdctl/README.md share/doc/nerdctl/docs/ share/doc/nerdctl/docs/cni.md share/doc/nerdctl/docs/compose.md share/doc/nerdctl/docs/config.md share/doc/nerdctl/docs/cosign.md share/doc/nerdctl/docs/dir.md share/doc/nerdctl/docs/experimental.md share/doc/nerdctl/docs/faq.md share/doc/nerdctl/docs/freebsd.md share/doc/nerdctl/docs/gpu.md share/doc/nerdctl/docs/ipfs.md share/doc/nerdctl/docs/multi-platform.md share/doc/nerdctl/docs/ocicrypt.md share/doc/nerdctl/docs/registry.md share/doc/nerdctl/docs/rootless.md share/doc/nerdctl/docs/stargz.md share/doc/nerdctl-full/ share/doc/nerdctl-full/README.md share/doc/nerdctl-full/SHA256SUMS 2.1.3 解压缩包 ⚠️解压缩包至 /usr/local 下，buildctl 、 buildkitd 、containerd 、ctr 、nerdctl 、runc 等命令就会全部包含，同时使用systemd管理buildkit文件buildkit.service 、使用systemd管理containerd文件 containerd.service 也会包含 tar xf nerdctl-full-${NERDCTL_VERSION}-linux-amd64.tar.gz -C /usr/local ⚠️也可以只复制部分文件到相关目录，例如 buildctl 、 buildkitd 、containerd 、ctr 、nerdctl 、runc 等命令以及 buildkit.service 、containerd.service 文件 2.1.4 查看目录结构 $ tree . ├── bin │ ├── buildctl │ ├── buildkitd │ ├── containerd │ ├── containerd-fuse-overlayfs-grpc │ ├── containerd-rootless-setuptool.sh │ ├── containerd-rootless.sh │ ├── containerd-shim-runc-v2 │ ├── containerd-stargz-grpc │ ├── ctd-decoder │ ├── ctr │ ├── ctr-enc │ ├── ctr-remote │ ├── fuse-overlayfs │ ├── ipfs │ ├── nerdctl │ ├── rootlessctl │ ├── rootlesskit │ ├── runc │ └── slirp4netns ├── lib │ └── systemd │ └── system │ ├── buildkit.service │ ├── containerd.service │ └── stargz-snapshotter.service ├── libexec │ └── cni │ ├── bandwidth │ ├── bridge │ ├── dhcp │ ├── firewall │ ├── host-device │ ├── host-local │ ├── ipvlan │ ├── isolation │ ├── loopback │ ├── macvlan │ ├── portmap │ ├── ptp │ ├── sbr │ ├── static │ ├── tuning │ ├── vlan │ └── vrf └── share └── doc ├── nerdctl │ ├── docs │ │ ├── cni.md │ │ ├── compose.md │ │ ├── config.md │ │ ├── cosign.md │ │ ├── dir.md │ │ ├── experimental.md │ │ ├── faq.md │ │ ├── freebsd.md │ │ ├── gpu.md │ │ ├── ipfs.md │ │ ├── multi-platform.md │ │ ├── ocicrypt.md │ │ ├── registry.md │ │ ├── rootless.md │ │ └── stargz.md │ └── README.md └── nerdctl-full ├── README.md └── SHA256SUMS 11 directories, 57 files 2.1.5 启动containerd与buildkit systemctl enable containerd buildkit && systemctl start containerd buildkit 2.1.6 验证 $ ctr version Client: Version: v1.5.9 Revision: 1407cab509ff0d96baa4f0eb6ff9980270e6e620 Go version: go1.17.6 Server: Version: v1.5.9 Revision: 1407cab509ff0d96baa4f0eb6ff9980270e6e620 UUID: 0fd8e6f7-ba7a-4715-844f-c8261932a201 $ nerdctl version Client: Version: v0.16.1 Git commit: c4bd56b3aa220db037cc6c0a4e0c8cc062f2cc4c Server: containerd: Version: v1.5.9 GitCommit: 1407cab509ff0d96baa4f0eb6ff9980270e6e620 $ buildctl -v buildctl github.com/moby/buildkit v0.9.3 8d2625494a6a3d413e3d875a2ff7dd9b1ed1b1a9 $ buildkitd -v buildkitd github.com/moby/buildkit v0.9.3 8d2625494a6a3d413e3d875a2ff7dd9b1ed1b1a9 2.1.7 卸载containerd、nerdctl、buildkit 2.1.7.1 停止containerd与buildkit服务 systemctl disable containerd buildkit && systemctl stop containerd buildkit && systemctl status containerd buildkit 2.1.7.2 卸载containerd、nerdctl、buildkit 卸载containerd、nerdctl、buildkit删除相关目录文件即可 rm -rf /usr/local/bin/{buildctl,buildkitd,containerd,containerd-fuse-overlayfs-grpc,containerd-rootless-setuptool.sh,containerd-rootless.sh,containerd-shim-runc-v2,containerd-stargz-grpc,ctd-decoder,ctd-decoder,ctr,ctr-enc,ctr-remote,fuse-overlayfs,ipfs,nerdctl,rootlessctl,rootlesskit,runc,slirp4netns} \\ /usr/local/lib/systemd/system/{buildkit.service,containerd.service,stargz-snapshotter.service} \\ /usr/local/libexec/cni \\ /usr/local/share/doc/{nerdctl,nerdctl-full} 2.2 已安装containerd 如果已经安装过了containerd，则直接下载 nerdctl--linux-amd64.tar.gz 2.2.1 下载包 export NERDCTL_VERSION=0.16.1 wget https://github.com/containerd/nerdctl/releases/download/v${NERDCTL_VERSION}/nerdctl-${NERDCTL_VERSION}-linux-amd64.tar.gz 2.2.2 查看包内容 # 查看包内容，解压缩后是2个sh文件和nerdctl二进制命令 $ tar tf nerdctl-${NERDCTL_VERSION}-linux-amd64.tar.gz nerdctl containerd-rootless-setuptool.sh containerd-rootless.sh 2.2.3 解压缩包 tar xf nerdctl-${NERDCTL_VERSION}-linux-amd64.tar.gz 2.2.4 导出 nerdctl 命令 mv nerdctl /usr/local/bin 2.2.5 验证 $ nerdctl version Client: Version: v0.16.1 Git commit: c4bd56b3aa220db037cc6c0a4e0c8cc062f2cc4c Server: containerd: Version: v1.5.9 GitCommit: 1407cab509ff0d96baa4f0eb6ff9980270e6e620 3.nerdctl使用 nerdctl与docker命令几乎一致 3.1 构建镜像 nerdctl build 编辑dockerfile cat > Dockerfile /usr/share/nginx/html/index.html EOF 执行构建，发现有报错，需要我们安装 buildctl 并运行 buildkitd，这是因为 nerdctl build 需要依赖 buildkit 工具 $ nerdctl build -t nginx:nerdctl -f Dockerfile . FATA[0000] `buildctl` needs to be installed and `buildkitd` needs to be running, see https://github.com/moby/buildkit: exec: \"buildctl\": executable file not found in $PATH buildkit 项目也是 Docker 公司开源的一个构建工具包，支持 OCI 标准的镜像构建。它主要包含以下部分: 服务端 buildkitd：当前支持 runc 和 containerd 作为 worker，默认是 runc，我们这里使用 containerd 客户端 buildctl：负责解析 Dockerfile，并向服务端 buildkitd 发出构建请求 buildkit 是典型的 C/S 架构，客户端和服务端是可以不在一台服务器上，而 nerdctl 在构建镜像的时候也作为 buildkitd 的客户端，所以需要我们安装并运行 buildkitd 安装 buildkit export BUILDKIT_VERSION=0.9.3 wget https://github.com/moby/buildkit/releases/download/v${BUILDKIT_VERSION}/buildkit-v${BUILDKIT_VERSION}.linux-amd64.tar.gz 解压缩包 tar xf buildkit-v${BUILDKIT_VERSION}.linux-amd64.tar.gz 解压后是一个 bin 目录， $ tree bin bin ├── buildctl ├── buildkitd ├── buildkit-qemu-aarch64 ├── buildkit-qemu-arm ├── buildkit-qemu-i386 ├── buildkit-qemu-mips64 ├── buildkit-qemu-mips64el ├── buildkit-qemu-ppc64le ├── buildkit-qemu-riscv64 ├── buildkit-qemu-s390x └── buildkit-runc 0 directories, 11 files 我们需要2个命令 buildctl 、buildkitd mv bin/{buildctl,buildkitd} /usr/local/bin/ 验证 $ buildctl -v buildctl github.com/moby/buildkit v0.9.3 8d2625494a6a3d413e3d875a2ff7dd9b1ed1b1a9 $ buildkitd -v buildkitd github.com/moby/buildkit v0.9.3 8d2625494a6a3d413e3d875a2ff7dd9b1ed1b1a9 使用 systemd 管理 buildkitd cat > /etc/systemd/system/buildkit.service 启动 buildkit systemctl daemon-reload && systemctl enable buildkit && systemctl start buildkit 查看启动状态 $ systemctl status buildkit ● buildkit.service - BuildKit Loaded: loaded (/etc/systemd/system/buildkit.service; enabled; vendor preset: disabled) Active: active (running) since Sat 2022-02-05 20:23:38 CST; 8s ago Docs: https://github.com/moby/buildkit Main PID: 16599 (buildkitd) Tasks: 6 Memory: 8.6M CGroup: /system.slice/buildkit.service └─16599 /usr/local/bin/buildkitd --oci-worker=false --containerd-worker=true Feb 05 20:23:38 k8s-test systemd[1]: Started BuildKit. 再次构建 $ nerdctl build -t nginx:nerdctl -f Dockerfile . [+] Building 28.1s (6/6) FINISHED => [internal] load build definition from Dockerfile 1.1s => => transferring dockerfile: 99B 0.0s => [internal] load .dockerignore 1.0s => => transferring context: 2B 0.0s => [internal] load metadata for docker.io/library/nginx:latest 4.9s => [1/2] FROM docker.io/library/nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767 5.9s => => resolve docker.io/library/nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767 0.2s => => extracting sha256:5eb5b503b37671af16371272f9c5313a3e82f1d0756e14506704489ad9900803 2.1s => => extracting sha256:1ae07ab881bd848493ad54c2ba32017f94d1d8dbfd0ba41b618f17e80f834a0f 2.3s => => extracting sha256:78091884b7bea0fa918527207924e9993bcc21bf7f1c9687da40042ceca31ac9 0.3s => => extracting sha256:091c283c6a66ad0edd2ab84cb10edacc00a1a7bc5277f5365c0d5c5457a75aff 0.2s => => extracting sha256:55de5851019b8f65ed6e28120c6300e35e556689d021e4b3411c7f4e90a9704b 0.2s => => extracting sha256:b559bad762bec166fd028483dd2a03f086d363ee827d8c98b7268112c508665a 0.2s => [2/2] RUN echo 'hehe' > /usr/share/nginx/html/index.html 5.8s => exporting to oci image format 7.3s => => exporting layers 1.7s => => exporting manifest sha256:4d1e59bbbfb8ee312290b9e377e6b116f7957ee623e2d38b0e75c99d1ba3ece4 0.2s => => exporting config sha256:b3d0a6dd26eb0181ec1cc98d21b580fb61b96d656de91245d1ff342d9963a1e2 0.2s => => sending tarball 4.9s unpacking docker.io/library/nginx:nerdctl (sha256:4d1e59bbbfb8ee312290b9e377e6b116f7957ee623e2d38b0e75c99d1ba3ece4)...done 查看构建 $ nerdctl images REPOSITORY TAG IMAGE ID CREATED PLATFORM SIZE nginx nerdctl 4d1e59bbbfb8 29 seconds ago linux/amd64 146.2 MiB 根据构建的镜像启动一个容器 $ nerdctl run -d -p 80:80 --name=nginx --restart=always nginx:nerdctl 4476ea62f983e67108ccc1c208a2ac8aaa80af63d439e8d701ab1633d037e1f5 查看启动的容器 $ nerdctl ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4476ea62f983 docker.io/library/nginx:nerdctl \"/docker-entrypoint.…\" 2 minutes ago Up 0.0.0.0:80->80/tcp nginx 访问容器 $ curl 127.0.0.1:80 hehe 这样我们就使用 nerdctl + buildkitd 轻松完成了容器镜像的构建。当然如果你还想在单机环境下使用 Docker Compose，在 containerd 模式下，我们也可以使用 nerdctl 来兼容该功能。同样我们可以使用 nerdctl compose、nerdctl compose up、nerdctl compose logs、nerdctl compose build、nerdctl compose down 等命令来管理 Compose 服务。这样使用 containerd、nerdctl 结合 buildkit 等工具就完全可以替代 docker 在镜像构建、镜像容器方面的管理功能了 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s介绍及组件/Kubernetes介绍.html":{"url":"linux/k8s/k8s介绍及组件/Kubernetes介绍.html","title":"k8s介绍","keywords":"","body":"[toc] 一、Kubernetes介绍 参考文档： What is Kubernetes Kubernetes是一个可以移植、可扩展的开源平台，使用 声明式的配置 并依据配置信息自动地执行容器化应用程序的管理。在所有的容器编排工具中（类似的还有 docker swarm / mesos等），Kubernetes的生态系统更大、增长更快，有更多的支持、服务和工具可供用户选择。 Kubernetes的名字起源于希腊语，含义是 舵手、领航员、向导。Google于2014年将Brog系统开源为Kubernetes。Kubernetes构建在Google Brog 十五年运行大规模分布式系统的经验 基础之上，并结合了开源社区最好的想法和实践。 以下是使用 google trends 对比 kubernetes 、 docker swarm、 mesos 三个关键词的截图。 回顾 为了理解Kubernetes的用处，我们先回顾一下历史。 大致来说，在部署应用程序的方式上，我们主要经历了三个时代： 传统部署时代：早期，企业直接将应用程序部署在物理机上。由于物理机上不能为应用程序定义资源使用边界，我们也就很难合理地分配计算资源。例如：如果多个应用程序运行在同一台物理机上，可能发生这样的情况：其中的一个应用程序消耗了大多数的计算资源，导致其他应用程序不能正常运行。应对此问题的一种解决办法是，将每一个应用程序运行在不同的物理机上。然而，这种做法无法大规模实施，因为资源利用率很低，且企业维护更多物理机的成本昂贵。 虚拟化部署时代：针对上述问题，虚拟化技术应运而生。用户可以在单台物理机的CPU上运行多个虚拟机（Virtual Machine）。 虚拟化技术使得应用程序被虚拟机相互分隔开，限制了应用程序之间的非法访问，进而提供了一定程度的安全性。 虚拟化技术提高了物理机的资源利用率，可以更容易地安装或更新应用程序，降低了硬件成本，因此可以更好地规模化实施。 每一个虚拟机可以认为是被虚拟化的物理机之上的一台完整的机器，其中运行了一台机器的所有组件，包括虚拟机自身的操作系统。 容器化部署时代：容器与虚拟机类似，但是降低了隔离层级，共享了操作系统。因此，容器可以认为是轻量级的。 与虚拟机相似，每个容器拥有自己的文件系统、CPU、内存、进程空间等 运行应用程序所需要的资源都被容器包装，并和底层基础架构解耦 容器化的应用程序可以跨云服务商、跨Linux操作系统发行版进行部署 容器化越来越流行，主要原因是它带来的诸多好处： 敏捷地创建和部署应用程序：相较于创建虚拟机镜像，创建容器镜像更加容易和快速 持续构建集成：可以更快更频繁地构建容器镜像、部署容器化的应用程序、并且轻松地回滚应用程序 分离开发和运维的关注点：在开发构建阶段就完成容器镜像的构建，构建好的镜像可以部署到多种基础设施上。这种做法将开发阶段需要关注的内容包含在如何构建容器镜像的过程中，将部署阶段需要关注的内容聚焦在如何提供基础设施以及如何使用容器镜像的过程中。降低了开发和运维的耦合度 可监控性：不仅可以查看操作系统级别的资源监控信息，还可以查看应用程序健康状态以及其他信号的监控信息 开发、测试、生产不同阶段的环境一致性：开发阶段在笔记本上运行的容器与测试、生产环境中运行的容器一致 跨云服务商、跨操作系统发行版的可移植性：容器可运行在 Ubuntu、RHEL、CoreOS、CentOS等不同的操作系统发行版上，可以运行在私有化部署、Google Kubernetes Engine、AWS、阿里云等不同的云供应商的环境中 以应用程序为中心的管理：虚拟机时代的考虑的问题是在虚拟硬件上运行一个操作系统，而容器化时代，问题的焦点则是在操作系统的逻辑资源上运行一个应用程序 松耦合、分布式、弹性、无约束的微服务：应用程序被切分成更小的、独立的微服务，并可以动态部署和管理，而不是一个部署在专属机器上的庞大的单片应用程序 资源隔离：确保应用程序性能不受干扰 资源利用：资源高效、高密度利用 二、Kubernetes的功能 容器是一个非常好的打包并运行应用程序的方式。在生产环境中，您需要管理容器化应用程序，并且确保其不停机地连续运行。例如：一个容器故障停机，另外一个容器需要立刻启动以替补停机的容器。类似的这种对容器的管理动作由系统来执行会更好更快速。 Kubernetes针对此类问题，提供了容器化编排解决方案，可以使你非常健壮地运行分布式系统。Kubernetes可以处理应用程序的伸缩、failover、部署模式等多种需求。例如，Kubernetes可以轻易地管理系统的金丝雀发布（灰度发布）。 Kubernetes提供的特性有： 服务发现和负载均衡 Kubernetes 可以通过 DNS 名称或 IP 地址暴露容器的访问方式。并且可以在同组容器内分发负载以实现负载均衡 存储编排 Kubernetes可以自动挂载指定的存储系统，例如 local stroage/nfs/云存储等 自动发布和回滚 您可以在 Kubernetes 中声明您期望应用程序容器应该达到的状态，Kubernetes将以合适的速率调整容器的实际状态，并逐步达到最终期望的结果。请参考 声明式的配置 自愈 Kubernetes提供如下自愈能力： 重启已经停机的容器 替换、kill 那些不满足自定义健康检查条件的容器 在容器就绪之前，避免调用者发现该容器 密钥及配置管理 Kubernetes可以存储和管理敏感信息（例如，密码、OAuth token、ssh密钥等）。您可以更新容器应用程序的密钥、配置等信息，而无需： 重新构建容器的镜像 在不合适的地方暴露密码信息 三、Kubernetes的边界 Kubernetes不是一个传统意义的、保罗万象的 PaaS（Platform as a Service）系统。Kubernetes在容器层面工作，而不是硬件层面，它提供了与 PaaS 平台相似的通用特性，例如：部署、伸缩、负载均衡、日志、监控等。然而，Kubernetes并不是一个单一整体，这些特性都是可选、可插拔的。Kubernetes提供用于搭建开发平台的基础模块，同时为用户提供了不同模块的选择性和多样性。 Kubernetes： 不限制应用程序的类型。Kubernetes的目标是广泛支持不同类型的工作负载，包括：有状态、无状态、数据处理等类型的应用。只要应用可以在容器中运行，就能够非常好地在 Kubernetes 上运行 不部署源码、不编译或构建应用程序。持续集成、分发、部署（CI/CD）的工作流极大程度上取决于组织的文化、偏好以及技术要求。Kubernetes可以作为部署平台参与到 CI/CD 流程，但是不涉及镜像构建和分发的过程 译者注：可选的有 Jenkins / Gitlab Runner / docker registry / harbour 等 不提供应用程序级别的服务，包括：中间件（例如，消息总线）、数据处理框架（例如，Spark）、数据库（例如，mysql）、缓存（例如，Redis），或者分布式存储（例如，Ceph）。此类组件可以在 Kubernetes 上运行，或者可以被运行在 Kubernetes 上的应用程序访问 不限定日志、监控、报警的解决方案。Kubernetes 提供一些样例展示如何与日志、监控、报警等组件集成，同时提供收集、导出监控度量（metrics）的一套机制。您可以根据自己的需要选择日志、监控、报警组件 译者注：可选的有 ELK / Prometheus / Graphana / Pinpoint / Skywalking / Metrics Server 等 不提供或者限定配置语言（例如，jsonnet）。Kubernetes提供一组声明式的 API，您可以按照自己的方式定义部署信息。 译者注：可选的有 helm/kustomize/kubectl/kubernetes dashboard/kuboard/octant/k9s 等 不提供或限定任何机器的配置、维护、管理或自愈的系统。 译者注：在这个级别上，可选的组件有 puppet、ansible、open stack 等 此外，Kubernetes不是一个纯粹意义上的容器编排系统。事实上，Kubernetes 消除了容器编排的需求。容器编排的技术定义是预定义流程的执行（先做A、再做B、然后做C）。与此相对应，Kubernetes构建了一系列相互独立、可预排的控制过程，以持续不断地将系统从当前状态调整到声明的目标状态。如何从 A 达到 C，并不重要。集中化的控制也就不需要了。这个设计思想使得Kubernetes使用更简单、更强大、稳健、反脆弱和可扩展。 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s介绍及组件/Kubernetes组件.html":{"url":"linux/k8s/k8s介绍及组件/Kubernetes组件.html","title":"k8s组件","keywords":"","body":"[toc] Kubernetes组件 本文严重抄袭于kuboard.cn) 参考文档： Kubernetes Components 本文档描述了 Kubernetes 的主要组件。 Master组件 Master组件是集群的控制平台（control plane）： master 组件负责集群中的全局决策（例如，调度） master 组件探测并响应集群事件（例如，当 Deployment 的实际 Pod 副本数未达到 replicas 字段的规定时，启动一个新的 Pod） Master组件可以运行于集群中的任何机器上。但是，为了简洁性，通常在同一台机器上运行所有的 master 组件，且不在此机器上运行用户的容器。参考 安装Kubernetes高可用。 kube-apiserver 此 master 组件提供 Kubernetes API。这是Kubernetes控制平台的前端（front-end），可以水平扩展（通过部署更多的实例以达到性能要求）。kubectl / kubernetes dashboard / kuboard 等Kubernetes管理工具就是通过 kubernetes API 实现对 Kubernetes 集群的管理。 etcd 支持一致性和高可用的名值对存储组件，Kubernetes集群的所有配置信息都存储在 etcd 中。请确保您 备份 了 etcd 的数据。关于 etcd 的更多信息，可参考 etcd 官方文档 kube-scheduler 此 master 组件监控所有新创建尚未分配到节点上的 Pod，并且自动选择为 Pod 选择一个合适的节点去运行。 影响调度的因素有： 单个或多个 Pod 的资源需求 硬件、软件、策略的限制 亲和与反亲和（affinity and anti-affinity）的约定 数据本地化要求 工作负载间的相互作用 kube-controller-manager 此 master 组件运行了所有的控制器 逻辑上来说，每一个控制器是一个独立的进程，但是为了降低复杂度，这些控制器都被合并运行在一个进程里。 kube-controller-manager 中包含的控制器有： 节点控制器： 负责监听节点停机的事件并作出对应响应 副本控制器： 负责为集群中每一个 副本控制器对象（Replication Controller Object）维护期望的 Pod 副本数 端点（Endpoints）控制器：负责为端点对象（Endpoints Object，连接 Service 和 Pod）赋值 Service Account & Token控制器： 负责为新的名称空间创建 default Service Account 以及 API Access Token cloud-controller-manager cloud-controller-manager 中运行了与具体云基础设施供应商互动的控制器。这是 Kubernetes 1.6 版本中引入的特性，尚处在 alpha 阶段。 cloud-controller-manager 只运行特定于云基础设施供应商的控制器。如果您参考 www.kuboard.cn 上提供的文档安装 Kubernetes 集群，默认不安装 cloud-controller-manager。 cloud-controller-manager 使得云供应商的代码和 Kubernetes 的代码可以各自独立的演化。在此之前的版本中，Kubernetes的核心代码是依赖于云供应商的代码的。在后续的版本中，特定于云供应商的代码将由云供应商自行维护，并在运行Kubernetes时链接到 cloud-controller-manager。 以下控制器中包含与云供应商相关的依赖： 节点控制器：当某一个节点停止响应时，调用云供应商的接口，以检查该节点的虚拟机是否已经被云供应商删除 译者注：私有化部署Kubernetes时，我们不知道节点的操作系统是否删除，所以在移除节点后，要自行通过 kubectl delete node 将节点对象从 Kubernetes 中删除 路由控制器：在云供应商的基础设施中设定网络路由 译者注：私有化部署Kubernetes时，需要自行规划Kubernetes的拓扑结构，并做好路由配置，例如 安装Kubernetes单Master节点 中所作的 服务（Service）控制器：创建、更新、删除云供应商提供的负载均衡器 译者注：私有化部署Kubernetes时，不支持 LoadBalancer 类型的 Service，如需要此特性，需要创建 NodePort 类型的 Service，并自行配置负载均衡器 数据卷（Volume）控制器：创建、绑定、挂载数据卷，并协调云供应商编排数据卷 译者注：私有化部署Kubernetes时，需要自行创建和管理存储资源，并通过Kubernetes的存储类、存储卷、数据卷等与之关联 译者注：通过 cloud-controller-manager，Kubernetes可以更好地与云供应商结合，例如，在阿里云的 Kubernetes 服务里，您可以在云控制台界面上轻松点击鼠标，即可完成 Kubernetes 集群的创建和管理。在私有化部署环境时，您必须自行处理更多的内容。幸运的是，通过合适的教程指引，这些任务的达成并不困难。 Node 组件 Node 组件运行在每一个节点上（包括 master 节点和 worker 节点），负责维护运行中的 Pod 并提供 Kubernetes 运行时环境。 kubelet 此组件是运行在每一个集群节点上的代理程序。它确保 Pod 中的容器处于运行状态。Kubelet 通过多种途径获得 PodSpec 定义，并确保 PodSpec 定义中所描述的容器处于运行和健康的状态。Kubelet不管理不是通过 Kubernetes 创建的容器。 kube-proxy kube-proxy 是一个网络代理程序，运行在集群中的每一个节点上，是实现 Kubernetes Service 概念的重要部分。 kube-proxy 在节点上维护网络规则。这些网络规则使得您可以在集群内、集群外正确地与 Pod 进行网络通信。如果操作系统中存在 packet filtering layer，kube-proxy 将使用这一特性（iptables代理模式），否则，kube-proxy将自行转发网络请求（User space代理模式） 容器引擎 容器引擎负责运行容器。Kubernetes支持多种容器引擎：Docker、containerd、cri-o、rktlet 以及任何实现了 Kubernetes容器引擎接口 的容器引擎 Addons Addons 使用 Kubernetes 资源（DaemonSet、Deployment等）实现集群的功能特性。由于他们提供集群级别的功能特性，addons使用到的Kubernetes资源都放置在 kube-system 名称空间下。 下面描述了一些经常用到的 addons，参考 Addons 查看更多列表。 DNS 除了 DNS Addon 以外，其他的 addon 都不是必须的，所有 Kubernetes 集群都应该有 Cluster DNS Cluster DNS 是一个 DNS 服务器，是对您已有环境中其他 DNS 服务器的一个补充，存放了 Kubernetes Service 的 DNS 记录。 Kubernetes 启动容器时，自动将该 DNS 服务器加入到容器的 DNS 搜索列表中。 如果您参考 www.kuboard.cn 上提供的文档安装 Kubernetes，默认已经安装了 Core DNS Web UI（Dashboard） Dashboard 是一个Kubernetes集群的 Web 管理界面。用户可以通过该界面管理集群。 Kuboard Kuboard 是一款基于Kubernetes的微服务管理界面，相较于 Dashboard，Kuboard 强调： 无需手工编写 YAML 文件 微服务参考架构 上下文相关的监控 场景化的设计 导出配置 导入配置 ContainerResource Monitoring Container Resource Monitoring 将容器的度量指标（metrics）记录在时间序列数据库中，并提供了 UI 界面查看这些数据 Cluster-level Logging Cluster-level logging 机制负责将容器的日志存储到一个统一存储中，并提供搜索浏览的界面 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s知识点/k8s知识图谱.html":{"url":"linux/k8s/k8s知识点/k8s知识图谱.html","title":"k8s知识图谱","keywords":"","body":"[toc] k8s知识图谱 1.部署k8s集群 Ansible kubeadm 二进制 第三方工具(sealos) 2.数据库 etcd：存储k8s对象信息 3.配置管理 configmap secret 4.包管理器 helm 5.微服务 istio 6.存储 volumes PV、PVC storage classse rook基于ceph 分布式存储 glusterFS ceph nfs 7.网络 flannel calico cannel 8.机器学习 kubeflow 9.日志收集 elasticsearch+fluentd+kibana elasticearch+logstash+filebeat+kibana 10.开放接口 CRI（Container Runtime Interface）：容器运行时接口，提供计算资源 CNI（Container Network Interface）：容器网络接口，提供网络资源 CSI（Container Storage Interface）：容器存储接口，提供存储资源 11.容器引擎（docker） 镜像、容器、网络、数据持久化 Dockerfile Supervisor多进程管理 GPU nvidia-docker device-plugin 12.镜像仓库 harbor nexus 13.常用资源对象 Pod namespaces Labels and Selectors Annotations 14.控制器 ReplicaSet Deployment SatefulSet DaemonSet job CronJob HPA Operater 15.DNS CoreDNS KubeDNS 16.服务发现与负载均衡 Ingress nginx Traefik Service 17.安全 Namespace ServiceAccout RBAC 18.监控 cAdvisor+Prometheus+Grafana Metrice-server kube-state-metrics Heapster 19.CI/CD jenkins gitlab git 发布策略 滚动更新 蓝绿发布 灰度发布 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s知识点/k8s必会知识点梳理.html":{"url":"linux/k8s/k8s知识点/k8s必会知识点梳理.html","title":"k8s必会知识点梳理","keywords":"","body":"[toc] k8s必会知识点梳理 本文严重抄袭至互联网 kube-apiserver 对外暴露了Kubernetes API。它是的 Kubernetes 核心控制层。它被设计为水平扩展，即通过部署更多实例来横向扩展。API Server 负责和 etcd 交互（其他组件不会直接操作 etcd，只有 API Server 这么做），是整个 kubernetes 集群的数据中心，所有的交互都是以 API Server 为核心的。API Server 提供了以下的功能： 整个集群管理的 API 接口：所有对集群进行的查询和管理都要通过 API 来进行。 集群内部各个模块之间通信的枢纽：所有模块之间并不会互相调用，而是通过和 API Server 打交道来完成各自的工作。 集群安全控制：API Server 提供的验证和授权保证了整个集群的安全。 kube-controller-manager和kube-scheduler的高可用选主机制 看这里 csdn博客 在k8s的组件中，其中有kube-scheduler和kube-controller-manager两个组件是有leader选举的，这个选举机制是k8s对于这两个组件的高可用保障。需要--leader-elect=true启动参数。即正常情况下kube-scheduler或kube-manager-controller组件的多个副本只有一个是处于业务逻辑运行状态，其它副本则不断的尝试去获取锁，去竞争leader，直到自己成为leader。如果正在运行的leader因某种原因导致当前进程退出，或者锁丢失，则由其它副本去竞争新的leader，获取leader继而执行业务逻辑。 在K8s中， 通过创建资源对象（当前的实现中实现了 ConfigMap 和 Endpoint 两种类型的资源）来维护锁的状态。这两种资源对象存在etcd里，也可以说是用etcd来实现的。 分布式锁一般实现原理就是大家先去抢锁，抢到的成为 leader ，然后 leader 会定期更新锁的状态，声明自己的活动状态，不让其他人把锁抢走。K8s 的资源锁也类似，抢到锁的节点会将自己的标记。设为锁的持有者，其他人则需要通过对比锁的更新时间和持有者来判断自己是否能成为新的 leader ，而 leader 则可以通过更新RenewTime来确保持续保有该锁。 主要调用client-go包中的： k8s.io/client-go/tools/leaderelection 总共有7个leader选举参数： lock-object-namespace和lock-object-name是锁对象的命名空间和名称。 leader-elect表示该组件运行时是否需要leader选举(如果集群中运行多副本，需要设置该选项为true，否则每个副本都将参与实际工作)。 leader-elect-lease-duration为资源锁租约观察时间，如果其它竞争者在该时间间隔过后发现leader没更新获取锁时间，则其它副本可以认为leader已经挂掉不参与工作了，将重新选举leader。 leader-elect-renew-deadline leader在该时间内没有更新则失去leader身份。 leader-elect-retry-period为其它副本获取锁的时间间隔(竞争leader)和leader更新间隔。 leader-elect-resource-lock是k8s分布式资源锁的资源对象，目前只支持endpoints和configmaps。 etcd Etcd使用的是raft一致性算法来实现的，是一款分布式的一致性KV存储，主要用于共享配置和服务发现。用于 Kubernetes 的后端存储。所有集群数据都存储在此处，ETCD在k8s技术栈的地位，就仿佛数据库（Mysql、Postgresql或oracle等）在Web应用中的地位，它存储了k8s集群中所有的元数据（以key-value的方式）。整个kubernetes系统需要用到etcd用来协同和存储配置的有： 网络插件flannel、calico等网络插件也需要用到etcd存储网络的配置信息 kubernetes本身，包括各种对象的状态和元信息配置 注意：flannel操作etcd使用的是v2的API，而kubernetes操作etcd使用的v3的API，所以在下面我们执行etcdctl的时候需要设置ETCDCTL_API环境变量，该变量默认值为2。 K8s中所有元数据的增删改查都是由kube-apiserver来执行的。ETCD中key值通过观察可以简单得出下面几个规律： k8s主要把自己的数据注册在/registry/前缀下面（在ETCD-v3版本后没有了目录的概念，只能一切皆前缀了）。通过观察k8s中deployment、namespace、pod等在ETCD中的表示，可以知道这部分资源的key的格式为/registry/{k8s对象}/{命名空间}/{具体实例名}。 kube-controller-manager kube-controller-manager运行控制器，它们是处理集群中常规任务的后台线程。逻辑上，每个控制器是一个单独的协程。用于监视 apiserver 暴露的集群状态，并且不断地尝试把当前状态向集群的目标状态迁移。为了避免频繁查询 apiserver，apiserver 提供了 watch 接口用于监视资源的增加删除和更新，client-go 对此作了抽象，封装一层 informer 来表示本地 apiserver 状态的 cache 。 参考: 看这里 这些控制器包括: 节点控制器（node-controller）: kubelet在启动时会通过API Server注册自身的节点信息，并定时向API Server汇报状态信息，API Server接收到信息后将信息更新到etcd中。Node Controller通过API Server实时获取Node的相关信息，实现管理和监控集群中的各个Node节点的相关控制功能。 副本控制器（Replication Controller）: 负责维护系统中每个副本控制器对象正确数量的 Pod。副本控制器的作用即保证集群中一个RC所关联的Pod副本数始终保持预设值。只有当Pod的重启策略是Always的时候（RestartPolicy=Always），副本控制器才会管理该Pod的操作（创建、销毁、重启等）。 服务帐户和令牌控制器（ServiceAccount Controller ）: 为新的命名空间创建默认帐户和 API 访问令牌。 资源配额管理控制器ResourceQuota Controller：资源配额管理确保指定的资源对象在任何时候都不会超量占用系统物理资源。支持三个层次的资源配置管理： 容器级别：对CPU和Memory进行限制; Pod级别：对一个Pod内所有容器的可用资源进行限制; Namespace级别：包括Pod数量、Replication Controller数量、Service数量、ResourceQuota数量、Secret数量、可持有的PV（Persistent Volume）数量 Namespace Controller：用户通过API Server可以创建新的Namespace并保存在etcd中，NamespaceController定时通过API Server读取这些Namespace信息。如果Namespace被API标记为优雅删除（即设置删除期限，DeletionTimestamp）,则将该Namespace状态设置为“Terminating”,并保存到etcd中。同时Namespace Controller删除该Namespace下的ServiceAccount、RC、Pod等资源对象。 Service Controller：属于kubernetes集群与外部的云平台之间的一个接口控制器。Service Controller监听Service变化，如果是一个LoadBalancer类型的Service，则确保外部的云平台上对该Service对应的LoadBalancer实例被相应地创建、删除及更新路由转发表。 deployment controller：用来替代以前的ReplicationController来方便的管理应用。只需要在 Deployment 中描述您想要的目标状态是什么，Deployment controller 就会帮您将 Pod 和ReplicaSet 的实际状态改变到您的目标状态。您可以定义一个全新的 Deployment 来创建 ReplicaSet 或者删除已有的 Deployment 并创建一个新的来替换。 定义Deployment来创建Pod和ReplicaSet 滚动升级和回滚应用 扩容和缩容 暂停和运行Deployment statefulset controller：StatefulSet是为了解决有状态服务的问题（对应Deployments和ReplicaSets是为无状态服务而设计），其应用场景包括： 稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC来实现; 稳定的网络标志，即Pod重新调度后其PodName和HostName不变，基于Headless Service（即没有Cluster IP的Service）来实现。StatefulSet中每个Pod的DNS格式为： statefulSetPodName-{0..N-1}.serviceName.namespace.svc.cluster.local 有序部署，有序扩展，即Pod是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依次进行（即从0到N-1，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态），基于init containers来实现； 有序收缩，有序删除（即从N-1到0） daemonset controller：DaemonSet确保全部（或者一些）Node 上运行一个 Pod 的副本。当有 Node 加入集群时，也会为他们新增一个 Pod 。当有 Node 从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。 Horizontal Pod Autoscaling：仅适用于Deployment和ReplicaSet，在V1版本中仅支持根据Pod的CPU利用率扩所容，在v1alpha版本中，支持根据内存和用户自定义的metric扩缩容。 persistentvolume-binder: 定期同步磁盘卷挂载信息，负责pv和pvc的绑定。 Endpoints controller：表示了一个Service对应的所有Pod副本的访问地址，而EndpointsController负责生成和维护所有Endpoints对象的控制器。它负责监听Service和对应的Pod副本的变化。 如果监测到Service被删除，则删除和该Service同名的Endpoints对象； 如果监测到新的Service被创建或修改，则根据该Service信息获得相关的Pod列表，然后创建或更新Service对应的Endpoints对象; 如果监测到Pod的事件，则更新它对应的Service的Endpoints对象。 kube-proxy进程获取每个Service的Endpoints，实现Service的负载均衡功能。 以上只是部分控制器，都是一个独立的协程，被controller-manager这个进程所管理。 Statefulset和Deployment的区别 Deployment用于部署无状态服务，StatefulSet用来部署有状态服务。 如果部署的应用满足以下一个或多个部署需求，则建议使用StatefulSet。 稳定的、唯一的网络标识; 稳定的、持久的存储; 有序的、优雅的部署和伸缩; 有序的、优雅的删除和停止; 有序的、自动的滚动更新; 实现固定的Pod IP方案, 可以优先考虑基于StatefulSet 稳定的：主要是针对Pod发生re-schedule后仍然要保持之前的网络标识和持久化存储。这里所说的网络标识包括hostname、集群内DNS中该Pod对应的A Record，并不能保证Pod re-schedule之后IP不变。要想保持Pod IP不变，我们可以借助稳定的Pod hostname定制IPAM获取固定的Pod IP。借助StatefulSet的稳定的唯一的网络标识特性，我们能比较轻松的实现Pod的固定IP需求，然后如果使用Deployment，那么将会复杂的多，你需要考虑滚动更新的过程中的参数控制(maxSurge、maxUnavailable)、每个应用的IP池预留造成的IP浪费等等问题。 存储：StatefulSet对应Pod的存储最好通过StorageClass来动态创建：每个Pod都会根据StatefulSet中定义的VolumeClaimTemplate来创建一个对应的PVC，然后PVS通过StorageClass自动创建对应的PV，并挂载给Pod。所以这种方式，需要事先创建好对应的StorageClass。当然，你也可以通过预先由管理员手动创建好对应的PV，只要能保证自动创建的PVC能和这些PV匹配上。 为了数据安全，当删除StatefulSet中Pods或者对StatefulSet进行缩容时，Kubernetes并不会自动删除StatefulSet对应的PV，而且这些PV默认也不能被其他PVC Bound。当你确认数据无用之后再手动去删除PV的时候，数据是否删除取决于PV的ReclaimPolicy配置。Reclaim Policy支持以下三种： Retain，意味着需要你手动清理； Recycle，等同于rm -rf /thevolume/* Delete，默认值，依赖于后端的存储系统自己实现。 部署和伸缩时与Deployment的区别 当部署有N个副本的StatefulSet应用时，严格按照index从0到N-1的递增顺序创建，下一个Pod创建必须是前一个Pod Ready为前提。 当删除有N个副本的StatefulSet应用时，严格按照index从N-1到0的递减顺序删除，下一个Pod删除必须是前一个Pod shutdown并完全删除为前提。 当扩容StatefulSet应用时，每新增一个Pod必须是前一个Pod Ready为前提。 当缩容StatefulSet应用时，没删除一个Pod必须是前一个Pod shutdown并成功删除为前提。 kube-scheduler kube-scheduler监视没有分配节点的新创建的 Pod，选择一个节点供他们运行。调度节点分配主要可以分为预选（Predicates）与优选（Priorities）两个环节： 预选 根据配置的PredicatesPolicies（默认为DefaultProvider中定义的default predicates policies集合）过滤掉那些不满足这些Policies 的 Node，预选的输出作为优选的输入； 优选 根据配置的PrioritiesPolicies（默认为DefaultProvider中定义的default priorities policies集合）给预选后的 Node 进行打分排名，得分最高的 Node 即作为最适合的 Node ，该 Pod 就绑定（Bind）到这个 Node 。 注：如果经过优选将 Node 打分排名后，有多个 Node 并列得分最高，那么kube-scheduler将随机从中选择一个 Node 作为目标 Node 。 预选阶段算法 NoDiskConflict: 评估是否存在volume冲突。如果该 volume 已经 mount 过了，k8s可能会不允许重复mount(取决于volume类型)； NoVolumeZoneConflict: 评估该节点上是否存在 Pod 请求的 volume； PodFitsResources: 检查节点剩余资源(CPU、内存)是否能满足 Pod 的需求。剩余资源=总容量-所有 Pod 请求的资源； MatchNodeSelector: 判断是否满足 Pod 设置的 NodeSelector； CheckNodeMemoryPressure: 检查 Pod 是否可以调度到存在内存压力的节点； CheckNodeDiskPressure: 检查 Pod 是否可以调度到存在硬盘压力的节点； 优选阶段算法 依次计算该 Pod 运行在每一个 Node 上的得分。主要算法有： LeastRequestedPriority：最低请求优先级，即 Node 使用率越低，得分越高； BalancedResourceAllocation：资源平衡分配，即CPU/内存配比合适的 Node 得分更高； SelectorSpreadPriority: 尽量将同一 RC/Replica 的多个 Pod 分配到不同的 Node 上； CalculateAntiAffinityPriority: 尽量将同一 Service 下的多个相同 Label 的 Pod 分配到不同的 Node； ImageLocalityPriority: Image本地优先，Node 上如果已经存在 Pod 需要的镜像，并且镜像越大，得分越高，从而减少 Pod 拉取镜像的开销(时间)； NodeAffinityPriority: 根据亲和性标签进行选择； 默认的预选、优选调度算法远不止以上这些。可以通过kube-scheduler的启动参数中加policy-config-file文件、configmaps（过时）、或者--config指定调度器用哪些预选、优选算法。 调度算法的扩展 如果kube-scheduler提供的调度算法不满足调度要求，也可以自己开发扩展调度器，在kube-scheduler启动参数的policy-config中指定扩展调度器的地址，包括（预选接口、优选接口、优先级抢占，pod和node绑定的Bind接口）。 扩展调度器示例代码： 看这里 由于默认调度器kube-scheduler需要调用扩展调度程序kube-scheduler-extender，故需要在kube-scheduler的启动参数里配置扩展调度器的地址。需要在master节点主机的/etc/kubernetes目录下的scheduler.yaml中配置如下内容：（static pod方式部署的kube-scheduler不能用configmaps的方式挂载配置文件） apiVersion: kubescheduler.config.k8s.io/v1alpha1 kind: KubeSchedulerConfiguration algorithmSource: policy: file: path: /etc/kubernetes/scheduler-policy.json clientConnection: kubeconfig: /etc/kubernetes/scheduler.conf leaderElection: leaderElect: true 主要配置是否启用选举机制，以及与API Server交互时认证用的scheduler.conf文件地址，调度策略选择用的scheduler-policy.json： { \"kind\":\"Policy\", \"apiVersion\":\"v1\", \"predicates\":[ { \"name\":\"NoVolumeZoneConflict\" }, { \"name\":\"MatchInterPodAffinity\" }, { \"name\":\"NoDiskConflict\" }, { \"name\":\"GeneralPredicates\" }, { \"name\":\"PodToleratesNodeTaints\" }, { \"name\":\"CheckVolumeBinding\" } ], \"priorities\":[ { \"name\":\"SelectorSpreadPriority\", \"weight\":1 }, { \"name\":\"InterPodAffinityPriority\", \"weight\":1 }, { \"name\":\"LeastRequestedPriority\", \"weight\":1 }, { \"name\":\"NodeAffinityPriority\", \"weight\":1 }, { \"name\":\"BalancedResourceAllocation\", \"weight\":1 }, { \"name\":\"NodePreferAvoidPodsPriority\", \"weight\":10000 }, { \"name\":\"TaintTolerationPriority\", \"weight\":1 } ], \"extenders\":[ { \"urlPrefix\":\"http://kube-scheduler-extender:80/scheduler\", \"filterVerb\":\"predicates/middleware_predicate\", \"prioritizeVerb\":\"\", \"preemptVerb\":\"\", \"bindVerb\":\"bind\", \"weight\":1, \"enableHttps\":false, \"nodeCacheCapable\":false } ], \"hardPodAffinitySymmetricWeight\":10, \"alwaysCheckAllPredicates\":false } 里面指定了默认调度器用到的预选、优选算法，以及调用扩展调度器的service地址，预选和Bind接口URI。 在/etc/kubernetes/manifests目录下的kube-scheduler.yaml中启动参数中加--config=/etc/kubernetes/scheduler.yaml，该文件通过hostPath的方式挂载到容器内。 DNS kube-dns这个插件是官方推荐安装的。通过将 Service 注册到 DNS 中，k8s 可以为我们提供一种简单的服务注册发现与负载均衡方式。 kube-dns内部通过监听services和endpoints的变更事件将域名和IP对应信息同步到本地缓存。比如服务 a 访问服务 b，dns解析依赖a容器内 /etc/resolv.conf 文件的配置 cat/etc/resolv.conf nameserver 10.233.0.3search default.svc.cluster.local svc.cluster.localcluster.local 这个文件中，配置的 DNS Server，一般就是 K8S 中，kubedns 的 Service 的 ClusterIP，这个IP是虚拟IP，无法ping。 [root@node4 user1]#kubectl get svc -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkube-dns ClusterIP 10.233.0.3 53/UDP,53/TCP 270dkubernetes-dashboard ClusterIP 10.233.22.223 443/TCP 124d 所有域名的解析，其实都要经过 kubedns 的虚拟IP 10.233.0.3 ，负载到某一个kube-dns pod上去解析。如果不能解析，则会去kube-dns pod所在的主机上的dns服务（/etc/resolv.conf）做解析。Kubernetes 启动的容器自动将 DNS 服务器包含在容器内的/etc/resolv.conf 中。 域名格式如下： statefulset一般使用Headless Service，如statefulset名为test，创建2个pod，则域名为test-0.test.kube-system.svc.cluster.local和test-1.test.kube-system.svc.cluster.local 节点组件 节点组件在每个节点上运行，维护运行的 Pod 并提供Kubernetes 运行时环境。kubelet一般作为二进制运行到每个k8s节点；kube-proxy作为daemonset pod运行到每个k8s节点。 kubelet 在kubernetes集群中，每个Node节点都会启动kubelet进程，用来处理Master节点下发到本节点的任务，管理Pod和其中的容器。kubelet会在API Server上注册节点信息，定期向Master汇报节点资源使用情况，并通过cAdvisor监控容器和节点资源。 节点管理 节点通过设置kubelet的启动参数--register-node来决定是否向API Server注册自己，默认为true，可以通过kubelet --help或者查看k8s源码(直接放弃) kubelet的配置文件 默认配置文件在/etc/kubernetes/kubelet中，其中 --api-servers: 用来配置master节点的IP和端口 --kukeconfig:用来配置kubeconfig的路径，kubeconfig文件常用来指定证书 --hostname-override：用来配置该节点在集群中显示的主机名 --node-status-update-frequency：配置kubeler向master心跳上报的频率，默认为10s pod管理 kubelet有几种方式获取自身node上所需运行的pod清单，但本文只讨论通过API Server监听etcd目录，同步pod列表的方式 kubelet通过API Server Client使用WatchAndList方式监听etcd中/registry/nodes/$(当前节点名称)和/registry/pods的目录，将获取的信息同步到本地缓存中 kubelet监听etcd，执行对pod的操作，对容器的操作则是通过docker client执行，例如自动删除容器等 kubelet创建和修改pod流程 1.为该pod创建一个数据目录 2.从API Server读取该pod清单 3.为该pod挂载外部卷 4.下载pod到用到的secret 5.检查运行的pod，执行pod中未完成的任务 6.先创建一个pause容器，该容器接管pod的网络，再创建其他容器 7.pod中容器的处理流程 ​ 7.1比较容器hash值并做相应处理 ​ 7.2如果容器被终止了且没有指定重启策略，则不做任何处理 ​ 7.3调用docker client下载容器镜像，调用docker clienr运行容器 pod被调度到kubelet所在节点时，调用CNI（Docker 运行或通过 rkt)运行 Pod 的容器; 周期性的对容器生命周期进行探测。（健康检查readness-隔离、liveness-重启）; 检查节点状态，将节点的状态报告给kube-apiserver; 容器监控所在节点的资源使用情况，并定时向 kube-apiserver报告。知道整个集群所有节点的资源情况，对于 pod 的调度和正常运行至关重要。kubelet 使用cAdvisor进行资源使用率的监控。 kube-proxy 看这里 service是一组pod的服务抽象，相当于一组pod的负载均衡器，负责将请求分发给对应的pod。service会提供一个clusterIP。kube-proxy的作用主要是负责service的实现，具体来说，就是实现了内部请求到service和外部的从node port向service的访问，转发到后端某个pod。 举个例子，现在有podA，podB，podC和serviceAB。serviceAB是podA，podB的服务抽象(service)。那么kube-proxy的作用就是可以将某一个发往（如podC发起的请求）向serviceAB的请求，进行转发到service所代表的一个具体pod(podA或者podB)上。请求的分配方法一般分配是采用轮询方法进行分配。 kube-proxy提供了三种负载均衡器（LB）模式: 一种是基于用户态的模式userspace, 一种是iptables模式，一种是ipvs模式。 userspace：是以socket的方式实现代理的，userspace这种模式最大的问题是，service的请求会先从用户空间进入内核iptables，然后再回到用户空间，由kube-proxy完成后端Endpoints的选择和代理工作，这样流量从用户空间进出内核带来的性能损耗是不可接受的； iptables mode：因为使用iptable NAT来完成转发，也存在不可忽视的性能损耗。另外，如果集群中存在上万的Service/Endpoint，那么Node上的iptables rules将会非常庞大，性能还会再打折扣； IPVS 模式：工作原理其实跟 iptables 模式类似，当我们创建了前面的Service 之后，kube-proxy首先会在宿主机上创建一个虚拟网卡（kube-ipvs0）并为他分配service VIP作为IP地址，kube-proxy会通过linux的IPVS模块为这个IP设置三个虚拟主机（后端的三个POD IP），使用轮询作为LB策略（ipvsadm命令查看），IPVS模块会负责请求的转发。 以下截图来自于极客时间张磊的课程描述： iptables模式和ipvs模式的对比 服务暴露方式 看这里 NodePort NodePort服务是引导外部流量到你的服务的最原始方式。可以通过访问集群内的每个NodeIP:NodePort的方式，访问到对应Service后端的Endpoint。在所有节点（虚拟机）上开放一个特定端口，任何发送到该端口的流量都被转发到对应服务。 NodePort 服务的 YAML 文件类似如下： apiVersion: v1 kind: Service metadata: name: my-nodeport-service selector: app: my-app spec: type: NodePort ports: - name: http port: 80 targetPort: 80 nodePort: 30036 protocol: TCP NodePort 服务主要有两点区别于普通的“ClusterIP”服务。第一，它的类型是“NodePort”。有一个额外的端口，称为 nodePort，它指定节点上开放的端口值。如果你不指定这个端口，系统将选择一个随机端口。 何时使用这种方式？ 这种方法有许多缺点： 每个端口只能是一种服务 端口范围只能是 30000-32767 如果节点/VM 的 IP 地址发生变化，你需要能处理这种情况。 基于以上原因，我不建议在生产环境上用这种方式暴露服务。如果你运行的服务不要求一直可用，或者对成本比较敏感，你可以使用这种方法。这样的应用的最佳例子是 demo 应用，或者某些临时应用。 hostNetwork 这种方式在创建pod时的yaml中spec.hostNetwork: true指定走主机网络，这种方式pod使用的端口必须是宿主机上没有被占用的端口。外部可以直接通过pod所在宿主机IP:Pod端口访问。 LoadBalancer 这也是用来对集群外暴露服务的，不同的是这需要云服务商的支持，比如亚马逊等。这个方式的最大缺点是每一个用 LoadBalancer 暴露的服务都会有它自己的 IP 地址，每个用到的 LoadBalancer 都需要付费，这是非常昂贵的。 Ingress ingress配置一种路由转发规则，ingress controller会根据ingress中的规则，生成路由转发配置。如nginx-ingress-controller，控制循环会检测ingress对象的添加，通过其规则和service、pod信息生成nginx的配置，通过nginx实现对外服务和负载均衡。 pod创建流程 1、客户端提交创建请求，通过API Server的Restful API，或者用kubectl命令行工具。支持的数据类型包括JSON和YAML。 2、API Server处理用户请求，存储Pod数据到etcd。 3、kube-scheduler通过API Server查看未绑定的Pod。尝试为Pod分配主机。 4、kube-scheduler通过预选算法过滤掉不符合要求的主机。比如Pod指定了所需要的资源量，那么可用资源比Pod需要的资源量少的主机会被过滤掉，端口被占用的也被过滤掉； 5、kube-scheduler通过优选算法给主机打分，对预选筛选出的符合要求的主机进行打分，在主机打分阶段，调度器会考虑一些整体优化策略，比如把一个deployment类型的pod分布到不同的主机上，使得资源均衡；或者将两个亲和的服务分配到同一个主机上。 6、选择主机：选择打分最高的主机，进行binding（调用apiserver将pod和node绑定）操作，结果存储到etcd中。 7、kubelet监听Api Server，根据调度结果执行Pod创建操作：绑定成功后，scheduler会调用API Server的API在etcd中创建一个bound pod对象，描述在一个工作节点上绑定运行的所有pod信息。运行在每个工作节点上的kubelet也会定期与etcd同步bound pod信息，一旦发现应该在该工作节点上运行的bound pod对象没有更新，则调用Docker API创建并启动pod内的容器。 8、kubelet调用CNI（Docker 运行或通过 rkt)运行 Pod 的容器。并周期性的对容器生命周期进行探测。（健康检查readness-隔离、liveness-重启） 各组件基本都是通过API Server提供的list-watch API进行监听资源对象变化，进行自己的控制循环，这些核心功能都被封装到了client-go包中。我们可以根据自己的需求，通过CRD编写controller、operator进行自己的控制循环逻辑、运维自动化部署，很轻松的扩展k8s能力。 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s知识点/k8s知识点.html":{"url":"linux/k8s/k8s知识点/k8s知识点.html","title":"k8s知识点","keywords":"","body":"[toc] k8s知识点 k8s命令自动补全 yum install -y bash-completion source /usr/share/bash-completion/bash_completion source > ~/.bashrc 一、k8s特性 服务发现和负载均衡 Kubernetes 可以使用 DNS 名称或自己的 IP 地址公开容器，如果到容器的流量很大，Kubernetes 可以负载均衡并分配网络流量，从而使部署稳定。 存储编排 Kubernetes 允许您自动挂载您选择的存储系统，例如本地存储、公共云提供商等。 自动部署和回滚 您可以使用 Kubernetes 描述已部署容器的所需状态，它可以以受控的速率将实际状态更改为所需状态。例如，您可以自动化 Kubernetes 来为您的部署创建新容器，删除现有容器并将它们的所有资源用于新容器。 自动二进制打包 Kubernetes 允许您指定每个容器所需 CPU 和内存（RAM）。当容器指定了资源请求时，Kubernetes 可以做出更好的决策来管理容器的资源。 自我修复 Kubernetes 重新启动失败的容器、替换容器、杀死不响应用户定义的运行状况检查的容器，并且在准备好服务之前不将其通告给客户端。 密钥与配置管理 Kubernetes 允许您存储和管理敏感信息，例如密码、OAuth 令牌和 ssh 密钥。您可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥。 二、k8s概念和术语 2.1 k8s角色 2.1.1 master k8s使用共享网路将多个物理机或者虚拟机汇集到一个集群中，在各服务器之间进行通信，该集群是是配置k8s的所有组件、功能和工作负载的物理平台 集群中的一台机器(或者高可用部署中的一组服务器)用作master，负责管理整个集群 2.1.2 node 集群中除master节点外的其余机器用作node，他们是使用本地和外部资源接收和运行工作负载的服务器 集群中的这些机器可以是物理服务器，也可以是虚拟机 2.2 k8s资源 2.2.1 pod k8s并不直接运行容器，而是使用一个抽象的资源对象来封装一个或者多个容器，这个抽象即为pod，pod是k8s的最小调度单元 同一pod中的容器共享网络名称空间和存储资源，这些容器可经由本地回环接口lo直接通信，但彼此之间又在mount、user及PID等名称空间上保持隔离 pod示意图 2.2.2 label 资源标签 标签是将资源进行分类的标识符，资源标签其实就是一个健值型数据 标签是用于指定对象(如pod)辨识性的属性，这些属性仅对用户存在特定的意义，对k8s集群来说并不直接表达核心系统语义 标签可以在对象创建时附加其上，并能够在创建后的任意时间进行添加和修改 一个对象可以拥有多个标签，一个标签也可以附加于多个对象之上 2.2.3 selector 标签选择器 标签选择器全称为label selector，是一种根据lable来过滤符合条件的资源对象的机制，例如将附有标签role:backend的所有pod对象挑选出来归为一组就是标签选择器的一种应用 2.2.4 controller pod控制器 尽管pod是k8s中的最小调度单元，但用户通常并不会直接部署及管理pod对象，而是要借助于控制器对其进行管理 用于工作负载的控制器是一种管理pod生命周期的资源抽象，是k8s上的一类对象而非单个资源对象 使用控制器之后就不再需要手动管理pod对象了，用户只需要声明应用的期望状态，控制器就会自动对其进行进程管理 pod控制器包括ReplicationController、ReplicaSet、Deployment、Statefulset、Job等 2.2.5 service 服务资源 service是建立在一组pod对象之上的资源抽象，它通过标签选择器选择一组pod对象，并为这组pod对象定义一个统一的固定访问入口(通常是一个IP地址)，若k8s集群存在DNS附件，它就会在service创建时为其自动配置一个DNS名称以便客户端进行服务发现 到达service IP的请求将负载均衡至其后的端点-->各个pod对象之上，因此service从本质上来讲是一个四层代理服务 service还可以将集群外部流量引入到集群中来 2.2.6 volume 存储卷 存储卷是独立于容器文件系统之外的存储空间，常用于扩展容器的存储空间并为它提供持久存储能力 k8s集群上的存储卷大体可分为临时卷、本地卷和网路卷 临时卷和本地卷都位于node本地，一旦pod被调度至其他node，此种类型的存储卷将无法访问，因此临时卷和本地卷通常用于数据缓存，持久化的数据则需要放置于持久卷(persistent volume)之上 2.2.7 name(名称)和namespace(名称空间) 名称是k8s集群中资源对象的标识符，它们的作用域通常是名称空间(namespace)，因此名称空间是名称的额外限定机制 在同一个名称空间中，同一类型资源对象的名称必须具有唯一性，名称空间通常用于实现项目的资源隔离，从而形成逻辑分组 创建的pod和service等资源对象都属于名称空间级别，未指定时，它们都属于默认的名称空间default 2.2.8 annotaion 注解 注解时另一种附加在对象之上的键值型的数据，但它拥有更大的数据容量 注解常用于将各种非标识型元数据(metadata)附加到对象上，但它不能用于标识和选择对象，通常也不会被k8s直接使用，其主要目的是方便工具或用户的阅读及查找 2.2.9 ingress k8s将pod对象和外部网路环境进行了隔离，pod和service等对象间的通信都使用其内部专用地址进行，如若需要开放某些pod对象提供给外部用户访问，则需要为其请求流量打开一个通往k8s集群内部的通道，除了service之外，ingress也是这类通道的实现方式之一 三、k8s架构 3.1k8s架构图 3.2 master、node组件 master节点 负责集群管理，接受用户请求，将请求分散到node节点 scheduler 调度器，负责资源的调度，按照预定的调度策略将pod调度到相应的机器上 apiserver 整个架构的核心，协调者的角色，所有组件都要与apiserver通信，提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制 controller manager 控制器，负责维护集群的状态，比如故障检测，自动扩展，滚动更新等 etcd(独立组件，不属于k8s集群) key value分布式存储，存储k8s的数据，主要是master节点的apiserver连接etcd node节点 计算节点，接受master指令，运行任务 kubelet master的agent，负责接收master的任务并完成任务，负责维护容器等生命周期，同时也负责volume和网络的管理 kube-proxy 负责为service提供集群内部的服务发现和负载均衡 容器运行时环境 k8s不具备容器引擎，不能直接运行容器，需要借助第三方容器引擎来运行和管理容器，例如docker 3.3 核心附件 3.3.1 coreDNS 在k8s集群中调度运行提供DNS服务的pod，同一集群中的其他pod可使用此DNS服务解决主机名，k8s1.11版本开始默认使用coreDNS项目为集群提供服务注册和服务发现的动态名称解析服务，之前的版本中用到的是kubeDNS，而skyDNS则是更早版本使用的 3.3.2 dashboard k8s集群的全部功能都要基于web的UI，来管理集群中的应用和集群自身 3.3.3 heapster 容器和节点的性能监控与分析系统，它收集并解析多种指标数据，如资源利用率、生命周期等 由于监控项比较少，已由prometheus代替 3.3.4 ingress controller 为服务提供外网入口，7层负载均衡，默认kube-process只能提供4层负载均衡 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s知识点/kubeadm 专题 一 init 究竟干了些什么.html":{"url":"linux/k8s/k8s知识点/kubeadm 专题 一 init 究竟干了些什么.html","title":"kubeadm init流程","keywords":"","body":"[toc] kubeadm 专题 一 init 究竟干了些什么 本文严重抄袭至互联网 kubeadm 所在层次 kubeadm 属于第二层，用于管理集群。 kubeadm init 的流程（phase）介绍 phase 阶段 preflight 预置检查 kubelet-start 生成 kubelet 配置，并重启kubelet certs 生成认证 /etcd-ca 生成自签名CA以为etcd配置标识 /apiserver-etcd-client 生成apiserver用于访问etcd的证书 /etcd-healthcheck-client 生成liveness探针使用的证书，用于检查etcd 的 healtcheck 状态 /etcd-server 生成 etcd 服务使用的的证书 /etcd-peer 为etcd节点生成证书以相互通信 /ca 生成自签名的 Kubernetes CA，为其他 Kubernetes 组件预配标识 /apiserver 生成用于提供 Kubernetes API 的证书 api server端证书 /apiserver-kubelet-client 为 API 服务器生成证书以连接到 kubelet /front-proxy-ca 生成自签名 CA 以预配front proxy 标识 /front-proxy-client 为前端代理客户端生成证书 /sa 生成用于对服务帐户令牌及其公钥进行签名的私钥 kubeconfig 生成 control plane 和 admin 管理员相关的kubeconfig 文件 /admin 生成admin 管理员和kubeadm 自身使用的kubeconfig文件 /kubelet 生成kebelet使用的，仅用于引导集群（bootstrap）的kubeconfig 文件 /controller-manager 生成 controller manager 使用的kubeconfig文件 /scheduler 生成 scheduler 使用的kubeconfig文件 kubelet-start 生成kubelet的环境变量文件/var/lib/kubelet/kubeadm-flags.env 和 配置信息文件 /var/lib/kubelet/config.yaml，然后 启动/重启 kubelet（systemd 模式） control-plane 生成拉起 control plane（master）static Pod 的 manifest 文件 /apiserver 生成拉起 kube-apiserver 的 static Pod manifest /controller-manager 生成拉起 kube-controller-manager 的static Pod manifest /scheduler 生成拉起 kube-scheduler 的 static Pod manifest etcd 生成本地 ETCD的 static Pod manifest 文件 /local 生成单节点本地 ETCD static Pod manifest 文件 upload-config 上传kubeadm和kubelet配置为 ConfigMap /kubeadm 上传 kubeadm ClusterConfiguration 为 ConfigMap /kubelet 上传 kubelet component config 为 ConfigMap upload-certs 上传证书到 kubeadm-certs mark-control-plane 标识节点为 control-plane bootstrap-token 生成 bootstrap tokens 用于其他节点加入集群 addon 安装所需的插件以通过一致性测试 /coredns 安装 CoreDNS 插件 /kube-proxy 安装 kube-proxy 插件 kubeadm 命令行参数 命令用法 kubeadm init [flags] 参数说明 --apiserver-advertise-address string 设置 apiserver 绑定的 IP. --apiserver-bind-port int32 设置apiserver 监听的端口. (默认 6443) --apiserver-cert-extra-sans strings api证书中指定额外的Subject Alternative Names (SANs) 可以是IP 也可以是DNS名称。 证书是和SAN绑定的。 --cert-dir string 证书存放的目录 (默认 \"/etc/kubernetes/pki\") --certificate-key string kubeadm-cert secret 中 用于加密 control-plane 证书的key --config string kubeadm 配置文件的路径. --cri-socket string CRI socket 文件路径，如果为空 kubeadm 将自动发现相关的socket文件; 只有当机器中存在多个 CRI socket 或者 存在非标准 CRI socket 时才指定. --dry-run 测试，并不真正执行;输出运行后的结果. --feature-gates string 指定启用哪些额外的feature 使用 key=value 对的形式。 --help 帮助文档 --ignore-preflight-errors strings 忽略前置检查错误，被忽略的错误将被显示为警告. 例子: 'IsPrivilegedUser,Swap'. Value 'all' ignores errors from all checks. --image-repository string 选择拉取 control plane images 的镜像repo (default \"k8s.gcr.io\") --kubernetes-version string 选择K8S版本. (default \"stable-1\") --node-name string 指定node的名称，默认使用 node 的 hostname. --pod-network-cidr string 指定 pod 的网络， control plane 会自动将 网络发布到其他节点的node，让其上启动的容器使用此网络 --service-cidr string 指定service 的IP 范围. (default \"10.96.0.0/12\") --service-dns-domain string 指定 service 的 dns 后缀, e.g. \"myorg.internal\". (default \"cluster.local\") --skip-certificate-key-print 不打印 control-plane 用于加密证书的key. --skip-phases strings 跳过指定的阶段（phase） --skip-token-print 不打印 kubeadm init 生成的 default bootstrap token --token string 指定 node 和control plane 之间，简历双向认证的token ，格式为 [a-z0-9]{6}\\.[a-z0-9]{16} - e.g. abcdef.0123456789abcdef --token-ttl duration token 自动删除的时间间隔。 (e.g. 1s, 2m, 3h). 如果设置为 '0', token 永不过期 (default 24h0m0s) --upload-certs 上传 control-plane 证书到 kubeadm-certs Secret. init 工作流 kubeadm init通过执行以下步骤来引导Kubernetes控制平面节点： 1.在进行更改之前运行一系列飞行前检查以验证系统状态。某些检查仅触发警告，其他检查被视为错误，并将退出kubeadm，直到问题得到纠正或用户指定--ignore-preflight-errors = 。 来忽略错误。 2.生成自签名 CA（或使用现有CA），以便为群集中的每个组件设置标识。如果用户通过将其放在通过--cert-dir配置的cert目录（默认情况下为/etc/kubernetes/pki）中提供了自己的CA证书和/或密钥，则会跳过此步骤，如使用自定义证书文档中所述。 APIServer证书将--apiserver-cert-extra-sans参数提供的额外SAN条目添加到证书信息中，如果需要，可以小写。 3.在/etc/kubernetes/中为 kubelet， controller-manager和scheduler 写入kubeconfig文件，用于连接到API服务器，每个都有自己的标识，以及另一个名为admin.conf的管理员kubeconfig文件。 4.生成启动 kubelet 服务所需的配置文件和环境变量，并启动kubelet （systemd 方式）生成文件如下 /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf 环境变量 /etc/kubernetes/bootstrap-kubelet.conf/ /etc/kubernetes/kubelet.conf /var/lib/kubelet/config.yaml /var/lib/kubelet/kubeadm-flags.env 环境变量 kubelet 使用4个文件的方式如下 [root@rancher ~]# systemctl status kubelet ● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/usr/lib/systemd/system/kubelet.service; disabled; vendor preset: disabled) Drop-In: /usr/lib/systemd/system/kubelet.service.d └─10-kubeadm.conf Active: active (running) since Sun 2019-08-18 14:16:37 CST; 13min ago Docs: https://kubernetes.io/docs/ Main PID: 14980 (kubelet) CGroup: /system.slice/kubelet.service └─14980 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml 1.为apiserver，control-manager 和scheduler 生成静态Pod清单。 如果未提供外部etcd，则会为etcd生成其他静态Pod清单。 Static Pod清单写入/etc/kubernetes/manifests; kubelet监视此目录以便Pods在启动时创建。 control plane 的pod 启动后，init 开始继续执行后面的流程。 2.将标签和污点应用于控制平面节点，以便不会在那里运行其他工作负载。 3.生成其他节点将来可用于向控件平面注册自己的令牌。或者，用户可以通过--token 参数来指定一个令牌。,具体在 kubeadm token 文档中. 4.进行所有必要的配置，以允许其他节点通过引导令牌 bootstrap token 和 TLS boostrap 机制进行加入 写入加入集群的所需的信息到configmap， 设置相关的RBAC 规则。 让bootstrap token 访问 CSR（证书签名请求） 签名 API Configure auto-approval for new CSR requests. 对CSR 请求（证书签名请求）配置自动同意。 通过 API server 安装 DNS 服务器 （CoreDNS） 和 kube-proxy 组件。在 Kubernetes 版本 1.11 和更高版本中，CoreDNS 是默认 DNS 服务器。要安装 kube-dns 而不是 CoreDNS，必须在 kubeadm 配置文件的ClusterConfiguration 字段中配置 DNS 附加组件(通过 kubeadm config 文件）。请注意，虽然已部署 DNS 服务器，但安装 CNI 前该POD 不会被调度到节点（可以理解为不回被实际部署，或不会生效）。 kubeadm init phase 的用法 查看 kubeadm init phase 列表 [root@rancher ~]# kubeadm init phase Use this command to invoke single phase of the init workflow Usage: kubeadm init phase [command] Available Commands: addon Install required addons for passing Conformance tests bootstrap-token Generates bootstrap tokens used to join a node to a cluster certs Certificate generation control-plane Generate all static Pod manifest files necessary to establish the control plane etcd Generate static Pod manifest file for local etcd kubeconfig Generate all kubeconfig files necessary to establish the control plane and the admin kubeconfig file kubelet-start Write kubelet settings and (re)start the kubelet mark-control-plane Mark a node as a control-plane preflight Run pre-flight checks upload-certs Upload certificates to kubeadm-certs upload-config Upload the kubeadm and kubelet configuration to a ConfigMap 可以查看某个具体的phase下的子phase 列表 [root@rancher ~]# kubeadm init phase control-plane --help This command is not meant to be run on its own. See list of available subcommands. Usage: kubeadm init phase control-plane [flags] kubeadm init phase control-plane [command] Available Commands: #下面的就是子phase all Generate all static Pod manifest files apiserver Generates the kube-apiserver static Pod manifest controller-manager Generates the kube-controller-manager static Pod manifest scheduler Generates the kube-scheduler static Pod manifest 查看 control-plane phase 下 controller-manager 子 phase 的用法详情 [root@rancher ~]# kubeadm init phase control-plane controller-manager --help Generates the kube-controller-manager static Pod manifest Usage: kubeadm init phase control-plane controller-manager [flags] Flags: --cert-dir string The path where to save and store the certificates. (default \"/etc/kubernetes/pki\") --config string Path to a kubeadm configuration file. --controller-manager-extra-args mapStringString A set of extra flags to pass to the Controller Manager or override default ones in form of = -h, --help help for controller-manager --image-repository string Choose a container registry to pull control plane images from (default \"k8s.gcr.io\") --kubernetes-version string Choose a specific Kubernetes version for the control plane. (default \"stable-1\") --pod-network-cidr string Specify range of IP addresses for the pod network. If set, the control plane will automatically allocate CIDRs for every node. 执行某个 phase 或者跳过某个 phase sudo kubeadm init phase control-plane all --config=configfile.yaml sudo kubeadm init phase etcd local --config=configfile.yaml # you can now modify the control plane and etcd manifest files sudo kubeadm init --skip-phases=control-plane,etcd --config=configfile.yaml kubeadm config 文件 注意，这个 config 文件特性在1.15 中依然是 beta，在将来可能改变 查看 kubeadm config print的帮助 [root@rancher ~]# kubeadm config print -h This command prints configurations for subcommands provided. Usage: kubeadm config print [flags] kubeadm config print [command] Available Commands: init-defaults Print default init configuration, that can be used for 'kubeadm init' join-defaults Print default join configuration, that can be used for 'kubeadm join' 打印默认的init 配置文件 [root@rancher ~]# kubeadm config print init-defaults > initconfig.yaml 打开 initconfig， 内容如下 apiVersion: kubeadm.k8s.io/v1beta2 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 1.2.3.4 bindPort: 6443 nodeRegistration: criSocket: /var/run/dockershim.sock name: rancher.local taints: - effect: NoSchedule key: node-role.kubernetes.io/master --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta2 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns: type: CoreDNS etcd: local: dataDir: /var/lib/etcd imageRepository: k8s.gcr.io kind: ClusterConfiguration kubernetesVersion: v1.14.0 networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 scheduler: {} 上面的内容只包含额了最简话的InitConfiguration type 的内容，kubeadm 完整的内容包含5大部分，如下，每个type 之间，需要用yaml的 --- 文档隔离进行分离。 init-full-config.yaml 文件结构 --- apiVersion: kubeadm.k8s.io/v1beta2 kind: InitConfiguration --- apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration --- apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration --- apiVersion: kubeadm.k8s.io/v1beta2 kind: JoinConfiguration 想细的内容可以参阅kubeadm api，kube-proxy配置部分的内容细节在这里KubeProxyConfiguration 比如我要修改kube-proxy的模式为IPVS 那么修改后的init-full-config.yaml 内容为如下 --- apiVersion: kubeadm.k8s.io/v1beta2 kind: InitConfiguration --- apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration --- apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: \"ipvs\" 修改在这里 --- apiVersion: kubeadm.k8s.io/v1beta2 kind: JoinConfiguration 关于如何通过kubeadm 配置启用 IPVS 请参阅 关于如何定制化 control plane 请参阅 使用自定义镜像仓库 对于google 提供的镜像，在众所周知的原因下，无法访问。所以需要使用国内镜像或者自建的镜像仓库。 kubeadm 提供了参数，同事也支持修改 kubeadm config 文件来指定定制化的仓库 # imageRepository: k8s.gcr.io imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers 通过如下命令，可以查看和拉取 init 所需的镜像。 kubeadm config images list kubeadm config images pull --config init-full-config.yaml kubeadm 配置 cri runtime kubelet 默认使用 docker 作为runtime 并使用内建的 dockershim 进行交互。 其他的runtime包括: cri-containerd cri-o frakti rkt 安装文档 CRI installation instructions 根据安装文档安装好runtime后，需要对kubeadm 和kubelet 做如下配置 1.在每个节点安装runtime 对应的 shim，如何安装在runtime 说明文档中有介绍 2.配置 kubelet 使用远程 CRI runtime （实际是使用linux sockets），记得修改 RUNTIME_ENDPOINT 为你自己对应的值，比如 /var/run/{your_runtime}.sock: 比如，如下是cri的配置文件。 cat > /etc/systemd/system/kubelet.service.d/20-cri.conf 你也可以通过kubeadm init/reset 的 --cri-socket 参数来是先同样的事情。 kubeadm 自动化 与其像kubeadm 基础教程中那样，将从 kubeadm init 获得的令牌复制到每个节点，不如并行化令牌分发，以便更轻松地实现自动化。要实现此自动化，您必须知道控制平面节点在启动后将具有的 IP 地址。 步骤 1.使用 kubeadm 生成令牌（token） kubeadm token generate 2.用此令牌(token)同时启动控制平面节点和工作节点。当它们启动时，它们会找到彼此并形成集群。相同的 --token 参数可以在kubeadm init和kubeadm join上使用 3.可以用同样的方法来添加master节点，通过设置 --certificate-key 参数来达到加入的目的。可以通过如下命令来生成key，给每个master 节点使用 kubeadm alpha certs certificate-key 集群启动之后，可以通过/etc/kubernetes/admin.conf中的凭证来和集群信。 这种方式不允许root ca 通过--discovery-token-ca-cert-hash 来验证证书hash 所以有一定的安全隐患。 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s下载/下载k8s二进制包.html":{"url":"linux/k8s/k8s下载/下载k8s二进制包.html","title":"k8s二进制包下载","keywords":"","body":"[toc] 下载k8s二进制包 登陆github，搜索kubernetes，然后点击项目进入 点击releases 选择要下载的版本，然后点击CHANGELOG/CHANGELOG-1.18.md 注意看准版本，在Server Binaries下下载二进制包 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s知识点/k8s设置通过kubeconfig登陆dashboard.html":{"url":"linux/k8s/k8s知识点/k8s设置通过kubeconfig登陆dashboard.html","title":"k8s设置通过kubeconfig登陆dashboard","keywords":"","body":"[toc] k8s设置通过kubeconfig登陆dashboard k8s 官方的dashboard每次登陆都需要输入token，而这个token一会特么就过期了，我就是本机实验，每次都得手动粘贴一大串命令获取token然后再登陆，非常麻烦 使用如下命令获取登陆dashboard的token kubectl get secret `kubectl get secret -n kube-system|grep admin-token|awk '{print $1}'` -o jsonpath={.data.token} -n kube-system |base64 -d && echo 查看secrets $ kubectl get secrets NAME TYPE DATA AGE default-token-jlz9f kubernetes.io/service-account-token 3 45h 1.创建cluster kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.crt --server=10.0.0.130:6443 --kubeconfig=/root/dashbord-admin.conf 2.获取token DASH_TOCKEN=$(kubectl get secret -n kube-system `kubectl get secret -n kube-system |grep dashboard |awk '{print $1}'` -o jsonpath={.data.token}|base64 -d) 3.创建credentials kubectl config set-credentials dashboard-admin --token=$DASH_TOCKEN --kubeconfig=/root/dashbord-admin.conf 4.创建context kubectl config set-context dashboard-admin@kubernetes --cluster=kubernetes --user=dashboard-admin --kubeconfig=/root/dashbord-admin.conf 5.切换context的current-context是dashboard-admin@kubernetes kubectl config use-context dashboard-admin@kubernetes --kubeconfig=/root/dashbord-admin.conf 下载dashboard-admin.conf然后登陆的时候选择这个文件即可 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s知识点/k8s给node节点添加角色标签.html":{"url":"linux/k8s/k8s知识点/k8s给node节点添加角色标签.html","title":"k8s给node节点添加角色标签","keywords":"","body":"[toc] k8s给node节点添加角色标签 使用命令 kubectl get nodes 查看k8s集群中的node节点时， ROLES 处中的 node 节点显示为 $ kubectl get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 133m v1.19.3 k8s-node1 Ready 128m v1.19.3 k8s-node2 Ready 128m v1.19.3 语法 =代表增加标签 -代表删除标签 增加标签 kubectl label nodes node主机名 node-role.kubernetes.io/标签名= 删除标签 kubectl label nodes node主机名 node-role.kubernetes.io/标签名- 现在添加一个标签 kubectl label nodes k8s-node1 node-role.kubernetes.io/node1= kubectl label nodes k8s-node2 node-role.kubernetes.io/node2= 查看 $ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-node1 Ready node1 133m v1.19.3 k8s-node2 Ready node2 133m v1.19.3 master1 Ready master 138m v1.19.3 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s知识点/本机通过config文件连接多个k8s集群.html":{"url":"linux/k8s/k8s知识点/本机通过config文件连接多个k8s集群.html","title":"本机通过config文件连接多个k8s集群","keywords":"","body":"本机通过config文件连接多个k8s集群 背景说明 我们有多个k8s集群，并且想要通过config文件来连接多个k8s集群，方法就是将2个集群的 config 文件合并到一个 config 文件中，通过使用 kubectl config use-context context_name 来访问集群。简而言之就是通过设置 context 来让 kubectl 访问不同的k8s集群。 config文件信息 现在有2个 config 文件，一个是mac本机k8s集群文件 config-mac，另外一个是公司内网测试集群文件 config-company， 通过 config 信息，可以看到两个集群的 cluster name 、context name 以及用户信息。 config文件1 config-mac 内容 apiVersion: v1 clusters: - cluster: certificate-authority-data: xxx server: https://kubernetes.docker.internal:6443 name: docker-desktop contexts: - context: cluster: docker-desktop user: docker-desktop name: docker-desktop current-context: docker-desktop kind: Config preferences: {} users: - name: docker-desktop user: client-certificate-data: xxx client-key-data: xxx config文件2 config-company 内容 apiVersion: v1 clusters: - cluster: certificate-authority-data: xxx server: https://10.0.19.31:6443 name: kubernetes contexts: - context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetes current-context: kubernetes-admin@kubernetes kind: Config preferences: {} users: - name: kubernetes-admin user: client-certificate-data: xxx client-key-data: xxx 合并config文件 cd $HOME/.kube/config KUBECONFIG=config-mac:config-company kubectl config view --flatten > $HOME/.kube/config 使用说明 查看 cluster name 以及 context name $ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://kubernetes.docker.internal:6443 name: docker-desktop - cluster: certificate-authority-data: DATA+OMITTED server: https://10.0.19.31:6443 name: kubernetes contexts: - context: cluster: docker-desktop user: docker-desktop name: docker-desktop - context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetes current-context: kubernetes-admin@kubernetes kind: Config preferences: {} users: - name: docker-desktop user: client-certificate-data: REDACTED client-key-data: REDACTED - name: kubernetes-admin user: client-certificate-data: REDACTED client-key-data: REDACTED 查看当前使用的集群 $ kubectl config current-context docker-desktop 修改当前使用的集群 kubernetes-admin@kubernetes 是通过命令 kubectl config view 查询结果中与 context 同级的 name 中的集群名字 $ kubectl config use-context kubernetes-admin@kubernetes Switched to context \"kubernetes-admin@kubernetes\". 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s知识点/k8s强制删除namespace.html":{"url":"linux/k8s/k8s知识点/k8s强制删除namespace.html","title":"k8s强制删除命名空间","keywords":"","body":"k8s强制删除namespace 背景说明 k8s1.22.2，安装 kubespheres 后，通过官方提供的yaml文件删除，结果发现有一部分的命名空间无法删除，一直处于 Terminating 状态，无法通过 1.启动proxy 使用 --port 参数指定端口，默认8001 kubectl proxy 2.导出json格式到文件 export MYNS=xxx kubectl get namespace ${MYNS} -o json >tmp.json 3.编辑 tmp.josn，删除 finalizers 字段的值 删除以下内容 \"finalizers\": [ \"finalizers.kubesphere.io/namespaces\" ], 4.删除命名空间 curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @tmp.json http://127.0.0.1:8001/api/v1/namespaces/${MYNS}/finalize 5.验证 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s安装/1.使用 kubeadm 搭建 v1.15.3 版本 Kubernetes 集群.html":{"url":"linux/k8s/k8s安装/1.使用 kubeadm 搭建 v1.15.3 版本 Kubernetes 集群.html","title":"1.15","keywords":"","body":"[toc] 使用 kubeadm 搭建 v1.15.3 版本 Kubernetes 集群 一、环境准备 1.1实验环境 角色 IP地址 主机名 docker版本 硬件配置 系统 内核 master 10.0.0.130 k8s-master 18.09.9 2c2g CentOS7.6 3.10.0-957.el7.x86_64 node1 10.0.0.131 k8s-node1 18.09.9 2c1g CentOS7.6 3.10.0-957.el7.x86_64 node2 10.0.0.132 k8s-node2 18.09.9 2c1g CentOS7.6 3.10.0-957.el7.x86_64 1.2每个节点配置host信息 cat >> /etc/hosts 1.3禁用防火墙和selinux //禁用防火墙 systemctl stop firewalld && systemctl disable firewalld //禁用selinux #临时修改 setenforce 0 #永久修改，重启服务器后生效 sed -i '7s/enforcing/disabled/' /etc/selinux/config 1.4创建/etc/sysctl.d/k8s.conf文件，添加如下内容 //向文件中写入以下内容 cat >/etc/sysctl.d/k8s.conf 1.5安装ipvs 脚本创建了的/etc/sysconfig/modules/ipvs.modules文件，保证在节点重启后能自动加载所需模块。使用lsmod | grep -e ip_vs -e nf_conntrack_ipv4命令查看是否已经正确加载所需的内核模块 //向文件中写入以下内容 cat > /etc/sysconfig/modules/ipvs.modules 安装ipset和ipvsadm(便于查看 ipvs 的代理规则) yum -y install ipset ipvsadm 1.6同步服务器时间 //安装chrony yum -y install chrony //修改同步服务器地址为阿里云 sed -i.bak '3,6d' /etc/chrony.conf && sed -i '3cserver ntp1.aliyun.com iburst' \\ /etc/chrony.conf //启动chronyd及加入开机自启 systemctl start chronyd && systemctl enable chronyd //查看同步结果 chronyc sources 210 Number of sources = 1 MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== ^* 120.25.115.20 2 6 37 13 +194us[+6131us] +/- 33ms 1.7关闭swap分区 修改/etc/fstab文件，注释掉 SWAP 的自动挂载，使用free -m确认 swap 已经关闭 //手动关闭swap swapoff -a //修改fstab文件，注释swap自动挂载 sed -i '/^\\/dev\\/mapper\\/centos-swap/c#/dev/mapper/centos-swap swap swap defaults 0 0' /etc/fstab //查看swap是否关闭 free -m total used free shared buff/cache available Mem: 1994 682 612 9 699 1086 Swap: 0 0 0 swappiness 参数调整，修改/etc/sysctl.d/k8s.conf添加下面一行 cat >>/etc/sysctl.d/k8s.conf 1.8安装docker18.09.9 1.添加阿里云yum源 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 2.查看可用版本 yum list docker-ce --showduplicates | sort -r 已加载插件：fastestmirror, langpacks 可安装的软件包 * updates: mirrors.aliyun.com Loading mirror speeds from cached hostfile * extras: mirrors.aliyun.com docker-ce.x86_64 3:19.03.5-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.4-3.el7 docker-ce-stable 。。。。。。 docker-ce.x86_64 3:18.09.9-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.8-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.7-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.6-3.el7 docker-ce-stable 。。。。。。 3.安装docker18.09.9 yum -y install docker-ce-18.09.9-3.el7 docker-ce-cli-18.09.9 4.启动docker并设置开机自启 systemctl enable docker && systemctl start docker 5.配置阿里云docker镜像加速 cat > /etc/docker/daemon.json 1.9安装Kubeadm 需要科学上网 cat >/etc/yum.repos.d/kubernetes.repo 使用阿里云yum源 cat >/etc/yum.repos.d/kubernetes.repo 安装 kubeadm、kubelet、kubectl(阿里云yum源会随官方更新最新版，因此指定版本) yum -y install kubelet-1.15.3 kubeadm-1.15.3 kubectl-1.15.3 --disableexcludes=kubernetes //查看版本 kubeadm version kubeadm version: &version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:11:18Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"} 设置kubelet开机自启 systemctl enable kubelet 设置k8s命令自动补全 yum -y install bash-completion source /usr/share/bash-completion/bash_completion source > ~/.bashrc 到此，基本环境安装完成!!! 二、初始化集群 2.1master节点操作，配置 kubeadm 初始化文件 //初始化kubeadm文件，这里的路径为/root kubeadm config print init-defaults > kubeadm.yaml //查看生成的默认文件 cat kubeadm.yaml apiVersion: kubeadm.k8s.io/v1beta2 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 1.2.3.4 bindPort: 6443 nodeRegistration: criSocket: /var/run/dockershim.sock name: k8s-master taints: - effect: NoSchedule key: node-role.kubernetes.io/master --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta2 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns: type: CoreDNS etcd: local: dataDir: /var/lib/etcd imageRepository: k8s.gcr.io kind: ClusterConfiguration kubernetesVersion: v1.15.0 networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 scheduler: {} 然后根据我们自己的需求修改配置，比如修改 imageRepository 的值，kube-proxy 的模式为 ipvs，另外需要注意的是我们这里是准备安装 calico 网络插件的，需要将 networking.podSubnet 设置为192.168.0.0/16 修改后的文件内容如下 cat > kubeadm.yaml 2.2初始化master kubeadm init --config kubeadm.yaml 完整输出结果 [init] Using Kubernetes version: v1.15.3 [preflight] Running pre-flight checks [WARNING IsDockerSystemdCheck]: detected \"cgroupfs\" as the Docker cgroup driv er. The recommended driver is \"systemd\". Please follow the guide at https://kubernete s.io/docs/setup/cri/ [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using 'kubeadm config imag es pull' [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet /kubeadm-flags.env\" [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Activating the kubelet service [certs] Using certificateDir folder \"/etc/kubernetes/pki\" [certs] Generating \"ca\" certificate and key [certs] Generating \"apiserver\" certificate and key [certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kuberne tes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10. 96.0.1 10.0.0.130] [certs] Generating \"apiserver-kubelet-client\" certificate and key [certs] Generating \"etcd/ca\" certificate and key [certs] Generating \"apiserver-etcd-client\" certificate and key [certs] Generating \"etcd/healthcheck-client\" certificate and key [certs] Generating \"etcd/server\" certificate and key [certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [10.0.0.130 127.0.0.1 ::1] [certs] Generating \"etcd/peer\" certificate and key [certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [10.0.0.130 127.0.0.1 ::1] [certs] Generating \"front-proxy-ca\" certificate and key [certs] Generating \"front-proxy-client\" certificate and key [certs] Generating \"sa\" key and public key [kubeconfig] Using kubeconfig folder \"/etc/kubernetes\" [kubeconfig] Writing \"admin.conf\" kubeconfig file [kubeconfig] Writing \"kubelet.conf\" kubeconfig file [kubeconfig] Writing \"controller-manager.conf\" kubeconfig file [kubeconfig] Writing \"scheduler.conf\" kubeconfig file [control-plane] Using manifest folder \"/etc/kubernetes/manifests\" [control-plane] Creating static Pod manifest for \"kube-apiserver\" [control-plane] Creating static Pod manifest for \"kube-controller-manager\" [control-plane] Creating static Pod manifest for \"kube-scheduler\" [etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\" [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s [apiclient] All control plane components are healthy after 19.003902 seconds [upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [kubelet] Creating a ConfigMap \"kubelet-config-1.15\" in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --upload-certs [mark-control-plane] Marking the node k8s-master as control-plane by adding the label \"node-role.kubernetes.io/master=''\" [mark-control-plane] Marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [bootstrap-token] Using token: abcdef.0123456789abcdef [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10.0.0.130:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:b29cbd05af6924b95b74964f949a65e3367402d76a268295c333e792c3eb7ad2 #kubeadm init --config kubeadm.yaml初始化master下载的镜像 gcr.azk8s.cn/google_containers/kube-proxy v1.15.3 232b5c793146 6 months ago 82.4MB gcr.azk8s.cn/google_containers/kube-apiserver v1.15.3 5eb2d3fc7a44 6 months ago 207MB gcr.azk8s.cn/google_containers/kube-controller-manager v1.15.3 e77c31de5547 6 months ago 159MB gcr.azk8s.cn/google_containers/kube-scheduler v1.15.3 703f9c69a5d5 6 months ago 81.1MB gcr.azk8s.cn/google_containers/coredns 1.3.1 eb516548c180 14 months ago 40.3MB gcr.azk8s.cn/google_containers/etcd 3.3.10 2c4adeb21b4f 15 months ago 258MB gcr.azk8s.cn/google_containers/pause 3.1 da86e6ba6ca1 2 years ago 742kB #启动的容器 ab18a4a6f78c 232b5c793146 \"/usr/local/bin/kube…\" About an hour ago Up About an hour k8s_kube-proxy_kube-proxy-zns59_kube-system_7b431cd3-6572-4a5e-981e-c536bbad32ea_0 b8b3f6cf6702 gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" About an hour ago Up About an hour k8s_POD_kube-proxy-zns59_kube-system_7b431cd3-6572-4a5e-981e-c536bbad32ea_0 dc2db22d2a84 e77c31de5547 \"kube-controller-man…\" About an hour ago Up About an hour k8s_kube-controller-manager_kube-controller-manager-k8s-master_kube-system_5a41bf631425471a02fc0cca5725518d_0 8a25922bf185 2c4adeb21b4f \"etcd --advertise-cl…\" About an hour ago Up About an hour k8s_etcd_etcd-k8s-master_kube-system_3c2a1c3e9bec3ffe168c550fd855d63f_0 f90159ea56c4 5eb2d3fc7a44 \"kube-apiserver --ad…\" About an hour ago Up About an hour k8s_kube-apiserver_kube-apiserver-k8s-master_kube-system_6eb25fe257b281826a14a7e276a75d61_0 b70fe56aab46 703f9c69a5d5 \"kube-scheduler --bi…\" About an hour ago Up About an hour k8s_kube-scheduler_kube-scheduler-k8s-master_kube-system_f1217bcf1fd2d00391bfd0ec41d69a25_0 f084bc0cf4ae gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" About an hour ago Up About an hour k8s_POD_kube-controller-manager-k8s-master_kube-system_5a41bf631425471a02fc0cca5725518d_0 e93a8bebe038 gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" About an hour ago Up About an hour k8s_POD_kube-scheduler-k8s-master_kube-system_f1217bcf1fd2d00391bfd0ec41d69a25_0 4d6cac595d0b gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" About an hour ago Up About an hour k8s_POD_kube-apiserver-k8s-master_kube-system_6eb25fe257b281826a14a7e276a75d61_0 f49e9fa0ba1d gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" About an hour ago Up About an hour k8s_POD_etcd-k8s-master_kube-system_3c2a1c3e9bec3ffe168c550fd855d63f_0 拷贝 kubeconfig 文件 //这里的路径为/root mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 2.3master添加节点 node1和node2相同操作 将master节点上的$HOME/.kube/config 文件拷贝到node节点对应的文件中 1.创建目录，这里的路径为/root mkdir -p $HOME/.kube 2.把master节点上的config文件拷贝到node1和node2的$HOME/.kube scp k8s-master:~/.kube/config $HOME/.kube 3.修改权限 chown $(id -u):$(id -g) $HOME/.kube/config 将node1和node2加入到集群中 这里需要用到2.2中初始化master最后生成的token和sha256值 kubeadm join 10.0.0.130:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:b29cbd05af6924b95b74964f949a65e3367402d76a268295c333e792c3eb7ad2 输出结果 [preflight] Running pre-flight checks [WARNING IsDockerSystemdCheck]: detected \"cgroupfs\" as the Docker cgroup driver. The recommended driver is \"systemd\". Please follow the guide at https://kubernetes.io/docs/setup/cri/ [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [kubelet-start] Downloading configuration for the kubelet from the \"kubelet-config-1.15\" ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Activating the kubelet service [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster. 如果忘记了token和sha256值，可以在master节点使用如下命令查看 //查看token kubeadm token list TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS abcdef.0123456789abcdef 23h 2019-12-11T16:52:56+08:00 authentication,signing system:bootstrappers:kubeadm:default-node-token //查看sha256 openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' b29cbd05af6924b95b74964f949a65e3367402d76a268295c333e792c3eb7ad2 //同时查看token和sha256 kubeadm token create --print-join-command kubeadm join 10.0.0.130:6443 --token h8lbi6.27rrq8c6khonopqe --discovery-token-ca-cert-hash sha256:b29cbd05af6924b95b74964f949a65e3367402d76a268295c333e792c3eb7ad2 master节点查看node，发现状态都是NotReady，因为还没有安装网络插件，这里我们安装calio官方插件文档 kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master NotReady master 19m v1.15.3 k8s-node1 NotReady 4m10s v1.15.3 k8s-node2 NotReady 4m3s v1.15.3 2.4master节点安装网络插件calio //下载文件 wget https://docs.projectcalico.org/v3.8/manifests/calico.yaml //因为有节点是多网卡，所以需要在资源清单文件中指定内网网卡，修改calico.yaml文件中以下几处 582行-594行，原内容如下 containers: 583 # Runs calico-node container on each Kubernetes node. This 584 # container programs network policy and routes on each 585 # host. 586 - name: calico-node 587 image: calico/node:v3.8.5 588 env: 589 # Use Kubernetes API as the backing datastore. 590 - name: DATASTORE_TYPE 591 value: \"kubernetes\" 592 # Wait for the datastore. 593 - name: WAIT_FOR_DATASTORE 594 value: \"true\" 现修改为如下（增加了一个环境变量） containers: # Runs calico-node container on each Kubernetes node. This # container programs network policy and routes on each # host. - name: calico-node image: calico/node:v3.8.5 env: # Use Kubernetes API as the backing datastore. - name: DATASTORE_TYPE value: \"kubernetes\" - name: IP_AUTODETECTION_METHOD # DaemonSet中添加该环境变量 value: interface=eth0 # 指定内网网卡 # Wait for the datastore. - name: WAIT_FOR_DATASTORE value: \"true\" //修改完成后安装calico网络插件 kubectl apply -f calico.yaml //安装完成后稍等一会查看pods状态 kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-5bbc8f45cb-rpm96 1/1 Running 0 6m17s calico-node-cbmcz 1/1 Running 0 6m17s calico-node-jbjw4 1/1 Running 0 6m17s calico-node-qfnfm 1/1 Running 0 6m17s coredns-cf8fb6d7f-vkst4 1/1 Running 0 50m coredns-cf8fb6d7f-z622t 1/1 Running 0 50m etcd-k8s-master 1/1 Running 0 49m kube-apiserver-k8s-master 1/1 Running 0 49m kube-controller-manager-k8s-master 1/1 Running 0 50m kube-proxy-68wzg 1/1 Running 0 35m kube-proxy-fbc95 1/1 Running 0 35m kube-proxy-g4xkc 1/1 Running 0 50m kube-scheduler-k8s-master 1/1 Running 0 49m //查看node状态 kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready master 55m v1.15.3 k8s-node1 Ready 39m v1.15.3 k8s-node2 Ready 39m v1.15.3 #kubectl apply -f calico.yaml安装网络插件calico下载的镜像 calico/node v3.8.6 1b9ca446b4da 2 months ago 192MB calico/pod2daemon-flexvol v3.8.6 97bfbee02d48 2 months ago 9.38MB calico/cni v3.8.6 33af7d7d46b6 2 months ago 161MB #启动的容器 4cfb387bd9a0 calico/node \"start_runit\" 5 minutes ago Up 5 minutes k8s_calico-node_calico-node-xc2tg_kube-system_0c89a070-b9e4-419e-916e-058b32ac64d1_0 b75783c7934c calico/pod2daemon-flexvol \"/usr/local/bin/flex…\" 5 minutes ago Exited (0) 5 minutes ago k8s_flexvol-driver_calico-node-xc2tg_kube-system_0c89a070-b9e4-419e-916e-058b32ac64d1_0 9e4766d2fa80 33af7d7d46b6 \"/install-cni.sh\" 6 minutes ago Exited (0) 6 minutes ago k8s_install-cni_calico-node-xc2tg_kube-system_0c89a070-b9e4-419e-916e-058b32ac64d1_0 f7f752fd62dc calico/cni \"/opt/cni/bin/calico…\" 6 minutes ago Exited (0) 6 minutes ago k8s_upgrade-ipam_calico-node-xc2tg_kube-system_0c89a070-b9e4-419e-916e-058b32ac64d1_0 2.5安装Dashboard 下载文件及修改内容 //下载文件 wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml //修改镜像名称 注释112行，添加如下两行 在这一行下添加如下两行 - name: kubernetes-dashboard image: gcr.azk8s.cn/google_containers/kubernetes-dashboard-amd64:v1.10.1 imagePullPolicy: IfNotPresent //修改Service为NodePort类型，文件最后一行加入以下内容 selector: k8s-app: kubernetes-dashboard type: NodePort #新增这一行，注意与selector在同一级 部署dashboard kubectl apply -f kubernetes-dashboard.yaml #下载的镜像 gcr.azk8s.cn/google_containers/kubernetes-dashboard-amd64 v1.10.1 f9aed6605b81 15 months ago 122MB #启动的容器 7fddf75bb284 gcr.azk8s.cn/google_containers/kubernetes-dashboard-amd64 \"/dashboard --insecu…\" 9 seconds ago Up 9 seconds k8s_kubernetes-dashboard_kubernetes-dashboard-fcfb4cbc-v7lfh_kube-system_268913c8-f9df-4e9a-9f89-c9dda9462d38_0 查看dashboard的运行状态及外网访问端口 //查看dashboard运行状态 kubectl get pods -n kube-system -l k8s-app=kubernetes-dashboard NAME READY STATUS RESTARTS AGE kubernetes-dashboard-fcfb4cbc-fcczh 1/1 Running 0 3m40s //查看dashboard外网访问端口 kubectl get svc -n kube-system -l k8s-app=kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard NodePort 10.96.100.138 443:31758/TCP 4m42s 通过上边的31758端口访问dashboard，注意是https 由于 dashboard 默认是自建的 https 证书，该证书是不受浏览器信任的，所以我们需要强制跳转就可以了 然后创建一个具有全局所有权限的用户来登录Dashboard //编辑admin.yaml文件 cat > admin.yaml 然后粘贴上述生成的base64字符串登陆dashboard，在登陆页面选择令牌一项 登陆后的首界面 到此，使用kubeadm安装k8s 1.15.3完成！！！ 获取令牌命令 kubectl get secret `kubectl get secret -n kube-system|grep admin-token|awk '{print $1}'` -o jsonpath={.data.token} -n kube-system |base64 -d 1.15.3启动的容器及下载的镜像 #启动的容器 7fddf75bb284 gcr.azk8s.cn/google_containers/kubernetes-dashboard-amd64 \"/dashboard --insecu…\" 8 minutes ago Up 8 minutes k8s_kubernetes-dashboard_kubernetes-dashboard-fcfb4cbc-v7lfh_kube-system_268913c8-f9df-4e9a-9f89-c9dda9462d38_0 fa592650d469 gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" 8 minutes ago Up 8 minutes k8s_POD_kubernetes-dashboard-fcfb4cbc-v7lfh_kube-system_268913c8-f9df-4e9a-9f89-c9dda9462d38_0 4cfb387bd9a0 calico/node \"start_runit\" 18 minutes ago Up 18 minutes k8s_calico-node_calico-node-xc2tg_kube-system_0c89a070-b9e4-419e-916e-058b32ac64d1_0 b75783c7934c calico/pod2daemon-flexvol \"/usr/local/bin/flex…\" 18 minutes ago Exited (0) 18 minutes ago k8s_flexvol-driver_calico-node-xc2tg_kube-system_0c89a070-b9e4-419e-916e-058b32ac64d1_0 9e4766d2fa80 33af7d7d46b6 \"/install-cni.sh\" 18 minutes ago Exited (0) 18 minutes ago k8s_install-cni_calico-node-xc2tg_kube-system_0c89a070-b9e4-419e-916e-058b32ac64d1_0 f7f752fd62dc calico/cni \"/opt/cni/bin/calico…\" 18 minutes ago Exited (0) 18 minutes ago k8s_upgrade-ipam_calico-node-xc2tg_kube-system_0c89a070-b9e4-419e-916e-058b32ac64d1_0 2cd3749c5b37 gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" 19 minutes ago Up 19 minutes k8s_POD_calico-node-xc2tg_kube-system_0c89a070-b9e4-419e-916e-058b32ac64d1_0 ab18a4a6f78c 232b5c793146 \"/usr/local/bin/kube…\" 2 hours ago Up 2 hours k8s_kube-proxy_kube-proxy-zns59_kube-system_7b431cd3-6572-4a5e-981e-c536bbad32ea_0 b8b3f6cf6702 gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" 2 hours ago Up 2 hours k8s_POD_kube-proxy-zns59_kube-system_7b431cd3-6572-4a5e-981e-c536bbad32ea_0 dc2db22d2a84 e77c31de5547 \"kube-controller-man…\" 2 hours ago Up 2 hours k8s_kube-controller-manager_kube-controller-manager-k8s-master_kube-system_5a41bf631425471a02fc0cca5725518d_0 8a25922bf185 2c4adeb21b4f \"etcd --advertise-cl…\" 2 hours ago Up 2 hours k8s_etcd_etcd-k8s-master_kube-system_3c2a1c3e9bec3ffe168c550fd855d63f_0 f90159ea56c4 5eb2d3fc7a44 \"kube-apiserver --ad…\" 2 hours ago Up 2 hours k8s_kube-apiserver_kube-apiserver-k8s-master_kube-system_6eb25fe257b281826a14a7e276a75d61_0 b70fe56aab46 703f9c69a5d5 \"kube-scheduler --bi…\" 2 hours ago Up 2 hours k8s_kube-scheduler_kube-scheduler-k8s-master_kube-system_f1217bcf1fd2d00391bfd0ec41d69a25_0 f084bc0cf4ae gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" 2 hours ago Up 2 hours k8s_POD_kube-controller-manager-k8s-master_kube-system_5a41bf631425471a02fc0cca5725518d_0 e93a8bebe038 gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" 2 hours ago Up 2 hours k8s_POD_kube-scheduler-k8s-master_kube-system_f1217bcf1fd2d00391bfd0ec41d69a25_0 4d6cac595d0b gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" 2 hours ago Up 2 hours k8s_POD_kube-apiserver-k8s-master_kube-system_6eb25fe257b281826a14a7e276a75d61_0 f49e9fa0ba1d gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" 2 hours ago Up 2 hours k8s_POD_etcd-k8s-master_kube-system_3c2a1c3e9bec3ffe168c550fd855d63f_0 #下载的镜像 calico/node v3.8.6 1b9ca446b4da 2 months ago 192MB calico/pod2daemon-flexvol v3.8.6 97bfbee02d48 2 months ago 9.38MB calico/cni v3.8.6 33af7d7d46b6 2 months ago 161MB gcr.azk8s.cn/google_containers/kube-proxy v1.15.3 232b5c793146 6 months ago 82.4MB gcr.azk8s.cn/google_containers/kube-apiserver v1.15.3 5eb2d3fc7a44 6 months ago 207MB gcr.azk8s.cn/google_containers/kube-scheduler v1.15.3 703f9c69a5d5 6 months ago 81.1MB gcr.azk8s.cn/google_containers/kube-controller-manager v1.15.3 e77c31de5547 6 months ago 159MB gcr.azk8s.cn/google_containers/coredns 1.3.1 eb516548c180 14 months ago 40.3MB gcr.azk8s.cn/google_containers/kubernetes-dashboard-amd64 v1.10.1 f9aed6605b81 15 months ago 122MB gcr.azk8s.cn/google_containers/etcd 3.3.10 2c4adeb21b4f 15 months ago 258MB gcr.azk8s.cn/google_containers/pause 3.1 da86e6ba6ca1 2 years ago 742kB k8s切换命名空间工具 git clone https://github.com.cnpmjs.org/ahmetb/kubectx cp kubectx/kubens /usr/local/bin #查看所有命名空间 kubens #切换到kube-system命名空间 kubens kube-system Context \"kubernetes-admin@kubernetes\" modified. Active namespace is \"kube-system\". 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s安装/2.使用 kubeadm 搭建 v1.16.3 版本 Kubernetes 集群.html":{"url":"linux/k8s/k8s安装/2.使用 kubeadm 搭建 v1.16.3 版本 Kubernetes 集群.html","title":"1.16","keywords":"","body":"[toc] 使用 kubeadm 搭建 v1.16.3 版本 Kubernetes 集群 一、环境准备 1.1实验环境 角色 IP地址 主机名 docker版本 硬件配置 系统 内核 master 10.0.0.130 k8s-master 18.09.9 2c2g CentOS7.6 3.10.0-957.el7.x86_64 node1 10.0.0.131 k8s-node1 18.09.9 2c1g CentOS7.6 3.10.0-957.el7.x86_64 node2 10.0.0.132 k8s-node2 18.09.9 2c1g CentOS7.6 3.10.0-957.el7.x86_64 1.2每个节点配置host信息 cat >> /etc/hosts 1.3禁用防火墙和selinux //禁用防火墙 systemctl stop firewalld && systemctl disable firewalld //禁用selinux #临时修改 setenforce 0 #永久修改，重启服务器后生效 sed -i '7s/enforcing/disabled/' /etc/selinux/config 1.4创建/etc/sysctl.d/k8s.conf文件，添加如下内容 //向文件中写入以下内容 cat >/etc/sysctl.d/k8s.conf 1.5安装ipvs 脚本创建了的/etc/sysconfig/modules/ipvs.modules文件，保证在节点重启后能自动加载所需模块。使用lsmod | grep -e ip_vs -e nf_conntrack_ipv4命令查看是否已经正确加载所需的内核模块 //向文件中写入以下内容 cat > /etc/sysconfig/modules/ipvs.modules 安装ipset和ipvsadm(便于查看 ipvs 的代理规则) yum -y install ipset ipvsadm 1.6同步服务器时间 //安装chrony yum -y install chrony //修改同步服务器地址为阿里云 sed -i.bak '3,6d' /etc/chrony.conf && sed -i '3cserver ntp1.aliyun.com iburst' \\ /etc/chrony.conf //启动chronyd及加入开机自启 systemctl start chronyd && systemctl enable chronyd //查看同步结果 chronyc sources 210 Number of sources = 1 MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== ^* 120.25.115.20 2 6 37 13 +194us[+6131us] +/- 33ms 1.7关闭swap分区 修改/etc/fstab文件，注释掉 SWAP 的自动挂载，使用free -m确认 swap 已经关闭 //手动关闭swap swapoff -a //修改fstab文件，注释swap自动挂载 sed -i '/^\\/dev\\/mapper\\/centos-swap/c#/dev/mapper/centos-swap swap swap defaults 0 0' /etc/fstab //查看swap是否关闭 free -m total used free shared buff/cache available Mem: 1994 682 612 9 699 1086 Swap: 0 0 0 swappiness 参数调整，修改/etc/sysctl.d/k8s.conf添加下面一行 cat >>/etc/sysctl.d/k8s.conf 1.8安装docker18.09.9 1.添加阿里云yum源 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 2.查看可用版本 yum list docker-ce --showduplicates | sort -r 已加载插件：fastestmirror, langpacks 可安装的软件包 * updates: mirrors.aliyun.com Loading mirror speeds from cached hostfile * extras: mirrors.aliyun.com docker-ce.x86_64 3:19.03.5-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.4-3.el7 docker-ce-stable 。。。。。。 docker-ce.x86_64 3:18.09.9-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.8-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.7-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.6-3.el7 docker-ce-stable 。。。。。。 3.安装docker18.09.9 yum -y install docker-ce-18.09.9-3.el7 docker-ce-cli-18.09.9 4.启动docker并设置开机自启 systemctl enable docker && systemctl start docker 5.配置阿里云docker镜像加速 cat > /etc/docker/daemon.json 1.9修改docker Cgroup Driver为systemd #修改docker Cgroup Driver为systemd 将/usr/lib/systemd/system/docker.service文件中的这一行 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock 修改为 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd 如果不修改，在添加 worker 节点时可能会碰到如下错误 [WARNING IsDockerSystemdCheck]: detected \"cgroupfs\" as the Docker cgroup driver. The recommended driver is \"systemd\". Please follow the guide at https://kubernetes.io/docs/setup/cri/ //使用如下命令修改 sed -i.bak \"s#^ExecStart=/usr/bin/dockerd.*#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd#g\" /usr/lib/systemd/system/docker.service //重启docker systemctl daemon-reload && systemctl restart docker 1.10安装Kubeadm 需要科学上网 cat >/etc/yum.repos.d/kubernetes.repo 使用阿里云yum源 cat >/etc/yum.repos.d/kubernetes.repo 安装 kubeadm、kubelet、kubectl(阿里云yum源会随官方更新最新版，因此指定版本) //安装1.16.3版本 yum -y install kubelet-1.16.3 kubeadm-1.16.3 kubectl-1.16.3 //查看版本 kubeadm version kubeadm version: &version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:11:18Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"} 设置kubelet开机自启 systemctl enable kubelet 设置k8s命令自动补全 yum -y install bash-completion source /usr/share/bash-completion/bash_completion source > ~/.bashrc 到此，基本环境安装完成!!! 二、初始化集群 2.1master节点操作，配置 kubeadm 初始化文件 cat ./kubeadm-config.yaml apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration kubernetesVersion: v1.16.3 imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers #dnsName controlPlaneEndpoint: \"10.0.0.130:6443\" networking: serviceSubnet: \"10.96.0.0/16\" #k8s容器组所在的网段 podSubnet: \"10.100.0.1/16\" dnsDomain: \"cluster.local\" EOF 2.2初始化master ⚠️如果想要重新初始化，需要执行命令kubeadm reset -f #kubeadm init --config=kubeadm-config.yaml --upload-certs 完整输出结果 kubeadm init --config=kubeadm-config.yaml [init] Using Kubernetes version: v1.16.3 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using 'kubeadm config images pull' [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Activating the kubelet service [certs] Using certificateDir folder \"/etc/kubernetes/pki\" [certs] Generating \"ca\" certificate and key [certs] Generating \"apiserver\" certificate and key [certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.0.0.130 10.0.0.130] [certs] Generating \"apiserver-kubelet-client\" certificate and key [certs] Generating \"front-proxy-ca\" certificate and key [certs] Generating \"front-proxy-client\" certificate and key [certs] Generating \"etcd/ca\" certificate and key [certs] Generating \"etcd/server\" certificate and key [certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [10.0.0.130 127.0.0.1 ::1] [certs] Generating \"etcd/peer\" certificate and key [certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [10.0.0.130 127.0.0.1 ::1] [certs] Generating \"etcd/healthcheck-client\" certificate and key [certs] Generating \"apiserver-etcd-client\" certificate and key [certs] Generating \"sa\" key and public key [kubeconfig] Using kubeconfig folder \"/etc/kubernetes\" [kubeconfig] Writing \"admin.conf\" kubeconfig file [kubeconfig] Writing \"kubelet.conf\" kubeconfig file [kubeconfig] Writing \"controller-manager.conf\" kubeconfig file [kubeconfig] Writing \"scheduler.conf\" kubeconfig file [control-plane] Using manifest folder \"/etc/kubernetes/manifests\" [control-plane] Creating static Pod manifest for \"kube-apiserver\" [control-plane] Creating static Pod manifest for \"kube-controller-manager\" [control-plane] Creating static Pod manifest for \"kube-scheduler\" [etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\" [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s [apiclient] All control plane components are healthy after 16.501777 seconds [upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [kubelet] Creating a ConfigMap \"kubelet-config-1.16\" in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --upload-certs [mark-control-plane] Marking the node k8s-master as control-plane by adding the label \"node-role.kubernetes.io/master=''\" [mark-control-plane] Marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [bootstrap-token] Using token: px979r.mphk9ee5ya8fgy44 [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of control-plane nodes by copying certificate authorities and service account keys on each node and then running the following as root: kubeadm join 10.0.0.130:6443 --token px979r.mphk9ee5ya8fgy44 \\ --discovery-token-ca-cert-hash sha256:5e7c7cd1cc1f86c0761e54b9380de22968b6b221cb98939c14ab2942223f6f51 \\ --control-plane Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10.0.0.130:6443 --token px979r.mphk9ee5ya8fgy44 \\ --discovery-token-ca-cert-hash sha256:5e7c7cd1cc1f86c0761e54b9380de22968b6b221cb98939c14ab2942223f6f51 #kubeadm init --config=kubeadm-config.yaml --upload-certs初始化master下载的镜像 gcr.azk8s.cn/google_containers/kube-proxy v1.16.3 9b65a0f78b09 4 months ago 86.1MB gcr.azk8s.cn/google_containers/kube-apiserver v1.16.3 df60c7526a3d 4 months ago 217MB gcr.azk8s.cn/google_containers/kube-controller-manager v1.16.3 bb16442bcd94 4 months ago 163MB gcr.azk8s.cn/google_containers/kube-scheduler v1.16.3 98fecf43a54f 4 months ago 87.3MB gcr.azk8s.cn/google_containers/etcd 3.3.15-0 b2756210eeab 6 months ago 247MB gcr.azk8s.cn/google_containers/coredns 1.6.2 bf261d157914 7 months ago 44.1MB gcr.azk8s.cn/google_containers/pause 3.1 da86e6ba6ca1 2 years ago 742kB 拷贝 kubeconfig 文件 //这里的路径为/root mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 2.3master添加节点 node1和node2相同操作 将master节点上的$HOME/.kube/config 文件拷贝到node节点对应的文件中 1.创建目录，这里的路径为/root mkdir -p $HOME/.kube 2.把master节点上的config文件拷贝到node1和node2的$HOME/.kube scp k8s-master:~/.kube/config $HOME/.kube 3.修改权限 chown $(id -u):$(id -g) $HOME/.kube/config 将node1和node2加入到集群中 这里需要用到2.2中初始化master最后生成的token和sha256值 #kubeadm join 10.0.0.130:6443 --token px979r.mphk9ee5ya8fgy44 --discovery-token-ca-cert-hash sha256:5e7c7cd1cc1f86c0761e54b9380de22968b6b221cb98939c14ab2942223f6f51 输出结果 [preflight] Running pre-flight checks [WARNING IsDockerSystemdCheck]: detected \"cgroupfs\" as the Docker cgroup driver. The recommended driver is \"systemd\". Please follow the guide at https://kubernetes.io/docs/setup/cri/ [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [kubelet-start] Downloading configuration for the kubelet from the \"kubelet-config-1.16\" ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Activating the kubelet service [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster. 如果忘记了token和sha256值，可以在master节点使用如下命令查看 //查看token #kubeadm token list TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS px979r.mphk9ee5ya8fgy44 20h 2020-03-18T13:49:48+08:00 authentication,signing system:bootstrappers:kubeadm:default-node-token //查看sha256 #openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' 5e7c7cd1cc1f86c0761e54b9380de22968b6b221cb98939c14ab2942223f6f51 //同时查看token和sha256 #kubeadm token create --print-join-command kubeadm join 10.0.0.130:6443 --token 9b28zg.oyt0kvvpmtrem4bg --discovery-token-ca-cert-hash sha256:5e7c7cd1cc1f86c0761e54b9380de22968b6b221cb98939c14ab2942223f6f51 master节点查看node，发现状态都是NotReady，因为还没有安装网络插件，这里我们安装calio官方插件文档 kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master NotReady master 19m v1.16.3 k8s-node1 NotReady 4m10s v1.16.3 k8s-node2 NotReady 4m3s v1.16.3 2.4master节点安装网络插件calio //下载文件 wget https://docs.projectcalico.org/v3.8/manifests/calico.yaml wget https://kuboard.cn/install-script/calico/calico-3.9.2.yaml 将文件中的620行改为如下，因为在上边kubeadm-config.yaml配置文件中指定了容器组IP 620行 value: \"10.100.0.1/16\" //修改完成后安装calico网络插件 #kubectl apply -f calico-3.9.2.yaml //安装完成后稍等一会查看pods状态 #kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-dc6cb64cb-8sh59 1/1 Running 0 6m22s calico-node-89s9k 1/1 Running 0 6m22s calico-node-dkt7w 1/1 Running 0 6m22s calico-node-tgg2h 1/1 Running 0 6m22s coredns-667f964f9b-7hrj9 1/1 Running 0 33m coredns-667f964f9b-8q7sh 1/1 Running 0 33m etcd-k8s-master 1/1 Running 0 33m kube-apiserver-k8s-master 1/1 Running 0 32m kube-controller-manager-k8s-master 1/1 Running 0 33m kube-proxy-b2r5d 1/1 Running 0 12m kube-proxy-nd982 1/1 Running 0 11m kube-proxy-zh6cz 1/1 Running 0 33m kube-scheduler-k8s-master 1/1 Running 0 32m //查看node状态 [root@k8s-master ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready master 31m v1.16.3 k8s-node1 Ready 9m46s v1.16.3 k8s-node2 Ready 9m22s v1.16.3 #kubectl apply -f calico.yaml安装网络插件calico下载的镜像 calico/node v3.9.2 14a380c92c40 5 months ago 195MB calico/cni v3.9.2 c0d73dd53e71 5 months ago 160MB calico/pod2daemon-flexvol v3.9.2 523f0356e07b 5 months ago 9.78MB #启动的容器 03df44242d90 calico/node \"start_runit\" 8 minutes ago Up 8 minutes k8s_calico-node_calico-node-89s9k_kube-system_ff5384c4-72b2-4bfa-b8ea-77f38c2a2112_0 c2a56feedc7c calico/pod2daemon-flexvol \"/usr/local/bin/flex…\" 9 minutes ago Exited (0) 9 minutes ago k8s_flexvol-driver_calico-node-89s9k_kube-system_ff5384c4-72b2-4bfa-b8ea-77f38c2a2112_0 ca9febcebaa8 c0d73dd53e71 \"/install-cni.sh\" 10 minutes ago Exited (0) 10 minutes ago k8s_install-cni_calico-node-89s9k_kube-system_ff5384c4-72b2-4bfa-b8ea-77f38c2a2112_0 b581894b1b91 calico/cni \"/opt/cni/bin/calico…\" 10 minutes ago Exited (0) 10 minutes ago k8s_upgrade-ipam_calico-node-89s9k_kube-system_ff5384c4-72b2-4bfa-b8ea-77f38c2a2112_0 2.5安装Dashboard(可选) 下载文件及修改内容 这里查看dashboard对应的k8s版本 //下载文件 v2.0.0-rc3是中文版本，beta8是英文版本 wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc3/aio/deploy/recommended.yaml //修改Service为NodePort类型 42行下增加一行 nodePort: 30001 44行下增加一行 type: NodePort //原先内容 spec: ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard //修改后内容 spec: ports: - port: 443 targetPort: 8443 nodePort: 30001 #增加，指定nodeport端口 selector: k8s-app: kubernetes-dashboard type: NodePort #增加，修改类型为nodeport 部署dashboard #kubectl apply -f recommended.yaml #下载的镜像 kubernetesui/metrics-scraper v1.0.3 3327f0dbcb4a 6 weeks ago 40.1MB kubernetesui/dashboard v2.0.0-beta8 eb51a3597525 3 months ago 90.8MB #启动的容器 8bc24b355d78 kubernetesui/dashboard \"/dashboard --insecu…\" 7 minutes ago Up 7 minutes k8s_kubernetes-dashboard_kubernetes-dashboard-5996555fd8-2ppc5_kubernetes-dashboard_7f46632c-8b7b-41e4-aeb3-a1fbadd29782_0 查看dashboard的运行状态及外网访问端口 //查看dashboard运行状态 #kubectl get pods -n kubernetes-dashboard -l k8s-app=kubernetes-dashboard NAME READY STATUS RESTARTS AGE kubernetes-dashboard-5996555fd8-2ppc5 1/1 Running 0 8m16s //查看dashboard外网访问端口，命名空间为kubernetes-dashboard #kubectl get svc -n kubernetes-dashboard -l k8s-app=kubernetes-dashboard kubectl get svc -n kubernetes-dashboard -l k8s-app=kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard NodePort 10.96.142.172 443:30001/TCP 8m37s 通过上边的30001端口访问dashboard，注意是https ⚠️k8s1.16.3这个版本中，使用的dashboard版本是2.0.0-beta8，只有火狐浏览器可以访问，其余浏览器都不能访问，会报错如下 使用火狐浏览器访问，由于 dashboard 默认是自建的 https 证书，该证书是不受浏览器信任的，所以我们需要强制跳转就可以了 然后创建一个具有全局所有权限的用户来登录Dashboard //编辑admin.yaml文件 cat > admin.yaml 然后粘贴上述生成的base64字符串登陆dashboard，在登陆页面选择令牌一项 登陆后的首界面 获取令牌命令 kubectl get secret `kubectl get secret -n kube-system|grep admin-token|awk '{print $1}'` -o jsonpath={.data.token} -n kube-system |base64 -d 2.6安装kuboard(可选) kuboard是Kubernetes 的一款图形化管理界面 安装kuboard kubectl apply -f https://kuboard.cn/install-script/kuboard.yaml kubectl apply -f https://addons.kuboard.cn/metrics-server/0.3.6/metrics-server.yaml 查看运行状态 #kubectl get pods -l k8s.eip.work/name=kuboard -n kube-system NAME READY STATUS RESTARTS AGE kuboard-756d46c4d4-tvhjq 1/1 Running 0 40s 获取token //获取管理员token kubectl -n kube-system get secret $(kubectl -n kube-system get secret | grep kuboard-user | awk '{print $1}') -o go-template='{{.data.token}}' | base64 -d && echo //获取只读token kubectl -n kube-system get secret $(kubectl -n kube-system get secret | grep kuboard-viewer | awk '{print $1}') -o go-template='{{.data.token}}' | base64 -d && echo 访问kuboard Kuboard Service 使用了 NodePort 的方式暴露服务，NodePort 为 32567 k8s集群的任意一个节点都可以访问到 登陆首界面 kuboard下载的镜像和启动的容器 #启动的容器 5d800a6ee8ad eipwork/kuboard \"/entrypoint.sh\" 9 minutes ago Up 9 minutes k8s_kuboard_kuboard-756d46c4d4-tvhjq_kube-system_6b88b7b5-64d8-4af3-8065-999b19722c86_0 #下载的镜像，本文中为1.0.8.2版本 eipwork/kuboard latest c6d652bbdf90 About an hour ago 180MB 到此，使用kubeadm安装k8s 1.16.3完成！！！ 1.16.3master启动的容器及下载的镜像 #下载的镜像 kubernetesui/metrics-scraper v1.0.3 3327f0dbcb4a 6 weeks ago 40.1MB kubernetesui/dashboard v2.0.0-beta8 eb51a3597525 3 months ago 90.8MB gcr.azk8s.cn/google_containers/kube-apiserver v1.16.3 df60c7526a3d 4 months ago 217MB gcr.azk8s.cn/google_containers/kube-proxy v1.16.3 9b65a0f78b09 4 months ago 86.1MB gcr.azk8s.cn/google_containers/kube-controller-manager v1.16.3 bb16442bcd94 4 months ago 163MB gcr.azk8s.cn/google_containers/kube-scheduler v1.16.3 98fecf43a54f 4 months ago 87.3MB calico/node v3.9.2 14a380c92c40 5 months ago 195MB calico/cni v3.9.2 c0d73dd53e71 5 months ago 160MB calico/pod2daemon-flexvol v3.9.2 523f0356e07b 5 months ago 9.78MB gcr.azk8s.cn/google_containers/etcd 3.3.15-0 b2756210eeab 6 months ago 247MB gcr.azk8s.cn/google_containers/coredns 1.6.2 bf261d157914 7 months ago 44.1MB kubernetesui/metrics-scraper v1.0.1 709901356c11 8 months ago 40.1MB gcr.azk8s.cn/google_containers/pause 3.1 da86e6ba6ca1 2 years ago 742kB #启动的容器 8bc24b355d78 kubernetesui/dashboard \"/dashboard --insecu…\" 19 minutes ago Up 19 minutes k8s_kubernetes-dashboard_kubernetes-dashboard-5996555fd8-2ppc5_kubernetes-dashboard_7f46632c-8b7b-41e4-aeb3-a1fbadd29782_0 4ebd793dc31f gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" 19 minutes ago Up 19 minutes k8s_POD_kubernetes-dashboard-5996555fd8-2ppc5_kubernetes-dashboard_7f46632c-8b7b-41e4-aeb3-a1fbadd29782_0 03df44242d90 calico/node \"start_runit\" 6 hours ago Up 6 hours k8s_calico-node_calico-node-89s9k_kube-system_ff5384c4-72b2-4bfa-b8ea-77f38c2a2112_0 c2a56feedc7c calico/pod2daemon-flexvol \"/usr/local/bin/flex…\" 6 hours ago Exited (0) 6 hours ago k8s_flexvol-driver_calico-node-89s9k_kube-system_ff5384c4-72b2-4bfa-b8ea-77f38c2a2112_0 ca9febcebaa8 c0d73dd53e71 \"/install-cni.sh\" 6 hours ago Exited (0) 6 hours ago k8s_install-cni_calico-node-89s9k_kube-system_ff5384c4-72b2-4bfa-b8ea-77f38c2a2112_0 b581894b1b91 calico/cni \"/opt/cni/bin/calico…\" 6 hours ago Exited (0) 6 hours ago k8s_upgrade-ipam_calico-node-89s9k_kube-system_ff5384c4-72b2-4bfa-b8ea-77f38c2a2112_0 2b58aa8cbc01 gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" 6 hours ago Up 6 hours k8s_POD_calico-node-89s9k_kube-system_ff5384c4-72b2-4bfa-b8ea-77f38c2a2112_0 8ff547ee27a4 9b65a0f78b09 \"/usr/local/bin/kube…\" 7 hours ago Up 7 hours k8s_kube-proxy_kube-proxy-zh6cz_kube-system_7a02864d-77cd-4636-ac70-aa56ad8c1df0_0 4c5be9765eaf gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" 7 hours ago Up 7 hours k8s_POD_kube-proxy-zh6cz_kube-system_7a02864d-77cd-4636-ac70-aa56ad8c1df0_0 09ae85c8b8f5 df60c7526a3d \"kube-apiserver --ad…\" 7 hours ago Up 7 hours k8s_kube-apiserver_kube-apiserver-k8s-master_kube-system_a8b1669c873ef7b41dacef818eb65aa9_0 6f17c0d51694 98fecf43a54f \"kube-scheduler --au…\" 7 hours ago Up 7 hours k8s_kube-scheduler_kube-scheduler-k8s-master_kube-system_8abfe947f8f4e810a2308b65bb933780_0 da743be3ec06 bb16442bcd94 \"kube-controller-man…\" 7 hours ago Up 7 hours k8s_kube-controller-manager_kube-controller-manager-k8s-master_kube-system_018dbc8aa2984b8851a80fc82254cb97_0 123d4d44a34f b2756210eeab \"etcd --advertise-cl…\" 7 hours ago Up 7 hours k8s_etcd_etcd-k8s-master_kube-system_6ba6d49a08f1a79cf377b8d0029e9a22_0 c4e04b936c8e gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" 7 hours ago Up 7 hours k8s_POD_kube-apiserver-k8s-master_kube-system_a8b1669c873ef7b41dacef818eb65aa9_0 f9411212bc50 gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" 7 hours ago Up 7 hours k8s_POD_kube-scheduler-k8s-master_kube-system_8abfe947f8f4e810a2308b65bb933780_0 20c18e3366ae gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" 7 hours ago Up 7 hours k8s_POD_etcd-k8s-master_kube-system_6ba6d49a08f1a79cf377b8d0029e9a22_0 1e74a3ddf17b gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" 7 hours ago Up 7 hours k8s_POD_kube-controller-manager-k8s-master_kube-system_018dbc8aa2984b8851a80fc82254cb97_0 1.16.3node启动的容器及下载的镜像 每个node都有的镜像 kube-proxy pause calico/node calico/cni calico/pod2daemon-flexvol coredns、calico/kube-controllers会随机在一个node上 //node1 #下载的镜像 gcr.azk8s.cn/google_containers/kube-proxy v1.16.3 9b65a0f78b09 4 months ago 86.1MB calico/node v3.9.2 14a380c92c40 5 months ago 195MB calico/cni v3.9.2 c0d73dd53e71 5 months ago 160MB calico/kube-controllers v3.9.2 7f7ed50db9fb 5 months ago 56MB calico/pod2daemon-flexvol v3.9.2 523f0356e07b 5 months ago 9.78MB gcr.azk8s.cn/google_containers/coredns 1.6.2 bf261d157914 7 months ago 44.1MB gcr.azk8s.cn/google_containers/pause 3.1 da86e6ba6ca1 2 years ago 742kB #启动的容器 958f160a7e79 calico/kube-controllers \"/usr/bin/kube-contr…\" 4 minutes ago Up 4 minutes k8s_calico-kube-controllers_calico-kube-controllers-dc6cb64cb-c6cng_kube-system_072a1596-42e8-4375-983c-fd6f9b173424_0 f3a7fbbb8e49 gcr.azk8s.cn/google_containers/coredns \"/coredns -conf /etc…\" 5 minutes ago Up 5 minutes k8s_coredns_coredns-667f964f9b-qz88x_kube-system_f18743bb-37af-4e8a-8268-9e328d3e0000_0 18150cf23c39 gcr.azk8s.cn/google_containers/coredns \"/coredns -conf /etc…\" 5 minutes ago Up 5 minutes k8s_coredns_coredns-667f964f9b-sdkc9_kube-system_712e4a15-923a-437b-a1dc-bfefaf30f920_0 e120da514c18 gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" 5 minutes ago Up 5 minutes k8s_POD_calico-kube-controllers-dc6cb64cb-c6cng_kube-system_072a1596-42e8-4375-983c-fd6f9b173424_24 b5e3c36224ad gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" 5 minutes ago Up 5 minutes k8s_POD_coredns-667f964f9b-qz88x_kube-system_f18743bb-37af-4e8a-8268-9e328d3e0000_23 c4cd7571e098 gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" 5 minutes ago Up 5 minutes k8s_POD_coredns-667f964f9b-sdkc9_kube-system_712e4a15-923a-437b-a1dc-bfefaf30f920_24 f497eb8c1b1a calico/node \"start_runit\" 5 minutes ago Up 5 minutes k8s_calico-node_calico-node-rcnts_kube-system_ebfce9a1-ad69-46e7-8e24-e9d3c51678bc_0 a55cab0bd81c calico/pod2daemon-flexvol \"/usr/local/bin/flex…\" 5 minutes ago Exited (0) 5 minutes ago k8s_flexvol-driver_calico-node-rcnts_kube-system_ebfce9a1-ad69-46e7-8e24-e9d3c51678bc_0 4b3f60e0d519 c0d73dd53e71 \"/install-cni.sh\" 6 minutes ago Exited (0) 6 minutes ago k8s_install-cni_calico-node-rcnts_kube-system_ebfce9a1-ad69-46e7-8e24-e9d3c51678bc_0 ee4493f6c482 calico/cni \"/opt/cni/bin/calico…\" 6 minutes ago Exited (0) 6 minutes ago k8s_upgrade-ipam_calico-node-rcnts_kube-system_ebfce9a1-ad69-46e7-8e24-e9d3c51678bc_0 87fe130ab10d gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" 6 minutes ago Up 6 minutes k8s_POD_calico-node-rcnts_kube-system_ebfce9a1-ad69-46e7-8e24-e9d3c51678bc_0 ba408469e7bc gcr.azk8s.cn/google_containers/kube-proxy \"/usr/local/bin/kube…\" 9 minutes ago Up 9 minutes k8s_kube-proxy_kube-proxy-q2tj7_kube-system_3c629796-9972-491a-87e7-3cc14264088e_0 272708e9251a gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" 9 minutes ago Up 9 minutes k8s_POD_kube-proxy-q2tj7_kube-system_3c629796-9972-491a-87e7-3cc14264088e_0 //node2 #下载的镜像 gcr.azk8s.cn/google_containers/kube-proxy v1.16.3 9b65a0f78b09 4 months ago 86.1MB calico/node v3.9.2 14a380c92c40 5 months ago 195MB calico/cni v3.9.2 c0d73dd53e71 5 months ago 160MB calico/pod2daemon-flexvol v3.9.2 523f0356e07b 5 months ago 9.78MB gcr.azk8s.cn/google_containers/pause 3.1 da86e6ba6ca1 2 years ago 742kB #启动的容器 acc3f199861e calico/node \"start_runit\" 6 minutes ago Up 6 minutes k8s_calico-node_calico-node-kdlz8_kube-system_e879923e-1028-4108-bdcc-29b8c7044127_0 8ff828ca0423 calico/pod2daemon-flexvol \"/usr/local/bin/flex…\" 6 minutes ago Exited (0) 6 minutes ago k8s_flexvol-driver_calico-node-kdlz8_kube-system_e879923e-1028-4108-bdcc-29b8c7044127_0 0bd0e1544cf7 c0d73dd53e71 \"/install-cni.sh\" 6 minutes ago Exited (0) 6 minutes ago k8s_install-cni_calico-node-kdlz8_kube-system_e879923e-1028-4108-bdcc-29b8c7044127_0 7bc2a912fda6 calico/cni \"/opt/cni/bin/calico…\" 6 minutes ago Exited (0) 6 minutes ago k8s_upgrade-ipam_calico-node-kdlz8_kube-system_e879923e-1028-4108-bdcc-29b8c7044127_0 1409a1dbc779 gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" 7 minutes ago Up 7 minutes k8s_POD_calico-node-kdlz8_kube-system_e879923e-1028-4108-bdcc-29b8c7044127_0 dda871e9daab gcr.azk8s.cn/google_containers/kube-proxy \"/usr/local/bin/kube…\" 10 minutes ago Up 10 minutes k8s_kube-proxy_kube-proxy-trqdv_kube-system_c572710b-0867-4baf-a629-8262f6511d15_0 a5c5c619ec67 gcr.azk8s.cn/google_containers/pause:3.1 \"/pause\" 10 minutes ago Up 10 minutes k8s_POD_kube-proxy-trqdv_kube-system_c572710b-0867-4baf-a629-8262f6511d15_0 k8s切换命名空间工具 git clone https://github.com.cnpmjs.org/ahmetb/kubectx cp kubectx/kubens /usr/local/bin #查看所有命名空间 kubens #切换到kube-system命名空间 kubens kube-system Context \"kubernetes-admin@kubernetes\" modified. Active namespace is \"kube-system\". 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s安装/3.使用 kubeadm 搭建 v1.17.4 版本 Kubernetes 集群.html":{"url":"linux/k8s/k8s安装/3.使用 kubeadm 搭建 v1.17.4 版本 Kubernetes 集群.html","title":"1.17","keywords":"","body":"[toc] 使用 kubeadm 搭建 v1.17.4 版本 Kubernetes 集群 一、环境准备 1.1实验环境 角色 IP地址 主机名 docker版本 硬件配置 系统 内核 master 10.0.0.130 k8s-master 19.03.4 2c2g CentOS7.6 3.10.0-957.el7.x86_64 node1 10.0.0.131 k8s-node1 19.03.4 2c1g CentOS7.6 3.10.0-957.el7.x86_64 node2 10.0.0.132 k8s-node2 19.03.4 2c1g CentOS7.6 3.10.0-957.el7.x86_64 1.2每个节点配置host信息 cat >> /etc/hosts 1.3禁用防火墙和selinux //禁用防火墙 systemctl stop firewalld && systemctl disable firewalld //禁用selinux #临时修改 setenforce 0 #永久修改，重启服务器后生效 sed -i '7s/enforcing/disabled/' /etc/selinux/config 1.4创建/etc/sysctl.d/k8s.conf文件，添加如下内容 //向文件中写入以下内容 cat >/etc/sysctl.d/k8s.conf 1.5安装ipvs(可选) 脚本创建了的/etc/sysconfig/modules/ipvs.modules文件，保证在节点重启后能自动加载所需模块。使用lsmod | grep -e ip_vs -e nf_conntrack_ipv4命令查看是否已经正确加载所需的内核模块 //向文件中写入以下内容 cat > /etc/sysconfig/modules/ipvs.modules 安装ipset和ipvsadm(便于查看 ipvs 的代理规则) yum -y install ipset ipvsadm 1.6同步服务器时间 //安装chrony yum -y install chrony //修改同步服务器地址为阿里云 sed -i.bak '3,6d' /etc/chrony.conf && sed -i '3cserver ntp1.aliyun.com iburst' \\ /etc/chrony.conf //启动chronyd及加入开机自启 systemctl start chronyd && systemctl enable chronyd //查看同步结果 chronyc sources 210 Number of sources = 1 MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== ^* 120.25.115.20 2 6 37 13 +194us[+6131us] +/- 33ms 1.7关闭swap分区 修改/etc/fstab文件，注释掉 SWAP 的自动挂载，使用free -m确认 swap 已经关闭 //手动关闭swap swapoff -a //修改fstab文件，注释swap自动挂载 sed -i '/^\\/dev\\/mapper\\/centos-swap/c#/dev/mapper/centos-swap swap swap defaults 0 0' /etc/fstab //查看swap是否关闭 free -m total used free shared buff/cache available Mem: 1994 682 612 9 699 1086 Swap: 0 0 0 swappiness 参数调整，修改/etc/sysctl.d/k8s.conf添加下面一行 cat >>/etc/sysctl.d/k8s.conf 1.8安装docker19.03.4 1.卸载旧版本 yum remove -y docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine 2.安装依赖包 yum install -y yum-utils device-mapper-persistent-data lvm2 3.添加阿里云yum源 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 4.查看可用版本 yum list docker-ce --showduplicates | sort -r 已加载插件：fastestmirror, langpacks 可安装的软件包 * updates: mirrors.aliyun.com Loading mirror speeds from cached hostfile * extras: mirrors.aliyun.com docker-ce.x86_64 3:19.03.5-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.4-3.el7 docker-ce-stable 。。。。。。 docker-ce.x86_64 3:18.09.9-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.8-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.7-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.6-3.el7 docker-ce-stable 。。。。。。 5.安装docker19.03.4 yum install -y docker-ce-19.03.4 docker-ce-cli-19.03.4 containerd.io 6.启动docker并设置开机自启 systemctl enable docker && systemctl start docker 7.配置阿里云docker镜像加速 cat > /etc/docker/daemon.json 1.9安装Kubeadm 需要科学上网 cat >/etc/yum.repos.d/kubernetes.repo 使用阿里云yum源 cat >/etc/yum.repos.d/kubernetes.repo 安装 kubeadm、kubelet、kubectl(阿里云yum源会随官方更新最新版，因此指定版本) yum -y install kubelet-1.17.4 kubeadm-1.17.4 kubectl-1.17.4 //查看版本 kubeadm version kubeadm version: &version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.4\", GitCommit:\"8d8aa39598534325ad77120c120a22b3a990b5ea\", GitTreeState:\"clean\", BuildDate:\"2020-03-12T21:01:11Z\", GoVersion:\"go1.13.8\", Compiler:\"gc\", Platform:\"linux/amd64\"} 设置kubelet开机自启 ⚠️此时kubelet是无法启动的，因为只有完成master的kubeadm init 的操作，kubelet才能正常启动 systemctl enable kubelet 设置k8s命令自动补全 yum -y install bash-completion source /usr/share/bash-completion/bash_completion source > ~/.bashrc 1.10修改docker Cgroup Driver为systemd #修改docker Cgroup Driver为systemd 将/usr/lib/systemd/system/docker.service文件中的这一行 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock 修改为 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd 如果不修改，在添加 worker 节点时可能会碰到如下错误 [WARNING IsDockerSystemdCheck]: detected \"cgroupfs\" as the Docker cgroup driver. The recommended driver is \"systemd\". Please follow the guide at https://kubernetes.io/docs/setup/cri/ //使用如下命令修改 sed -i.bak \"s#^ExecStart=/usr/bin/dockerd.*#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd#g\" /usr/lib/systemd/system/docker.service //重启docker systemctl daemon-reload && systemctl restart docker 到此，基本环境安装完成!!! 二、初始化集群 2.1master节点操作，配置 kubeadm 初始化文件 方法一 命令初始化 kubeadm init --apiserver-advertise-address=10.0.0.130 --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.17.4 --service-cidr=10.96.0.0/16 --pod-network-cidr=10.20.0.0/16 --apiserver-advertise-address= #master节点IP --image-repository registry.aliyuncs.com/google_containers #镜像仓库 --kubernetes-version v1.17.4 #k8s版本 --service-cidr=10.96.0.0/16 #service IP网段 --pod-network-cidr=10.20.0.0/16 #pod IP网段，后续网络插件会用到 方法二 文件初始化 cat > kubeadm.yaml 2.2初始化master kubeadm init --config kubeadm.yaml 完整输出结果 W0319 22:59:56.631407 12367 validation.go:28] Cannot validate kubelet config - no validator is available W0319 22:59:56.631444 12367 validation.go:28] Cannot validate kube-proxy config - no validator is available [init] Using Kubernetes version: v1.17.4 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using 'kubeadm config images pull' [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Starting the kubelet [certs] Using certificateDir folder \"/etc/kubernetes/pki\" [certs] Generating \"ca\" certificate and key [certs] Generating \"apiserver\" certificate and key [certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.0.0.130] [certs] Generating \"apiserver-kubelet-client\" certificate and key [certs] Generating \"front-proxy-ca\" certificate and key [certs] Generating \"front-proxy-client\" certificate and key [certs] Generating \"etcd/ca\" certificate and key [certs] Generating \"etcd/server\" certificate and key [certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [10.0.0.130 127.0.0.1 ::1] [certs] Generating \"etcd/peer\" certificate and key [certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [10.0.0.130 127.0.0.1 ::1] [certs] Generating \"etcd/healthcheck-client\" certificate and key [certs] Generating \"apiserver-etcd-client\" certificate and key [certs] Generating \"sa\" key and public key [kubeconfig] Using kubeconfig folder \"/etc/kubernetes\" [kubeconfig] Writing \"admin.conf\" kubeconfig file [kubeconfig] Writing \"kubelet.conf\" kubeconfig file [kubeconfig] Writing \"controller-manager.conf\" kubeconfig file [kubeconfig] Writing \"scheduler.conf\" kubeconfig file [control-plane] Using manifest folder \"/etc/kubernetes/manifests\" [control-plane] Creating static Pod manifest for \"kube-apiserver\" [control-plane] Creating static Pod manifest for \"kube-controller-manager\" W0319 23:01:45.692056 12367 manifests.go:214] the default kube-apiserver authorization-mode is \"Node,RBAC\"; using \"Node,RBAC\" [control-plane] Creating static Pod manifest for \"kube-scheduler\" W0319 23:01:45.692762 12367 manifests.go:214] the default kube-apiserver authorization-mode is \"Node,RBAC\"; using \"Node,RBAC\" [etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\" [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s [apiclient] All control plane components are healthy after 16.502774 seconds [upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [kubelet] Creating a ConfigMap \"kubelet-config-1.17\" in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --upload-certs [mark-control-plane] Marking the node k8s-master as control-plane by adding the label \"node-role.kubernetes.io/master=''\" [mark-control-plane] Marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [bootstrap-token] Using token: abcdef.0123456789abcdef [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace [kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10.0.0.130:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:27b37276c4451b54c99106f73cb38c3b0bb6e2e178c142808e9ff00300ac7eaf #kubeadm init --config kubeadm.yaml初始化master下载的镜像 registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy v1.17.4 6dec7cfde1e5 6 days ago 116MB registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver v1.17.4 2e1ba57fe95a 6 days ago 171MB registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager v1.17.4 7f997fcf3e94 6 days ago 161MB registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler v1.17.4 5db16c1c7aff 6 days ago 94.4MB registry.cn-hangzhou.aliyuncs.com/google_containers/coredns 1.6.5 70f311871ae1 4 months ago 41.6MB registry.cn-hangzhou.aliyuncs.com/google_containers/etcd 3.4.3-0 303ce5db0e90 4 months ago 288MB registry.cn-hangzhou.aliyuncs.com/google_containers/pause 3.1 da86e6ba6ca1 2 years ago 742kB #启动的容器 5220dac932a8 6dec7cfde1e5 \"/usr/local/bin/kube…\" 2 minutes ago Up 2 minutes k8s_kube-proxy_kube-proxy-jlwfd_kube-system_5af2149d-dc6c-4a6b-a99b-fb0af734d1fb_0 20a9003d1a55 registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \"/pause\" 2 minutes ago Up 2 minutes k8s_POD_kube-proxy-jlwfd_kube-system_5af2149d-dc6c-4a6b-a99b-fb0af734d1fb_0 48c86b9adbf1 7f997fcf3e94 \"kube-controller-man…\" 3 minutes ago Up 3 minutes k8s_kube-controller-manager_kube-controller-manager-k8s-master_kube-system_c06ae2244175d69b82dd8327536a3f47_0 beed6b3671bd 5db16c1c7aff \"kube-scheduler --au…\" 3 minutes ago Up 3 minutes k8s_kube-scheduler_kube-scheduler-k8s-master_kube-system_0621ae8690c69d1d72f746bc2de0667e_0 fe47bd324d10 2e1ba57fe95a \"kube-apiserver --ad…\" 3 minutes ago Up 3 minutes k8s_kube-apiserver_kube-apiserver-k8s-master_kube-system_ffa9ef4a4e85500fbc6d35b5a88efb43_0 cfbf4d126eae 303ce5db0e90 \"etcd --advertise-cl…\" 3 minutes ago Up 3 minutes k8s_etcd_etcd-k8s-master_kube-system_ba1d164d5064efe1d13e5645859a6dec_0 aff7b239f5ef registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \"/pause\" 3 minutes ago Up 3 minutes k8s_POD_kube-scheduler-k8s-master_kube-system_0621ae8690c69d1d72f746bc2de0667e_0 1266cf8da7d9 registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \"/pause\" 3 minutes ago Up 3 minutes k8s_POD_kube-controller-manager-k8s-master_kube-system_c06ae2244175d69b82dd8327536a3f47_0 7415d07612a9 registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \"/pause\" 3 minutes ago Up 3 minutes k8s_POD_kube-apiserver-k8s-master_kube-system_ffa9ef4a4e85500fbc6d35b5a88efb43_0 fef33ef8bb46 registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \"/pause\" 3 minutes ago Up 3 minutes k8s_POD_etcd-k8s-master_kube-system_ba1d164d5064efe1d13e5645859a6dec_0 拷贝 kubeconfig 文件 //这里的路径为/root mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 2.3master添加节点 node1和node2相同操作 将master节点上的$HOME/.kube/config 文件拷贝到node节点对应的文件中 1.创建目录，这里的路径为/root mkdir -p $HOME/.kube 2.把master节点上的config文件拷贝到node1和node2的$HOME/.kube scp k8s-master:~/.kube/config $HOME/.kube 3.修改权限 chown $(id -u):$(id -g) $HOME/.kube/config 将node1和node2加入到集群中 这里需要用到2.2中初始化master最后生成的token和sha256值 kubeadm join 10.0.0.130:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:27b37276c4451b54c99106f73cb38c3b0bb6e2e178c142808e9ff00300ac7eaf 输出结果 W0319 23:06:51.048079 4137 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set. [preflight] Running pre-flight checks [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [kubelet-start] Downloading configuration for the kubelet from the \"kubelet-config-1.17\" ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Starting the kubelet [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster. 将node节点加入到k8s集群后，node节点会运行kube-proxy #下载的镜像 registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy v1.17.4 6dec7cfde1e5 6 days ago 116MB registry.cn-hangzhou.aliyuncs.com/google_containers/pause 3.1 da86e6ba6ca1 2 years ago 742kB #启动的容器 f8dff6a9caf5 registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy \"/usr/local/bin/kube…\" About a minute ago Up About a minute k8s_kube-proxy_kube-proxy-g8lfq_kube-system_72e60b6a-0023-4d25-b8e0-73e98bb4b1ea_0 64752d1e78b3 registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \"/pause\" 2 minutes ago Up 2 minutes k8s_POD_kube-proxy-g8lfq_kube-system_72e60b6a-0023-4d25-b8e0-73e98bb4b1ea_0 如果忘记了token和sha256值，可以在master节点使用如下命令查看 //查看token kubeadm token list TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS abcdef.0123456789abcdef 23h 2020-03-20T23:02:03+08:00 authentication,signing system:bootstrappers:kubeadm:default-node-token //查看sha256 openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' 27b37276c4451b54c99106f73cb38c3b0bb6e2e178c142808e9ff00300ac7eaf //同时查看token和sha256 kubeadm token create --print-join-command W0319 23:07:44.720182 16201 validation.go:28] Cannot validate kube-proxy config - no validator is available W0319 23:07:44.720215 16201 validation.go:28] Cannot validate kubelet config - no validator is available kubeadm join 10.0.0.130:6443 --token 36t7ur.rtigrd344emr5urd --discovery-token-ca-cert-hash sha256:27b37276c4451b54c99106f73cb38c3b0bb6e2e178c142808e9ff00300ac7eaf master节点查看node，发现状态都是NotReady，因为还没有安装网络插件，这里我们安装calio官方插件文档 kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master NotReady master 19m v1.15.3 k8s-node1 NotReady 4m10s v1.15.3 k8s-node2 NotReady 4m3s v1.15.3 2.4master节点安装网络插件calio //下载文件 wget https://kuboard.cn/install-script/calico/calico-3.13.1.yaml https://docs.projectcalico.org //修改完成后安装calico网络插件 kubectl apply -f calico-3.13.1.yaml //安装完成后稍等一会查看pods状态 kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-788d6b9876-4lls7 1/1 Running 0 5m58s calico-node-jfn5g 1/1 Running 0 5m59s calico-node-k6f84 1/1 Running 0 5m59s calico-node-vm4m6 1/1 Running 0 5m59s coredns-7f9c544f75-5p2mf 1/1 Running 0 30m coredns-7f9c544f75-prdlb 1/1 Running 0 30m etcd-k8s-master 1/1 Running 0 30m kube-apiserver-k8s-master 1/1 Running 0 30m kube-controller-manager-k8s-master 1/1 Running 0 30m kube-proxy-g8lfq 1/1 Running 0 25m kube-proxy-jlwfd 1/1 Running 0 30m kube-proxy-x8twc 1/1 Running 0 25m kube-scheduler-k8s-master 1/1 Running 0 30m //查看node状态 kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready master 30m v1.17.4 k8s-node1 Ready 25m v1.17.4 k8s-node2 Ready 25m v1.17.4 #kubectl apply -f calico-3.13.1.yaml安装网络插件calico下载的镜像 calico/node v3.13.1 2e5029b93d4a 6 days ago 260MB calico/pod2daemon-flexvol v3.13.1 e8c600448aae 6 days ago 111MB calico/cni v3.13.1 6912ec2cfae6 6 days ago 207MB #启动的容器 70d8a967cc69 calico/node \"start_runit\" 2 minutes ago Up 2 minutes k8s_calico-node_calico-node-jfn5g_kube-system_b86bd096-addf-4665-8a72-ec7c9e4a97f9_0 f9830c017734 calico/pod2daemon-flexvol \"/usr/local/bin/flex…\" 4 minutes ago Exited (0) 4 minutes ago k8s_flexvol-driver_calico-node-jfn5g_kube-system_b86bd096-addf-4665-8a72-ec7c9e4a97f9_0 8fe868ba10d4 6912ec2cfae6 \"/install-cni.sh\" 5 minutes ago Exited (0) 5 minutes ago k8s_install-cni_calico-node-jfn5g_kube-system_b86bd096-addf-4665-8a72-ec7c9e4a97f9_0 ae71689904ca calico/cni \"/opt/cni/bin/calico…\" 5 minutes ago Exited (0) 5 minutes ago k8s_upgrade-ipam_calico-node-jfn5g_kube-system_b86bd096-addf-4665-8a72-ec7c9e4a97f9_0 node节点 #都有的镜像 calico/node calico/pod2daemon-flexvol calico/cni #calico/kube-controllers 、registry.cn-hangzhou.aliyuncs.com/google_containers/coredns会随机在一个node上 2.5安装Dashboard 下载文件及修改内容 这里查看dashboard对应的k8s版本 //下载文件 v2.0.0-rc3是中文版本，beta8是英文版本 wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc3/aio/deploy/recommended.yaml //修改Service为NodePort类型 42行下增加一行 nodePort: 30001 44行下增加一行 type: NodePort //原先内容 spec: ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard //修改后内容 spec: ports: - port: 443 targetPort: 8443 nodePort: 30001 #增加，指定nodeport端口 selector: k8s-app: kubernetes-dashboard type: NodePort #增加，修改类型为nodeport 部署dashboard #kubectl apply -f recommended.yaml #下载的镜像 kubernetesui/dashboard v2.0.0-rc3 4a0a1cf1b459 6 weeks ago 126MB #启动的容器 62a551c0a0e6 kubernetesui/dashboard \"/dashboard --insecu…\" 9 seconds ago Up 8 seconds k8s_kubernetes-dashboard_kubernetes-dashboard-7867cbccbb-z9vzj_kubernetes-dashboard_de95d4c3-6234-4f7a-8d80-d326c52b65c0_0 查看dashboard的运行状态及外网访问端口 //查看dashboard运行状态 kubectl get pods -n kubernetes-dashboard -l k8s-app=kubernetes-dashboard NAME READY STATUS RESTARTS AGE kubernetes-dashboard-7867cbccbb-z9vzj 1/1 Running 0 50s //查看dashboard外网访问端口 kubectl get svc -n kubernetes-dashboard -l k8s-app=kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard NodePort 10.96.222.222 443:30001/TCP 82s 通过上边的30001端口访问dashboard，注意是https ⚠️k8s1.16.3这个版本中，使用的dashboard版本是2.0.0-beta8，只有火狐浏览器可以访问，其余浏览器都不能访问，会报错如下 只有火狐浏览器可以！ 然后创建一个具有全局所有权限的用户来登录Dashboard //编辑admin.yaml文件 cat > admin.yaml 然后粘贴上述生成的base64字符串登陆dashboard，在登陆页面选择令牌一项 登陆后的首界面 2.6安装kuboard(可选) kuboard是Kubernetes 的一款图形化管理界面 安装kuboard kubectl apply -f https://kuboard.cn/install-script/kuboard.yaml kubectl apply -f https://addons.kuboard.cn/metrics-server/0.3.6/metrics-server.yaml 查看运行状态 #kubectl get pods -l k8s.eip.work/name=kuboard -n kube-system NAME READY STATUS RESTARTS AGE kuboard-756d46c4d4-tvhjq 1/1 Running 0 40s 获取token //获取管理员token kubectl -n kube-system get secret $(kubectl -n kube-system get secret | grep kuboard-user | awk '{print $1}') -o go-template='{{.data.token}}' | base64 -d && echo //获取只读token kubectl -n kube-system get secret $(kubectl -n kube-system get secret | grep kuboard-viewer | awk '{print $1}') -o go-template='{{.data.token}}' | base64 -d && echo 访问kuboard Kuboard Service 使用了 NodePort 的方式暴露服务，NodePort 为 32567 k8s集群的任意一个节点都可以访问到 kuboard下载的镜像和启动的容器 #启动的容器 11171ab07891 eipwork/kuboard \"/entrypoint.sh\" 3 minutes ago Up 3 minutes k8s_kuboard_kuboard-756d46c4d4-jfx4t_kube-system_8f509c5b-fbee-40b0-8d74-9f2b8485639a_0d #下载的镜像，本文中为1.0.8.2版本 eipwork/kuboard latest c6d652bbdf90 About an hour ago 180MB 到此，使用kubeadm安装k8s 1.17.4完成！！！ 获取令牌命令 kubectl get secret `kubectl get secret -n kube-system|grep admin-token|awk '{print $1}'` -o jsonpath={.data.token} -n kube-system |base64 -d && echo 1.17.4master启动的容器及下载的镜像 #启动的容器 11171ab07891 eipwork/kuboard \"/entrypoint.sh\" 4 minutes ago Up 4 minutes k8s_kuboard_kuboard-756d46c4d4-jfx4t_kube-system_8f509c5b-fbee-40b0-8d74-9f2b8485639a_0 f8e5d82cf8ee registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \"/pause\" 6 minutes ago Up 6 minutes k8s_POD_kuboard-756d46c4d4-jfx4t_kube-system_8f509c5b-fbee-40b0-8d74-9f2b8485639a_0 62a551c0a0e6 kubernetesui/dashboard \"/dashboard --insecu…\" 10 hours ago Up 10 hours k8s_kubernetes-dashboard_kubernetes-dashboard-7867cbccbb-z9vzj_kubernetes-dashboard_de95d4c3-6234-4f7a-8d80-d326c52b65c0_0 40ad4724f6f1 registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \"/pause\" 10 hours ago Up 10 hours k8s_POD_kubernetes-dashboard-7867cbccbb-z9vzj_kubernetes-dashboard_de95d4c3-6234-4f7a-8d80-d326c52b65c0_0 70d8a967cc69 calico/node \"start_runit\" 10 hours ago Up 10 hours k8s_calico-node_calico-node-jfn5g_kube-system_b86bd096-addf-4665-8a72-ec7c9e4a97f9_0 f9830c017734 calico/pod2daemon-flexvol \"/usr/local/bin/flex…\" 10 hours ago Exited (0) 10 hours ago k8s_flexvol-driver_calico-node-jfn5g_kube-system_b86bd096-addf-4665-8a72-ec7c9e4a97f9_0 8fe868ba10d4 6912ec2cfae6 \"/install-cni.sh\" 10 hours ago Exited (0) 10 hours ago k8s_install-cni_calico-node-jfn5g_kube-system_b86bd096-addf-4665-8a72-ec7c9e4a97f9_0 ae71689904ca calico/cni \"/opt/cni/bin/calico…\" 10 hours ago Exited (0) 10 hours ago k8s_upgrade-ipam_calico-node-jfn5g_kube-system_b86bd096-addf-4665-8a72-ec7c9e4a97f9_0 cd04477d7393 registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \"/pause\" 10 hours ago Up 10 hours k8s_POD_calico-node-jfn5g_kube-system_b86bd096-addf-4665-8a72-ec7c9e4a97f9_0 5220dac932a8 6dec7cfde1e5 \"/usr/local/bin/kube…\" 10 hours ago Up 10 hours k8s_kube-proxy_kube-proxy-jlwfd_kube-system_5af2149d-dc6c-4a6b-a99b-fb0af734d1fb_0 20a9003d1a55 registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \"/pause\" 10 hours ago Up 10 hours k8s_POD_kube-proxy-jlwfd_kube-system_5af2149d-dc6c-4a6b-a99b-fb0af734d1fb_0 48c86b9adbf1 7f997fcf3e94 \"kube-controller-man…\" 10 hours ago Up 10 hours k8s_kube-controller-manager_kube-controller-manager-k8s-master_kube-system_c06ae2244175d69b82dd8327536a3f47_0 beed6b3671bd 5db16c1c7aff \"kube-scheduler --au…\" 10 hours ago Up 10 hours k8s_kube-scheduler_kube-scheduler-k8s-master_kube-system_0621ae8690c69d1d72f746bc2de0667e_0 fe47bd324d10 2e1ba57fe95a \"kube-apiserver --ad…\" 10 hours ago Up 10 hours k8s_kube-apiserver_kube-apiserver-k8s-master_kube-system_ffa9ef4a4e85500fbc6d35b5a88efb43_0 cfbf4d126eae 303ce5db0e90 \"etcd --advertise-cl…\" 10 hours ago Up 10 hours k8s_etcd_etcd-k8s-master_kube-system_ba1d164d5064efe1d13e5645859a6dec_0 aff7b239f5ef registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \"/pause\" 10 hours ago Up 10 hours k8s_POD_kube-scheduler-k8s-master_kube-system_0621ae8690c69d1d72f746bc2de0667e_0 1266cf8da7d9 registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \"/pause\" 10 hours ago Up 10 hours k8s_POD_kube-controller-manager-k8s-master_kube-system_c06ae2244175d69b82dd8327536a3f47_0 7415d07612a9 registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \"/pause\" 10 hours ago Up 10 hours k8s_POD_kube-apiserver-k8s-master_kube-system_ffa9ef4a4e85500fbc6d35b5a88efb43_0 fef33ef8bb46 registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \"/pause\" 10 hours ago Up 10 hours k8s_POD_etcd-k8s-master_kube-system_ba1d164d5064efe1d13e5645859a6dec_0 #下载的镜像 eipwork/kuboard latest c6d652bbdf90 2 days ago 180MB registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy v1.17.4 6dec7cfde1e5 7 days ago 116MB registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver v1.17.4 2e1ba57fe95a 7 days ago 171MB registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager v1.17.4 7f997fcf3e94 7 days ago 161MB registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler v1.17.4 5db16c1c7aff 7 days ago 94.4MB calico/node v3.13.1 2e5029b93d4a 7 days ago 260MB calico/pod2daemon-flexvol v3.13.1 e8c600448aae 7 days ago 111MB calico/cni v3.13.1 6912ec2cfae6 7 days ago 207MB kubernetesui/dashboard v2.0.0-rc3 4a0a1cf1b459 7 weeks ago 126MB registry.cn-hangzhou.aliyuncs.com/google_containers/coredns 1.6.5 70f311871ae1 4 months ago 41.6MB registry.cn-hangzhou.aliyuncs.com/google_containers/etcd 3.4.3-0 303ce5db0e90 4 months ago 288MB registry.cn-hangzhou.aliyuncs.com/google_containers/pause 3.1 da86e6ba6ca1 2 years ago 742kB 1.17.4node启动的容器和下载的镜像 #启动的容器 59b6a4cd2f44 registry.cn-hangzhou.aliyuncs.com/google_containers/coredns \"/coredns -conf /etc…\" 10 hours ago Up 10 hours k8s_coredns_coredns-7f9c544f75-prdlb_kube-system_77f42046-8348-4247-b37b-6ce3dc62db18_0 c73eac71d7e4 calico/kube-controllers \"/usr/bin/kube-contr…\" 10 hours ago Up 10 hours k8s_calico-kube-controllers_calico-kube-controllers-788d6b9876-4lls7_kube-system_25992c41-0bfe-4e84-8a04-70b63c969627_0 947909b16d67 registry.cn-hangzhou.aliyuncs.com/google_containers/coredns \"/coredns -conf /etc…\" 10 hours ago Up 10 hours k8s_coredns_coredns-7f9c544f75-5p2mf_kube-system_305e69d6-6ae6-4c48-a32d-6dddc290fa8a_0 e2b467f4fb14 registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \"/pause\" 10 hours ago Up 10 hours k8s_POD_coredns-7f9c544f75-prdlb_kube-system_77f42046-8348-4247-b37b-6ce3dc62db18_104 a5683b681459 registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \"/pause\" 10 hours ago Up 10 hours k8s_POD_coredns-7f9c544f75-5p2mf_kube-system_305e69d6-6ae6-4c48-a32d-6dddc290fa8a_105 11a2725d9030 registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \"/pause\" 10 hours ago Up 10 hours k8s_POD_calico-kube-controllers-788d6b9876-4lls7_kube-system_25992c41-0bfe-4e84-8a04-70b63c969627_104 a0064abf6347 calico/node \"start_runit\" 10 hours ago Up 10 hours k8s_calico-node_calico-node-vm4m6_kube-system_de38b605-5188-4db0-8123-061bb253be9d_0 ad86e37926e4 calico/pod2daemon-flexvol \"/usr/local/bin/flex…\" 10 hours ago Exited (0) 10 hours ago k8s_flexvol-driver_calico-node-vm4m6_kube-system_de38b605-5188-4db0-8123-061bb253be9d_0 0946ccc79ba8 6912ec2cfae6 \"/install-cni.sh\" 10 hours ago Exited (0) 10 hours ago k8s_install-cni_calico-node-vm4m6_kube-system_de38b605-5188-4db0-8123-061bb253be9d_0 fee74eaf355d calico/cni \"/opt/cni/bin/calico…\" 10 hours ago Exited (0) 10 hours ago k8s_upgrade-ipam_calico-node-vm4m6_kube-system_de38b605-5188-4db0-8123-061bb253be9d_0 7635ac40c020 registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \"/pause\" 10 hours ago Up 10 hours k8s_POD_calico-node-vm4m6_kube-system_de38b605-5188-4db0-8123-061bb253be9d_0 f8dff6a9caf5 registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy \"/usr/local/bin/kube…\" 10 hours ago Up 10 hours k8s_kube-proxy_kube-proxy-g8lfq_kube-system_72e60b6a-0023-4d25-b8e0-73e98bb4b1ea_0 64752d1e78b3 registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \"/pause\" 10 hours ago Up 10 hours k8s_POD_kube-proxy-g8lfq_kube-system_72e60b6a-0023-4d25-b8e0-73e98bb4b1ea_0 #下载的镜像 registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy v1.17.4 6dec7cfde1e5 7 days ago 116MB calico/node v3.13.1 2e5029b93d4a 7 days ago 260MB calico/pod2daemon-flexvol v3.13.1 e8c600448aae 7 days ago 111MB calico/cni v3.13.1 6912ec2cfae6 7 days ago 207MB calico/kube-controllers v3.13.1 3971f13f2c6c 7 days ago 56.6MB registry.cn-hangzhou.aliyuncs.com/google_containers/coredns 1.6.5 70f311871ae1 4 months ago 41.6MB registry.cn-hangzhou.aliyuncs.com/google_containers/pause 3.1 da86e6ba6ca1 2 years ago 742kB k8s切换命名空间工具 git clone https://github.com.cnpmjs.org/ahmetb/kubectx cp kubectx/kubens /usr/local/bin #查看所有命名空间 kubens #切换到kube-system命名空间 kubens kube-system Context \"kubernetes-admin@kubernetes\" modified. Active namespace is \"kube-system\". 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s安装/5.使用 kubeadm 搭建 v1.19.3 版本 Kubernetes 集群.html":{"url":"linux/k8s/k8s安装/5.使用 kubeadm 搭建 v1.19.3 版本 Kubernetes 集群.html","title":"1.19","keywords":"","body":"[toc] 使用 kubeadm 搭建 v1.19.3 版本 Kubernetes 集群 一、环境准备 1.1 实验环境 角色 IP地址 主机名 docker版本 硬件配置 系统 内核 master 10.0.0.130 k8s-master1 19.03.11 2C4G CentOS7.8 5.9.1-1.el7.elrepo.x86_64 node1 10.0.0.131 k8s-node1 19.03.11 2C4G CentOS7.8 5.9.1-1.el7.elrepo.x86_64 node2 10.0.0.132 k8s-node2 19.03.11 2C4G CentOS7.8 5.9.1-1.el7.elrepo.x86_64 1.2 每个节点配置host信息 cat >> /etc/hosts 1.3 禁用防火墙和selinux # 禁用防火墙 systemctl stop firewalld && systemctl disable firewalld # 禁用selinux # 临时修改 setenforce 0 # 永久修改，重启服务器后生效 sed -i '7s/enforcing/disabled/' /etc/selinux/config 1.4 创建 /etc/sysctl.d/k8s.conf 文件，添加如下内容 # 向文件中写入以下内容 cat >/etc/sysctl.d/k8s.conf bridge-nf 说明 bridge-nf 使得 netfilter 可以对 Linux 网桥上的 IPv4/ARP/IPv6 包过滤。比如，设置 net.bridge.bridge-nf-call-iptables＝1 后，二层的网桥在转发包时也会被 iptables的 FORWARD 规则所过滤。常用的选项包括： net.bridge.bridge-nf-call-arptables：是否在 arptables 的 FORWARD 中过滤网桥的 ARP 包 net.bridge.bridge-nf-call-ip6tables：是否在 ip6tables 链中过滤 IPv6 包 net.bridge.bridge-nf-call-iptables：是否在 iptables 链中过滤 IPv4 包 net.bridge.bridge-nf-filter-vlan-tagged：是否在 iptables/arptables 中过滤打了 vlan 标签的包。 1.5 安装ipvs 脚本创建的/etc/sysconfig/modules/ipvs.modules文件，保证在节点重启后能自动加载所需模块。使用lsmod | grep -e ip_vs -e nf_conntrack_ipv4命令查看是否已经正确加载所需的内核模块 ⚠️由于内核使用的是最新的版本5.9.1(发布日期2020.10.17)，因此无法加载nf_conntrack_ipv4 模块 google到的答案 看这个isseu # 向文件中写入以下内容 cat > /etc/sysconfig/modules/ipvs.modules 安装ipset和ipvsadm(便于查看 ipvs 的代理规则) yum -y install ipset ipvsadm 1.6 同步服务器时间 master节点操作 # 安装chrony yum -y install chrony # 修改同步服务器地址为阿里云 sed -i.bak '3,6d' /etc/chrony.conf && sed -i -e '3cserver ntp1.aliyun.com iburst' -i -e '/^#allow/callow 10.0.0.0/24' \\ /etc/chrony.conf # 启动chronyd及加入开机自启 systemctl start chronyd && systemctl enable chronyd # 查看同步结果 $ chronyc sources 210 Number of sources = 1 MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== ^* 120.25.115.20 2 6 37 13 +194us[+6131us] +/- 33ms node节点操作 # 安装chrony yum -y install chrony # 修改同步服务器地址为k8s-master1 sed -i.bak '3,6d' /etc/chrony.conf && sed -i '3cserver k8s-master1 iburst' /etc/chrony.conf # 启动chronyd及加入开机自启 systemctl start chronyd && systemctl enable chronyd # 查看同步结果 $ chronyc sources 210 Number of sources = 1 MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== ^* k8s-master1 3 6 17 5 +14us[ -17us] +/- 29ms 1.7 关闭swap分区 修改/etc/fstab文件，注释掉 SWAP 的自动挂载，使用free -m确认 swap 已经关闭 # 手动关闭swap swapoff -a # 修改fstab文件，注释swap自动挂载 sed -i '/^\\/dev\\/mapper\\/centos-swap/c#/dev/mapper/centos-swap swap swap defaults 0 0' /etc/fstab # 查看swap是否关闭 $ free -m total used free shared buff/cache available Mem: 1994 682 612 9 699 1086 Swap: 0 0 0 swappiness 参数调整，修改/etc/sysctl.d/k8s.conf添加下面一行 # 向文件中追加内容 cat >> /etc/sysctl.d/k8s.conf 1.8 安装docker19.03.11 k8s-v1.19.3官方推荐安装的docker版本是19.03.11 1.卸载旧版本 yum remove -y docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine 2.安装依赖包 yum install -y yum-utils device-mapper-persistent-data lvm2 3.添加阿里云yum源 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 4.查看可用版本 yum list docker-ce --showduplicates | sort -r 已加载插件：fastestmirror, langpacks 可安装的软件包 * updates: mirrors.aliyun.com Loading mirror speeds from cached hostfile * extras: mirrors.aliyun.com docker-ce.x86_64 3:19.03.5-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.4-3.el7 docker-ce-stable 。。。。。。 docker-ce.x86_64 3:18.09.9-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.8-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.7-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.6-3.el7 docker-ce-stable 。。。。。。 5.安装docker19.03.11 yum install -y docker-ce-19.03.11 docker-ce-cli-19.03.11 containerd.io 6.启动docker并设置开机自启 systemctl enable docker && systemctl start docker 7.配置阿里云docker镜像加速 cat > /etc/docker/daemon.json 1.9 安装Kubeadm 需要科学上网 cat >/etc/yum.repos.d/kubernetes.repo 使用阿里云yum源 cat >/etc/yum.repos.d/kubernetes.repo 安装 kubeadm、kubelet、kubectl(阿里云yum源会随官方更新最新版，因此指定版本) # 指定版本安装 yum -y install kubelet-1.19.3 kubeadm-1.19.3 kubectl-1.19.3 # 查看版本 $ kubeadm version kubeadm version: &version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.4\", GitCommit:\"8d8aa39598534325ad77120c120a22b3a990b5ea\", GitTreeState:\"clean\", BuildDate:\"2020-03-12T21:01:11Z\", GoVersion:\"go1.13.8\", Compiler:\"gc\", Platform:\"linux/amd64\"} 设置kubelet开机自启 ⚠️此时kubelet是无法启动的，因为只有完成master的kubeadm init 的操作，kubelet才能正常启动 systemctl enable kubelet 设置k8s命令自动补全 yum -y install bash-completion source /usr/share/bash-completion/bash_completion source > ~/.bashrc 1.10 修改docker Cgroup Driver为systemd 由于默认情况下 kubelet 使用的 cgroupdriver 是 systemd，所以需要保持 docker 和kubelet 的 cgroupdriver 一致，我们这里修改 docker 的 cgroupdriver=systemd。如果不修改 docker 则需要修改 kubelet 的启动配置，需要保证两者一致。 # 修改docker Cgroup Driver为systemd 将/usr/lib/systemd/system/docker.service文件中的这一行 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock 修改为 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd 如果不修改，在添加 worker 节点时可能会碰到如下错误 [WARNING IsDockerSystemdCheck]: detected \"cgroupfs\" as the Docker cgroup driver. The recommended driver is \"systemd\". Please follow the guide at https://kubernetes.io/docs/setup/cri/ # 使用如下命令修改 sed -i.bak \"s#^ExecStart=/usr/bin/dockerd.*#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd#g\" /usr/lib/systemd/system/docker.service # 重启docker systemctl daemon-reload && systemctl restart docker 1.11 升级内核 centos7内核下载地址 可根据自己实际需求下载对应版本 现在升级到最新版，编辑安装最新内核脚本 cat > /opt/kernel_update.sh 到此，基本环境安装完成!!! 二、初始化集群 2.1 master节点操作，配置 kubeadm 初始化文件 可以通过如下命令导出默认的初始化配置 kubeadm config print init-defaults > kubeadm.yaml 方法一 命令初始化 kubeadm init --apiserver-advertise-address=10.0.0.130 --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.19.3 --service-cidr=10.96.0.0/16 --pod-network-cidr=10.20.0.0/16 --apiserver-advertise-address= #master节点IP --image-repository registry.aliyuncs.com/google_containers #镜像仓库 --kubernetes-version v1.19.3 #k8s版本 --service-cidr=10.96.0.0/16 #service IP网段 --pod-network-cidr=10.20.0.0/16 #pod IP网段，后续网络插件会用到 方法二 文件初始化 官方清单说明文档 cat > kubeadm.yaml 2.2 初始化master 可以先将需要的镜像pull下来 kubeadm config images pull --config kubeadm.yaml # 下载的镜像 [config/images] Pulled registry.aliyuncs.com/google_containers/kube-apiserver:v1.19.3 [config/images] Pulled registry.aliyuncs.com/google_containers/kube-controller-manager:v1.19.3 [config/images] Pulled registry.aliyuncs.com/google_containers/kube-scheduler:v1.19.3 [config/images] Pulled registry.aliyuncs.com/google_containers/kube-proxy:v1.19.3 [config/images] Pulled registry.aliyuncs.com/google_containers/pause:3.2 [config/images] Pulled registry.aliyuncs.com/google_containers/etcd:3.4.13-0 [config/images] Pulled registry.aliyuncs.com/google_containers/coredns:1.7.0 可以使用如下命令查看kubeadm需要的镜像列表 # 默认使用的是谷歌的镜像仓库 $ kubeadm config images list W1020 15:34:00.813012 3391 configset.go:348] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] k8s.gcr.io/kube-apiserver:v1.19.3 k8s.gcr.io/kube-controller-manager:v1.19.3 k8s.gcr.io/kube-scheduler:v1.19.3 k8s.gcr.io/kube-proxy:v1.19.3 k8s.gcr.io/pause:3.2 k8s.gcr.io/etcd:3.4.13-0 k8s.gcr.io/coredns:1.7.0 # 指定配置文件查看kubeadm需要下载的镜像 $ kubeadm config images list --config kubeadm.yaml W1020 15:48:24.612354 3760 configset.go:348] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] registry.aliyuncs.com/google_containers/kube-apiserver:v1.19.3 registry.aliyuncs.com/google_containers/kube-controller-manager:v1.19.3 registry.aliyuncs.com/google_containers/kube-scheduler:v1.19.3 registry.aliyuncs.com/google_containers/kube-proxy:v1.19.3 registry.aliyuncs.com/google_containers/pause:3.2 registry.aliyuncs.com/google_containers/etcd:3.4.13-0 registry.aliyuncs.com/google_containers/coredns:1.7.0 开始初始化集群 kubeadm init --config kubeadm.yaml 完整输出结果 W1020 15:53:56.503466 4046 configset.go:348] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] [init] Using Kubernetes version: v1.19.3 [preflight] Running pre-flight checks [WARNING Hostname]: hostname \"master1\" could not be reached [WARNING Hostname]: hostname \"master1\": lookup master1 on 223.5.5.5:53: no such host [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using 'kubeadm config images pull' [certs] Using certificateDir folder \"/etc/kubernetes/pki\" [certs] Generating \"ca\" certificate and key [certs] Generating \"apiserver\" certificate and key [certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local master1] and IPs [10.96.0.1 10.0.0.130] [certs] Generating \"apiserver-kubelet-client\" certificate and key [certs] Generating \"front-proxy-ca\" certificate and key [certs] Generating \"front-proxy-client\" certificate and key [certs] Generating \"etcd/ca\" certificate and key [certs] Generating \"etcd/server\" certificate and key [certs] etcd/server serving cert is signed for DNS names [localhost master1] and IPs [10.0.0.130 127.0.0.1 ::1] [certs] Generating \"etcd/peer\" certificate and key [certs] etcd/peer serving cert is signed for DNS names [localhost master1] and IPs [10.0.0.130 127.0.0.1 ::1] [certs] Generating \"etcd/healthcheck-client\" certificate and key [certs] Generating \"apiserver-etcd-client\" certificate and key [certs] Generating \"sa\" key and public key [kubeconfig] Using kubeconfig folder \"/etc/kubernetes\" [kubeconfig] Writing \"admin.conf\" kubeconfig file [kubeconfig] Writing \"kubelet.conf\" kubeconfig file [kubeconfig] Writing \"controller-manager.conf\" kubeconfig file [kubeconfig] Writing \"scheduler.conf\" kubeconfig file [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Starting the kubelet [control-plane] Using manifest folder \"/etc/kubernetes/manifests\" [control-plane] Creating static Pod manifest for \"kube-apiserver\" [control-plane] Creating static Pod manifest for \"kube-controller-manager\" [control-plane] Creating static Pod manifest for \"kube-scheduler\" [etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\" [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s [apiclient] All control plane components are healthy after 13.002462 seconds [upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [kubelet] Creating a ConfigMap \"kubelet-config-1.19\" in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --upload-certs [mark-control-plane] Marking the node master1 as control-plane by adding the label \"node-role.kubernetes.io/master=''\" [mark-control-plane] Marking the node master1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [bootstrap-token] Using token: abcdef.0123456789abcdef [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace [kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10.0.0.130:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:9ef5d816512489579f88785e67a80fbd7cc73c65457dde935c69ff2bcaa6cd96 kubeadm init 命令执行流程如下图所示 # kubeadm config images pull --config kubeadm.yaml 下载的镜像 registry.aliyuncs.com/google_containers/kube-proxy v1.19.3 cdef7632a242 5 days ago 118MB registry.aliyuncs.com/google_containers/kube-controller-manager v1.19.3 9b60aca1d818 5 days ago 111MB registry.aliyuncs.com/google_containers/kube-scheduler v1.19.3 aaefbfa906bd 5 days ago 45.7MB registry.aliyuncs.com/google_containers/kube-apiserver v1.19.3 a301be0cd44b 5 days ago 119MB registry.aliyuncs.com/google_containers/etcd 3.4.13-0 0369cf4303ff 7 weeks ago 253MB registry.aliyuncs.com/google_containers/coredns 1.7.0 bfe3a36ebd25 4 months ago 45.2MB registry.aliyuncs.com/google_containers/pause 3.2 80d28bedfe5d 8 months ago 683kB # kubeadm init --config kubeadm.yaml启动的容器 CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c79c4892da0b cdef7632a242 \"/usr/local/bin/kube…\" About a minute ago Up About a minute k8s_kube-proxy_kube-proxy-tqtqg_kube-system_e9dcc574-d430-436c-9b30-04b72cd47b17_0 26a2ae82ef0e registry.aliyuncs.com/google_containers/pause:3.2 \"/pause\" About a minute ago Up About a minute k8s_POD_kube-proxy-tqtqg_kube-system_e9dcc574-d430-436c-9b30-04b72cd47b17_0 f77d83a7b547 aaefbfa906bd \"kube-scheduler --au…\" About a minute ago Up About a minute k8s_kube-scheduler_kube-scheduler-master1_kube-system_285062c53852ebaf796eba8548d69e43_0 8c2c0fed9792 9b60aca1d818 \"kube-controller-man…\" About a minute ago Up About a minute k8s_kube-controller-manager_kube-controller-manager-master1_kube-system_8f99a56fb3eeae0c61283d6071bfb1f4_0 adab35c64481 a301be0cd44b \"kube-apiserver --ad…\" About a minute ago Up About a minute k8s_kube-apiserver_kube-apiserver-master1_kube-system_20c1cf16863ba23269117625a41bbd61_0 107a0f8678b9 0369cf4303ff \"etcd --advertise-cl…\" About a minute ago Up About a minute k8s_etcd_etcd-master1_kube-system_f98110a6164b8f0481572a8f9382951a_0 91c9349800c7 registry.aliyuncs.com/google_containers/pause:3.2 \"/pause\" About a minute ago Up About a minute k8s_POD_kube-scheduler-master1_kube-system_285062c53852ebaf796eba8548d69e43_0 3585b984e1b9 registry.aliyuncs.com/google_containers/pause:3.2 \"/pause\" About a minute ago Up About a minute k8s_POD_kube-controller-manager-master1_kube-system_8f99a56fb3eeae0c61283d6071bfb1f4_0 f23fe4ef6069 registry.aliyuncs.com/google_containers/pause:3.2 \"/pause\" About a minute ago Up About a minute k8s_POD_kube-apiserver-master1_kube-system_20c1cf16863ba23269117625a41bbd61_0 09271e8da969 registry.aliyuncs.com/google_containers/pause:3.2 \"/pause\" About a minute ago Up About a minute k8s_POD_etcd-master1_kube-system_f98110a6164b8f0481572a8f9382951a_0 拷贝 kubeconfig 文件 # 这里的路径为/root mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 2.3 master添加节点 node1和node2相同操作 将master节点上的$HOME/.kube/config 文件拷贝到node节点对应的文件中 # 创建目录，这里的路径为/root mkdir -p $HOME/.kube # 把master节点上的config文件拷贝到node1和node2的$HOME/.kube scp k8s-master1:~/.kube/config $HOME/.kube # 修改权限 chown $(id -u):$(id -g) $HOME/.kube/config 将node1和node2加入到集群中 这里需要用到2.2中初始化master最后生成的token和sha256值 kubeadm join 10.0.0.130:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:9ef5d816512489579f88785e67a80fbd7cc73c65457dde935c69ff2bcaa6cd96 输出结果 [preflight] Running pre-flight checks [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Starting the kubelet [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster. kubeadm join 命令执行流程如下所示 将node节点加入到k8s集群后，node节点会运行kube-proxy # 下载的镜像 CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5d72febd6273 registry.aliyuncs.com/google_containers/kube-proxy \"/usr/local/bin/kube…\" About a minute ago Up About a minute k8s_kube-proxy_kube-proxy-44bsb_kube-system_1501673e-9840-4f37-9ab3-690a2c673d0f_0 b94dc5408576 registry.aliyuncs.com/google_containers/pause:3.2 \"/pause\" 2 minutes ago Up # 启动的容器 CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5d72febd6273 registry.aliyuncs.com/google_containers/kube-proxy \"/usr/local/bin/kube…\" About a minute ago Up About a minute k8s_kube-proxy_kube-proxy-44bsb_kube-system_1501673e-9840-4f37-9ab3-690a2c673d0f_0 b94dc5408576 registry.aliyuncs.com/google_containers/pause:3.2 \"/pause\" 2 minutes ago Up 2 minutes k8s_POD_kube-proxy-44bsb_kube-system_1501673e-9840-4f37-9ab3-690a2c673d0f_0 如果忘记了token和sha256值，可以在master节点使用如下命令查看 # 查看token $ kubeadm token list TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS abcdef.0123456789abcdef 23h 2020-03-20T23:02:03+08:00 authentication,signing system:bootstrappers:kubeadm:default-node-token # 查看sha256 $ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' 27b37276c4451b54c99106f73cb38c3b0bb6e2e178c142808e9ff00300ac7eaf # 同时查看token和sha256 $ kubeadm token create --print-join-command W1020 16:01:33.668459 7695 configset.go:348] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] kubeadm join 10.0.0.130:6443 --token gmrizd.neccboizuq5ik6t4 --discovery-token-ca-cert-hash sha256:9ef5d816512489579f88785e67a80fbd7cc73c65457dde935c69ff2bcaa6cd96 master节点查看node，发现状态都是NotReady，因为还没有安装网络插件，这里我们安装 k8s网络插件官方文档 calio插件官方文档 $ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master NotReady master 19m v1.19.3 k8s-node1 NotReady 4m10s v1.19.3 k8s-node2 NotReady 4m3s v1.19.3 2.4 master节点安装网络插件flannel # 下载文件 wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml # 上述连接需要科学上网才能下载，所以将文件上传至码云 wget https://gitee.com/pptfz/k8s-yaml/blob/master/kube-flannel.yml # 修改文件内容 搜索 kube-flannel-ds ，kube-flannel-ds 中 containers 下的 args 添加如下一行 - --iface=eth0 这么做是为了避免机器中是多网卡的情况，这里手动指定一下网卡名称 # 修改完成后安装flannel网络插件 kubectl apply -f kube-flannel.yml # 安装完成后稍等一会查看pods状态，全部为running即为正确 kubectl get pods -n kube-system # 查看node状态 kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready master 30m v1.19.3 k8s-node1 Ready 25m v1.19.3 k8s-node2 Ready 25m v1.19.3 # kubectl apply -f kube-flannel.yml 安装网络插件flannel下载的镜像 REPOSITORY TAG IMAGE ID CREATED SIZE quay.io/coreos/flannel v0.13.0 e708f4bb69e3 5 days ago 57.2MB # kubectl apply -f kube-flannel.yml 安装网络插件flannel启动的容器 CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 902867d66cdc bfe3a36ebd25 \"/coredns -conf /etc…\" 2 minutes ago Up 2 minutes k8s_coredns_coredns-6d56c8448f-sm772_kube-system_12eb4ed7-e150-46bd-9269-44f67f757c10_0 b32d7e15eddc bfe3a36ebd25 \"/coredns -conf /etc…\" 2 minutes ago Up 2 minutes k8s_coredns_coredns-6d56c8448f-pjdd7_kube-system_5ad83254-04b8-45b4-9c63-b811cccb1f60_0 5f2076eb19b0 registry.aliyuncs.com/google_containers/pause:3.2 \"/pause\" 2 minutes ago Up 2 minutes k8s_POD_coredns-6d56c8448f-sm772_kube-system_12eb4ed7-e150-46bd-9269-44f67f757c10_0 e29468c30f1d registry.aliyuncs.com/google_containers/pause:3.2 \"/pause\" 2 minutes ago Up 2 minutes k8s_POD_coredns-6d56c8448f-pjdd7_kube-system_5ad83254-04b8-45b4-9c63-b811cccb1f60_0 48f9771ad558 e708f4bb69e3 \"/opt/bin/flanneld -…\" 2 minutes ago Up 2 minutes k8s_kube-flannel_kube-flannel-ds-njvkx_kube-system_9d9bbb3a-49bc-40cd-af35-8a1592534fb5_0 726ad34dd6ee quay.io/coreos/flannel \"cp -f /etc/kube-fla…\" 2 minutes ago Exited (0) 2 minutes ago k8s_install-cni_kube-flannel-ds-njvkx_kube-system_9d9bbb3a-49bc-40cd-af35-8a1592534fb5_0 4e5d65b8c80a registry.aliyuncs.com/google_containers/pause:3.2 \"/pause\" 2 minutes ago Up 2 minutes k8s_POD_kube-flannel-ds-njvkx_kube-system_9d9bbb3a-49bc-40cd-af35-8a1592534fb5_0 node节点 # 下载的镜像 REPOSITORY TAG IMAGE ID CREATED SIZE quay.io/coreos/flannel v0.13.0 e708f4bb69e3 5 days ago 57.2MB # 启动的容器 CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6af151942c77 e708f4bb69e3 \"/opt/bin/flanneld -…\" 3 minutes ago Up 3 minutes k8s_kube-flannel_kube-flannel-ds-b5l9b_kube-system_d13d64c1-4564-4995-bbc1-aaabc34f76c0_0 9faa5e1b769f quay.io/coreos/flannel \"cp -f /etc/kube-fla…\" 3 minutes ago Exited (0) 3 minutes ago k8s_install-cni_kube-flannel-ds-b5l9b_kube-system_d13d64c1-4564-4995-bbc1-aaabc34f76c0_0 c16b48d309a2 registry.aliyuncs.com/google_containers/pause:3.2 \"/pause\" 3 minutes ago Up 3 minutes 2.5 安装Dashboard(可选) 下载文件及修改内容 这里查看dashboard对应的k8s版本 # 下载文件 wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.4/aio/deploy/recommended.yaml # 上述连接需要科学上网才能下载，所以将文件上传至码云 wget https://gitee.com/pptfz/k8s-yaml/blob/master/dashboard-2.0.4.yaml # 修改Service为NodePort类型 42行下增加一行 nodePort: 30001 44行下增加一行 type: NodePort //原先内容 spec: ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard //修改后内容 spec: type: NodePort #增加，修改类型为nodeport ports: - port: 443 targetPort: 8443 nodePort: 30001 #增加，指定nodeport端口 selector: k8s-app: kubernetes-dashboard 部署dashboard # 创建dashboard kubectl apply -f recommended.yaml # 下载的镜像 kubernetesui/dashboard v2.0.4 46d0a29c3f61 6 weeks ago 225MB # 启动的容器 CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES f960c868c3e0 kubernetesui/dashboard \"/dashboard --insecu…\" 41 seconds ago Up 40 seconds k8s_kubernetes-dashboard_kubernetes-dashboard-665f4c5ff-8q4bt_kubernetes-dashboard_192980de-d658-4367-a720-fab16ebd62d0_0 查看dashboard的运行状态及外网访问端口 # 查看dashboard运行状态 $ kubectl get pods -n kubernetes-dashboard -l k8s-app=kubernetes-dashboard NAME READY STATUS RESTARTS AGE kubernetes-dashboard-7867cbccbb-z9vzj 1/1 Running 0 50s # 查看dashboard外网访问端口 $ kubectl get svc -n kubernetes-dashboard -l k8s-app=kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard NodePort 10.102.116.245 443:30001/TCP 2m56s 通过上边的30001端口访问dashboard，注意是https ⚠️k8s1.19.3这个版本中，使用的dashboard版本是2.0.4，只有火狐浏览器可以访问，其余浏览器都不能访问，会报错如下 只有火狐浏览器可以！ 然后创建一个具有全局所有权限的用户来登录Dashboard # 编辑admin.yaml文件 cat > admin.yaml 然后粘贴上述生成的base64字符串登陆dashboard，在登陆页面选择令牌一项 登陆后的首界面 2.6 安装kuboard(可选) kuboard 是Kubernetes 的一款图形化管理界面 安装kuboard kubectl apply -f https://kuboard.cn/install-script/kuboard.yaml kubectl apply -f https://addons.kuboard.cn/metrics-server/0.3.7/metrics-server.yaml 查看运行状态 $ kubectl get pods -l k8s.kuboard.cn/name=kuboard -n kube-system NAME READY STATUS RESTARTS AGE kuboard-756d46c4d4-tvhjq 1/1 Running 0 40s 获取token # 获取管理员token kubectl -n kube-system get secret $(kubectl -n kube-system get secret | grep kuboard-user | awk '{print $1}') -o go-template='{{.data.token}}' | base64 -d && echo # 获取只读token kubectl -n kube-system get secret $(kubectl -n kube-system get secret | grep kuboard-viewer | awk '{print $1}') -o go-template='{{.data.token}}' | base64 -d && echo 访问kuboard Kuboard Service 使用了 NodePort 的方式暴露服务，NodePort 为 32567 k8s集群的任意一个节点都可以访问到 登陆后首界面 kuboard下载的镜像和启动的容器 # 启动的容器 CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6d765b5835b1 eipwork/kuboard \"/entrypoint.sh\" 2 minutes ago Up 2 minutes k8s_kuboard_kuboard-655785f55c-db5c4_kube-system_10f752b5-3434-468d-9e93-e8c76f81501a_0 # 下载的镜像，本文中为2.0.5版本 REPOSITORY TAG IMAGE ID CREATED SIZE eipwork/kuboard latest 23b68e80aa82 12 days ago 182MB 到此，使用kubeadm安装k8s 1.19.3完成！！！ 获取令牌命令 kubectl get secret `kubectl get secret -n kube-system|grep admin-token|awk '{print $1}'` -o jsonpath={.data.token} -n kube-system |base64 -d && echo 1.19.3master启动的容器及下载的镜像 下载的镜像 启动的容器 1.19.3node启动的容器和下载的镜像 node下载的镜像 node启动的容器 2.7 k8s切换命名空间工具(可选) # 下载包 git clone https://github.com.cnpmjs.org/ahmetb/kubectx cp kubectx/kubens /usr/local/bin # 查看所有命名空间 $ kubens # 切换到kube-system命名空间 $ kubens kube-system Context \"kubernetes-admin@kubernetes\" modified. Active namespace is \"kube-system\". 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s安装/6.使用 kubeadm 搭建 v1.21.5 版本 Kubernetes 集群.html":{"url":"linux/k8s/k8s安装/6.使用 kubeadm 搭建 v1.21.5 版本 Kubernetes 集群.html","title":"1.21","keywords":"","body":"[toc] 使用 kubeadm 搭建 v1.21.5 版本 Kubernetes 集群 1.环境准备 单master架构图 1.1 实验环境 角色 IP地址 主机名 docker版本 硬件配置 系统 硬盘 内核 安装组件 master 172.30.100.101 k8s-master01 20.10.8 2C4G CentOS7.6 40g+50g 5.4.150-1.el7.elrepo.x86_64 kube-apiserver，kube-controller-manager，kube-scheduler，etcd node1 172.30.100.102 k8s-node01 20.10.8 2C4G CentOS7.6 40g+50g 5.4.150-1.el7.elrepo.x86_64 kubelet，kube-proxy，docker，etcd node2 172.30.100.103 k8s-node02 20.10.8 2C4G CentOS7.6 40g+50g 5.4.150-1.el7.elrepo.x86_64 kubelet，kube-proxy，docker，etcd ⚠️如无特殊说明，以下所有操作均在master节点 ⚠️已提前配置好master可以免密登陆node节点 ⚠️实验开始前已临时开启root远程登陆，实验结束后关闭root远程登陆，采用sudo用户密钥登陆方式 1.2 编辑环境变量文件 [ -d /opt/k8s/script ] || mkdir -p /opt/k8s/script && cd /opt/k8s/script cat > /opt/k8s/script/env.sh 1.3 每个节点配置host信息 # 加载变量文件 source /opt/k8s/script/env.sh # master节点编辑hosts文件 cat >> /etc/hosts >> ${node_ip}\" scp -o StrictHostKeyChecking=no -P${SSH_PORT} -i${SSH_KEY_FILE} /etc/hosts ${SSH_USER}@${node_ip}:/etc done 1.4 创建 /etc/sysctl.d/k8s.conf 文件，添加如下内容 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'cat > /etc/sysctl.d/k8s.conf >> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'modprobe br_netfilter && sysctl -p /etc/sysctl.d/k8s.conf' done bridge-nf 说明 bridge-nf 使得 netfilter 可以对 Linux 网桥上的 IPv4/ARP/IPv6 包过滤。比如，设置 net.bridge.bridge-nf-call-iptables＝1 后，二层的网桥在转发包时也会被 iptables的 FORWARD 规则所过滤。常用的选项包括： net.bridge.bridge-nf-call-arptables：是否在 arptables 的 FORWARD 中过滤网桥的 ARP 包 net.bridge.bridge-nf-call-ip6tables：是否在 ip6tables 链中过滤 IPv6 包 net.bridge.bridge-nf-call-iptables：是否在 iptables 链中过滤 IPv4 包 net.bridge.bridge-nf-filter-vlan-tagged：是否在 iptables/arptables 中过滤打了 vlan 标签的包 1.5 安装ipvs 创建/etc/sysconfig/modules/ipvs.modules文件，目的是保证在节点重启后能自动加载所需模块。使用lsmod | grep -e ip_vs -e nf_conntrack_ipv4命令查看是否已经正确加载所需的内核模块 ⚠️由于内核使用的是最新的版本5.4.150，因此无法加载nf_conntrack_ipv4 模块 google到的答案 看这个isseu # 编辑文件 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'cat >> /etc/sysconfig/modules/ipvs.modules >> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4' done 安装ipset和ipvsadm(便于查看 ipvs 的代理规则) for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'yum -y install ipset ipvsadm' done 1.6 安装docker20.10.8 1.6.1 添加阿里云yum源 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo' done 1.6.2 安装docker 使用 yum list docker-ce --showduplicates | sort -r 查看可用版本列表 for node_ip in ${NODE_IPS[@]} do { echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'export DOCKER_VERSION=20.10.8 && yum -y install docker-ce-${DOCKER_VERSION} docker-ce-cli-${DOCKER_VERSION} containerd.io' }& done wait 1.6.3 启动docker并设置开机自启 for node_ip in ${NODE_IPS[@]} do { echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'systemctl enable docker && systemctl start docker' }& done 1.6.4 配置docker镜像加速、修改docker镜像存储路径 3台机器都改，暂时没有批量修改的方法 [ -d /data/docker-image ] || mkdir /data/docker-image && \\ cat > /etc/docker/daemon.json >> ${node_ip}\" scp -P${SSH_PORT} -i${SSH_KEY_FILE} /etc/docker/daemon.json ${SSH_USER}@${node_ip}:/etc/docker }& done 1.6.5 修改docker Cgroup Driver为systemd 由于默认情况下 kubelet 使用的 cgroupdriver 是 systemd，所以需要保持 docker 和kubelet 的 cgroupdriver 一致，我们这里修改 docker 的 cgroupdriver=systemd。如果不修改 docker 则需要修改 kubelet 的启动配置，需要保证两者一致。 # 修改docker Cgroup Driver为systemd 将/usr/lib/systemd/system/docker.service文件中的这一行 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock 修改为 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd 如果不修改，在添加 worker 节点时可能会碰到如下错误 [WARNING IsDockerSystemdCheck]: detected \"cgroupfs\" as the Docker cgroup driver. The recommended driver is \"systemd\". Please follow the guide at https://kubernetes.io/docs/setup/cri/ 3台机器都改，暂时没有批量修改的方法 sed -i.bak 's#^ExecStart=/usr/bin/dockerd.*#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd#g' /usr/lib/systemd/system/docker.service for node_ip in ${NODE_IPS[@]} do { echo \">>> ${node_ip}\" scp -P${SSH_PORT} -i${SSH_KEY_FILE} /usr/lib/systemd/system/docker.service ${SSH_USER}@${node_ip}:/usr/lib/systemd/system }& done 1.7 安装Kubeadm 1.7.1 使用阿里云yum源 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'cat > /etc/yum.repos.d/kubernetes.repo 1.7.2 安装 kubeadm、kubelet、kubectl ⚠️阿里云yum源会随官方更新最新版，因此指定版本 # 指定版本安装 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'export K8S_VERSION=1.21.5 && yum -y install kubelet-${K8S_VERSION} kubeadm-${K8S_VERSION} kubectl-${K8S_VERSION}' done # 查看版本 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'sudo bash -c \"kubeadm version\"' done 1.7.3 设置kubelet开机自启 ⚠️此时kubelet是无法启动的，因为只有完成master的kubeadm init 的操作，kubelet才能正常启动 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'systemctl enable kubelet' done 1.7.4 设置k8s命令自动补全 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'yum -y install bash-completion && \\ source /usr/share/bash-completion/bash_completion && \\ source > ~/.bashrc' done 1.8 升级内核 linux内核官网 centos7内核下载地址 linux内核历史版本下载地址 可根据自己实际需求下载对应版本 1.8.1 导入elrepo的key并安装elrepo的yum源 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i ${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org && rpm -Uvh https://elrepo.org/linux/kernel/el7/x86_64/RPMS/elrepo-release-7.0-5.el7.elrepo.noarch.rpm' done 1.8.2 安装最新内核 执行此命令为安装最新稳定版 yum -y install --enablerepo=elrepo-kernel kernel-ml 执行此命令为安装最新长期维护版 yum -y install --enablerepo=elrepo-kernel kernel-lt for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'yum -y install --enablerepo=elrepo-kernel kernel-lt' done 1.8.3 修改内核顺序 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'grub2-set-default 0 && grub2-mkconfig -o /etc/grub2.cfg' done 1.8.4 确认是否启动默认内核指向最新安装的内核 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'grubby --default-kernel' done 1.8.5 重启机器生效 for node_ip in ${NODE_IPS[@]} do { echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'reboot' }& done 1.8.6 验证内核版本 source /opt/k8s/script/env.sh for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'uname -r' done 到此，基本环境安装完成!!! 2.初始化集群 2.1 master节点操作，配置 kubeadm 初始化文件 可以通过如下命令导出默认的初始化配置 kubeadm config print init-defaults > kubeadm.yaml 方法一 命令初始化 kubeadm init \\ --apiserver-advertise-address=172.30.100.101 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v2.21.5 \\ --service-cidr=10.96.0.0/16 \\ --pod-network-cidr=10.20.0.0/16 参数 说明 --apiserver-advertise-address=172.30.100.101 master节点IP --image-repository registry.aliyuncs.com/google_containers 指定阿里云镜像仓库 --kubernetes-version v2.21.5 k8s版本 --service-cidr=10.96.0.0/16 service IP网段 --pod-network-cidr=10.20.0.0/16 pod IP网段，后续网络插件会用到 方法二 文件初始化 官方清单说明文档 [ -d /opt/k8s/yaml ] || mkdir -p /opt/k8s/yaml && cd /opt/k8s/yaml cat > kubeadm.yaml 2.2 初始化master 可以先将需要的镜像pull下来 $ kubeadm config images pull --config kubeadm.yaml # 下载的镜像 [config/images] Pulled registry.aliyuncs.com/google_containers/kube-apiserver:v1.21.5 [config/images] Pulled registry.aliyuncs.com/google_containers/kube-controller-manager:v1.21.5 [config/images] Pulled registry.aliyuncs.com/google_containers/kube-scheduler:v1.21.5 [config/images] Pulled registry.aliyuncs.com/google_containers/kube-proxy:v1.21.5 [config/images] Pulled registry.aliyuncs.com/google_containers/pause:3.4.1 [config/images] Pulled registry.aliyuncs.com/google_containers/etcd:3.4.13-0 [config/images] Pulled registry.aliyuncs.com/google_containers/coredns:v1.8.0 可以使用如下命令查看kubeadm需要的镜像列表 # 默认使用的是谷歌的镜像仓库 $ kubeadm config images list I1001 12:37:55.538500 3567 version.go:254] remote version is much newer: v1.22.2; falling back to: stable-1.21 k8s.gcr.io/kube-apiserver:v1.21.5 k8s.gcr.io/kube-controller-manager:v1.21.5 k8s.gcr.io/kube-scheduler:v1.21.5 k8s.gcr.io/kube-proxy:v1.21.5 k8s.gcr.io/pause:3.4.1 k8s.gcr.io/etcd:3.4.13-0 k8s.gcr.io/coredns/coredns:v1.8.0 # 指定配置文件查看kubeadm需要下载的镜像 $ kubeadm config images list --config kubeadm.yaml registry.aliyuncs.com/google_containers/kube-apiserver:v1.21.5 registry.aliyuncs.com/google_containers/kube-controller-manager:v1.21.5 registry.aliyuncs.com/google_containers/kube-scheduler:v1.21.5 registry.aliyuncs.com/google_containers/kube-proxy:v1.21.5 registry.aliyuncs.com/google_containers/pause:3.4.1 registry.aliyuncs.com/google_containers/etcd:3.4.13-0 registry.aliyuncs.com/google_containers/coredns:v1.8.0 开始初始化集群 kubeadm init --config kubeadm.yaml # 以下为完整输出结果 [init] Using Kubernetes version: v1.21.5 [preflight] Running pre-flight checks [WARNING Hostname]: hostname \"master1\" could not be reached [WARNING Hostname]: hostname \"master1\": lookup master1 on 100.100.2.138:53: no such host [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using 'kubeadm config images pull' [certs] Using certificateDir folder \"/etc/kubernetes/pki\" [certs] Generating \"ca\" certificate and key [certs] Generating \"apiserver\" certificate and key [certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local master1] and IPs [10.96.0.1 172.30.100.101] [certs] Generating \"apiserver-kubelet-client\" certificate and key [certs] Generating \"front-proxy-ca\" certificate and key [certs] Generating \"front-proxy-client\" certificate and key [certs] Generating \"etcd/ca\" certificate and key [certs] Generating \"etcd/server\" certificate and key [certs] etcd/server serving cert is signed for DNS names [localhost master1] and IPs [172.30.100.101 127.0.0.1 ::1] [certs] Generating \"etcd/peer\" certificate and key [certs] etcd/peer serving cert is signed for DNS names [localhost master1] and IPs [172.30.100.101 127.0.0.1 ::1] [certs] Generating \"etcd/healthcheck-client\" certificate and key [certs] Generating \"apiserver-etcd-client\" certificate and key [certs] Generating \"sa\" key and public key [kubeconfig] Using kubeconfig folder \"/etc/kubernetes\" [kubeconfig] Writing \"admin.conf\" kubeconfig file [kubeconfig] Writing \"kubelet.conf\" kubeconfig file [kubeconfig] Writing \"controller-manager.conf\" kubeconfig file [kubeconfig] Writing \"scheduler.conf\" kubeconfig file [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Starting the kubelet [control-plane] Using manifest folder \"/etc/kubernetes/manifests\" [control-plane] Creating static Pod manifest for \"kube-apiserver\" [control-plane] Creating static Pod manifest for \"kube-controller-manager\" [control-plane] Creating static Pod manifest for \"kube-scheduler\" [etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\" [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s [apiclient] All control plane components are healthy after 15.501460 seconds [upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [kubelet] Creating a ConfigMap \"kubelet-config-1.21\" in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --upload-certs [mark-control-plane] Marking the node master1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers] [mark-control-plane] Marking the node master1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [bootstrap-token] Using token: abcdef.0123456789abcdef [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace [kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 172.30.100.101:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:e4a3403c0f8496f53ed90ba5435bd79df40da1804d324a7d97928744c66c5c25 kubeadm init 命令执行流程如下图所示 # kubeadm config images pull --config kubeadm.yaml 下载的镜像 REPOSITORY TAG IMAGE ID CREATED SIZE registry.aliyuncs.com/google_containers/kube-apiserver v1.21.5 7b2ac941d4c3 2 weeks ago 126MB registry.aliyuncs.com/google_containers/kube-proxy v1.21.5 e08abd2be730 2 weeks ago 104MB registry.aliyuncs.com/google_containers/kube-controller-manager v1.21.5 184ef4d127b4 2 weeks ago 120MB registry.aliyuncs.com/google_containers/kube-scheduler v1.21.5 8e60ea3644d6 2 weeks ago 50.8MB registry.aliyuncs.com/google_containers/pause 3.4.1 0f8457a4c2ec 8 months ago 683kB registry.aliyuncs.com/google_containers/coredns v1.8.0 296a6d5035e2 11 months ago 42.5MB registry.aliyuncs.com/google_containers/etcd 3.4.13-0 0369cf4303ff 13 months ago 253MB # kubeadm init --config kubeadm.yaml启动的容器 613c480c6b7e e08abd2be730 \"/usr/local/bin/kube…\" 50 seconds ago Up 50 seconds k8s_kube-proxy_kube-proxy-csfth_kube-system_ff2078df-57df-456d-a7c9-c1af4422bf5d_0 e60a9284820f registry.aliyuncs.com/google_containers/pause:3.4.1 \"/pause\" 50 seconds ago Up 50 seconds k8s_POD_kube-proxy-csfth_kube-system_ff2078df-57df-456d-a7c9-c1af4422bf5d_0 ae3d78eb273a 8e60ea3644d6 \"kube-scheduler --au…\" About a minute ago Up About a minute k8s_kube-scheduler_kube-scheduler-master1_kube-system_2fb7c32f95eac5c30e00e43feb43d8c0_0 f127286862b6 184ef4d127b4 \"kube-controller-man…\" About a minute ago Up About a minute k8s_kube-controller-manager_kube-controller-manager-master1_kube-system_3b907e705e4157a55244132169909d32_0 cb172134f689 0369cf4303ff \"etcd --advertise-cl…\" About a minute ago Up About a minute k8s_etcd_etcd-master1_kube-system_00ba2ed6f0d10f06abb6423169eca485_0 d3808a82f447 7b2ac941d4c3 \"kube-apiserver --ad…\" About a minute ago Up About a minute k8s_kube-apiserver_kube-apiserver-master1_kube-system_92a2138420cba276ed6b4bc76b6a8f1a_0 cd131dbe4fab registry.aliyuncs.com/google_containers/pause:3.4.1 \"/pause\" About a minute ago Up About a minute k8s_POD_kube-scheduler-master1_kube-system_2fb7c32f95eac5c30e00e43feb43d8c0_0 c757028f4427 registry.aliyuncs.com/google_containers/pause:3.4.1 \"/pause\" About a minute ago Up About a minute k8s_POD_kube-controller-manager-master1_kube-system_3b907e705e4157a55244132169909d32_0 6619a1b415d4 registry.aliyuncs.com/google_containers/pause:3.4.1 \"/pause\" About a minute ago Up About a minute k8s_POD_kube-apiserver-master1_kube-system_92a2138420cba276ed6b4bc76b6a8f1a_0 99e793cea0be registry.aliyuncs.com/google_containers/pause:3.4.1 \"/pause\" About a minute ago Up About a minute k8s_POD_etcd-master1_kube-system_00ba2ed6f0d10f06abb6423169eca485_0 拷贝 kubeconfig 文件 # $HOME路径为/root mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 2.3 master添加节点 node1和node2相同操作 将master节点上的 /root/.kube/config 文件拷贝到node节点对应的文件中 for node_ip in ${NODE_IPS[@]} do { echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'mkdir -p $HOME/.kube' scp -P${SSH_PORT} -i${SSH_KEY_FILE} $HOME/.kube/config ${SSH_USER}@${node_ip}:$HOME/.kube ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'chown $(id -u):$(id -g) $HOME/.kube/config' }& done 将node1和node2加入到集群中 这里需要用到2.2中初始化master最后生成的token和sha256值 # 设置node节点数组变量 export NODE_IPS=(172.30.100.102 172.30.100.103) for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'kubeadm join 172.30.100.101:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:e4a3403c0f8496f53ed90ba5435bd79df40da1804d324a7d97928744c66c5c25' done 输出结果 [preflight] Running pre-flight checks [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml' [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Starting the kubelet [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster. kubeadm join 命令执行流程如下所示 将node节点加入到k8s集群后，node节点会运行kube-proxy # 下载的镜像 registry.aliyuncs.com/google_containers/kube-proxy v1.21.5 e08abd2be730 2 weeks ago 104MB registry.aliyuncs.com/google_containers/pause 3.4.1 0f8457a4c2ec 8 months ago 683kB # 启动的容器 7f09c2d3b75f registry.aliyuncs.com/google_containers/kube-proxy \"/usr/local/bin/kube…\" 33 seconds ago Up 32 seconds k8s_kube-proxy_kube-proxy-lvcgt_kube-system_e5b226e0-99cf-4164-80b2-7533487322ae_0 091c6e597bbe registry.aliyuncs.com/google_containers/pause:3.4.1 \"/pause\" 37 seconds ago Up 37 seconds k8s_POD_kube-proxy-lvcgt_kube-system_e5b226e0-99cf-4164-80b2-7533487322ae_0 如果忘记了token和sha256值，可以在master节点使用如下命令查看 # 查看token $ kubeadm token list TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS abcdef.0123456789abcdef 23h 2021-10-02T14:38:16+08:00 authentication,signing system:bootstrappers:kubeadm:default-node-token # 查看sha256 $ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' e4a3403c0f8496f53ed90ba5435bd79df40da1804d324a7d97928744c66c5c25 # 同时查看token和sha256 $ kubeadm token create --print-join-command kubeadm join 172.30.100.101:6443 --token 6vpmr9.es2wa9gtromi0aek --discovery-token-ca-cert-hash sha256:e4a3403c0f8496f53ed90ba5435bd79df40da1804d324a7d97928744c66c5c25 master节点查看node，发现状态都是NotReady，因为还没有安装网络插件，这里我们安装 k8s网络插件官方文档 calio插件官方文档 $ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-node01 NotReady 2m22s v1.21.5 k8s-node02 NotReady 2m14s v1.21.5 master1 NotReady control-plane,master 15m v1.21.5 2.4 master节点安装网络插件flannel 2.4.1 下载文件 cd /opt/k8s/yaml wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 2.4.2 修改文件内容 搜索 kube-flannel-ds ，在 kube-flannel-ds 中 containers 下的 args 添加一行 - --iface=eth0，这么做是为了避免机器中是多网卡的情况，这里手动指定一下网卡名称 containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.14.0 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=eth0 # 增加一行 2.4.3 修改完成后安装flannel网络插件 kubectl apply -f kube-flannel.yml 2.4.4 安装完成后稍等一会查看pods状态，全部为running即为正确 $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-59d64cd4d4-9l75q 1/1 Running 0 34m coredns-59d64cd4d4-plj25 1/1 Running 0 34m etcd-master1 1/1 Running 0 34m kube-apiserver-master1 1/1 Running 0 34m kube-controller-manager-master1 1/1 Running 0 34m kube-flannel-ds-4p5kf 1/1 Running 0 16m kube-flannel-ds-7rb45 1/1 Running 0 16m kube-flannel-ds-sqt66 1/1 Running 0 16m kube-proxy-hvrl8 1/1 Running 0 34m kube-proxy-lvcgt 1/1 Running 0 21m kube-proxy-qdszc 1/1 Running 0 21m kube-scheduler-master1 1/1 Running 0 34m 2.4.5 查看node状态 $ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-node01 Ready 22m v1.21.5 k8s-node02 Ready 22m v1.21.5 master1 Ready control-plane,master 36m v1.21.5 2.4.5 安装flannel网络插件下载的镜像和启动的容器 master节点 # kubectl apply -f kube-flannel.yml 安装网络插件flannel下载的镜像 quay.io/coreos/flannel v0.14.0 8522d622299c 4 months ago 67.9MB # kubectl apply -f kube-flannel.yml 安装网络插件flannel启动的容器 fa65c5816d45 8522d622299c \"/opt/bin/flanneld -…\" 20 minutes ago Up 20 minutes k8s_kube-flannel_kube-flannel-ds-4p5kf_kube-system_49c57fc9-41d9-4c77-a73c-8822ff407c84_0 40b91da4838d quay.io/coreos/flannel \"cp -f /etc/kube-fla…\" 20 minutes ago Exited (0) 20 minutes ago k8s_install-cni_kube-flannel-ds-4p5kf_kube-system_49c57fc9-41d9-4c77-a73c-8822ff407c84_0 f221355e7362 registry.aliyuncs.com/google_containers/pause:3.4.1 \"/pause\" 20 minutes ago Up 20 minutes k8s_POD_kube-flannel-ds-4p5kf_kube-system_49c57fc9-41d9-4c77-a73c-8822ff407c84_0 node节点 # 下载的镜像 quay.io/coreos/flannel v0.14.0 8522d622299c 4 months ago 67.9MB registry.aliyuncs.com/google_containers/coredns v1.8.0 296a6d5035e2 11 months ago 42.5MB # 启动的容器 858dcf4bc524 registry.aliyuncs.com/google_containers/coredns \"/coredns -conf /etc…\" 22 minutes ago Up 22 minutes k8s_coredns_coredns-59d64cd4d4-9l75q_kube-system_03015b62-2c0b-4859-b9a5-3dbc49c02877_0 768b0c91b52e registry.aliyuncs.com/google_containers/pause:3.4.1 \"/pause\" 23 minutes ago Up 22 minutes k8s_POD_coredns-59d64cd4d4-9l75q_kube-system_03015b62-2c0b-4859-b9a5-3dbc49c02877_0 7b9ab983793b 8522d622299c \"/opt/bin/flanneld -…\" 23 minutes ago Up 23 minutes k8s_kube-flannel_kube-flannel-ds-sqt66_kube-system_2dab0071-e15e-4615-90b9-ed121675150d_0 e99be342113a quay.io/coreos/flannel \"cp -f /etc/kube-fla…\" 23 minutes ago Exited (0) 23 minutes ago k8s_install-cni_kube-flannel-ds-sqt66_kube-system_2dab0071-e15e-4615-90b9-ed121675150d_0 26f8110dc18e registry.aliyuncs.com/google_containers/pause:3.4.1 \"/pause\" 23 minutes ago Up 23 minutes k8s_POD_kube-flannel-ds-sqt66_kube-system_2dab0071-e15e-4615-90b9-ed121675150d_0 2.5 安装Dashboard(可选) 2.5.1 下载yaml文件 这里查看dashboard对应的k8s版本 cd /opt/k8s/yaml wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml mv recommended.yaml dashboard-v2.3.1.yaml 2.5.2 修改文件 修改Service为NodePort类型 # 原先内容 spec: ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard # 修改后内容 spec: type: NodePort # 新增一行，修改类型为nodeport ports: - port: 443 targetPort: 8443 nodePort: 30001 # 新增一行，指定nodeport端口 selector: k8s-app: kubernetes-dashboard 2.5.3 部署dashboard kubectl apply -f dashboard-v2.3.1.yaml kubectl apply -f dashboard-v2.3.1.yaml 下载的镜像和启动的容器 # 下载的镜像 kubernetesui/dashboard v2.3.1 e1482a24335a 3 months ago 220MB # 启动的容器 0c94aae98603 kubernetesui/dashboard \"/dashboard --insecu…\" About a minute ago Up About a minute k8s_kubernetes-dashboard_kubernetes-dashboard-67484c44f6-lz4r4_kubernetes-dashboard_4139d465-2e82-4048-a983-f3ee1f6b92c7_0 2.5.4 查看dashboard的运行状态及外网访问端口 # 查看dashboard运行状态 $ kubectl get pods -n kubernetes-dashboard -l k8s-app=kubernetes-dashboard NAME READY STATUS RESTARTS AGE kubernetes-dashboard-67484c44f6-lz4r4 1/1 Running 0 2m42s # 查看dashboard外网访问端口 $ kubectl get svc -n kubernetes-dashboard -l k8s-app=kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard NodePort 10.98.151.186 443:30001/TCP 2m56s 2.5.5 创建一个具有全局所有权限的用户来登录Dashboard # 编辑admin.yaml文件 cd /opt/k8s/yaml cat > admin.yaml 2.5.6 查看dashboard token # 查看token $ kubectl get secret -n kubernetes-dashboard|grep admin-token admin-token-852sr kubernetes.io/service-account-token 3 103s # 获取base64解码后的字符串，注意需要用到上边命令查看到的token，会生成很长的一串字符串 kubectl get secret admin-token-zcwfb -o jsonpath={.data.token} -n kubernetes-dashboard |base64 -d # 直接用这条命令搞定 kubectl get secret `kubectl get secret -n kubernetes-dashboard | grep admin-token|awk '{print $1}'` -o jsonpath={.data.token} -n kubernetes-dashboard |base64 -d && echo 2.5.6 访问dashboard 浏览器访问 https://ip:30001，注意是https 谷歌浏览器访问会提示如下 通过火狐浏览器访问 然后粘贴2.5.6步骤中生成的base64字符串登陆dashboard，在登陆页面选择令牌一项 登陆后的首界面 2.6 安装kuboard(可选) kuboard 是Kubernetes 的一款图形化管理界面 2.6.1 安装kuboard kuboard可以安装在k8s集群中，也可以单独以容器运行，这里选择单独以容器运行 docker run -d \\ --restart=unless-stopped \\ --name=kuboard \\ -p 80:80/tcp \\ -p 10081:10081/tcp \\ -e KUBOARD_ENDPOINT=\"http://172.30.100.101:80\" \\ -e KUBOARD_AGENT_SERVER_TCP_PORT=\"10081\" \\ -v /root/kuboard-data:/data \\ eipwork/kuboard:v3 启动参数说明 查看运行状态 $ docker ps -a|grep kuboard bcb69ab63a48 eipwork/kuboard:v3 \"/entrypoint.sh\" 2 minutes ago Up 2 minutes 443/tcp, 0.0.0.0:10081->10081/tcp, 0.0.0.0:80->80/tcp kuboard 2.6.2 访问kuboard 浏览器访问 ip:80 用户名： admin 密 码： Kuboard123 登陆后首界面 2.6.3 添加k8s集群 点击 添加集群 填写名称和描述 按照提示 cd /opt/k8s/yaml curl -k 'http://172.30.100.101:80/kuboard-api/cluster/ops-k8s/kind/KubernetesCluster/ops-k8s/resource/installAgentToKubernetes?token=zSWL5I5l1POTgRVV80FlVDyoWy8HJz0P' > kuboard-agent.yaml kubectl apply -f ./kuboard-agent.yaml 等待几秒，当状态变为已就绪即为成功 2.7 安装k8s切换命名空间工具(可选) # 下载包 git clone https://github.com.cnpmjs.org/ahmetb/kubectx cp kubectx/kubens /usr/local/bin # 查看所有命名空间 $ kubens # 切换到kube-system命名空间 $ kubens kube-system Context \"kubernetes-admin@kubernetes\" modified. Active namespace is \"kube-system\". 到此，使用kubeadm安装k8s 1.21.5完成！！！ k8s 1.21.5master启动的容器及下载的镜像 下载的镜像 启动的容器 k8s 1.21.5node启动的容器和下载的镜像 node下载的镜像 node启动的容器 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s安装/7.使用 kubeadm 搭建 v1.22.2 版本 Kubernetes 集群.html":{"url":"linux/k8s/k8s安装/7.使用 kubeadm 搭建 v1.22.2 版本 Kubernetes 集群.html","title":"1.22","keywords":"","body":"[toc] 使用 kubeadm 搭建 v1.22.2 版本 Kubernetes 集群 1.环境准备 单master架构图 1.1 实验环境 角色 IP地址 主机名 containerd版本 硬件配置 系统 硬盘 内核 安装组件 master 172.30.100.101 k8s-master01 1.5.5 2C4G CentOS7.6 40g+50g 3.10.0-957.21.3.el7.x86_64 kube-apiserver，kube-controller-manager，kube-scheduler，etcd node1 172.30.100.102 k8s-node01 1.5.5 2C4G CentOS7.6 40g+50g 3.10.0-957.21.3.el7.x86_64 kubelet，kube-proxy，docker，etcd node2 172.30.100.103 k8s-node02 1.5.5 2C4G CentOS7.6 40g+50g 3.10.0-957.21.3.el7.x86_64 kubelet，kube-proxy，docker，etcd ⚠️如无特殊说明，以下所有操作均在master节点 ⚠️已提前配置好master可以免密登陆node节点 ⚠️实验开始前已临时开启root远程登陆，实验结束后关闭root远程登陆，采用sudo用户密钥登陆方式 1.2 编辑环境变量文件 [ -d /opt/k8s/script ] || mkdir -p /opt/k8s/script && cd /opt/k8s/script cat > /opt/k8s/script/env.sh 1.3 每个节点配置host信息 # 加载变量文件 source /opt/k8s/script/env.sh # master节点编辑hosts文件 cat >> /etc/hosts >> ${node_ip}\" scp -o StrictHostKeyChecking=no -P${SSH_PORT} -i${SSH_KEY_FILE} /etc/hosts ${SSH_USER}@${node_ip}:/etc done 1.4 创建 /etc/sysctl.d/k8s.conf 文件，添加如下内容 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'cat > /etc/sysctl.d/k8s.conf >> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'modprobe br_netfilter && sysctl -p /etc/sysctl.d/k8s.conf' done bridge-nf 说明 bridge-nf 使得 netfilter 可以对 Linux 网桥上的 IPv4/ARP/IPv6 包过滤。比如，设置 net.bridge.bridge-nf-call-iptables＝1 后，二层的网桥在转发包时也会被 iptables的 FORWARD 规则所过滤。常用的选项包括： net.bridge.bridge-nf-call-arptables：是否在 arptables 的 FORWARD 中过滤网桥的 ARP 包 net.bridge.bridge-nf-call-ip6tables：是否在 ip6tables 链中过滤 IPv6 包 net.bridge.bridge-nf-call-iptables：是否在 iptables 链中过滤 IPv4 包 net.bridge.bridge-nf-filter-vlan-tagged：是否在 iptables/arptables 中过滤打了 vlan 标签的包 1.5 安装ipvs 创建 /etc/sysconfig/modules/ipvs.modules 文件，目的是保证在节点重启后能自动加载所需模块。使用 lsmod | grep -e ip_vs -e nf_conntrack_ipv4 命令查看是否已经正确加载所需的内核模块 # 编辑文件 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'cat > /etc/sysconfig/modules/ipvs.modules >> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4' done 安装ipset和ipvsadm(便于查看 ipvs 的代理规则) for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'yum -y install ipset ipvsadm' done 1.6 安装Containerd containerd github地址 containerd官网 1.6.1 下载安装包 由于 containerd 需要调用 runc，所以我们也需要先安装 runc，不过 containerd 提供了一个包含相关依赖的压缩包 cri-containerd-cni-${VERSION}.${OS}-${ARCH}.tar.gz，可以直接使用这个包来进行安装。 如果github下载速度较慢，可以使用 https://download.fastgit.org/containerd/containerd/releases/download/v1.5.5/cri-containerd-cni-1.5.5-linux-amd64.tar.gz 加速下载 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'wget -P /opt https://github.com/containerd/containerd/releases/download/v1.5.5/cri-containerd-cni-1.5.5-linux-amd64.tar.gz' done 1.6.2 安装配置containerd 1.6.2.1 解压缩包 ⚠️tar包解压缩后是3个目录 etc 、 opt 、 usr for node_ip in ${NODE_IPS[@]} do { echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'tar xf /opt/cri-containerd-cni-1.5.5-linux-amd64.tar.gz -C /' } done 1.6.2.2 导出命令 PATH 环境变量 for node_ip in ${NODE_IPS[@]} do { echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'echo \"export PATH=$PATH:/usr/local/bin:/usr/local/sbin\" > /etc/profile.d/containerd.sh && source /etc/profile' } done 1.6.2.3 创建containerd配置文件 containerd 的默认配置文件为 /etc/containerd/config.toml，我们可以通过 containerd config default > /etc/containerd/config.toml 命令生成一个默认的配置 for node_ip in ${NODE_IPS[@]} do { echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'mkdir -p /etc/containerd && containerd config default > /etc/containerd/config.toml' } done 1.6.2.4 修改containerd配置文件 对于使用 systemd 作为 init system 的 Linux 的发行版，使用 systemd 作为容器的 cgroup driver 可以确保节点在资源紧张的情况更加稳定，所以推荐将 containerd 的 cgroup driver 配置为 systemd 修改前面生成的配置文件 /etc/containerd/config.toml，在 plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options 配置块下面将 SystemdCgroup 设置为 true 修改 SystemdCgroup = false 修改为 SystemdCgroup = true 1.6.2.5 配置containerd仓库加速 需要在 cri 配置块下面的 registry 配置块下面进行配置 registry.mirrors 如果我们的节点不能正常获取 k8s.gcr.io 的镜像，那么我们需要在上面重新配置 sandbox_image 镜像，否则后面 kubelet 覆盖该镜像不会生效：Warning: For remote container runtime, --pod-infra-container-image is ignored in kubelet, which should be set in that remote runtime instead。因此将 sandbox_image 中的镜像仓库地址修改为阿里云地址 [plugins.\"io.containerd.grpc.v1.cri\"] ... # sandbox_image = \"k8s.gcr.io/pause:3.5\" sandbox_image = \"registry.aliyuncs.com/k8sxio/pause:3.5\" ... [plugins.\"io.containerd.grpc.v1.cri\".registry] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"docker.io\"] endpoint = [\"https://bqr1dr1n.mirror.aliyuncs.com\"] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"k8s.gcr.io\"] endpoint = [\"https://registry.aliyuncs.com/k8sxio\"] /etc/containerd/config.toml 最终内容 cat > /etc/containerd/config.toml 拷贝文件到node节点 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" scp -o StrictHostKeyChecking=no -P${SSH_PORT} -i${SSH_KEY_FILE} /etc/containerd/config.toml ${SSH_USER}@${node_ip}:/etc/containerd done 1.6.3 启动containerd for node_ip in ${NODE_IPS[@]} do { echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'systemctl daemon-reload && systemctl enable containerd && systemctl start containerd' } done 1.6.4 验证 启动完成后就可以使用 containerd 的本地 CLI 工具 ctr 和 crictl 了，比如查看版本 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'ctr version' done $ ctr version Client: Version: v1.5.5 Revision: 72cec4be58a9eb6b2910f5d10f1c01ca47d231c0 Go version: go1.16.6 Server: Version: v1.5.5 Revision: 72cec4be58a9eb6b2910f5d10f1c01ca47d231c0 UUID: 992ed032-c65e-4aa1-b2dc-e15453b3ab42 1.7 安装Kubeadm 1.7.1 使用阿里云yum源 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'cat > /etc/yum.repos.d/kubernetes.repo 1.7.2 安装 kubeadm、kubelet、kubectl ⚠️阿里云yum源会随官方更新最新版，因此指定版本 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'export K8S_VERSION=1.22.2 && yum -y install kubelet-${K8S_VERSION} kubeadm-${K8S_VERSION} kubectl-${K8S_VERSION}' done 查看版本 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'kubeadm version' done 1.7.3 设置kubelet开机自启 ⚠️此时kubelet是无法启动的，因为只有完成master的 kubeadm init 的操作，kubelet才能正常启动 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'systemctl enable kubelet' done 1.7.4 设置k8s命令自动补全 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'yum -y reinstall bash-completion && \\ source /usr/share/bash-completion/bash_completion && \\ source > ~/.bashrc' done 2.初始化集群 2.1 master节点操作，配置 kubeadm 初始化文件 当我们执行 kubelet --help 命令的时候可以看到原来大部分命令行参数都被 DEPRECATED了，这是因为官方推荐我们使用 --config 来指定配置文件，在配置文件中指定原来这些参数的配置，可以通过官方文档 Set Kubelet parameters via a config file 了解更多相关信息，这样 Kubernetes 就可以支持动态 Kubelet 配置（Dynamic Kubelet Configuration）了，参考 Reconfigure a Node’s Kubelet in a Live Cluster。 可以通过如下命令导出默认的初始化配置 kubeadm config print init-defaults > kubeadm.yaml 方法一 命令初始化 kubeadm init \\ --apiserver-advertise-address=172.30.100.101 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.22.2 \\ --service-cidr=10.96.0.0/16 \\ --pod-network-cidr=10.20.0.0/16 参数 说明 --apiserver-advertise-address=172.30.100.101 master节点IP --image-repository registry.aliyuncs.com/google_containers 指定阿里云镜像仓库 --kubernetes-version v1.22.2 k8s版本 --service-cidr=10.96.0.0/16 service IP网段 --pod-network-cidr=10.20.0.0/16 pod IP网段，后续网络插件会用到 方法二 文件初始化 官方清单说明文档 [ -d /opt/k8s/yaml ] || mkdir -p /opt/k8s/yaml && cd /opt/k8s/yaml cat > kubeadm.yaml 2.2 初始化master 可以先将需要的镜像pull下来 $ kubeadm config images pull --config kubeadm.yaml [config/images] Pulled registry.aliyuncs.com/k8sxio/kube-apiserver:v1.22.2 [config/images] Pulled registry.aliyuncs.com/k8sxio/kube-controller-manager:v1.22.2 [config/images] Pulled registry.aliyuncs.com/k8sxio/kube-scheduler:v1.22.2 [config/images] Pulled registry.aliyuncs.com/k8sxio/kube-proxy:v1.22.2 [config/images] Pulled registry.aliyuncs.com/k8sxio/pause:3.5 [config/images] Pulled registry.aliyuncs.com/k8sxio/etcd:3.5.0-0 failed to pull image \"registry.aliyuncs.com/k8sxio/coredns:v1.8.4\": output: time=\"2021-11-07T17:23:37+08:00\" level=fatal msg=\"pulling image: rpc error: code = NotFound desc = failed to pull and unpack image \\\"registry.aliyuncs.com/k8sxio/coredns:v1.8.4\\\": failed to resolve reference \\\"registry.aliyuncs.com/k8sxio/coredns:v1.8.4\\\": registry.aliyuncs.com/k8sxio/coredns:v1.8.4: not found\" , error: exit status 1 To see the stack trace of this error execute with --v=5 or higher 上面在拉取 coredns 镜像的时候出错了，没有找到这个镜像，我们可以手动 pull 该镜像，然后重新 tag 下镜像地址即可 # 手动pull镜像 $ ctr -n k8s.io i pull docker.io/coredns/coredns:1.8.4 docker.io/coredns/coredns:1.8.4: resolved |++++++++++++++++++++++++++++++++++++++| index-sha256:6e5a02c21641597998b4be7cb5eb1e7b02c0d8d23cce4dd09f4682d463798890: done |++++++++++++++++++++++++++++++++++++++| manifest-sha256:10683d82b024a58cc248c468c2632f9d1b260500f7cd9bb8e73f751048d7d6d4: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:bc38a22c706b427217bcbd1a7ac7c8873e75efdd0e59d6b9f069b4b243db4b4b: done |++++++++++++++++++++++++++++++++++++++| config-sha256:8d147537fb7d1ac8895da4d55a5e53621949981e2e6460976dae812f83d84a44: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:c6568d217a0023041ef9f729e8836b19f863bcdb612bb3a329ebc165539f5a80: exists |++++++++++++++++++++++++++++++++++++++| elapsed: 12.4s total: 12.0 M (991.3 KiB/s) unpacking linux/amd64 sha256:6e5a02c21641597998b4be7cb5eb1e7b02c0d8d23cce4dd09f4682d463798890... done: 410.185888ms # 手动打tag ctr -n k8s.io i tag docker.io/coredns/coredns:1.8.4 registry.aliyuncs.com/k8sxio/coredns:v1.8.4 开始初始化集群 kubeadm init --config kubeadm.yaml # 以下为完整输出结果 [init] Using Kubernetes version: v1.22.2 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using 'kubeadm config images pull' [certs] Using certificateDir folder \"/etc/kubernetes/pki\" [certs] Generating \"ca\" certificate and key [certs] Generating \"apiserver\" certificate and key [certs] apiserver serving cert is signed for DNS names [k8s-master01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.30.100.101] [certs] Generating \"apiserver-kubelet-client\" certificate and key [certs] Generating \"front-proxy-ca\" certificate and key [certs] Generating \"front-proxy-client\" certificate and key [certs] Generating \"etcd/ca\" certificate and key [certs] Generating \"etcd/server\" certificate and key [certs] etcd/server serving cert is signed for DNS names [k8s-master01 localhost] and IPs [172.30.100.101 127.0.0.1 ::1] [certs] Generating \"etcd/peer\" certificate and key [certs] etcd/peer serving cert is signed for DNS names [k8s-master01 localhost] and IPs [172.30.100.101 127.0.0.1 ::1] [certs] Generating \"etcd/healthcheck-client\" certificate and key [certs] Generating \"apiserver-etcd-client\" certificate and key [certs] Generating \"sa\" key and public key [kubeconfig] Using kubeconfig folder \"/etc/kubernetes\" [kubeconfig] Writing \"admin.conf\" kubeconfig file [kubeconfig] Writing \"kubelet.conf\" kubeconfig file [kubeconfig] Writing \"controller-manager.conf\" kubeconfig file [kubeconfig] Writing \"scheduler.conf\" kubeconfig file [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Starting the kubelet [control-plane] Using manifest folder \"/etc/kubernetes/manifests\" [control-plane] Creating static Pod manifest for \"kube-apiserver\" [control-plane] Creating static Pod manifest for \"kube-controller-manager\" [control-plane] Creating static Pod manifest for \"kube-scheduler\" [etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\" [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s [apiclient] All control plane components are healthy after 12.002784 seconds [upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [kubelet] Creating a ConfigMap \"kubelet-config-1.22\" in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --upload-certs [mark-control-plane] Marking the node k8s-master01 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers] [mark-control-plane] Marking the node k8s-master01 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [bootstrap-token] Using token: abcdef.0123456789abcdef [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace [kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 172.30.100.101:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:4dbb97534e8304bb78023352236b898730062a35569e1c7f9247e6c7e81f250d kubeadm init 命令执行流程如下图所示 拷贝 kubeconfig 文件 # $HOME路径为/root mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 2.3 master添加节点 node1和node2相同操作 将master节点上的 /root/.kube/config 文件拷贝到node节点对应的文件中 for node_ip in ${NODE_IPS[@]} do { echo \">>> ${node_ip}\" ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'mkdir -p $HOME/.kube' scp -P${SSH_PORT} -i${SSH_KEY_FILE} $HOME/.kube/config ${SSH_USER}@${node_ip}:$HOME/.kube ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'chown $(id -u):$(id -g) $HOME/.kube/config' } done 将node1和node2加入到集群中 这里需要用到2.2中初始化master最后生成的token和sha256值 kubeadm join 172.30.100.101:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:4dbb97534e8304bb78023352236b898730062a35569e1c7f9247e6c7e81f250d 输出结果 [preflight] Running pre-flight checks [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml' [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Starting the kubelet [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster. kubeadm join 命令执行流程如下所示 如果忘记了token和sha256值，可以在master节点使用如下命令查看 # 查看token $ kubeadm token list TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS abcdef.0123456789abcdef 23h 2021-11-08T10:22:16Z authentication,signing system:bootstrappers:kubeadm:default-node-token # 查看sha256 $ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' 4dbb97534e8304bb78023352236b898730062a35569e1c7f9247e6c7e81f250d # 同时查看token和sha256 $ kubeadm token create --print-join-command kubeadm join 172.30.100.101:6443 --token ztnnf4.rnk33s1lpfyc4w98 --discovery-token-ca-cert-hash sha256:4dbb97534e8304bb78023352236b898730062a35569e1c7f9247e6c7e81f250d 这个时候其实集群还不能正常使用，因为还没有安装网络插件，接下来安装网络插件，可以在文档 https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/ 中选择我们自己的网络插件，这里我们安装 flannel k8s网络插件官方文档 calio插件官方文档 $ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master01 Ready control-plane,master 24m v1.22.2 k8s-node01 Ready 101s v1.22.2 k8s-node02 Ready 112s v1.22.2 2.4 master节点安装网络插件flannel 2.4.1 下载文件 wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 2.4.2 修改文件内容 搜索 kube-flannel-ds ，在 kube-flannel-ds 中 containers 下的 args 添加一行 - --iface=eth0，这么做是为了避免机器中是多网卡的情况，这里手动指定一下网卡名称 containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.15.0 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=eth0 # 增加一行 2.4.3 修改完成后安装flannel网络插件 kubectl apply -f kube-flannel.yml ⚠️⚠️⚠️这里有坑！！！，执行完 kubectl apply -f kube-flannel.yml 命令后查看所有pod状态，这个时候coredns一定是有问题的，查看coredns会发现如下报错 could not add IP address to \"cni0\": permission denied 执行 ip a 命令查看网卡信息，可以看到有一个虚拟网卡 cni0 ，这个虚拟网卡的IP段是 10.88.0.0/16 段的，这个就是应用flannel后创建的虚拟网卡 查看cni配置文件 $ ls /etc/cni/net.d/ 10-containerd-net.conflist 10-flannel.conflist 10.88.0.0./16 就是由 /etc/cni/net.d/10-containerd-net.conflist 这个文件中定义的，而 10-flannel.conflist 才是我们想要的文件 $ cat /etc/cni/net.d/10-containerd-net.conflist { \"cniVersion\": \"0.4.0\", \"name\": \"containerd-net\", \"plugins\": [ { \"type\": \"bridge\", \"bridge\": \"cni0\", \"isGateway\": true, \"ipMasq\": true, \"promiscMode\": true, \"ipam\": { \"type\": \"host-local\", \"ranges\": [ [{ \"subnet\": \"10.88.0.0/16\" }], [{ \"subnet\": \"2001:4860:4860::/64\" }] ], \"routes\": [ { \"dst\": \"0.0.0.0/0\" }, { \"dst\": \"::/0\" } ] } }, { \"type\": \"portmap\", \"capabilities\": {\"portMappings\": true} } ] } 这个 cni 插件类型是 bridge 网络，网桥的名称为 cni0，但是使用 bridge 网络的容器无法跨多个宿主机进行通信，跨主机通信需要借助其他的 cni 插件，比如上面我们安装的 Flannel，或者 Calico 等等，由于我们这里有两个 cni 配置，所以我们需要将 10-containerd-net.conflist 这个配置删除，因为如果这个目录中有多个 cni 配置文件，kubelet 将会使用按文件名的字典顺序排列的第一个作为配置文件，所以前面默认选择使用的是 containerd-net 这个插件。 mv /etc/cni/net.d/10-containerd-net.conflist /etc/cni/net.d/10-containerd-net.conflist.bak ifconfig cni0 down && ip link delete cni0 systemctl daemon-reload systemctl restart containerd kubelet 再次查看coredns就没有问题了 2.4.4 安装完成后稍等一会查看pods状态，全部为running即为正确 $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-7568f67dbd-26nkx 1/1 Running 0 6d3h coredns-7568f67dbd-n56bc 1/1 Running 0 6d3h etcd-k8s-master01 1/1 Running 0 6d3h kube-apiserver-k8s-master01 1/1 Running 0 6d3h kube-controller-manager-k8s-master01 1/1 Running 0 6d3h kube-flannel-ds-7rbqf 1/1 Running 0 20m kube-flannel-ds-kwtzt 1/1 Running 0 20m kube-flannel-ds-tswz7 1/1 Running 0 20m kube-proxy-blqbn 1/1 Running 0 6d3h kube-proxy-mwqmv 1/1 Running 0 6d3h kube-proxy-sgwhm 1/1 Running 0 6d3h kube-scheduler-k8s-master01 1/1 Running 0 6d3h 2.4.5 查看node状态 $ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master01 Ready control-plane,master 6d3h v1.22.2 k8s-node01 Ready 6d3h v1.22.2 k8s-node02 Ready 6d3h v1.22.2 2.5 安装Dashboard(可选) 2.5.1 下载yaml文件 这里查看dashboard对应的k8s版本 cd /opt/k8s/yaml wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.4.0/aio/deploy/recommended.yaml mv recommended.yaml dashboard-v2.4.0.yaml 2.5.2 修改文件 修改Service为NodePort类型 # 原先内容 spec: ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard # 修改后内容 spec: type: NodePort # 新增一行，修改类型为nodeport ports: - port: 443 targetPort: 8443 nodePort: 30001 # 新增一行，指定nodeport端口 selector: k8s-app: kubernetes-dashboard 2.5.3 部署dashboard kubectl apply -f dashboard-v2.4.0.yaml 2.5.4 查看dashboard的运行状态及外网访问端口 # 查看dashboard运行状态 $ kubectl get pods -n kubernetes-dashboard -l k8s-app=kubernetes-dashboard NAME READY STATUS RESTARTS AGE kubernetes-dashboard-576cb95f94-nmtkl 1/1 Running 0 6m55s # 查看dashboard外网访问端口 $ kubectl get svc -n kubernetes-dashboard -l k8s-app=kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard NodePort 10.99.205.24 443:30001/TCP 7m10s 2.5.5 创建一个具有全局所有权限的用户来登录Dashboard # 编辑admin.yaml文件 cat > /opt/k8s/yaml/admin.yaml 2.5.6 查看dashboard token # 查看token $ kubectl get secret -n kubernetes-dashboard|grep admin-token admin-token-852sr kubernetes.io/service-account-token 3 103s # 获取base64解码后的字符串，注意需要用到上边命令查看到的token，会生成很长的一串字符串 kubectl get secret admin-token-zcwfb -o jsonpath={.data.token} -n kubernetes-dashboard |base64 -d # 直接用这条命令搞定 kubectl get secret `kubectl get secret -n kubernetes-dashboard | grep admin-token|awk '{print $1}'` -o jsonpath={.data.token} -n kubernetes-dashboard |base64 -d && echo 2.5.6 访问dashboard 浏览器访问 https://ip:30001，注意是https 谷歌浏览器访问会提示如下 通过火狐浏览器访问 然后粘贴2.5.6步骤中生成的base64字符串登陆dashboard，在登陆页面选择令牌一项 登陆后的首界面 2.7 安装k8s切换命名空间工具(可选) # 下载包 git clone https://github.com/ahmetb/kubectx.git cp kubectx/kubens /usr/local/bin # 查看所有命名空间 $ kubens # 切换到kube-system命名空间 $ kubens kube-system Context \"kubernetes-admin@kubernetes\" modified. Active namespace is \"kube-system\". 到此，使用kubeadm安装k8s 1.22.2完成！！！ 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s安装/8.centos8.2二进制安装单master k8s1.18.5.html":{"url":"linux/k8s/k8s安装/8.centos8.2二进制安装单master k8s1.18.5.html","title":"1.18","keywords":"","body":"[toc] centos8.2二进制安装单master k8s1.18.5 一、环境准备 单master架构图 1.1 实验环境 角色 IP地址 主机名 docker版本 硬件配置 系统 内核 安装组件 master1 10.0.0.30 k8s-master1 19.03.4 2c4g CentOS8.2 4.18.0-193.el8.x86_64 kube-apiserver，kube-controller-manager，kube-scheduler，etcd node1 10.0.0.33 k8s-node1 19.03.4 2c4g CentOS8.2 4.18.0-193.el8.x86_64 kubelet，kube-proxy，docker，etcd node2 10.0.0.34 k8s-node2 19.03.4 2c4g CentOS8.2 4.18.0-193.el8.x86_64 kubelet，kube-proxy，docker，etcd CentOS8.2采用最小化安装，并执行了以下脚本 #!/usr/bin/env bash # #修改系统yum源为aliyun并添加epel源 mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak [ ! -e /etc/yum.repos.d/CentOS-Base.repo ] && curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-8.repo dnf clean all dnf makecache yum install -y https://mirrors.aliyun.com/epel/epel-release-latest-8.noarch.rpm sed -i 's|^#baseurl=https://download.fedoraproject.org/pub|baseurl=https://mirrors.aliyun.com|' /etc/yum.repos.d/epel* sed -i 's|^metalink|#metalink|' /etc/yum.repos.d/epel* dnf -y install tar wget net-tools git vim tree lrzsz htop iftop iotop psmisc python36 python3-devel zlib zlib-devel gcc gcc-c++ conntrack-tools jq socat bash-completion telnet nload strace tcpdump lsof sysstat #关闭防火墙、selinux、NetworkManager systemctl disable firewalld NetworkManager sed -i '7s/enforcing/disabled/' /etc/selinux/config #同步时间计划任务 #rpm -ivh http://mirrors.wlnmp.com/centos/wlnmp-release-centos.noarch.rpm && dnf -y install wntp #sed -i '/*\\/10 \\* \\* \\* \\* \\/usr\\/sbin\\/ntpdate ntp2\\.aliyun\\.com &>\\/dev\\/null/d' /var/spool/cron/root #echo \"*/10 * * * * /usr/local/bin/ntpdate ntp2.aliyun.com &>/dev/null\" >>/var/spool/cron/root #历史命令显示时间 sed -i '/HISTFILESIZE=2000/d' /etc/bashrc sed -i '/HISTSIZE=2000/d' /etc/bashrc sed -i '/HISTTIMEFORMAT=\"%Y-%m-%d %H:%M:%S \"/d' /etc/bashrc sed -i '/export HISTTIMEFORMAT/d' /etc/bashrc cat >>/etc/bashrc>/etc/security/limits.conf~/.pip/pip.conf 1.2 配置master节点可以免密登陆node节点 生成密钥对 ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa &>/dev/null 编辑expect自动化交互脚本 这里机器用户名是root，密码是国际标准通用密码1 cat >ssh.exp 编辑shell脚本循环执行expect脚本 #编辑脚本 cat > ssh.sh 1.3 编辑环境变量脚本 mkdir -p /opt/k8s/script cat >/opt/k8s/script/env.sh 1.4 每个节点配置host信息 source /opt/k8s/script/env.sh for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} 'cat >> /etc/hosts 1.5 禁用防火墙和selinux source /opt/k8s/script/env.sh for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} 'systemctl stop firewalld && systemctl disable firewalld && setenforce 0' done #永久修改，重启服务器后生效 sed -i '7s/enforcing/disabled/' /etc/selinux/config 1.6 关闭swap ⚠️centos7中是 /dev/mapper/centos-swap swap swap defaults 0 0 ⚠️centos8中/dev/mapper/cl-swap swap swap defaults 0 0 source /opt/k8s/script/env.sh for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} \"swapoff -a && sed -i 's/^\\/dev\\/mapper\\/cl-swap/#&/' /etc/fstab\" done 1.7 将桥接的IPv4流量传递到iptables的链 source /opt/k8s/script/env.sh for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} 'cat >/etc/sysctl.d/k8s.conf 1.6 配置时间同步 master节点操作 source /opt/k8s/script/env.sh for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} dnf -y install chrony done #修改服务器地址为阿里云 sed -i -e '/^pool/cserver ntp1.aliyun.com iburst' -e '/^#allow/callow 10.0.0.0/24' /etc/chrony.conf #node节点修改同步服务器为master节点 export NODE_IPS=(10.0.0.33 10.0.0.34) for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} sed -in '/^pool/cserver\\ k8s-master1\\ iburst' /etc/chrony.conf done #启动NTP服务并设置开机自启 source /opt/k8s/script/env.sh for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} 'systemctl enable chronyd && systemctl start chronyd' done #检查端口，chronyd监听udp323端口 source /opt/k8s/script/env.sh for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} netstat -nupl|grep chronyd done #正确输出 >>> 10.0.0.30 udp 0 0 127.0.0.1:323 0.0.0.0:* 4535/chronyd udp6 0 0 ::1:323 :::* 4535/chronyd >>> 10.0.0.33 udp 0 0 127.0.0.1:323 0.0.0.0:* 5984/chronyd udp6 0 0 ::1:323 :::* 5984/chronyd >>> 10.0.0.34 udp 0 0 127.0.0.1:323 0.0.0.0:* 5674/chronyd udp6 0 0 ::1:323 :::* 5674/chronyd #验证同步服务器 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} chronyc sources done #正确输出 >>> 10.0.0.30 210 Number of sources = 1 MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== ^* 120.25.115.20 2 6 17 7 +5664us[+8204us] +/- 34ms >>> 10.0.0.33 210 Number of sources = 1 MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== ^* k8s-master1 3 6 17 3 -632us[ +70us] +/- 4645ms >>> 10.0.0.34 210 Number of sources = 1 MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== ^* k8s-master1 3 6 17 2 -33us[ -93us] +/- 4940ms 二、创建CA根证书和秘钥 CA根证书说明 为确保安全，kubernetes 系统各组件需要使用 x509 证书对通信进行加密和认证。 CA (Certificate Authority) 是自签名的根证书，用来签名后续创建的其它证书。 CA证书是集群所有节点共享的，只需要创建一次，后续用它签名其它所有证书。 本文档使用 CloudFlare 的 PKI 工具集 cfssl 创建所有证书。 2.1 创建存放CA根证书的目录 mkdir -p /opt/k8s/cert && cd /opt/k8s/cert 2.2 下载并配置cfssl工具集 cfssl官网 cfssl github地址 cfssl是一个开源的证书管理工具，使用json文件生成证书，相比openssl更方便使用。 #下载cfssl工具集 wget https://github.com/cloudflare/cfssl/releases/download/v1.4.1/cfssl-certinfo_1.4.1_linux_amd64 wget https://github.com/cloudflare/cfssl/releases/download/v1.4.1/cfssljson_1.4.1_linux_amd64 wget https://github.com/cloudflare/cfssl/releases/download/v1.4.1/cfssl_1.4.1_linux_amd64 #给予执行权限 chmod +x cfssl* #修改名称并移动到/usr/local/bin mv cfssl_1.4.1_linux_amd64 /usr/local/bin/cfssl mv cfssljson_1.4.1_linux_amd64 /usr/local/bin/cfssljson mv cfssl-certinfo_1.4.1_linux_amd64 /usr/local/bin/cfssl-certinfo 2.3 创建CA根证书配置文件 cd /opt/k8s/cert cat > ca-config.json 配置文件中的一些参数说明 signing：表示该证书可用于签名其它证书（生成的 ca.pem 证书中 CA=TRUE）； server auth：表示 client 可以用该该证书对 server 提供的证书进行验证； client auth：表示 server 可以用该该证书对 client 提供的证书进行验证； \"expiry\": \"876000h\"：证书有效期设置为 100 年； 2.4 创建证书签名请求文件 cd /opt/k8s/cert cat > ca-csr.json 配置文件中的一些参数说明 CN：Common Name：kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)，浏览器使用该字段验证网站是否合法； O：Organization：kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)； kube-apiserver 将提取的 User、Group 作为 RBAC 授权的用户标识； ⚠️注意： 不同证书 csr 文件的 CN、C、ST、L、O、OU 组合必须不同，否则可能出现 PEER'S CERTIFICATE HAS AN INVALID SIGNATURE 错误； 后续创建证书的 csr 文件时，CN 都不相同（C、ST、L、O、OU 相同），以达到区分的目的； 2.5 生成 CA 证书和私钥 cd /opt/k8s/cert cfssl gencert -initca ca-csr.json | cfssljson -bare ca ls ca*pem #命令执行成功后会生成如下3个文件 ca.csr ca-key.pem ca.pem 三、部署etcd集群 etcd 是基于 Raft 的分布式 KV 存储系统，由 CoreOS 开发，常用于服务发现、共享配置以及并发控制（如 leader 选举、分布式锁等）。 kubernetes 使用 etcd 集群持久化存储所有 API 对象、运行数据。 本文档介绍部署一个三节点高可用 etcd 集群的步骤： 下载和分发 etcd 二进制文件； 创建 etcd 集群各节点的 x509 证书，用于加密客户端(如 etcdctl) 与 etcd 集群、etcd 集群之间的通信； 创建 etcd 的 systemd unit 文件，配置服务参数； 检查集群工作状态； etcd 集群节点名称和 IP 如下： k8s-master1：10.0.0.30 k8s-node1：10.0.0.33 k8s-node2：10.0.0.34 3.1 创建etcd证书和私钥 3.1.1 创建证书签名请求 hosts：指定授权使用该证书的 etcd 节点 IP 列表，需要将 etcd 集群所有节点 IP 都列在其中 ⚠️文件中的IP是etcd节点的IP，哪些节点部署了etcd就写哪些节点的IP，为了方便后期扩展这里还可以多写几个IP cd /opt/k8s/cert cat > etcd-csr.json 3.1.2 生成证书和私钥 cd /opt/k8s/cert cfssl gencert -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes etcd-csr.json | cfssljson -bare etcd ls etcd*pem #以上命令执行成功后会生成如下文件 etcd.csr etcd-key.pem etcd.pem 3.2 下载etcd二进制文件 etcd github地址 etcd官网 创建目录 mkdir /opt/k8s/etcd && cd /opt/k8s/etcd 下载二进制文件 wget https://github.com/etcd-io/etcd/releases/download/v3.4.9/etcd-v3.4.9-linux-amd64.tar.gzr tar xf etcd-v3.4.9-linux-amd64.tar.gz 拷贝etcd命令到/usr/local/bin cp etcd-v3.4.9-linux-amd64/{etcd,etcdctl} /usr/local/bin 3.3 创建etcd配置文件 这里先创建一个template模版，后续会用sed替换 mkdir /opt/k8s/etcd/cfg && cd /opt/k8s/etcd/cfg cat > etcd.conf.template etcd配置文件参数说明 参数 说明 ETCD_NAME 节点名称，集群中唯一 ETCD_DATA_DIR 数据目录 ETCD_LISTEN_PEER_URLS 集群通信监听地址 ETCD_LISTEN_CLIENT_URLS 客户端访问监听地址 ETCD_INITIAL_ADVERTISE_PEER_URLS 集群通告地址 ETCD_ADVERTISE_CLIENT_URLS 客户端通告地址 ETCD_INITIAL_CLUSTER 集群节点地址 ETCD_INITIAL_CLUSTER_TOKEN 集群Token ETCD_INITIAL_CLUSTER_STATE 加入集群的当前状态，new是新集群，existing表示加入已有集群 3.4 使用systemd管理etcd cat > /usr/lib/systemd/system/etcd.service 3.5 拷贝相关文件到其余node节点 拷贝证书 export NODE_IPS=(10.0.0.33 10.0.0.34) for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} mkdir -p /opt/k8s/cert scp -p /opt/k8s/cert/{etcd-key.pem,etcd.pem,ca-key.pem,ca.pem} root@${node_ip}:/opt/k8s/cert done 拷贝systemd文件 export NODE_IPS=(10.0.0.33 10.0.0.34) for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" scp -r /usr/lib/systemd/system/etcd.service root@${node_ip}:/usr/lib/systemd/system/ done 拷贝etcd配置文件 #先做sed替换，把之前的模版文件中的NODE_IP和ETCD_NAME替换成相对应的 source /opt/k8s/script/env.sh cd /opt/k8s/etcd/cfg for (( i=0; i etcd-${NODE_IPS[i]}.conf done #拷贝etcd配置文件 for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} mkdir -p /opt/k8s/etcd/cfg scp etcd-${node_ip}.conf root@${node_ip}:/opt/k8s/etcd/cfg/etcd.conf done 拷贝etcd命令 export NODE_IPS=(10.0.0.33 10.0.0.34) for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" scp -p /usr/local/bin/{etcd,etcdctl} root@${node_ip}:/usr/local/bin done 3.6 启动etcd source /opt/k8s/script/env.sh for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} \"systemctl daemon-reload && systemctl enable etcd && systemctl restart etcd \" & done 检查启动结果，确保状态为 active (running)，否则使用命令journalctl -u etcd查看日志，确认原因 source /opt/k8s/script/env.sh for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} \"systemctl status etcd|grep Active\" done 正确输出 >>> 10.0.0.30 Active: active (running) since Tue 2020-07-07 00:39:54 CST; 1min 5s ago >>> 10.0.0.33 Active: active (running) since Tue 2020-07-07 00:39:54 CST; 1min 5s ago >>> 10.0.0.34 Active: active (running) since Tue 2020-07-07 00:39:54 CST; 1min 5s ago 3.7 验证服务状态 部署完 etcd 集群后，在任一 etcd 节点上执行如下命令 source /opt/k8s/script/env.sh for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" /usr/local/bin/etcdctl \\ --endpoints=https://${node_ip}:2379 \\ --cacert=/opt/k8s/cert/ca.pem \\ --cert=/opt/k8s/cert/etcd.pem \\ --key=/opt/k8s/cert/etcd-key.pem endpoint health done 正确输出 >>> 10.0.0.30 https://10.0.0.30:2379 is healthy: successfully committed proposal: took = 7.228637ms >>> 10.0.0.33 https://10.0.0.33:2379 is healthy: successfully committed proposal: took = 7.620834ms >>> 10.0.0.34 https://10.0.0.34:2379 is healthy: successfully committed proposal: took = 6.689841ms 3.8 查看当前etcd集群 leader export ETCD_ENDPOINTS=\"https://10.0.0.30:2379,https://10.0.0.33:2379,https://10.0.0.34:2379\" etcdctl \\ -w table --cacert=/opt/k8s/cert/ca.pem \\ --cert=/opt/k8s/cert/etcd.pem \\ --key=/opt/k8s/cert/etcd-key.pem \\ --endpoints=${ETCD_ENDPOINTS} endpoint status 输出结果，可见当前的 etcd leader 是10.0.0.30 +------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | +------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | https://10.0.0.30:2379 | dd4b95995dc266b1 | 3.4.9 | 16 kB | true | false | 2 | 8 | 8 | | | https://10.0.0.33:2379 | f1ec1f6015c9d4a4 | 3.4.9 | 20 kB | false | false | 2 | 8 | 8 | | | https://10.0.0.34:2379 | 22353e8ece256e71 | 3.4.9 | 20 kB | false | false | 2 | 8 | 8 | | +------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ 四、安装docker docker官网 docker-ce github地址 docker官方二进制下载地址 所有节点操作 4.1 下载二进制安装包并解压缩 创建目录 mkdir /opt/k8s/docker && cd /opt/k8s/docker 下载包并解压缩 wget https://download.docker.com/linux/static/stable/x86_64/docker-19.03.12.tgz tar xf docker-19.03.12.tgz 4.2 导出docker命令环境变量 cd /opt/k8s/docker source /opt/k8s/script/env.sh for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" scp -p docker/* root@${node_ip}:/usr/local/bin done 4.3 使用systemd管理docker cat > /usr/lib/systemd/system/docker.service 把docker systemd文件拷贝到所有node节点 export NODE_IPS=(10.0.0.33 10.0.0.34) for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" scp -p /usr/lib/systemd/system/docker.service root@${node_ip}:/usr/lib/systemd/system/docker.service done 4.4 创建docker配置文件 所有机器配置加速源并配置docker的启动参数使用systemd，使用systemd是官方的建议 mkdir /etc/docker cat > /etc/docker/daemon.json 拷贝docker配置文件到node节点 export NODE_IPS=(10.0.0.33 10.0.0.34) for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} mkdir /etc/docker scp -p /etc/docker/daemon.json root@${node_ip}:/etc/docker/daemon.json done 4.5 启动docker并设置开机自启 source /opt/k8s/script/env.sh for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} \"systemctl daemon-reload && systemctl enable docker && systemctl start docker \" done 4.6 检查docker启动状态 确保所有节点docker都为running状态 source /opt/k8s/script/env.sh for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} \"systemctl status docker|grep Active\" done 正确输出 >>> 10.0.0.30 Active: active (running) since Tue 2020-07-07 09:07:57 CST; 21s ago >>> 10.0.0.33 Active: active (running) since Tue 2020-07-07 09:07:58 CST; 20s ago >>> 10.0.0.34 Active: active (running) since Tue 2020-07-07 09:07:59 CST; 20s ago 4.7 设置docker命令自动补全 yum安装的docker会有一个文件/usr/share/bash-completion/completions/docker，这个文件就是自动补全docker命令的文件，二进制安装的没有，把这个文件拷贝过来即可 五、部署Mster Node kubernetes master 节点运行如下组件： kube-apiserver kube-scheduler kube-controller-manager 5.1 部署 kube-apiserver 5.1.1 生成 kube-apiserver 证书和私钥 创建证书签名请求 cd /opt/k8s/cert cat > kube-apiserver-csr.json 生成证书和私钥 /opt/k8s/cert cfssl gencert -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes kube-apiserver-csr.json | cfssljson -bare kube-apiserver ls kube-api*pem #上述命令执行成功后会生成如下文件 kube-apiserver.csr kube-apiserver-key.pem kube-apiserver.pem 5.1.2 下载 kubernetes-server 二进制包 mkdir /opt/k8s/kubernetes-server && cd /opt/k8s/kubernetes-server wget https://dl.k8s.io/v1.18.5/kubernetes-server-linux-amd64.tar.gz tar xf kubernetes-server-linux-amd64.tar.gz 导出命令 cp kubernetes/server/bin/{apiextensions-apiserver,kubeadm,kube-apiserver,kube-controller-manager,kubectl,kubelet,kube-proxy,kube-scheduler,mounter} /usr/local/bin/ 5.1.3 创建 kube-apiserver 配置文件 mkdir /opt/k8s/{cfg,logs} cat > /opt/k8s/cfg/kube-apiserver.conf –logtostderr：启用日志 —v：日志等级 –log-dir：日志目录 –etcd-servers：etcd集群地址 –bind-address：监听地址 –secure-port：https安全端口 –advertise-address：集群通告地址 –allow-privileged：启用授权 –service-cluster-ip-range：Service虚拟IP地址段 –enable-admission-plugins：准入控制模块 –authorization-mode：认证授权，启用RBAC授权和节点自管理 –enable-bootstrap-token-auth：启用TLS bootstrap机制 –token-auth-file：bootstrap token文件 –service-node-port-range：Service nodeport类型默认分配端口范围 –kubelet-client-xxx：apiserver访问kubelet客户端证书 –tls-xxx-file：apiserver https证书 –etcd-xxxfile：连接Etcd集群证书 –audit-log-xxx：审计日志 5.1.4 启用 TLS Bootstrapping 机制 TLS Bootstraping： Master apiserver启用TLS认证后，Node节点kubelet和kube-proxy要与kube-apiserver进行通信，必须使用CA签发的有效证书才可以，当Node节点很多时，这种客户端证书颁发需要大量工作，同样也会增加集群扩展复杂度。为了简化流程，Kubernetes引入了TLS bootstraping机制来自动颁发客户端证书，kubelet会以一个低权限用户自动向apiserver申请证书，kubelet的证书由apiserver动态签署。所以强烈建议在Node上使用这种方式，目前主要用于kubelet，kube-proxy还是由我们统一颁发一个证书。 TLS bootstraping 工作流程： 创建kube-apiserver配置文件中/opt/k8s/cfg/kube-apiserver.conf指定的 --token-auth-file=/opt/k8s/cfg/token.csv 格式：token，用户名，UID，用户组 cd /opt/k8s/cfg export TOKEN_CSV=`head -c 16 /dev/urandom | od -An -t x | tr -d ' '` cat > /opt/k8s/cfg/token.csv 5.1.5 使用systemd管理kube-apiserver cat > /usr/lib/systemd/system/kube-apiserver.service 5.1.6 启动 kube-apiserver 并设置开机自启 systemctl daemon-reload systemctl start kube-apiserver && systemctl enable kube-apiserver 检查 kube-apiserver 是否正确启动，如果没有正确启动，使用命令journalctl -u kube-apiserver查看日志 $ systemctl status kube-apiserver |grep Active Active: active (running) since Tue 2020-07-07 10:01:15 CST; 37s ago 5.1.7 授权 kubelet-bootstrap 用户允许请求证书 kubectl create clusterrolebinding kubelet-bootstrap \\ --clusterrole=system:node-bootstrapper \\ --user=kubelet-bootstrap 5.2 部署 kube-controller-manager 5.2.1 创建配置文件 cat > /opt/k8s/cfg/kube-controller-manager.conf –master：通过本地非安全端口8080连接apiserver。 –leader-elect：当该组件启动多个时，自动选举（HA） –cluster-signing-cert-file/–cluster-signing-key-file：自动为kubelet颁发证书的CA，与apiserver保持一致 5.2.2 systemd管理 kube-controller-manager cat > /usr/lib/systemd/system/kube-controller-manager.service 5.2.3 启动 kube-controller-manager 并设置开机自启 systemctl daemon-reload systemctl start kube-controller-manager && systemctl enable kube-controller-manager 检查 kube-controller-manager 是否正确启动，如果没有正确启动，使用命令journalctl -u kube-controller-manager查看日志 $ systemctl status kube-controller-manager |grep Active Active: active (running) since Tue 2020-07-07 10:01:15 CST; 37s ago 5.3 部署 kube-scheduler 5.3.1 创建 kube-scheduler 配置文件 cat > /opt/k8s/cfg/kube-scheduler.conf –master：通过本地非安全本地端口8080连接apiserver。 –leader-elect：当该组件启动多个时，自动选举（HA） 5.3.2 使用systemd管理 kube-scheduler cat > /usr/lib/systemd/system/kube-scheduler.service 5.3.3 启动 kube-scheduler 并设置开机自启 systemctl daemon-reload systemctl start kube-scheduler && systemctl enable kube-scheduler 检查 kube-scheduler 是否正确启动，如果没有正确启动，使用命令journalctl -u kube-scheduler查看日志 $ systemctl status kube-scheduler |grep Active Active: active (running) since Wed 2020-07-08 09:45:23 CST; 44s ago 5.4 k8s命令自动补全、切换命名空间 5.4.1 设置k8s命令自动补全 dnf -y install bash-completion source /usr/share/bash-completion/bash_completion source > ~/.bashrc 5.4.2 配置k8s切换命名空间工具 #克隆工具 git clone https://github.com.cnpmjs.org/ahmetb/kubectx cp kubectx/kubens /usr/local/bin #查看所有命名空间 $ kubens #切换到kube-system命名空间 $ kubens kube-system Context \"kubernetes-admin@kubernetes\" modified. Active namespace is \"kube-system\". 5.5 查看Master Node节点集群状态 $ kubectl get cs NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-2 Healthy {\"health\":\"true\"} etcd-0 Healthy {\"health\":\"true\"} etcd-1 Healthy {\"health\":\"true\"} 六、部署Worker Node kubernetes worker 节点运行如下组件： kubelet kube-proxy flannel ⚠️如果后续有pod需要部署在master节点，则在master节点也需要部署kubelet和kube-proxy ⚠️这里把master节点也复用为node节点，即master节点上部署node 6.1 Worker Node节点创建工作目录 在所有worker node节点创建工作目录 export NODE_IPS=(10.0.0.33 10.0.0.34) for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} mkdir -p /opt/k8s/{logs,cfg} done 6.2 部署 kubelet 6.2.1 创建 kukelet 配置文件 pause-amd64:3.0原先地址是gcr.io/google-containers/pause-amd64:3.0 ⚠️kubelet配置文件中的--hostname-override要与主机名相对应 先做一个模版，后续会用sed替换文件中的主机名--hostname-override mkdir /opt/k8s/kubelet && cd /opt/k8s/kubelet cat > kubelet.template –hostname-override：显示名称，集群中唯一 –network-plugin：启用CNI –kubeconfig：空路径，会自动生成，后面用于连接apiserver –bootstrap-kubeconfig：首次启动向apiserver申请证书 –config：配置参数文件 –cert-dir：kubelet证书生成目录 –pod-infra-container-image：管理Pod网络容器的镜像 使用sed做替换 source /opt/k8s/script/env.sh cd /opt/k8s/kubelet for (( i=0; i kubelet-${NODE_IPS[i]}.conf done #替换完成后会生成如下文件，每一个节点文件中的--hostname-override就是自己的主机名 ls *.conf kubelet-10.0.0.30.conf kubelet-10.0.0.33.conf kubelet-10.0.0.34.conf 拷贝文件到所有节点 source /opt/k8s/script/env.sh cd /opt/k8s/kubelet for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" scp kubelet-${node_ip}.conf root@${node_ip}:/opt/k8s/cfg/kubelet.conf done 6.2.2 创建 kubelet 参数配置文件 ⚠️因为安装的docker已经使用了systemd，因此参数配置文件中的cgroupDriver: systemd也要做修改 source /opt/k8s/script/env.sh for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} 'cat > /opt/k8s/cfg/kubelet-config.yml 6.2.3 生成bootstrap.kubeconfig文件 这个配置文件就是能让kubelet连接kube-apiserver，去请求颁发证书 cd /opt/k8s/cfg export KUBE_APISERVER=\"https://10.0.0.30:6443\" # apiserver IP:PORT export TOKEN=`awk -F, '{print $1}' /opt/k8s/cfg/token.csv` # 与token.csv里保持一致 # 生成 kubelet bootstrap kubeconfig 配置文件 kubectl config set-cluster kubernetes \\ --certificate-authority=/opt/k8s/cert/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=bootstrap.kubeconfig kubectl config set-credentials \"kubelet-bootstrap\" \\ --token=${TOKEN} \\ --kubeconfig=bootstrap.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=\"kubelet-bootstrap\" \\ --kubeconfig=bootstrap.kubeconfig kubectl config use-context default --kubeconfig=bootstrap.kubeconfig 拷贝bootstrap.kubeconfig到node节点 export NODE_IPS=(10.0.0.33 10.0.0.34) /opt/k8s/cfg for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" scp bootstrap.kubeconfig root@${node_ip}:/opt/k8s/cfg done 6.2.4 拷贝kubelet、kube-proxy命令到node节点 export NODE_IPS=(10.0.0.33 10.0.0.34) for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" scp -p /usr/local/bin/{kubelet,kube-proxy} ${node_ip}:/usr/local/bin done 6.2.5 使用systemd管理 kubelet cat > /usr/lib/systemd/system/kubelet.service 拷贝文件到node节点 export NODE_IPS=(10.0.0.33 10.0.0.34) for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" scp -p /usr/lib/systemd/system/kubelet.service root@${node_ip}:/usr/lib/systemd/system done 6.2.6 启动 kubelet 并设置开机自启 source /opt/k8s/script/env.sh for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} 'systemctl daemon-reload && systemctl start kubelet && systemctl enable kubelet' done 验证 kubelet 启动状态 source /opt/k8s/script/env.sh for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} 'systemctl status kubelet|grep active' done 正确输出 >>> 10.0.0.30 Active: active (running) since Tue 2020-07-07 13:27:37 CST; 19s ago >>> 10.0.0.33 Active: active (running) since Tue 2020-07-07 13:27:52 CST; 4s ago >>> 10.0.0.34 Active: active (running) since Tue 2020-07-07 13:27:52 CST; 4s ago 6.2.7 批准kubelet证书申请并加入集群 ⚠️当kubelet启动成功的时候就会有节点过来请求颁发证书，使用命令kubectl get csr查看 查看kukelet证书请求，可以看到有3个节点请求颁发证书 $ kubectl get csr NAME AGE SIGNERNAME REQUESTOR CONDITION node-csr-4YfUpgFR7xndRzDFy4HHOfvRhO_p7iucRgB8dGptrIM 2m53s kubernetes.io/kube-apiserver-client-kubelet kubelet-bootstrap Pending node-csr-Fh0hY-oE3W6d7_TlO06d-eV9CD93iF3r3dSQSk9dtSo 2m37s kubernetes.io/kube-apiserver-client-kubelet kubelet-bootstrap Pending node-csr-kG8QBmV1SUcR39FGV2JtSDpJzEHxlRpJnEJhLv2W7fI 2m38s kubernetes.io/kube-apiserver-client-kubelet kubelet-bootstrap Pending 使用命令kubectl certificate approve csr名车批准申请 #使用如下命令批量操作 for i in `kubectl get csr|awk 'NR>1{print $1}'` do kubectl certificate approve $i done certificatesigningrequest.certificates.k8s.io/node-csr-4YfUpgFR7xndRzDFy4HHOfvRhO_p7iucRgB8dGptrIM approved certificatesigningrequest.certificates.k8s.io/node-csr-Fh0hY-oE3W6d7_TlO06d-eV9CD93iF3r3dSQSk9dtSo approved certificatesigningrequest.certificates.k8s.io/node-csr-kG8QBmV1SUcR39FGV2JtSDpJzEHxlRpJnEJhLv2W7fI approved 再次查看kubelet证书请求 $ kubectl get csr NAME AGE SIGNERNAME REQUESTOR CONDITION node-csr-4YfUpgFR7xndRzDFy4HHOfvRhO_p7iucRgB8dGptrIM 6m35s kubernetes.io/kube-apiserver-client-kubelet kubelet-bootstrap Approved,Issued node-csr-Fh0hY-oE3W6d7_TlO06d-eV9CD93iF3r3dSQSk9dtSo 6m19s kubernetes.io/kube-apiserver-client-kubelet kubelet-bootstrap Approved,Issued node-csr-kG8QBmV1SUcR39FGV2JtSDpJzEHxlRpJnEJhLv2W7fI 6m20s kubernetes.io/kube-apiserver-client-kubelet kubelet-bootstrap Approved,Issued 6.3 部署 kube-proxy 6.3.1 创建 kube-proxy 配置文件 export NODE_IPS=(10.0.0.30 10.0.0.33 10.0.0.34) for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} 'cat > /opt/k8s/cfg/kube-proxy.conf 6.3.2 创建 kube-proxy 参数文件 先生成一个模版文件，后续会用sed替换文件中的主机名hostnameOverride mkdir /opt/k8s/kube-proxy && cd /opt/k8s/kube-proxy cat > kube-proxy-config.yml.template 使用sed做替换 source /opt/k8s/script/env.sh cd /opt/k8s/kube-proxy for (( i=0; i kube-proxy-config-${NODE_IPS[i]}.yml done #替换完成后会生成如下文件，每一个节点文件中的hostnameOverride就是本机主机名 ls kube*.yml kube-proxy-config-10.0.0.30.yml kube-proxy-config-10.0.0.33.yml kube-proxy-config-10.0.0.34.yml 拷贝文件到所有节点 source /opt/k8s/script/env.sh cd /opt/k8s/kube-proxy for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" scp kube-proxy-config-${node_ip}.yml root@${node_ip}:/opt/k8s/cfg/kube-proxy-config.yml done 6.3.3 生成kube-proxy证书 创建证书请求文件 cd /opt/k8s/cert cat > kube-proxy-csr.json 生成证书 cfssl gencert -ca=/opt/k8s/cert/ca.pem -ca-key=/opt/k8s/cert/ca-key.pem -config=/opt/k8s/cert/ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy #以上命令执行成功后会生成如下文件 ls kube-proxy*pem kube-proxy-key.pem kube-proxy.pem kube-proxy.csr 6.3.3 生成 kube-proxy.kubeconfig 文件 export KUBE_APISERVER=\"https://10.0.0.30:6443\" cd /opt/k8s/cfg kubectl config set-cluster kubernetes \\ --certificate-authority=/opt/k8s/cert/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-credentials kube-proxy \\ --client-certificate=/opt/k8s/cert/kube-proxy.pem \\ --client-key=/opt/k8s/cert/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 拷贝 kube-proxy.kubeconfig 到node节点 export NODE_IPS=(10.0.0.33 10.0.0.34) cd /opt/k8s/cfg for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" scp -p kube-proxy.kubeconfig root@${node_ip}:/opt/k8s/cfg done 6.3.4 使用systemd管理 kube-proxy cat > /usr/lib/systemd/system/kube-proxy.service 拷贝文件到node节点 export NODE_IPS=(10.0.0.33 10.0.0.34) for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" scp -p /usr/lib/systemd/system/kube-proxy.service root@${node_ip}:/usr/lib/systemd/system done 6.3.5 启动 kube-proxy 并设置开机自启 source /opt/k8s/script/env.sh for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} 'systemctl daemon-reload && systemctl start kube-proxy && systemctl enable kube-proxy' done 验证 kube-proxy 启动状态 source /opt/k8s/script/env.sh for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} 'systemctl status kube-proxy|grep active' done 正确输出 >>> 10.0.0.30 Active: active (running) since Tue 2020-07-07 14:06:01 CST; 10s ago >>> 10.0.0.33 Active: active (running) since Tue 2020-07-07 14:03:18 CST; 2min 53s ago >>> 10.0.0.34 Active: active (running) since Tue 2020-07-07 14:03:18 CST; 2min 53s ago 6.4 部署CNI网络 创建目录 mkdir /opt/k8s/cni && cd /opt/k8s/cni 下载安装包 wget https://github.com/containernetworking/plugins/releases/download/v0.8.6/cni-plugins-linux-amd64-v0.8.6.tgz 导出命令 包解压后全是可执行的二进制文件 mkdir -p /opt/cni/bin tar xf cni-plugins-linux-amd64-v0.8.6.tgz -C /opt/cni/bin 拷贝命令到node节点 source /opt/k8s/script/env.sh for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" scp -rp /opt/cni root@${node_ip}:/opt done 下载yaml文件 #下载yaml文件并替换镜像仓库地址 cd /opt/k8s/cni wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml sed -i -r \"s#quay.io/coreos/flannel:.*-amd64#pptfz/flannel:v0.12.0-amd64#g\" kube-flannel.yml 部署CNI网络 kubectl apply -f kube-flannel.yml 6.5 授权apiserver访问kubelet cd /opt/k8s/cfg cat > apiserver-to-kubelet-rbac.yaml 6.6 验证 网络插件flannel pod状态必须全部为running $ kubectl -n kube-system get pods NAME READY STATUS RESTARTS AGE kube-flannel-ds-amd64-ncrzv 1/1 Running 0 11m kube-flannel-ds-amd64-spn7q 1/1 Running 0 11m kube-flannel-ds-amd64-vk5qr 1/1 Running 0 11m 网络部署完成后，所有节点状态就都变为了Ready $ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master1 Ready 18m v1.18.5 k8s-node1 Ready 18m v1.18.5 k8s-node2 Ready 18m v1.18.5 删除node节点的kubelet证书文件 ⚠️这几个文件是证书申请审批后自动生成的，每个Node不同，必须删除重新生成 export NODE_IPS=(10.0.0.33 10.0.0.34) for node_ip in ${NODE_IPS[@]} do echo \">>> ${node_ip}\" ssh root@${node_ip} rm -rf /opt/k8s/cert/kubelet* done 七、部署CoreDNS 创建目录 mkdir /opt/k8s/coredns && cd /opt/k8s/coredns 下载CoreDNS项目 git clone https://github.com.cnpmjs.org/coredns/deployment.git cd deployment/kubernetes 默认情况下 CLUSTER_DNS_IP 是自动获取kube-dns的集群ip的，但是由于没有部署kube-dns所以只能手动指定一个集群ip。 编辑文件deploy.sh，注释文件中的 CLUSTER_DNS_IP=$(kubectl get service --namespace kube-system kube-dns -o jsonpath=\"{.spec.clusterIP}\")，修改为CLUSTER_DNS_IP=172.16.0.2，因为在kubelet参数配置文件/opt/k8s/cfg/kubelet-config.yml中指定了clusterDNS=172.16.0.2 #原先内容 103行处 if [[ -z $CLUSTER_DNS_IP ]]; then # Default IP to kube-dns IP CLUSTER_DNS_IP=$(kubectl get service --namespace kube-system kube-dns -o jsonpath=\"{.spec.clusterIP}\") #修改为如下 if [[ -z $CLUSTER_DNS_IP ]]; then # Default IP to kube-dns IP # CLUSTER_DNS_IP=$(kubectl get service --namespace kube-system kube-dns -o jsonpath=\"{.spec.clusterIP}\") CLUSTER_DNS_IP=172.16.0.2 部署CoreDNS #查看执行效果，并未真正开始部署 ./deploy.sh #执行部署 ./deploy.sh | kubectl apply -f - # 查看 CoreDNS $ kubectl get svc,pods -n kube-system| grep coredns pod/coredns-85b4878f78-2ndvz 0/1 Running 0 9s 测试 CoreDNS 解析 mkdir /opt/k8s/yaml && cd /opt/k8s/yaml cat > busybox.yaml #返回以下内容说明解析正常 $ kubectl exec -i busybox -n default nslookup kubernetes kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead. Server: 172.16.0.2 Address 1: 172.16.0.2 kube-dns.kube-system.svc.cluster.local Name: kubernetes Address 1: 172.16.0.1 kubernetes.default.svc.cluster.local 八、部署dashboard 8.1 部署官方dashboard 下载yaml文件 cd /opt/k8s/yaml wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.3/aio/deploy/recommended.yaml 默认Dashboard只能集群内部访问，修改Service为NodePort类型，暴露到外部 #原先内容 spec: ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard #修改为如下 spec: ports: - port: 443 targetPort: 8443 nodePort: 30001 type: NodePort selector: k8s-app: kubernetes-dashboard 创建dashboard #应用文件 kubectl apply -f recommended.yaml #查看pod运行状态 $ kubectl get pods -A |grep kubernetes-dashboard kubernetes-dashboard dashboard-metrics-scraper-6b4884c9d5-lrz94 1/1 Running 0 47s kubernetes-dashboard kubernetes-dashboard-7f99b75bf4-ndztw 1/1 Running 0 47s 创建service account并绑定默认cluster-admin管理员集群角色 kubectl create serviceaccount dashboard-admin -n kube-system kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin 查看token kubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk '/dashboard-admin/{print $1}') 浏览器访问 https://集群任意节点IP:30001 ⚠️只能用火狐浏览器访问 登陆后首界面 8.2 部署kuboard kuboard官网 安装 kubectl apply -f https://kuboard.cn/install-script/kuboard.yaml kubectl apply -f https://addons.kuboard.cn/metrics-server/0.3.6/metrics-server.yaml 查看kuboard运行状态 $ kubectl get pods -l k8s.kuboard.cn/name=kuboard -n kube-system NAME READY STATUS RESTARTS AGE kuboard-7bb89b4cc4-7rqh4 1/1 Running 0 78s 获取管理员token，此Token拥有 ClusterAdmin 的权限，可以执行所有操作 echo $(kubectl -n kube-system get secret $(kubectl -n kube-system get secret | grep kuboard-user | awk '{print $1}') -o go-template='{{.data.token}}' | base64 -d) 访问kuboard Kuboard Service 使用了 NodePort 的方式暴露服务，NodePort 为 32567，浏览器访问http://任意一个Worker节点的IP地址:32567/ 登陆后首界面 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s证书/kubeam安装k8s生成的证书.html":{"url":"linux/k8s/k8s证书/kubeam安装k8s生成的证书.html","title":"kubeadm","keywords":"","body":"[toc] 本文严重抄袭至互联网 傻傻分不清楚的kubernetes证书 kubeadm 生成的一坨证书是不是让人很蒙逼，这些东西没那么神奇，来深入扒扒其内裤。 root@k8s-master:/etc/kubernetes/pki# tree . |-- apiserver.crt |-- apiserver-etcd-client.crt |-- apiserver-etcd-client.key |-- apiserver.key |-- apiserver-kubelet-client.crt |-- apiserver-kubelet-client.key |-- ca.crt |-- ca.key |-- etcd | |-- ca.crt | |-- ca.key | |-- healthcheck-client.crt | |-- healthcheck-client.key | |-- peer.crt | |-- peer.key | |-- server.crt | `-- server.key |-- front-proxy-ca.crt |-- front-proxy-ca.key |-- front-proxy-client.crt |-- front-proxy-client.key |-- sa.key `-- sa.pub 1 directory, 22 files 从RSA说起 要深入了解证书的作用，首先需要了解一些原理和具备一些基本知识，比如什么是非对称加密，什么是公钥，私钥，数字签名是啥等。先从RSA算法说起。 非对称加密会生成一个密钥对，如上面的sa.key sa.pub就是密钥对，一个用于加密一个用于解密。 明文 + 公钥 => 密文 密文 + 私钥 => 明文 那么此时没有私钥，就很难把密文解密。 进一步再详细看看其原理, 不想关注的可以跳过下面原理部分： 假设我们想加密一个单词Caesar, 先把它变成一串数字，比如Ascii码 X = 067097101115097114 这也就是我们需要加密的 明码。 现在来对X进行加密。 找两个很大的质数 P 和 Q 计算他们的乘积 N = P * Q 再令M = (P - 1)(Q - 1) 找到一个数E满足E和M除了1以外没有公约数 找到一个数D满足E乘以D除以M余1, E * D mod M = 1 现在 E就是公钥，可以公开给任何人进行加密 D就是私钥，用于解密，一定要自己保存好 联系公钥和私钥的N是公开的, 为什么这个可以公开，就是因为根据P Q算出N很简单，但是把N分解成P Q两个大质数非常的难，所以公开了现有的计算机算力也很难破解 现在来加密： pow(X,E) mod N = Y Y就是密文，现在没有D(私钥) 神仙也没法算出X(明文) 解密： pow(Y,D) mod N = X X是明文，明文就出来了。 数学是不是很神奇，现在可认为 sa.key = D sa.pub = E 数字签名 假设你写一封信给老板，内容是\"老板我崇拜你\"，然后让同事把信送给老板，怎么确定这信就是你写的，而且怎么防止同事送信过程中把信改成 \"老板你是个SB\"? 可以这样做，首先你生成一个密钥对，把公钥给老板，然后对信的内容做一个hash摘要，再用私钥对摘要进行加密，结果就是签名 这样老板拿到信之后用公钥进行解密，发现得到的hash值与信的hash值是一致的，这样确定了信就是你写的 所以数字签名是加密技术的一种运用，与完全加密信息的区别是这里信息是公开的，你的同事可以看到你吹捧老板。 数字证书 根证书与证书 通常我们配置https服务时需要到\"权威机构\"申请证书。 过程是这样的： 网站创建一个密钥对，提供公钥和组织以及个人信息给权威机构 权威机构颁发证书 浏览网页的朋友利用权威机构的根证书公钥解密签名，对比摘要，确定合法性 客户端验证域名信息有效时间等（浏览器基本都内置各大权威机构的CA公钥） 这个证书包含如下内容： 申请者公钥 申请者组织和个人信息 签发机构CA信息，有效时间，序列号等 以上信息的签名 根证书又名自签名证书，也就是自己给自己颁发的证书。CA(Certificate Authority)被称为证书授权中心，k8s中的ca证书就是根证书。 kubernetes证书 有了以上基础，下面咱们正式开始。。。 先分类： 密钥对：sa.key sa.pub 根证书：ca.crt etcd/ca 私钥 ： ca.key 等 其它证书 首先其它证书都是由CA根证书颁发的，kubernetes与etcd使用了不同的CA, 很重要的一点是证书是用于客户端校验还是服务端校验。 下面一个一个来看： service Account密钥对 sa.key sa.pub 提供给 kube-controller-manager 使用. kube-controller-manager 通过 sa.key 对 token 进行签名, master 节点通过公钥 sa.pub 进行签名的验证 如 kube-proxy 是以 pod 形式运行的, 在 pod 中, 直接使用 service account 与 kube-apiserver 进行认证, 此时就不需要再单独为 kube-proxy 创建证书了, 会直接使用token校验 根证书 pki/ca.crt pki/ca.key 为k8s集群证书签发机构 apiserver 证书 pki/apiserver.crt pki/apiserver.key kubelet证书 pki/apiserver-kubelet-client.crt pki/apiserver-kubelet-client.key kubelet要主动访问kube-apiserver, kube-apiserver也需要主动向kubelet发起请求, 所以双方都需要有自己的根证书以及使用该根证书签发的服务端证书和客户端证书. 在kube-apiserver中, 一般明确指定用于https访问的服务端证书和带有CN用户名信息的客户端证书. 而在kubelet的启动配置中, 一般只指定了ca根证书, 而没有明确指定用于https访问的服务端证书,在生成服务端证书时, 一般会指定服务端地址或主机名, kube-apiserver相对变化不是很频繁, 所以在创建集群之初就可以预先分配好用作 kube-apiserver的IP 或主机名/域名, 但是由于部署在node节点上的kubelet会因为集群规模的变化而频繁变化, 而无法预知node的所有IP信息, 所以kubelet上一般不会明确指定服务端证书, 而是只指定ca根证书, 让kubelet根据本地主机信息自动生成服务端证书并保存到配置的cert-dir文件夹中 Aggregation 证书 代理根证书： pki/front-proxy-ca.crt pki/front-proxy-ca.key 由代理根证书签发的客户端证书： pki/front-proxy-client.crt pki/front-proxy-client.key 比如使用kubectl proxy代理访问时，kube-apiserver使用这个证书来验证客户端证书是否是自己签发的证书。 etcd 根证书 pki/etcd/ca.crt pki/etcd/ca.key etcd节点间相互通信 peer证书 由根证书签发 pki/etcd/peer.crt pki/etcd/peer.key pod中Liveness探针客户端证书 pki/etcd/healthcheck-client.crt pki/etcd/healthcheck-client.key 可查看yaml探活配置： Liveness: exec [/bin/sh -ec ETCDCTL_API=3 etcdctl \\ --endpoints=https://[127.0.0.1]:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \\ --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get foo] \\ delay=15s timeout=15s period=10s #success=1 #failure=8 apiserver访问etcd的证书 pki/apiserver-etcd-client.crt pki/apiserver-etcd-client.key 这里注意一下客户端证书与服务端证书区别，服务端证书通常会校验地址域名等。 代码实现 kubeadm把证书时间写死成了1年（client-go就写死了），这是个悲伤的故事，导致sealos不得不把证书生成的逻辑剥离出来以让安装支持任意过期时间。 下面根据源码来深入体验下kubeadm的证书生成，直接看kubeadm代码可能有点累，sealos/cert目录剥离出核心的代码更容易读懂一些。 以下为了突出核心逻辑，代码中删除一些错误处理细节，有兴趣可阅读github.com/fanux/sealos/cert源码 密钥对生成 // create sa.key sa.pub for service Account func GenerateServiceAccountKeyPaire(dir string) error { key, err := NewPrivateKey(x509.RSA) pub := key.Public() err = WriteKey(dir, \"sa\", key) return WritePublicKey(dir, \"sa\", pub) } 生成私钥, 这里的keyType是x509.RSA func NewPrivateKey(keyType x509.PublicKeyAlgorithm) (crypto.Signer, error) { if keyType == x509.ECDSA { return ecdsa.GenerateKey(elliptic.P256(), rand.Reader) } return rsa.GenerateKey(rand.Reader, rsaKeySize) } 生成CA证书 会返回ca.crt（自签名证书） ca.key(私钥) func NewCaCertAndKey(cfg Config) (*x509.Certificate, crypto.Signer, error) { key, err := NewPrivateKey(x509.UnknownPublicKeyAlgorithm) cert, err := NewSelfSignedCACert(key, cfg.CommonName, cfg.Organization, cfg.Year) return cert, key, nil } 根据私钥生成自签名证书, NotAfter就是证书过期时间，我们很友好的加了个变量而不是写死： // NewSelfSignedCACert creates a CA certificate func NewSelfSignedCACert(key crypto.Signer, commonName string, organization []string, year time.Duration) (*x509.Certificate, error) { now := time.Now() tmpl := x509.Certificate{ SerialNumber: new(big.Int).SetInt64(0), Subject: pkix.Name{ CommonName: commonName, Organization: organization, }, NotBefore: now.UTC(), NotAfter: now.Add(duration365d * year).UTC(), KeyUsage: x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature | x509.KeyUsageCertSign, BasicConstraintsValid: true, IsCA: true, } certDERBytes, err := x509.CreateCertificate(rand.Reader, &tmpl, &tmpl, key.Public(), key) return x509.ParseCertificate(certDERBytes) } 非常要注意里面的CommonName和Organization字段，非常有用，比如我们创建一个k8s用户指定该用户属于哪个用户组，对应上面这两个字段。 比如证书中 fanux 属于 sealyun这个组织，那么生成一个kubeconfig, 就相当于有了fanux这个用户，这样k8s在做认证时只需要校验签名就行，而不需要去访问 数据库来做认证，这非常有利于apiserver的横向扩展。 生成其它证书 密钥对还是自己生成，然后签证书时会把根证书信息带上 func NewCaCertAndKeyFromRoot(cfg Config, caCert *x509.Certificate, caKey crypto.Signer) (*x509.Certificate, crypto.Signer, error) { key, err := NewPrivateKey(x509.UnknownPublicKeyAlgorithm) cert, err := NewSignedCert(cfg, key, caCert, caKey) return cert, key, nil } 此时就必须要求有CommonName了，Usages也得指定是服务端使用还是客户端使用, 注意与上面SelfSign的区别 // NewSignedCert creates a signed certificate using the given CA certificate and key func NewSignedCert(cfg Config, key crypto.Signer, caCert *x509.Certificate, caKey crypto.Signer) (*x509.Certificate, error) { serial, err := rand.Int(rand.Reader, new(big.Int).SetInt64(math.MaxInt64)) if len(cfg.CommonName) == 0 { return nil, errors.New(\"must specify a CommonName\") } if len(cfg.Usages) == 0 { return nil, errors.New(\"must specify at least one ExtKeyUsage\") } certTmpl := x509.Certificate{ Subject: pkix.Name{ CommonName: cfg.CommonName, Organization: cfg.Organization, }, DNSNames: cfg.AltNames.DNSNames, IPAddresses: cfg.AltNames.IPs, SerialNumber: serial, NotBefore: caCert.NotBefore, NotAfter: time.Now().Add(duration365d * cfg.Year).UTC(), KeyUsage: x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature, ExtKeyUsage: cfg.Usages, } certDERBytes, err := x509.CreateCertificate(rand.Reader, &certTmpl, caCert, key.Public(), caKey) return x509.ParseCertificate(certDERBytes) } kubernetes中的所有证书 根证书列表 var caList = []Config{ { Path: BasePath, BaseName: \"ca\", CommonName: \"kubernetes\", Organization: nil, Year: 100, AltNames: AltNames{}, Usages: nil, }, { Path: BasePath, BaseName: \"front-proxy-ca\", CommonName: \"front-proxy-ca\", Organization: nil, Year: 100, AltNames: AltNames{}, Usages: nil, }, { Path: EtcdBasePath, BaseName: \"ca\", CommonName: \"etcd-ca\", Organization: nil, Year: 100, AltNames: AltNames{}, Usages: nil, }, } 其它签名证书列表 var certList = []Config{ { Path: BasePath, BaseName: \"apiserver\", CAName: \"kubernetes\", CommonName: \"kube-apiserver\", Organization: nil, Year: 100, AltNames: AltNames{// 实际安装时还需要把服务器IP用户自定义域名加上 DNSNames: []string{ \"apiserver.cluster.local\", \"localhost\", \"master\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", }, IPs: []net.IP{ {127,0,0,1}, }, }, Usages: []x509.ExtKeyUsage{x509.ExtKeyUsageServerAuth}, // 用途是服务端校验 }, { Path: BasePath, BaseName: \"apiserver-kubelet-client\", CAName: \"kubernetes\", CommonName: \"kube-apiserver-kubelet-client\", Organization: []string{\"system:masters\"}, Year: 100, AltNames: AltNames{}, Usages: []x509.ExtKeyUsage{x509.ExtKeyUsageClientAuth}, }, { Path: BasePath, BaseName: \"front-proxy-client\", CAName: \"front-proxy-ca\", CommonName: \"front-proxy-client\", Organization: nil, Year: 100, AltNames: AltNames{}, Usages: []x509.ExtKeyUsage{x509.ExtKeyUsageClientAuth}, }, { Path: BasePath, BaseName: \"apiserver-etcd-client\", CAName: \"etcd-ca\", CommonName: \"kube-apiserver-etcd-client\", Organization: []string{\"system:masters\"}, Year: 100, AltNames: AltNames{}, Usages: []x509.ExtKeyUsage{x509.ExtKeyUsageClientAuth}, }, { Path: EtcdBasePath, BaseName: \"server\", CAName: \"etcd-ca\", CommonName: \"etcd\", // kubeadm etcd server证书common name使用节点名，这也是调用时需要改动的 Organization: nil, Year: 100, AltNames: AltNames{}, // 调用时需要把节点名，节点IP等加上 Usages: []x509.ExtKeyUsage{x509.ExtKeyUsageServerAuth, x509.ExtKeyUsageClientAuth}, }, { Path: EtcdBasePath, BaseName: \"peer\", CAName: \"etcd-ca\", CommonName: \"etcd-peer\", // 与etcd server同理 Organization: nil, Year: 100, AltNames: AltNames{}, // 与etcd server同理 Usages: []x509.ExtKeyUsage{x509.ExtKeyUsageServerAuth, x509.ExtKeyUsageClientAuth}, }, { Path: EtcdBasePath, BaseName: \"healthcheck-client\", CAName: \"etcd-ca\", CommonName: \"kube-etcd-healthcheck-client\", Organization: []string{\"system:masters\"}, Year: 100, AltNames: AltNames{}, Usages: []x509.ExtKeyUsage{x509.ExtKeyUsageClientAuth}, }, } 上面非常要注意的是server端校验的证书安装时需要把IP和域名加上，etcd的commonName也要设置成node name。 看最后生成的证书信息： apiserver: [root@iZ2ze4ry74x8bh3cweeg69Z pki]# openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout Certificate: ... Signature Algorithm: sha256WithRSAEncryption Issuer: CN=kubernetes Validity Not Before: Mar 31 09:18:06 2020 GMT Not After : Mar 8 09:18:06 2119 GMT Subject: CN=kube-apiserver ... X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication X509v3 Subject Alternative Name: DNS:iz2ze4ry74x8bh3cweeg69z, DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, DNS:apiserver.cluster.local, DNS:apiserver.cluster.local, IP Address:10.96.0.1, IP Address:172.16.9.192, IP Address:127.0.0.1, IP Address:172.16.9.192, IP Address:172.16.9.193, IP Address:172.16.9.194, IP Address:10.103.97.2 Signature Algorithm: sha256WithRSAEncryption etcd server: [root@iZ2ze4ry74x8bh3cweeg69Z pki]# openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -text -noout Certificate: Data: Version: 3 (0x2) Serial Number: 1930981199811083392 (0x1acc392ba2b27c80) Signature Algorithm: sha256WithRSAEncryption Issuer: CN=etcd-ca Validity Not Before: Mar 31 09:18:07 2020 GMT Not After : Mar 8 09:18:07 2119 GMT Subject: CN=iz2ze4ry74x8bh3cweeg69z ... X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Subject Alternative Name: DNS:iz2ze4ry74x8bh3cweeg69z, DNS:localhost, IP Address:172.16.9.192, IP Address:127.0.0.1, IP Address:0:0:0:0:0:0:0:1 Signature Algorithm: sha256WithRSAEncryption 生成用户证书和kubeconfig 现在有个实习生小明来公司了，也想用用k8s，果断不放心把admin 的kubeconfig交给他，那怎么办？ 有了上面基础，再进一步教你怎么为实习生小明分配一个单独的kubeconfig 从磁盘加载根证书,和私钥 生成fanux这个用户的证书, common name就是fanux 编码成pem格式 写kubeconfig, 写磁盘 func GenerateKubeconfig(conf Config) error{ certs, err := cert.CertsFromFile(conf.CACrtFile) caCert := certs[0] cert := EncodeCertPEM(caCert) caKey,err := TryLoadKeyFromDisk(conf.CAKeyFile) // 这里conf.User就是fanux, conf.Groups就是用户组，可以是多个 clientCert,clientKey,err := NewCertAndKey(caCert,caKey,conf.User,conf.Groups,conf.DNSNames,conf.IPAddresses) encodedClientKey,err := keyutil.MarshalPrivateKeyToPEM(clientKey) encodedClientCert := EncodeCertPEM(clientCert) // 构建kubeconfig的三元组信息 config := &api.Config{ Clusters: map[string]*api.Cluster{ conf.ClusterName: { Server: conf.Apiserver, // 集群地址 如 https://apiserver.cluster.local:6443 CertificateAuthorityData: cert, // pem格式的根证书，用于https }, }, Contexts: map[string]*api.Context{ ctx: { // 三元组信息，用户名 fanux, 上面的cluster名，以及namespace这里没写 Cluster: conf.ClusterName, AuthInfo: conf.User, }, }, AuthInfos: map[string]*api.AuthInfo{ // 用户信息, 所以你直接改kubeconfig里的user是没用的，因为k8s只认证书里的名字 conf.User:&api.AuthInfo{ ClientCertificateData: encodedClientCert, // pem格式的用户证书 ClientKeyData: encodedClientKey, // pem格式的用户私钥 }, }, CurrentContext: ctx, // 当前上下文, kubeconfig可以很好支持多用户和多集群 } err = clientcmd.WriteToFile(*config, conf.OutPut) return nil } 用户证书和私钥生成, 和上面签名证书一样，user就是funax, group是用户组： func NewCertAndKey(caCert *x509.Certificate, caKey crypto.Signer, user string, groups []string, DNSNames []string,IPAddresses []net.IP) (*x509.Certificate, crypto.Signer, error) { key,err := rsa.GenerateKey(rand.Reader, 2048) serial, err := rand.Int(rand.Reader, new(big.Int).SetInt64(math.MaxInt64)) certTmpl := x509.Certificate{ Subject: pkix.Name{ CommonName: user, Organization: groups, }, DNSNames: DNSNames, IPAddresses: IPAddresses, SerialNumber: serial, NotBefore: caCert.NotBefore, NotAfter: time.Now().Add(time.Hour * 24 * 365 * 99).UTC(), KeyUsage: x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature, ExtKeyUsage: []x509.ExtKeyUsage{x509.ExtKeyUsageClientAuth}, } certDERBytes, err := x509.CreateCertificate(rand.Reader, &certTmpl, caCert, key.Public(), caKey) cert,err := x509.ParseCertificate(certDERBytes) return cert,key,nil } 然后这位小伙伴的kubeconfig就生成了，此时没有任何权限： kubectl --kubeconfig ./kube/config get pod Error from server (Forbidden): pods is forbidden: User \"fanux\" cannot list resource \"pods\" in API group ... 最后发挥一下RBAC就可以了，这里就直接绑定个管理员权限了 kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: user-admin-test subjects: - kind: User name: \"fanux\" # Name is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: cluster-admin # using admin role apiGroup: rbac.authorization.k8s.io 总结 证书与k8s的认证原理在集群安装以及开发多租户容器平台时非常有用，希望本文能让大家有个整体细致全面的了解。 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s资源对象/pod/yaml文件.html":{"url":"linux/k8s/k8s资源对象/pod/yaml文件.html","title":"yaml文件","keywords":"","body":"[toc] k8s YAML文件 本文严重抄袭至互联网 k8s 1.18 api官方文档 YAML 基础 它的基本语法规则如下： 大小写敏感 使用缩进表示层级关系 缩进时不允许使用Tab键，只允许使用空格。 缩进的空格数目不重要，只要相同层级的元素左侧对齐即可 # 表示注释，从这个字符一直到行尾，都会被解析器忽略。 在我们的 kubernetes 中，你只需要两种结构类型就行了： Lists Maps 也就是说，你可能会遇到 Lists 的 Maps 和 Maps 的 Lists，等等。不过不用担心，你只要掌握了这两种结构也就可以了，其他更加复杂的我们暂不讨论。 Maps 首先我们来看看 Maps，我们都知道 Map 是字典，就是一个key:value的键值对，Maps 可以让我们更加方便的去书写配置信息，例如： --- apiVersion: v1 kind: Pod 第一行的---是分隔符，是可选的，在单一文件中，可用连续三个连字号---区分多个文件。这里我们可以看到，我们有两个键： apiVersion和kind ，他们对应的值分别是：v1 和Pod。上面的 YAML 文件转换成 JSON 格式的话，你肯定就容易明白了： { \"apiVersion\": \"v1\", \"kind\": \"pod\" } 我们在创建一个相对复杂一点的 YAML 文件，创建一个 KEY 对应的值不是字符串而是一个 Maps： --- apiVersion: v1 kind: Pod metadata: name: kube100-site labels: app: web 上面的 YAML 文件，metadata 这个 KEY 对应的值就是一个 Maps 了，而且嵌套的 labels 这个 KEY 的值又是一个 Map，你可以根据你自己的情况进行多层嵌套。 上面我们也提到了 YAML 文件的语法规则，YAML 处理器是根据行缩进来知道内容之间的关联性的。比如我们上面的 YAML 文件，我用了两个空格作为缩进，空格的数量并不重要，但是你得保持一致，并且至少要求一个空格（什么意思？就是你别一会缩进两个空格，一会缩进4个空格）。 我们可以看到 name 和 labels 是相同级别的缩进，所以 YAML 处理器就知道了他们属于同一个 MAP，而 app 是 labels 的值是因为 app 的缩进更大。 注意：在 YAML 文件中绝对不要使用 tab 键。 同样的，我们可以将上面的 YAML 文件转换成 JSON 文件： { \"apiVersion\": \"v1\", \"kind\": \"Pod\", \"metadata\": { \"name\": \"kube100-site\", \"labels\": { \"app\": \"web\" } } } 或许你对上面的 JSON 文件更熟悉，但是你不得不承认 YAML 文件的语义化程度更高吧？ Lists Lists 就是列表，说白了就是数组，在 YAML 文件中我们可以这样定义： args - Cat - Dog - Fish 你可以有任何数量的项在列表中，每个项的定义以破折号（-）开头的，与父元素直接可以缩进一个空格。对应的 JSON 格式如下： { \"args\": [\"Cat\", \"Dog\", \"Fish\"] } 当然，list 的子项也可以是 Maps，Maps 的子项也可以是list如下所示： --- apiVersion: v1 kind: Pod metadata: name: kube100-site labels: app: web spec: containers: - name: front-end image: nginx ports: - containerPort: 80 - name: flaskapp-demo image: jcdemo/flaskapp ports: - containerPort: 5000 比如这个 YAML 文件，我们定义了一个叫 containers 的 List 对象，每个子项都由 name、image、ports 组成，每个 ports 都有一个 key 为 containerPort 的 Map 组成，同样的，我们可以转成如下 JSON 格式文件： { \"apiVersion\": \"v1\", \"kind\": \"Pod\", \"metadata\": { \"name\": \"kube100-site\", \"labels\": { \"app\": \"web\" } }, \"spec\": { \"containers\": [{ \"name\": \"front-end\", \"image\": \"nginx\", \"ports\": [{ \"containerPort\": 80 }] }, { \"name\": \"flaskapp-demo\", \"image\": \"jcdemo/flaskapp\", \"ports\": [{ \"containerPort\": 5000 }] }] } } 是不是觉得用 JSON 格式的话文件明显比 YAML 文件更复杂了呢？ 使用 YAML 创建 Pod 现在我们已经对 YAML 文件有了大概的了解了，我相信你应该没有之前那么懵逼了吧？我们还是来使用 YAML 文件来创建一个 Deployment 吧。 创建 Pod --- apiVersion: v1 kind: Pod metadata: name: kube100-site labels: app: web spec: containers: - name: front-end image: nginx ports: - containerPort: 80 - name: flaskapp-demo image: jcdemo/flaskapp ports: - containerPort: 5000 这是我们上面定义的一个普通的 POD 文件，我们先来简单分析下文件内容： apiVersion，这里它的值是 v1，这个版本号需要根据我们安装的 kubernetes 版本和资源类型进行变化的，记住不是写死的 kind，这里我们创建的是一个 Pod，当然根据你的实际情况，这里资源类型可以是 Deployment、Job、Ingress、Service 等待。 metadata：包含了我们定义的 Pod 的一些 meta 信息，比如名称、namespace、标签等等信息。 spec：包括一些 containers，storage，volumes，或者其他 Kubernetes 需要知道的参数，以及诸如是否在容器失败时重新启动容器的属性。你可以在特定 Kubernetes API 找到完整的 Kubernetes Pod 的属性。 让我们来看一个典型的容器的定义： …spec: containers: - name: front-end image: nginx ports: - containerPort: 80 … 在这个例子中，这是一个简单的最小定义：一个名字（front-end），基于 nginx 的镜像，以及容器 将会监听的一个端口（80）。在这些当中，只有名字是非常需要的，你也可以指定一个更加复杂的属性，例如在容器启动时运行的命令，应使用的参数，工作目录，或每次实例化时是否拉取映像的新副本。以下是一些容器可选的设置属性： name image command args workingDir ports env resources volumeMounts livenessProbe readinessProbe livecycle terminationMessagePath imagePullPolicy securityContext stdin stdinOnce tty 明白了 POD 的定义后，我们将上面创建 POD 的 YAML 文件保存成 pod.yaml，然后使用kubectl创建 POD： $ kubectl create -f pod.yaml pod \"kube100-site\" created 然后我们就可以使用我们前面比较熟悉的 kubectl 命令来查看 POD 的状态了： $ kubectl get pods NAME READY STATUS RESTARTS AGE kube100-site 2/2 Running 0 1m 到这里我们的 POD 就创建成功了，如果你在创建过程中有任何问题，我们同样可以使用前面的kubectl describe 进行排查。我们先删除上面创建的 POD： $ kubectl delete -f pod.yaml pod \"kube100-site\" deleted 创建 Deployment 现在我们可以来创建一个真正的 Deployment。在上面的例子中，我们只是单纯的创建了一个 POD 实例，但是如果这个 POD 出现了故障的话，我们的服务也就挂掉了，所以 kubernetes 提供了一个Deployment的概念，可以让 kubernetes 去管理一组 POD 的副本，也就是副本集，这样就可以保证一定数量的副本一直可用的，不会因为一个 POD 挂掉导致整个服务挂掉。我们可以这样定义一个 Deployment： --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: kube100-site spec: replicas: 2 注意这里的apiVersion对应的值是extensions/v1beta1，当然 kind 要指定为 Deployment，因为这就是我们需要的，然后我们可以指定一些 meta 信息，比如名字，或者标签之类的。最后，最重要的是spec配置选项，这里我们定义需要两个副本，当然还有很多可以设置的属性，比如一个 Pod 在没有任何错误变成准备的情况下必须达到的最小秒数。 我们可以在Kubernetes v1beta1 API(官方已修改，早就404了，天天瞎鸡吧改，都尼玛找不到了)参考中找到一个完整的 Depolyment 可指定的参数列表。 现在我们来定义一个完整的 Deployment 的 YAML 文件： --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: kube100-site spec: replicas: 2 template: metadata: labels: app: web spec: containers: - name: front-end image: nginx ports: - containerPort: 80 - name: flaskapp-demo image: jcdemo/flaskapp ports: - containerPort: 5000 看起来是不是和我们上面的 pod.yaml 很类似啊，注意其中的 template，其实就是对 POD 对象的定义。将上面的 YAML 文件保存为 deployment.yaml，然后创建 Deployment： $ kubectl create -f deployment.yaml deployment \"kube100-site\" created 同样的，想要查看它的状态，我们可以检查 Deployment的列表： $ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kube100-site 2 2 2 2 2m 我们可以看到所有的 Pods 都已经正常运行了。 到这里我们就完成了使用 YAML 文件创建 Kubernetes Deployment 的过程，在了解了 YAML 文件的基础后，定义 YAML 文件其实已经很简单了，最主要的是要根据实际情况去定义 YAML 文件，所以查阅 Kubernetes 文档很重要。 可以使用yaml文件检测网站去检验 YAML 文件的合法性。 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s资源对象/pod/pod简介.html":{"url":"linux/k8s/k8s资源对象/pod/pod简介.html","title":"pod简介","keywords":"","body":"pod简介 k8s pod官方文档 Pod 是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。 Pod （就像在鲸鱼荚或者豌豆荚中）是一组（一个或多个） 容器； 这些容器共享存储、网络、以及怎样运行这些容器的声明。 Pod 中的内容总是并置（colocated）的并且一同调度，在共享的上下文中运行。 Pod 所建模的是特定于应用的“逻辑主机”，其中包含一个或多个应用容器， 这些容器是相对紧密的耦合在一起的。 在非云环境中，在相同的物理机或虚拟机上运行的应用类似于 在同一逻辑主机上运行的云应用。 除了应用容器，Pod 还可以包含在 Pod 启动期间运行的 Init 容器。 你也可以在集群中支持临时性容器 的情况下，为调试的目的注入临时性容器。 什么是 Pod？ 说明： 除了 Docker 之外，Kubernetes 支持 很多其他容器运行时， Docker 是最有名的运行时， 使用 Docker 的术语来描述 Pod 会很有帮助。 Pod 的共享上下文包括一组 Linux 名字空间、控制组（cgroup）和可能一些其他的隔离 方面，即用来隔离 Docker 容器的技术。 在 Pod 的上下文中，每个独立的应用可能会进一步实施隔离。 就 Docker 概念的术语而言，Pod 类似于共享名字空间和文件系统卷的一组 Docker 容器。 使用 Pod 通常你不需要直接创建 Pod，甚至单实例 Pod。 相反，你会使用诸如 Deployment 或 Job 这类工作负载资源 来创建 Pod。如果 Pod 需要跟踪状态， 可以考虑 StatefulSet 资源。 Kubernetes 集群中的 Pod 主要有两种用法： 运行单个容器的 Pod。\"每个 Pod 一个容器\"模型是最常见的 Kubernetes 用例； 在这种情况下，可以将 Pod 看作单个容器的包装器，并且 Kubernetes 直接管理 Pod，而不是容器。 运行多个协同工作的容器的 Pod。 Pod 可能封装由多个紧密耦合且需要共享资源的共处容器组成的应用程序。 这些位于同一位置的容器可能形成单个内聚的服务单元 —— 一个容器将文件从共享卷提供给公众， 而另一个单独的“边车”（sidecar）容器则刷新或更新这些文件。 Pod 将这些容器和存储资源打包为一个可管理的实体。 说明： 将多个并置、同管的容器组织到一个 Pod 中是一种相对高级的使用场景。 只有在一些场景中，容器之间紧密关联时你才应该使用这种模式。 每个 Pod 都旨在运行给定应用程序的单个实例。如果希望横向扩展应用程序（例如，运行多个实例 以提供更多的资源），则应该使用多个 Pod，每个实例使用一个 Pod。 在 Kubernetes 中，这通常被称为 副本（Replication）。 通常使用一种工作负载资源及其控制器 来创建和管理一组 Pod 副本。 参见 Pod 和控制器以了解 Kubernetes 如何使用工作负载资源及其控制器以实现应用的扩缩和自动修复。 Pod 怎样管理多个容器 Pod 被设计成支持形成内聚服务单元的多个协作过程（形式为容器）。 Pod 中的容器被自动安排到集群中的同一物理机或虚拟机上，并可以一起进行调度。 容器之间可以共享资源和依赖、彼此通信、协调何时以及何种方式终止自身。 例如，你可能有一个容器，为共享卷中的文件提供 Web 服务器支持，以及一个单独的 “sidecar（挂斗）”容器负责从远端更新这些文件，如下图所示： 有些 Pod 具有 Init 容器 和 应用容器。 Init 容器会在启动应用容器之前运行并完成。 Pod 天生地为其成员容器提供了两种共享资源：网络和 存储。 使用 Pod 你很少在 Kubernetes 中直接创建一个个的 Pod，甚至是单实例（Singleton）的 Pod。 这是因为 Pod 被设计成了相对临时性的、用后即抛的一次性实体。 当 Pod 由你或者间接地由 控制器 创建时，它被调度在集群中的节点上运行。 Pod 会保持在该节点上运行，直到 Pod 结束执行、Pod 对象被删除、Pod 因资源不足而被 驱逐 或者节点失效为止。 说明： 重启 Pod 中的容器不应与重启 Pod 混淆。 Pod 不是进程，而是容器运行的环境。 在被删除之前，Pod 会一直存在。 当你为 Pod 对象创建清单时，要确保所指定的 Pod 名称是合法的 DNS 子域名。 Pod 和控制器 你可以使用工作负载资源来创建和管理多个 Pod。 资源的控制器能够处理副本的管理、上线，并在 Pod 失效时提供自愈能力。 例如，如果一个节点失败，控制器注意到该节点上的 Pod 已经停止工作， 就可以创建替换性的 Pod。调度器会将替身 Pod 调度到一个健康的节点执行。 下面是一些管理一个或者多个 Pod 的工作负载资源的示例： Deployment StatefulSet DaemonSet Pod 模版 负载资源的控制器通常使用 Pod 模板（Pod Template） 来替你创建 Pod 并管理它们。 Pod 模板是包含在工作负载对象中的规范，用来创建 Pod。这类负载资源包括 Deployment、 Job 和 DaemonSets等。 工作负载的控制器会使用负载对象中的 PodTemplate 来生成实际的 Pod。 PodTemplate 是你用来运行应用时指定的负载资源的目标状态的一部分。 下面的示例是一个简单的 Job 的清单，其中的 template 指示启动一个容器。 该 Pod 中的容器会打印一条消息之后暂停。 apiVersion: batch/v1 kind: Job metadata: name: hello spec: template: # 这里是 Pod 模版 spec: containers: - name: hello image: busybox command: ['sh', '-c', 'echo \"Hello, Kubernetes!\" && sleep 3600'] restartPolicy: OnFailure # 以上为 Pod 模版 修改 Pod 模版或者切换到新的 Pod 模版都不会对已经存在的 Pod 起作用。 Pod 不会直接收到模版的更新。相反， 新的 Pod 会被创建出来，与更改后的 Pod 模版匹配。 例如，Deployment 控制器针对每个 Deployment 对象确保运行中的 Pod 与当前的 Pod 模版匹配。如果模版被更新，则 Deployment 必须删除现有的 Pod，基于更新后的模版 创建新的 Pod。每个工作负载资源都实现了自己的规则，用来处理对 Pod 模版的更新。 在节点上，kubelet并不直接监测 或管理与 Pod 模版相关的细节或模版的更新，这些细节都被抽象出来。 这种抽象和关注点分离简化了整个系统的语义，并且使得用户可以在不改变现有代码的 前提下就能扩展集群的行为。 Pod 更新与替换 正如前面章节所述，当某工作负载的 Pod 模板被改变时，控制器会基于更新的模板 创建新的 Pod 对象而不是对现有 Pod 执行更新或者修补操作。 Kubernetes 并不禁止你直接管理 Pod。对运行中的 Pod 的某些字段执行就地更新操作 还是可能的。不过，类似 patch 和 replace 这类更新操作有一些限制： Pod 的绝大多数元数据都是不可变的。例如，你不可以改变其 namespace、name、 uid 或者 creationTimestamp 字段；generation 字段是比较特别的，如果更新 该字段，只能增加字段取值而不能减少。 如果 metadata.deletionTimestamp 已经被设置，则不可以向 metadata.finalizers 列表中添加新的条目。 Pod 更新不可以改变除 spec.containers[*].image、spec.initContainers[*].image、 spec.activeDeadlineSeconds 或 spec.tolerations 之外的字段。 对于 spec.tolerations，你只被允许添加新的条目到其中。 在更新spec.activeDeadlineSeconds 字段时，以下两种更新操作是被允许的： 如果该字段尚未设置，可以将其设置为一个正数； 如果该字段已经设置为一个正数，可以将其设置为一个更小的、非负的整数。 资源共享和通信 Pod 使它的成员容器间能够进行数据共享和通信。 Pod 中的存储 一个 Pod 可以设置一组共享的存储卷。 Pod 中的所有容器都可以访问该共享卷，从而允许这些容器共享数据。 卷还允许 Pod 中的持久数据保留下来，即使其中的容器需要重新启动。 有关 Kubernetes 如何在 Pod 中实现共享存储并将其提供给 Pod 的更多信息， 请参考卷。 Pod 联网 每个 Pod 都在每个地址族中获得一个唯一的 IP 地址。 Pod 中的每个容器共享网络名字空间，包括 IP 地址和网络端口。 Pod 内 的容器可以使用 localhost 互相通信。 当 Pod 中的容器与 Pod 之外 的实体通信时，它们必须协调如何使用共享的网络资源 （例如端口）。 在同一个 Pod 内，所有容器共享一个 IP 地址和端口空间，并且可以通过 localhost 发现对方。 他们也能通过如 SystemV 信号量或 POSIX 共享内存这类标准的进程间通信方式互相通信。 不同 Pod 中的容器的 IP 地址互不相同，没有 特殊配置 就不能使用 IPC 进行通信。 如果某容器希望与运行于其他 Pod 中的容器通信，可以通过 IP 联网的方式实现。 Pod 中的容器所看到的系统主机名与为 Pod 配置的 name 属性值相同。 网络部分提供了更多有关此内容的信息。 容器的特权模式 在 Linux 中，Pod 中的任何容器都可以使用容器规约中的 安全性上下文中的 privileged（Linux）参数启用特权模式。 这对于想要使用操作系统管理权能（Capabilities，如操纵网络堆栈和访问设备） 的容器很有用。 如果你的集群启用了 WindowsHostProcessContainers 特性，你可以使用 Pod 规约中安全上下文的 windowsOptions.hostProcess 参数来创建 Windows HostProcess Pod。 这些 Pod 中的所有容器都必须以 Windows HostProcess 容器方式运行。 HostProcess Pod 可以直接运行在主机上，它也能像 Linux 特权容器一样，用于执行管理任务。 说明： 你的容器运行时必须支持 特权容器的概念才能使用这一配置。 静态 Pod 静态 Pod（Static Pod） 直接由特定节点上的 kubelet 守护进程管理， 不需要API 服务器看到它们。 尽管大多数 Pod 都是通过控制面（例如，Deployment） 来管理的，对于静态 Pod 而言，kubelet 直接监控每个 Pod，并在其失效时重启之。 静态 Pod 通常绑定到某个节点上的 kubelet。 其主要用途是运行自托管的控制面。 在自托管场景中，使用 kubelet 来管理各个独立的 控制面组件。 kubelet 自动尝试为每个静态 Pod 在 Kubernetes API 服务器上创建一个 镜像 Pod。 这意味着在节点上运行的 Pod 在 API 服务器上是可见的，但不可以通过 API 服务器来控制。 说明：静态 Pod 的 spec 不能引用其他的 API 对象（例如：ServiceAccount、ConfigMap、Secret等）。 容器探针 Probe 是由 kubelet 对容器执行的定期诊断。要执行诊断，kubelet 可以执行三种动作： ExecAction（借助容器运行时执行） TCPSocketAction（由 kubelet 直接检测） HTTPGetAction（由 kubelet 直接检测） 你可以参阅 Pod 的生命周期文档中的探针部分。 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s资源对象/pod/pod hook.html":{"url":"linux/k8s/k8s资源对象/pod/pod hook.html","title":"pod hook","keywords":"","body":"pod hook Kubernetes 支持 postStart 和 preStop 事件。 当一个容器启动后，Kubernetes 将立即发送 postStart 事件；在容器被终结之前， Kubernetes 将发送一个 preStop 事件。容器可以为每个事件指定一个处理程序。 定义 postStart 和 preStop 处理函数 编辑yaml文件 cat > lifecycle-events.yaml /usr/share/message\"] preStop: exec: command: [\"/bin/sh\",\"-c\",\"nginx -s quit; while killall -0 nginx; do sleep 1; done\"] EOF 在上述配置文件中，你可以看到 postStart 命令在容器的 /usr/share 目录下写入文件 message。 命令 preStop 负责优雅地终止 nginx 服务。当因为失效而导致容器终止时，这一处理方式很有用。 创建 Pod： kubectl apply -f lifecycle-events.yaml 验证 Pod 中的容器已经运行： kubectl get pod lifecycle-demo 验证 postStart 处理函数创建了 message 文件 $ kubectl exec -it lifecycle-demo -- cat /usr/share/message Hello from the postStart handler 讨论 Kubernetes 在容器创建后立即发送 postStart 事件。 然而，postStart 处理函数的调用不保证早于容器的入口点（entrypoint） 的执行。postStart 处理函数与容器的代码是异步执行的，但 Kubernetes 的容器管理逻辑会一直阻塞等待 postStart 处理函数执行完毕。 只有 postStart 处理函数执行完毕，容器的状态才会变成 RUNNING。 Kubernetes 在容器结束前立即发送 preStop 事件。除非 Pod 宽限期限超时，Kubernetes 的容器管理逻辑 会一直阻塞等待 preStop 处理函数执行完毕。更多的相关细节，可以参阅 Pods 的结束。 说明： Kubernetes 只有在 Pod 结束（Terminated） 的时候才会发送 preStop 事件， 这意味着在 Pod 完成（Completed） 时 preStop 的事件处理逻辑不会被触发。这个限制在 issue #55087 中被追踪。 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s资源对象/pod/init容器.html":{"url":"linux/k8s/k8s资源对象/pod/init容器.html","title":"init容器","keywords":"","body":"init容器 k8s init容器官方文档 Init 容器 本页提供了 Init 容器的概览。Init 容器是一种特殊容器，在 Pod 内的应用容器启动之前运行。Init 容器可以包括一些应用镜像中不存在的实用工具和安装脚本。 你可以在 Pod 的规约中与用来描述应用容器的 containers 数组平行的位置指定 Init 容器。 理解 Init 容器 每个 Pod 中可以包含多个容器， 应用运行在这些容器里面，同时 Pod 也可以有一个或多个先于应用容器启动的 Init 容器。 Init 容器与普通的容器非常像，除了如下两点： 它们总是运行到完成。 每个都必须在下一个启动之前成功完成。 如果 Pod 的 Init 容器失败，kubelet 会不断地重启该 Init 容器直到该容器成功为止。 然而，如果 Pod 对应的 restartPolicy 值为 \"Never\"，并且 Pod 的 Init 容器失败， 则 Kubernetes 会将整个 Pod 状态设置为失败。 为 Pod 设置 Init 容器需要在 Pod 规约 中添加 initContainers 字段， 该字段以 Container 类型对象数组的形式组织，和应用的 containers 数组同级相邻。 参阅 API 参考的容器章节了解详情。 Init 容器的状态在 status.initContainerStatuses 字段中以容器状态数组的格式返回 （类似 status.containerStatuses 字段）。 与普通容器的不同之处 Init 容器支持应用容器的全部字段和特性，包括资源限制、数据卷和安全设置。 然而，Init 容器对资源请求和限制的处理稍有不同，在下面资源节有说明。 同时 Init 容器不支持 lifecycle、livenessProbe、readinessProbe 和 startupProbe， 因为它们必须在 Pod 就绪之前运行完成。 如果为一个 Pod 指定了多个 Init 容器，这些容器会按顺序逐个运行。 每个 Init 容器必须运行成功，下一个才能够运行。当所有的 Init 容器运行完成时， Kubernetes 才会为 Pod 初始化应用容器并像平常一样运行。 使用 Init 容器 因为 Init 容器具有与应用容器分离的单独镜像，其启动相关代码具有如下优势： Init 容器可以包含一些安装过程中应用容器中不存在的实用工具或个性化代码。 例如，没有必要仅为了在安装过程中使用类似 sed、awk、python 或 dig 这样的工具而去 FROM 一个镜像来生成一个新的镜像。 Init 容器可以安全地运行这些工具，避免这些工具导致应用镜像的安全性降低。 应用镜像的创建者和部署者可以各自独立工作，而没有必要联合构建一个单独的应用镜像。 Init 容器能以不同于 Pod 内应用容器的文件系统视图运行。因此，Init 容器可以访问 应用容器不能访问的 Secret 的权限。 由于 Init 容器必须在应用容器启动之前运行完成，因此 Init 容器 提供了一种机制来阻塞或延迟应用容器的启动，直到满足了一组先决条件。 一旦前置条件满足，Pod 内的所有的应用容器会并行启动。 示例 下面是一些如何使用 Init 容器的想法： 等待一个 Service 完成创建，通过类似如下 shell 命令： for i in {1..100}; do sleep 1; if dig myservice; then exit 0; fi; exit 1 注册这个 Pod 到远程服务器，通过在命令中调用 API，类似如下： curl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register \\ -d 'instance=$()&ip=$()' 在启动应用容器之前等一段时间，使用类似命令： sleep 60 克隆 Git 仓库到卷中。 将配置值放到配置文件中，运行模板工具为主应用容器动态地生成配置文件。 例如，在配置文件中存放 POD_IP 值，并使用 Jinja 生成主应用配置文件。 使用 Init 容器的情况 下面的例子定义了一个具有 2 个 Init 容器的简单 Pod。 第一个等待 myservice 启动， 第二个等待 mydb 启动。 一旦这两个 Init容器 都启动完成，Pod 将启动 spec 节中的应用容器。 apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox:1.28 command: ['sh', '-c', 'echo The app is running! && sleep 3600'] initContainers: - name: init-myservice image: busybox:1.28 command: ['sh', '-c', \"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done\"] - name: init-mydb image: busybox:1.28 command: ['sh', '-c', \"until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done\"] 你通过运行下面的命令启动 Pod： kubectl apply -f myapp.yaml 输出类似于： pod/myapp-pod created 使用下面的命令检查其状态： kubectl get -f myapp.yaml 输出类似于： NAME READY STATUS RESTARTS AGE myapp-pod 0/1 Init:0/2 0 6m 或者查看更多详细信息： kubectl describe -f myapp.yaml 输出类似于： Name: myapp-pod Namespace: default [...] Labels: app=myapp Status: Pending [...] Init Containers: init-myservice: [...] State: Running [...] init-mydb: [...] State: Waiting Reason: PodInitializing Ready: False [...] Containers: myapp-container: [...] State: Waiting Reason: PodInitializing Ready: False [...] Events: FirstSeen LastSeen Count From SubObjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 16s 16s 1 {default-scheduler } Normal Scheduled Successfully assigned myapp-pod to 172.17.4.201 16s 16s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Pulling pulling image \"busybox\" 13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Pulled Successfully pulled image \"busybox\" 13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Created Created container with docker id 5ced34a04634; Security:[seccomp=unconfined] 13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Started Started container with docker id 5ced34a046s34 如需查看 Pod 内 Init 容器的日志，请执行： kubectl logs myapp-pod -c init-myservice # 查看第一个 Init 容器 kubectl logs myapp-pod -c init-mydb # 查看第二个 Init 容器 在这一刻，Init 容器将会等待至发现名称为 mydb 和 myservice 的 Service。 如下为创建这些 Service 的配置文件： --- apiVersion: v1 kind: Service metadata: name: myservice spec: ports: - protocol: TCP port: 80 targetPort: 9376 --- apiVersion: v1 kind: Service metadata: name: mydb spec: ports: - protocol: TCP port: 80 targetPort: 9377 创建 mydb 和 myservice 服务的命令： kubectl create -f services.yaml 输出类似于： service \"myservice\" created service \"mydb\" created 这样你将能看到这些 Init 容器执行完毕，随后 my-app 的 Pod 进入 Running 状态： kubectl get -f myapp.yaml 输出类似于： NAME READY STATUS RESTARTS AGE myapp-pod 1/1 Running 0 9m 这个简单例子应该能为你创建自己的 Init 容器提供一些启发。 接下来节提供了更详细例子的链接。 具体行为 在 Pod 启动过程中，每个 Init 容器会在网络和数据卷初始化之后按顺序启动。 kubelet 运行依据 Init 容器在 Pod 规约中的出现顺序依次运行之。 每个 Init 容器成功退出后才会启动下一个 Init 容器。 如果某容器因为容器运行时的原因无法启动，或以错误状态退出，kubelet 会根据 Pod 的 restartPolicy 策略进行重试。 然而，如果 Pod 的 restartPolicy 设置为 \"Always\"，Init 容器失败时会使用 restartPolicy 的 \"OnFailure\" 策略。 在所有的 Init 容器没有成功之前，Pod 将不会变成 Ready 状态。 Init 容器的端口将不会在 Service 中进行聚集。正在初始化中的 Pod 处于 Pending 状态， 但会将状况 Initializing 设置为 false。 如果 Pod 重启，所有 Init 容器必须重新执行。 对 Init 容器规约的修改仅限于容器的 image 字段。 更改 Init 容器的 image 字段，等同于重启该 Pod。 因为 Init 容器可能会被重启、重试或者重新执行，所以 Init 容器的代码应该是幂等的。 特别地，基于 emptyDirs 写文件的代码，应该对输出文件可能已经存在做好准备。 Init 容器具有应用容器的所有字段。然而 Kubernetes 禁止使用 readinessProbe， 因为 Init 容器不能定义不同于完成态（Completion）的就绪态（Readiness）。 Kubernetes 会在校验时强制执行此检查。 在 Pod 上使用 activeDeadlineSeconds 和在容器上使用 livenessProbe 可以避免 Init 容器一直重复失败。 activeDeadlineSeconds 时间包含了 Init 容器启动的时间。 但建议仅在团队将其应用程序部署为 Job 时才使用 activeDeadlineSeconds， 因为 activeDeadlineSeconds 在 Init 容器结束后仍有效果。 如果你设置了 activeDeadlineSeconds，已经在正常运行的 Pod 会被杀死。 在 Pod 中的每个应用容器和 Init 容器的名称必须唯一； 与任何其它容器共享同一个名称，会在校验时抛出错误。 资源 在给定的 Init 容器执行顺序下，资源使用适用于如下规则： 所有 Init 容器上定义的任何特定资源的 limit 或 request 的最大值，作为 Pod 有效初始 request/limit。 如果任何资源没有指定资源限制，这被视为最高限制。 Pod 对资源的 有效 limit/request 是如下两者的较大者： 所有应用容器对某个资源的 limit/request 之和 对某个资源的有效初始 limit/request 基于有效 limit/request 完成调度，这意味着 Init 容器能够为初始化过程预留资源， 这些资源在 Pod 生命周期过程中并没有被使用。 Pod 的 有效 QoS 层 ，与 Init 容器和应用容器的一样。 配额和限制适用于有效 Pod 的请求和限制值。 Pod 级别的 cgroups 是基于有效 Pod 的请求和限制值，和调度器相同。 Pod 重启的原因 Pod 重启会导致 Init 容器重新执行，主要有如下几个原因： Pod 的基础设施容器 (译者注：如 pause 容器) 被重启。这种情况不多见， 必须由具备 root 权限访问节点的人员来完成。 当 restartPolicy 设置为 \"Always\"，Pod 中所有容器会终止而强制重启。 由于垃圾收集机制的原因，Init 容器的完成记录将会丢失。 当 Init 容器的镜像发生改变或者 Init 容器的完成记录因为垃圾收集等原因被丢失时， Pod 不会被重启。这一行为适用于 Kubernetes v1.20 及更新版本。如果你在使用较早 版本的 Kubernetes，可查阅你所使用的版本对应的文档。 创建一个包含 Init 容器的 Pod cat > init-containers.yaml 配置文件中，你可以看到应用容器和 Init 容器共享了一个卷。 Init 容器将共享卷挂载到了 /work-dir 目录，应用容器将共享卷挂载到了 /usr/share/nginx/html 目录。 Init 容器执行完下面的命令就终止： wget -O /work-dir/index.html http://info.cern.ch 请注意 Init 容器在 nginx 服务器的根目录写入 index.html。 创建 Pod： kubectl create -f init-containers.yaml 检查 nginx 容器运行正常： kubectl get pod init-demo 结果表明 nginx 容器运行正常： NAME READY STATUS RESTARTS AGE init-demo 1/1 Running 0 1m 通过 shell 进入 init-demo Pod 中的 nginx 容器： kubectl exec -it init-demo -- /bin/bash 在 shell 中，发送个 GET 请求到 nginx 服务器： root@nginx:~# apt-get update root@nginx:~# apt-get install curl root@nginx:~# curl localhost 结果表明 nginx 正在为 Init 容器编写的 web 页面服务： http://info.cern.ch http://info.cern.ch - home of the first website ... Browse the first website ... 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s资源对象/pod/pod生命周期.html":{"url":"linux/k8s/k8s资源对象/pod/pod生命周期.html","title":"pod生命周期","keywords":"","body":"pod生命周期 k8s pod生命周期官方文档 Pod 遵循一个预定义的生命周期，起始于 Pending 阶段，如果至少 其中有一个主要容器正常启动，则进入 Running，之后取决于 Pod 中是否有容器以 失败状态结束而进入 Succeeded 或者 Failed 阶段。 在 Pod 运行期间，kubelet 能够重启容器以处理一些失效场景。 在 Pod 内部，Kubernetes 跟踪不同容器的状态 并确定使 Pod 重新变得健康所需要采取的动作。 在 Kubernetes API 中，Pod 包含规约部分和实际状态部分。 Pod 对象的状态包含了一组 Pod 状况（Conditions）。 如果应用需要的话，你也可以向其中注入自定义的就绪性信息。 Pod 在其生命周期中只会被调度一次。 一旦 Pod 被调度（分派）到某个节点，Pod 会一直在该节点运行，直到 Pod 停止或者被终止。 Pod 生命期 和一个个独立的应用容器一样，Pod 也被认为是相对临时性（而不是长期存在）的实体。 Pod 会被创建、赋予一个唯一的 ID（UID）， 并被调度到节点，并在终止（根据重启策略）或删除之前一直运行在该节点。 如果一个节点死掉了，调度到该节点的 Pod 也被计划在给定超时期限结束后删除。 Pod 自身不具有自愈能力。如果 Pod 被调度到某节点 而该节点之后失效，Pod 会被删除；类似地，Pod 无法在因节点资源耗尽或者节点维护而被驱逐期间继续存活。Kubernetes 使用一种高级抽象来管理这些相对而言可随时丢弃的 Pod 实例，称作 控制器。 任何给定的 Pod （由 UID 定义）从不会被“重新调度（rescheduled）”到不同的节点； 相反，这一 Pod 可以被一个新的、几乎完全相同的 Pod 替换掉。 如果需要，新 Pod 的名字可以不变，但是其 UID 会不同。 如果某物声称其生命期与某 Pod 相同，例如存储卷， 这就意味着该对象在此 Pod （UID 亦相同）存在期间也一直存在。 如果 Pod 因为任何原因被删除，甚至某完全相同的替代 Pod 被创建时， 这个相关的对象（例如这里的卷）也会被删除并重建。 Pod 结构图例 一个包含多个容器的 Pod 中包含一个用来拉取文件的程序和一个 Web 服务器， 均使用持久卷作为容器间共享的存储。 Pod 阶段 Pod 的 status 字段是一个 PodStatus 对象，其中包含一个 phase 字段。 Pod 的阶段（Phase）是 Pod 在其生命周期中所处位置的简单宏观概述。 该阶段并不是对容器或 Pod 状态的综合汇总，也不是为了成为完整的状态机。 Pod 阶段的数量和含义是严格定义的。 除了本文档中列举的内容外，不应该再假定 Pod 有其他的 phase 值。 下面是 phase 可能的值： 取值 描述 Pending（悬决） Pod 已被 Kubernetes 系统接受，但有一个或者多个容器尚未创建亦未运行。此阶段包括等待 Pod 被调度的时间和通过网络下载镜像的时间。 Running（运行中） Pod 已经绑定到了某个节点，Pod 中所有的容器都已被创建。至少有一个容器仍在运行，或者正处于启动或重启状态。 Succeeded（成功） Pod 中的所有容器都已成功终止，并且不会再重启。 Failed（失败） Pod 中的所有容器都已终止，并且至少有一个容器是因为失败终止。也就是说，容器以非 0 状态退出或者被系统终止。 Unknown（未知） 因为某些原因无法取得 Pod 的状态。这种情况通常是因为与 Pod 所在主机通信失败。 如果某节点死掉或者与集群中其他节点失联，Kubernetes 会实施一种策略，将失去的节点上运行的所有 Pod 的 phase 设置为 Failed。 容器状态 Kubernetes 会跟踪 Pod 中每个容器的状态，就像它跟踪 Pod 总体上的阶段一样。 你可以使用容器生命周期回调 来在容器生命周期中的特定时间点触发事件。 一旦调度器将 Pod 分派给某个节点，kubelet 就通过 容器运行时 开始为 Pod 创建容器。 容器的状态有三种：Waiting（等待）、Running（运行中）和 Terminated（已终止）。 要检查 Pod 中容器的状态，你可以使用 kubectl describe pod 。 其输出中包含 Pod 中每个容器的状态。 每种状态都有特定的含义： Waiting （等待） 如果容器并不处在 Running 或 Terminated 状态之一，它就处在 Waiting 状态。 处于 Waiting 状态的容器仍在运行它完成启动所需要的操作：例如，从某个容器镜像 仓库拉取容器镜像，或者向容器应用 Secret 数据等等。 当你使用 kubectl 来查询包含 Waiting 状态的容器的 Pod 时，你也会看到一个 Reason 字段，其中给出了容器处于等待状态的原因。 Running（运行中） Running 状态表明容器正在执行状态并且没有问题发生。 如果配置了 postStart 回调，那么该回调已经执行且已完成。 如果你使用 kubectl 来查询包含 Running 状态的容器的 Pod 时，你也会看到 关于容器进入 Running 状态的信息。 Terminated（已终止） 处于 Terminated 状态的容器已经开始执行并且或者正常结束或者因为某些原因失败。 如果你使用 kubectl 来查询包含 Terminated 状态的容器的 Pod 时，你会看到 容器进入此状态的原因、退出代码以及容器执行期间的起止时间。 如果容器配置了 preStop 回调，则该回调会在容器进入 Terminated 状态之前执行。 容器重启策略 Pod 的 spec 中包含一个 restartPolicy 字段，其可能取值包括 Always、OnFailure 和 Never。默认值是 Always。 restartPolicy 适用于 Pod 中的所有容器。restartPolicy 仅针对同一节点上 kubelet 的容器重启动作。当 Pod 中的容器退出时，kubelet 会按指数回退 方式计算重启的延迟（10s、20s、40s、...），其最长延迟为 5 分钟。 一旦某容器执行了 10 分钟并且没有出现问题，kubelet 对该容器的重启回退计时器执行 重置操作。 Pod 状况 Pod 有一个 PodStatus 对象，其中包含一个 PodConditions 数组。Pod 可能通过也可能未通过其中的一些状况测试。 PodScheduled：Pod 已经被调度到某节点； ContainersReady：Pod 中所有容器都已就绪； Initialized：所有的 Init 容器 都已成功完成； Ready：Pod 可以为请求提供服务，并且应该被添加到对应服务的负载均衡池中。 字段名称 描述 type Pod 状况的名称 status 表明该状况是否适用，可能的取值有 \"True\", \"False\" 或 \"Unknown\" lastProbeTime 上次探测 Pod 状况时的时间戳 lastTransitionTime Pod 上次从一种状态转换到另一种状态时的时间戳 reason 机器可读的、驼峰编码（UpperCamelCase）的文字，表述上次状况变化的原因 message 人类可读的消息，给出上次状态转换的详细信息 Pod 就绪态 FEATURE STATE: Kubernetes v1.14 [stable] 你的应用可以向 PodStatus 中注入额外的反馈或者信号：Pod Readiness（Pod 就绪态）。 要使用这一特性，可以设置 Pod 规约中的 readinessGates 列表，为 kubelet 提供一组额外的状况供其评估 Pod 就绪态时使用。 就绪态门控基于 Pod 的 status.conditions 字段的当前值来做决定。 如果 Kubernetes 无法在 status.conditions 字段中找到某状况，则该状况的状态值默认为 \"False\"。 这里是一个例子： kind: Pod ... spec: readinessGates: - conditionType: \"www.example.com/feature-1\" status: conditions: - type: Ready # 内置的 Pod 状况 status: \"False\" lastProbeTime: null lastTransitionTime: 2018-01-01T00:00:00Z - type: \"www.example.com/feature-1\" # 额外的 Pod 状况 status: \"False\" lastProbeTime: null lastTransitionTime: 2018-01-01T00:00:00Z containerStatuses: - containerID: docker://abcd... ready: true ... 你所添加的 Pod 状况名称必须满足 Kubernetes 标签键名格式。 Pod 就绪态的状态 命令 kubectl patch 不支持修改对象的状态。 如果需要设置 Pod 的 status.conditions，应用或者 Operators 需要使用 PATCH 操作。 你可以使用 Kubernetes 客户端库 之一来编写代码，针对 Pod 就绪态设置定制的 Pod 状况。 对于使用定制状况的 Pod 而言，只有当下面的陈述都适用时，该 Pod 才会被评估为就绪： Pod 中所有容器都已就绪； readinessGates 中的所有状况都为 True 值。 当 Pod 的容器都已就绪，但至少一个定制状况没有取值或者取值为 False， kubelet 将 Pod 的状况设置为 ContainersReady。 容器探针 probe 是由 kubelet 对容器执行的定期诊断。 要执行诊断，kubelet 既可以在容器内执行代码，也可以发出一个网络请求。 检查机制 使用探针来检查容器有四种不同的方法。 每个探针都必须准确定义为这四种机制中的一种： exec 在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功。 grpc 使用 gRPC 执行一个远程过程调用。 目标应该实现 gRPC健康检查。 如果响应的状态是 \"SERVING\"，则认为诊断成功。 gRPC 探针是一个 alpha 特性，只有在你启用了 \"GRPCContainerProbe\" 特性门控时才能使用。 httpGet 对容器的 IP 地址上指定端口和路径执行 HTTP GET 请求。如果响应的状态码大于等于 200 且小于 400，则诊断被认为是成功的。 tcpSocket 对容器的 IP 地址上的指定端口执行 TCP 检查。如果端口打开，则诊断被认为是成功的。 如果远程系统（容器）在打开连接后立即将其关闭，这算作是健康的。 探测结果 每次探测都将获得以下三种结果之一： Success（成功） 容器通过了诊断。 Failure（失败） 容器未通过诊断。 Unknown（未知） 诊断失败，因此不会采取任何行动。 探测类型 针对运行中的容器，kubelet 可以选择是否执行以下三种探针，以及如何针对探测结果作出反应： livenessProbe 指示容器是否正在运行。如果存活态探测失败，则 kubelet 会杀死容器， 并且容器将根据其重启策略决定未来。如果容器不提供存活探针， 则默认状态为 Success。 readinessProbe 指示容器是否准备好为请求提供服务。如果就绪态探测失败， 端点控制器将从与 Pod 匹配的所有服务的端点列表中删除该 Pod 的 IP 地址。 初始延迟之前的就绪态的状态值默认为 Failure。 如果容器不提供就绪态探针，则默认状态为 Success。 startupProbe 指示容器中的应用是否已经启动。如果提供了启动探针，则所有其他探针都会被 禁用，直到此探针成功为止。如果启动探测失败，kubelet 将杀死容器，而容器依其 重启策略进行重启。 如果容器没有提供启动探测，则默认状态为 Success。 如欲了解如何设置存活态、就绪态和启动探针的进一步细节，可以参阅 配置存活态、就绪态和启动探针。 何时该使用存活态探针? FEATURE STATE: Kubernetes v1.0 [stable] 如果容器中的进程能够在遇到问题或不健康的情况下自行崩溃，则不一定需要存活态探针; kubelet 将根据 Pod 的restartPolicy 自动执行修复操作。 如果你希望容器在探测失败时被杀死并重新启动，那么请指定一个存活态探针， 并指定restartPolicy 为 \"Always\" 或 \"OnFailure\"。 何时该使用就绪态探针? FEATURE STATE: Kubernetes v1.0 [stable] 如果要仅在探测成功时才开始向 Pod 发送请求流量，请指定就绪态探针。 在这种情况下，就绪态探针可能与存活态探针相同，但是规约中的就绪态探针的存在意味着 Pod 将在启动阶段不接收任何数据，并且只有在探针探测成功后才开始接收数据。 如果你希望容器能够自行进入维护状态，也可以指定一个就绪态探针，检查某个特定于 就绪态的因此不同于存活态探测的端点。 如果你的应用程序对后端服务有严格的依赖性，你可以同时实现存活态和就绪态探针。 当应用程序本身是健康的，存活态探针检测通过后，就绪态探针会额外检查每个所需的后端服务是否可用。 这可以帮助你避免将流量导向只能返回错误信息的 Pod。 如果你的容器需要在启动期间加载大型数据、配置文件或执行迁移，你可以使用 启动探针。 然而，如果你想区分已经失败的应用和仍在处理其启动数据的应用，你可能更倾向于使用就绪探针。 说明： 请注意，如果你只是想在 Pod 被删除时能够排空请求，则不一定需要使用就绪态探针； 在删除 Pod 时，Pod 会自动将自身置于未就绪状态，无论就绪态探针是否存在。 等待 Pod 中的容器停止期间，Pod 会一直处于未就绪状态。 何时该使用启动探针？ FEATURE STATE: Kubernetes v1.18 [beta] 对于所包含的容器需要较长时间才能启动就绪的 Pod 而言，启动探针是有用的。 你不再需要配置一个较长的存活态探测时间间隔，只需要设置另一个独立的配置选定， 对启动期间的容器执行探测，从而允许使用远远超出存活态时间间隔所允许的时长。 如果你的容器启动时间通常超出 initialDelaySeconds + failureThreshold × periodSeconds 总值，你应该设置一个启动探测，对存活态探针所使用的同一端点执行检查。 periodSeconds 的默认值是 10 秒。你应该将其 failureThreshold 设置得足够高， 以便容器有充足的时间完成启动，并且避免更改存活态探针所使用的默认值。 这一设置有助于减少死锁状况的发生。 Pod 的终止 由于 Pod 所代表的是在集群中节点上运行的进程，当不再需要这些进程时允许其体面地 终止是很重要的。一般不应武断地使用 KILL 信号终止它们，导致这些进程没有机会 完成清理操作。 设计的目标是令你能够请求删除进程，并且知道进程何时被终止，同时也能够确保删除 操作终将完成。当你请求删除某个 Pod 时，集群会记录并跟踪 Pod 的体面终止周期， 而不是直接强制地杀死 Pod。在存在强制关闭设施的前提下， kubelet 会尝试体面地终止 Pod。 通常情况下，容器运行时会发送一个 TERM 信号到每个容器中的主进程。 很多容器运行时都能够注意到容器镜像中 STOPSIGNAL 的值，并发送该信号而不是 TERM。 一旦超出了体面终止限期，容器运行时会向所有剩余进程发送 KILL 信号，之后 Pod 就会被从 API 服务器 上移除。如果 kubelet 或者容器运行时的管理服务在等待进程终止期间被重启， 集群会从头开始重试，赋予 Pod 完整的体面终止限期。 下面是一个例子： 你使用 kubectl 工具手动删除某个特定的 Pod，而该 Pod 的体面终止限期是默认值（30 秒）。 API 服务器中的 Pod 对象被更新，记录涵盖体面终止限期在内 Pod 的最终死期，超出所计算时间点则认为 Pod 已死（dead）。 如果你使用 kubectl describe 来查验你正在删除的 Pod，该 Pod 会显示为 \"Terminating\" （正在终止）。 在 Pod 运行所在的节点上：kubelet 一旦看到 Pod 被标记为正在终止（已经设置了体面终止限期），kubelet 即开始本地的 Pod 关闭过程。 如果 Pod 中的容器之一定义了 preStop 回调， kubelet 开始在容器内运行该回调逻辑。如果超出体面终止限期时，preStop 回调逻辑 仍在运行，kubelet 会请求给予该 Pod 的宽限期一次性增加 2 秒钟。 说明： 如果 preStop 回调所需要的时间长于默认的体面终止限期，你必须修改 terminationGracePeriodSeconds 属性值来使其正常工作。 kubelet 接下来触发容器运行时发送 TERM 信号给每个容器中的进程 1。 说明： Pod 中的容器会在不同时刻收到 TERM 信号，接收顺序也是不确定的。 如果关闭的顺序很重要，可以考虑使用 preStop 回调逻辑来协调。 与此同时，kubelet 启动体面关闭逻辑，控制面会将 Pod 从对应的端点列表（以及端点切片列表， 如果启用了的话）中移除，过滤条件是 Pod 被对应的 服务以某 选择算符选定。 ReplicaSets和其他工作负载资源 不再将关闭进程中的 Pod 视为合法的、能够提供服务的副本。关闭动作很慢的 Pod 也无法继续处理请求数据，因为负载均衡器（例如服务代理）已经在终止宽限期开始的时候 将其从端点列表中移除。 超出终止宽限期限时，kubelet 会触发强制关闭过程。容器运行时会向 Pod 中所有容器内 仍在运行的进程发送 SIGKILL 信号。 kubelet 也会清理隐藏的 pause 容器，如果容器运行时使用了这种容器的话。 kubelet 触发强制从 API 服务器上删除 Pod 对象的逻辑，并将体面终止限期设置为 0 （这意味着马上删除）。 API 服务器删除 Pod 的 API 对象，从任何客户端都无法再看到该对象。 强制终止 Pod 注意： 对于某些工作负载及其 Pod 而言，强制删除很可能会带来某种破坏。 默认情况下，所有的删除操作都会附有 30 秒钟的宽限期限。 kubectl delete 命令支持 --grace-period= 选项，允许你重载默认值， 设定自己希望的期限值。 将宽限期限强制设置为 0 意味着立即从 API 服务器删除 Pod。 如果 Pod 仍然运行于某节点上，强制删除操作会触发 kubelet 立即执行清理操作。 说明： 你必须在设置 --grace-period=0 的同时额外设置 --force 参数才能发起强制删除请求。 执行强制删除操作时，API 服务器不再等待来自 kubelet 的、关于 Pod 已经在原来运行的节点上终止执行的确认消息。 API 服务器直接删除 Pod 对象，这样新的与之同名的 Pod 即可以被创建。 在节点侧，被设置为立即终止的 Pod 仍然会在被强行杀死之前获得一点点的宽限时间。 如果你需要强制删除 StatefulSet 的 Pod，请参阅 从 StatefulSet 中删除 Pod 的任务文档。 失效 Pod 的垃圾收集 对于已失败的 Pod 而言，对应的 API 对象仍然会保留在集群的 API 服务器上，直到 用户或者控制器进程显式地 将其删除。 控制面组件会在 Pod 个数超出所配置的阈值 （根据 kube-controller-manager 的 terminated-pod-gc-threshold 设置）时 删除已终止的 Pod（阶段值为 Succeeded 或 Failed）。 这一行为会避免随着时间演进不断创建和终止 Pod 而引起的资源泄露问题。 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s资源对象/pod/pod健康检查.html":{"url":"linux/k8s/k8s资源对象/pod/pod健康检查.html","title":"pod健康检查","keywords":"","body":"[toc] pod健康检查 pod健康检查官方文档 探针分类 liveness probe（存活探针） kubelet 通过使用 liveness probe 来确定你的应用程序是否正在运行，通俗点讲就是是否还活着。一般来说，如果你的程序一旦崩溃了， Kubernetes 就会立刻知道这个程序已经终止了，然后就会重启这个程序。而我们的 liveness probe 的目的就是来捕获到当前应用程序还有没有终止，还有没有崩溃，如果出现了这些情况，那么就重启处于该状态下的容器，使应用程序在存在 bug 的情况下依然能够继续运行下去。 readiness probe（可读性探针） kubelet 使用 readiness probe 来确定容器是否已经就绪可以接收流量过来了。这个探针通俗点讲就是说是否准备好了，现在可以开始工作了。只有当 Pod 中的容器都处于就绪状态的时候 kubelet 才会认定该 Pod 处于就绪状态，因为一个 Pod 下面可能会有多个容器。当然 Pod 如果处于非就绪状态，那么我们就会将他从我们的工作队列(实际上就是Service)中移除出来，这样我们的流量就不会被路由到这个 Pod 里面来了。 Pod中容器的生命周期的两个钩子函数，PostStart与PreStop，其中PostStart是在容器创建后立即执行的，而PreStop这个钩子函数则是在容器终止之前执行的。除了上面这两个钩子函数以外，还有一项配置会影响到容器的生命周期的，那就是健康检查的探针。 在kubernetes集群当中，我们可以通过配置liveness probe（存活探针）和readiness probe（可读性探针）来影响容器的生存周期。 和前面的钩子函数一样的，我们这两个探针的支持三种配置方式： exec：执行一段命令 http：检测某个 http 请求 tcpSocket：使用此配置， kubelet 将尝试在指定端口上打开容器的套接字。如果可以建立连接，容器被认为是健康的，如果不能就认为是失败的。实际上就是检查端口 探针配置方式 kubelet 使用存活探测器来知道什么时候要重启容器。 例如，存活探测器可以捕捉到死锁（应用程序在运行，但是无法继续执行后面的步骤）。 这样的情况下重启容器有助于让应用程序在有问题的情况下更可用。 kubelet 使用就绪探测器可以知道容器什么时候准备好了并可以开始接受请求流量， 当一个 Pod 内的所有容器都准备好了，才能把这个 Pod 看作就绪了。 这种信号的一个用途就是控制哪个 Pod 作为 Service 的后端。 在 Pod 还没有准备好的时候，会从 Service 的负载均衡器中被剔除的。 kubelet 使用启动探测器可以知道应用程序容器什么时候启动了。 如果配置了这类探测器，就可以控制容器在启动成功后再进行存活性和就绪检查， 确保这些存活、就绪探测器不会影响应用程序的启动。 这可以用于对慢启动容器进行存活性检测，避免它们在启动运行之前就被杀掉。 定义存活命令 exec 第一种类型的存活探测是运行特定命令 许多长时间运行的应用程序最终会过渡到断开的状态，除非重新启动，否则无法恢复。 Kubernetes 提供了存活探测器来发现并补救这种情况。 cat > exec-liveness.yaml 在这个配置文件中，可以看到 Pod 中只有一个容器。 periodSeconds 字段指定了 kubelet 应该每 5 秒执行一次存活探测。 initialDelaySeconds 字段告诉 kubelet 在执行第一次探测前应该等待 5 秒。 kubelet 在容器内执行命令 cat /tmp/healthy 来进行探测。 如果命令执行成功并且返回值为 0，kubelet 就会认为这个容器是健康存活的。 如果这个命令返回非 0 值，kubelet 会杀死这个容器并重新启动它。 当容器启动时，执行如下的命令： /bin/sh -c \"touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600\" 这个容器生命的前 30 秒， /tmp/healthy 文件是存在的。 所以在这最开始的 30 秒内，执行命令 cat /tmp/healthy 会返回成功代码。 30 秒之后，执行命令 cat /tmp/healthy 就会返回失败代码。 创建 Pod： kubectl apply -f exec-liveness.yaml 输出结果表明还没有存活探测器失败： Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 27s default-scheduler Successfully assigned hehe/liveness-exec to k8s-node02 Normal Pulling 27s kubelet Pulling image \"mirrorgooglecontainers/busybox\" Normal Pulled 6s kubelet Successfully pulled image \"mirrorgooglecontainers/busybox\" in 20.357008603s Normal Created 6s kubelet Created container liveness Normal Started 5s kubelet Started container liveness 35 秒之后，再来看 Pod 的事件： kubectl describe pod liveness-exec 在输出结果的最下面，有信息显示存活探测器失败了，这个容器被杀死并且被重建了。 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 64s default-scheduler Successfully assigned hehe/liveness-exec to k8s-node02 Normal Pulling 64s kubelet Pulling image \"mirrorgooglecontainers/busybox\" Normal Pulled 43s kubelet Successfully pulled image \"mirrorgooglecontainers/busybox\" in 20.357008603s Normal Created 43s kubelet Created container liveness Normal Started 42s kubelet Started container liveness Warning Unhealthy 5s (x2 over 10s) kubelet Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory 再等另外 30 秒，检查看这个容器被重启了： kubectl get pod liveness-exec 输出结果显示 RESTARTS 的值增加了 1。 NAME READY STATUS RESTARTS AGE liveness-exec 1/1 Running 1 (43s ago) 2m18s 定义一个存活态 HTTP 请求接口 http 第二种类型的存活探测方式是使用 HTTP GET 请求 cat > http-liveness.yaml 在这个配置文件中，可以看到 Pod 也只有一个容器。 periodSeconds 字段指定了 kubelet 每隔 3 秒执行一次存活探测。 initialDelaySeconds 字段告诉 kubelet 在执行第一次探测前应该等待 3 秒。 kubelet 会向容器内运行的服务（服务会监听 8080 端口）发送一个 HTTP GET 请求来执行探测。 如果服务器上 /healthz 路径下的处理程序返回成功代码，则 kubelet 认为容器是健康存活的。 如果处理程序返回失败代码，则 kubelet 会杀死这个容器并且重新启动它。 任何大于或等于 200 并且小于 400 的返回代码标示成功，其它返回代码都标示失败。 可以在这里看服务的源码 server.go。 容器存活的最开始 10 秒中，/healthz 处理程序返回一个 200 的状态码。之后处理程序返回 500 的状态码。 http.HandleFunc(\"/healthz\", func(w http.ResponseWriter, r *http.Request) { duration := time.Now().Sub(started) if duration.Seconds() > 10 { w.WriteHeader(500) w.Write([]byte(fmt.Sprintf(\"error: %v\", duration.Seconds()))) } else { w.WriteHeader(200) w.Write([]byte(\"ok\")) } }) kubelet 在容器启动之后 3 秒开始执行健康检测。所以前几次健康检查都是成功的。 但是 10 秒之后，健康检查会失败，并且 kubelet 会杀死容器再重新启动容器。 创建一个 Pod 来测试 HTTP 的存活检测： kubectl apply -f http-liveness.yaml 10 秒之后，通过看 Pod 事件来检测存活探测器已经失败了并且容器被重新启动了。 kubectl describe pod liveness-http Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 48s default-scheduler Successfully assigned hehe/liveness-http to k8s-node01 Normal Pulled 28s kubelet Successfully pulled image \"mirrorgooglecontainers/liveness\" in 18.956899406s Normal Created 27s kubelet Created container liveness Normal Started 27s kubelet Started container liveness Normal Pulling 9s (x2 over 47s) kubelet Pulling image \"mirrorgooglecontainers/liveness\" Warning Unhealthy 9s (x3 over 15s) kubelet Liveness probe failed: HTTP probe failed with statuscode: 500 Normal Killing 9s kubelet Container liveness failed liveness probe, will be restarted 在 1.13（包括 1.13版本）之前的版本中，如果在 Pod 运行的节点上设置了环境变量 http_proxy（或者 HTTP_PROXY），HTTP 的存活探测会使用这个代理。 在 1.13 之后的版本中，设置本地的 HTTP 代理环境变量不会影响 HTTP 的存活探测。 定义 TCP 的存活探测 tcpSocket 第三种类型的存活探测是使用 TCP 套接字。 通过配置，kubelet 会尝试在指定端口和容器建立套接字链接。 如果能建立连接，这个容器就被看作是健康的，如果不能则这个容器就被看作是有问题的。 cat > tcp-liveness-readiness.yaml 如你所见，TCP 检测的配置和 HTTP 检测非常相似。 下面这个例子同时使用就绪和存活探测器。kubelet 会在容器启动 5 秒后发送第一个就绪探测。 这会尝试连接 goproxy 容器的 8080 端口。 如果探测成功，这个 Pod 会被标记为就绪状态，kubelet 将继续每隔 10 秒运行一次检测。 除了就绪探测，这个配置包括了一个存活探测。 kubelet 会在容器启动 15 秒后进行第一次存活探测。 与就绪探测类似，会尝试连接 goproxy 容器的 8080 端口。 如果存活探测失败，这个容器会被重新启动。 kubectl apply -f tcp-liveness-readiness.yaml 15 秒之后，通过看 Pod 事件来检测存活探测器： kubectl describe pod goproxy 使用命名端口 对于 HTTP 或者 TCP 存活检测可以使用命名的 ContainerPort。 ports: - name: liveness-port containerPort: 8080 hostPort: 8080 livenessProbe: httpGet: path: /healthz port: liveness-port 使用启动探测器保护慢启动容器 有时候，会有一些现有的应用程序在启动时需要较多的初始化时间。 要不影响对引起探测死锁的快速响应，这种情况下，设置存活探测参数是要技巧的。 技巧就是使用一个命令来设置启动探测，针对HTTP 或者 TCP 检测，可以通过设置 failureThreshold * periodSeconds 参数来保证有足够长的时间应对糟糕情况下的启动时间。 所以，前面的例子就变成了： ports: - name: liveness-port containerPort: 8080 hostPort: 8080 livenessProbe: httpGet: path: /healthz port: liveness-port failureThreshold: 1 periodSeconds: 10 startupProbe: httpGet: path: /healthz port: liveness-port failureThreshold: 30 periodSeconds: 10 幸亏有启动探测，应用程序将会有最多 5 分钟(30 * 10 = 300s) 的时间来完成它的启动。 一旦启动探测成功一次，存活探测任务就会接管对容器的探测，对容器死锁可以快速响应。 如果启动探测一直没有成功，容器会在 300 秒后被杀死，并且根据 restartPolicy 来设置 Pod 状态。 定义就绪探测器 有时候，应用程序会暂时性的不能提供通信服务。 例如，应用程序在启动时可能需要加载很大的数据或配置文件，或是启动后要依赖等待外部服务。 在这种情况下，既不想杀死应用程序，也不想给它发送请求。 Kubernetes 提供了就绪探测器来发现并缓解这些情况。 容器所在 Pod 上报还未就绪的信息，并且不接受通过 Kubernetes Service 的流量。 说明： 就绪探测器在容器的整个生命周期中保持运行状态。 注意： 活跃性探测器 不等待 就绪性探测器成功。 如果要在执行活跃性探测器之前等待，应该使用 initialDelaySeconds 或 startupProbe。 就绪探测器的配置和存活探测器的配置相似。 唯一区别就是要使用 readinessProbe 字段，而不是 livenessProbe 字段。 readinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 HTTP 和 TCP 的就绪探测器配置也和存活探测器的配置一样的。 就绪和存活探测可以在同一个容器上并行使用。 两者都用可以确保流量不会发给还没有准备好的容器，并且容器会在它们失败的时候被重新启动。 配置探测器 Probe 有很多配置字段，可以使用这些字段精确的控制存活和就绪检测的行为： initialDelaySeconds：容器启动后要等待多少秒后存活和就绪探测器才被初始化，默认是 0 秒，最小值是 0。 periodSeconds：执行探测的时间间隔（单位是秒）。默认是 10 秒。最小值是 1。 timeoutSeconds：探测的超时后等待多少秒。默认值是 1 秒。最小值是 1。 successThreshold：探测器在失败后，被视为成功的最小连续成功数。默认值是 1。 存活和启动探测的这个值必须是 1。最小值是 1。 failureThreshold：当探测失败时，Kubernetes 的重试次数。 存活探测情况下的放弃就意味着重新启动容器。 就绪探测情况下的放弃 Pod 会被打上未就绪的标签。默认值是 3。最小值是 1。 说明： 在 Kubernetes 1.20 版本之前，exec 探针会忽略 timeoutSeconds：探针会无限期地 持续运行，甚至可能超过所配置的限期，直到返回结果为止。 这一缺陷在 Kubernetes v1.20 版本中得到修复。你可能一直依赖于之前错误的探测行为， 甚至你都没有觉察到这一问题的存在，因为默认的超时值是 1 秒钟。 作为集群管理员，你可以在所有的 kubelet 上禁用 ExecProbeTimeout 特性门控 （将其设置为 false），从而恢复之前版本中的运行行为，之后当集群中所有的 exec 探针都设置了 timeoutSeconds 参数后，移除此标志重载。 如果你有 Pods 受到此默认 1 秒钟超时值的影响，你应该更新 Pod 对应的探针的 超时值，这样才能为最终去除该特性门控做好准备。 当此缺陷被修复之后，在使用 dockershim 容器运行时的 Kubernetes 1.20+ 版本中，对于 exec 探针而言，容器中的进程可能会因为超时值的设置保持持续运行， 即使探针返回了失败状态。 注意： 如果就绪态探针的实现不正确，可能会导致容器中进程的数量不断上升。 如果不对其采取措施，很可能导致资源枯竭的状况。 HTTP 探测 HTTP Probes 可以在 httpGet 上配置额外的字段： host：连接使用的主机名，默认是 Pod 的 IP。也可以在 HTTP 头中设置 “Host” 来代替。 scheme ：用于设置连接主机的方式（HTTP 还是 HTTPS）。默认是 HTTP。 path：访问 HTTP 服务的路径。默认值为 \"/\"。 httpHeaders：请求中自定义的 HTTP 头。HTTP 头字段允许重复。 port：访问容器的端口号或者端口名。如果数字必须在 1 ～ 65535 之间。 对于 HTTP 探测，kubelet 发送一个 HTTP 请求到指定的路径和端口来执行检测。 除非 httpGet 中的 host 字段设置了，否则 kubelet 默认是给 Pod 的 IP 地址发送探测。 如果 scheme 字段设置为了 HTTPS，kubelet 会跳过证书验证发送 HTTPS 请求。 大多数情况下，不需要设置host 字段。 这里有个需要设置 host 字段的场景，假设容器监听 127.0.0.1，并且 Pod 的 hostNetwork 字段设置为了 true。那么 httpGet 中的 host 字段应该设置为 127.0.0.1。 可能更常见的情况是如果 Pod 依赖虚拟主机，你不应该设置 host 字段，而是应该在 httpHeaders 中设置 Host。 针对 HTTP 探针，kubelet 除了必需的 Host 头部之外还发送两个请求头部字段： User-Agent 和 Accept。这些头部的默认值分别是 kube-probe/{{ skew latestVersion >}} （其中 1.23 是 kubelet 的版本号）和 */*。 你可以通过为探测设置 .httpHeaders 来重载默认的头部字段值；例如： livenessProbe: httpGet: httpHeaders: - name: Accept value: application/json startupProbe: httpGet: httpHeaders: - name: User-Agent value: MyUserAgent 你也可以通过将这些头部字段定义为空值，从请求中去掉这些头部字段。 livenessProbe: httpGet: httpHeaders: - name: Accept value: \"\" startupProbe: httpGet: httpHeaders: - name: User-Agent value: \"\" TCP 探测 对于一次 TCP 探测，kubelet 在节点上（不是在 Pod 里面）建立探测连接， 这意味着你不能在 host 参数上配置服务名称，因为 kubelet 不能解析服务名称。 探测器级别 terminationGracePeriodSeconds FEATURE STATE: Kubernetes v1.22 [beta] 在 1.21 及更高版本中，当特性门控 ProbeTerminationGracePeriod 为 启用状态时，用户可以指定一个探测级别的 terminationGracePeriodSeconds 作为 探针规格的一部分。当特性门控被启用时，并且 Pod 级和探针级的 terminationGracePeriodSeconds 都已设置，kubelet 将 使用探针级设置的值。 说明： 从 Kubernetes 1.22 开始，ProbeTerminationGracePeriod 特性门控只 在 API 服务器上可用。 kubelet 始终遵守探针级别 terminationGracePeriodSeconds 字段（如果它存在于 Pod 上）。 如果你已经为现有 Pod 设置了 “terminationGracePeriodSeconds” 字段并且 不再希望使用针对每个探针的终止宽限期，则必须删除那些现有的 Pod。 当你（或控制平面或某些其他组件）创建替换 Pods，并且特性门控 “ProbeTerminationGracePeriod” 被禁用，那么 API 服务器会忽略 Pod 级别的 terminationGracePeriodSeconds 字段，即使 Pod 或 Pod 模板指定了它。 例如: spec: terminationGracePeriodSeconds: 3600 # pod-level containers: - name: test image: ... ports: - name: liveness-port containerPort: 8080 hostPort: 8080 livenessProbe: httpGet: path: /healthz port: liveness-port failureThreshold: 1 periodSeconds: 60 # Override pod-level terminationGracePeriodSeconds # terminationGracePeriodSeconds: 60 探测器级别的 terminationGracePeriodSeconds 不能用于设置就绪态探针。 它将被 API 服务器拒绝。 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s资源对象/pod/图解kubernetes Pod创建流程大揭秘.html":{"url":"linux/k8s/k8s资源对象/pod/图解kubernetes Pod创建流程大揭秘.html","title":"k8s 创建pod流程","keywords":"","body":"[toc] 图解kubernetes Pod创建流程大揭秘 本文严重抄袭至互联网 kubernetes中的容器创建无疑是个复杂的过程，涉及内部各种组件的统一协作，还有对接外部的CRI运行时，本文尝试初探一下容器创建流程中的各种细节，了解其各种组件协作流程，从而在后续出现问题的时候，也好能大概有点排查方向 1. 基础筑基 1.1 容器管理线程模型 kubelet中的线程模型属于master/wroker模型，通过单master来监听各种事件源，并为每个Pod创建一个goroutine来进行Pod业务逻辑的处理，master和wroker之间通过一个状态管道来进行通信 1.2 基于事件驱动的状态最终一致性 在通过yaml创建Pod之后，kubernetes会根据当前的事件和当前的Pod状态，来不断进行调整，从而达到最终目标状态的一致性 1.3 组件协作流程 kubelet的结构体声明就高达300多行代码，可见其复杂程度，但是我们按照容器创建这个流程，我们去观察其核心流程，其实主要可以概括为三部分：kubelet、containerRuntime、CRI容器运行时 2.Kubelet创建容器流程 2.1 获取Pod进行准入检查 kubelet的事件源主要包含两个部分：静态Pod和Apiserver，我们这里只考虑普通的Pod，则会直接将Pod加入到PodManager来进行管理，并且进行准入检查 准入检查主要包含两个关键的控制器：驱逐管理与预选检查驱逐管理主要是根据当前的资源压力，检测对应的Pod是否容忍当前的资源压力；预选检查则是根据当前活跃的容器和当前节点的信息来检查是否满足当前Pod的基础运行环境，例如亲和性检查，同时如果当前的Pod的优先级特别高或者是静态Pod，则会尝试为其进行资源抢占，会按照QOS等级逐级来进行抢占从而满足其运行环境 2.2 创建事件管道与容器管理主线程 kubelet接收到一个新创建的Pod首先会为其创建一个事件管道，并且启动一个容器管理的主线程消费管道里面的事件，并且会基于最后同步时间来等待当前kubelet中最新发生的事件(从本地的podCache中获取)，如果是一个新建的Pod，则主要是通过PLEG中更新时间操作，广播的默认空状态来作为最新的状态 2.3 同步最新状态 当从本地的podCache中获取到最新的状态信息和从事件源获取的Pod信息后，会结合当前当前statusManager和probeManager里面的Pod里面的容器状态来更新，从而获取当前感知到的最新的Pod状态 2.4 准入控制检查 之前的准入检查是Pod运行的资源硬性限制的检查，而这里的准入检查则是软状态即容器运行时和版本的一些软件运行环境检查，如果这里检查失败，则会讲对应的容器状态设置为Blocked 2.5 更新容器状态 在通过准入检查之后，会调用statusManager来进行POd最新状态的同步，此处可能会同步给apiserver 2.6 Cgroup配置 在更新完成状态之后会启动一个PodCOntainerManager主要作用则是为对应的Pod根据其QOS等级来进行Cgroup配置的更新 2.7Pod基础运行环境准备 接下来kubelet会为Pod的创建准备基础的环境，包括Pod数据目录的创建、镜像秘钥的获取、等待volume挂载完成等操作创建Pod的数据目录主要是创建 Pod运行所需要的Pod、插件、Volume目录，并且会通过Pod配置的镜像拉取秘钥生成秘钥信息，到此kubelet创建容器的工作就已经基本完成 3.ContainerRuntime 前面我们提到过针对Pod的操作，最终都是基于事件和状态的同步而完成，在containerRUntime并不会区分对应的事件是创建还是更新操作，只是根据当前的Pod的信息与目标状态来进行对比，从而构建出对应的操作，达到目标状态 3.1 计算Pod容器变更 计算容器变更主要包括：Pod的sandbox是否变更、短声明周期容器、初始化容器是否完成、业务容器是否已经完成，相应的我们会得到一个几个对应的容器列表：需要被kill掉的容器列表、需要启动的容器列表，注意如果我们的初始化容器未完成，则不会进行将要运行的业务容器加入到需要启动的容器列表，可以看到这个地方是两个阶段 3.2 初始化失败尝试终止 如果之前检测到之前的初始化容器失败，则会检查当前Pod的所有容器和sandbox关联的容器如果有在运行的容器，会全部进行Kill操作，并且等待操作完成 3.3 未知状态容器补偿 当一些Pod的容器已经运行，但是其状态仍然是Unknow的时候，在这个地方会进行统一的处理，全部kill掉，从而为接下来的重新启动做清理操作，此处和3.2只会进行一个分支，但核心的目标都是清理那些运行失败或者无法获取状态的容器 3.4 创建容器沙箱 在启动Pod的容器之前，首先会为其创建一个sandbox容器，当前Pod的所有容器都和Pod对应的sandbox共享同一个namespace从而共享一个namespace里面的资源，创建Sandbox比较复杂，后续会继续介绍 3.5 启动Pod相关容器 Pod的容器目前分为三大类：短生命周期容器、初始化容器、业务容器，启动顺序也是从左到右依次进行,如果对于的容器创建失败，则会通过backoff机制来延缓容器的创建，这里我们顺便介绍下containerRuntime启动容器的流程 3.5.1 检查容器镜像是否拉取 镜像的拉取首先会进行对应容器镜像的拼接，然后将之前获取的拉取的秘钥信息和镜像信息，一起交给CRI运行时来进行底层容器镜像的拉取，当然这里也会各种backoff机制，从而避免频繁拉取失败影响kubelet的性能 3.5.2 创建容器配置 创建容器配置主要是为了容器的运行创建对应的配置数据，主要包括：Pod的主机名、域名、挂载的volume、configMap、secret、环境变量、挂载的设备信息、要挂载的目录信息、端口映射信息、根据环境生成执行的命令、日志目录等信息 3.5.3 调用runtimeService完成容器的创建 调用runtimeService传递容器的配置信息，调用CRI，并且最终调用容器的创建接口完成容器的状态 3.5.4 调用runtimeService启动容器 通过之前创建容器返回的容器ID，来进行对应的容器的启动，并且会为容器创建对应的日志目录 3.5.5 执行容器的回调钩子 如果容器配置了PostStart钩子，则会在此处进行对应钩子的执行，如果钩子的类型是Exec类则会调用CNI的EXec接口完成在容器内的执行 4. 运行沙箱容器 4.1 拉取sandbox镜像 首先会拉取sandbox镜像 4.2 创建沙箱容器 4.2.1 应用SecurityContext 在创建容器之前会先根据SecurityContext里面的配资信息，来进行容器SecurityContext的配置，主要包括特权等级、只读目录、运行账户组等信息 4.2 其余基础信息 除了应用SecurityContext还会进行断开、OOMScoreAdj、Cgroup驱动等信息的映射 4.3 创建容器 根据上面的各种配置信息来进行容器的创建 4.3 创建checkpoint checkpoint主要是将当前sandbox的配置信息进行序列化，并且存储其当前的快照信息 4.4 启动sandbox容器 启动sandbox容器则会直接调用StartContainer同时传入之前创建容器返回的ID完成容器的启动，并且此时会重写覆盖容器的dns配置文件 4.5 容器网络设置 容器的网络配置主要是调用CNI插件来完成容器网络的配置，这里就先不展开了 5. Pod容器启动总结 kubelet是容器管理的核心大管家，其负责各种准入控制、状态管理、探测管理、volume管理、QOS管理、CSI对接的统一调度，并且为Runtime运行时准备基础的数据和并反馈Pod当前的最新状态 Runtime层则将kubelet组装的数据，按照CRI运行时的目标配置和kubelet管理的资源配置信息来进行资源的重组，并且根据Pod的容器的状态来决策容器的启停、创建等操作，并完成容器的基础配置环境的构建，并最终调用CRI完成容器的创建，而CRI运行时，则会讲传递过来的各种数据进行进一步的组合，并应用到主机和对应的namespace资源限制，并根据自己的容器服务组织数据，调用容器服务完成容器的最终创建 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/k8s资源对象/pod/Pod创建流程代码版本kubelet篇.html":{"url":"linux/k8s/k8s资源对象/pod/Pod创建流程代码版本kubelet篇.html","title":"pod创建流程代码版本kubelet篇","keywords":"","body":"[toc] Pod创建流程代码版本kubelet篇 本文严重抄袭至互联网 1. 前言 在k8s的面试中Pod的创建流程是一个常问的问题，而kubelet则无疑重中之重，之前也写过一篇Pod的运行，不过没有涉及到具体的代码，本文尝试用代码的方式，来复数整个核心的流程，同时为了方便记忆，又将整个过程分为：准备、配置、清理、构建运行四个阶段，让我们一起来看下吧， 文末有大图总结 2. 准备阶段 当获取到Pod添加的事件的时候，首先会进行一些基础的工作，我吧这个过程称为准备阶段，准备阶段主要做的事情有如下：1）加入PodManager 2）准入控制检查 3）分发事件 4）根据Pod添加对应的探针， 让我们一起来看下关键实现 2.1 加入PodManager PodManager中的功能除了存储Pod的信息，还会进行对应Pod的configMap和secret的管理，当心加入Pod的时候，会检查对应的Pod是否有对应的configMap和secret配置，如果有则就会创建对应的监听器，监听资源的变化，进行本地缓存 除此之外，如果对应的Pod的BootstrapCheckpointAnnotationKey有设定，则还会创建对应的checkpoint,即将pod的配置数据写入到本地磁盘 kl.podManager.AddPod(pod) 2.2 准入控制检查 准入控制检查主要是在运行Pod之前在kubelet上进行Pod运行条件的检查，检查当前节点在scheduler决策完成后到感知到Pod运行这段时间资源是否依旧满足，并且检查Pod的一些特殊资源比如比如sysctl、security等检查，这里我感觉比较重要的两个分别是eviction和predicate, 如果不满足准入检查，则会直接拒绝 2.2.1 eviction准入检查 如果当前节点只存在内存压力，则会根据对应的Pod的QOS等级来判断，如果说不是BestEffort或者容忍内存压力的污点，则会允许，否则则会拒绝运行 nodeOnlyHasMemoryPressureCondition := hasNodeCondition(m.nodeConditions, v1.NodeMemoryPressure) && len(m.nodeConditions) == 1 if nodeOnlyHasMemoryPressureCondition { // 如果不是PodQOSBestEffort, 则都会尝试运行 notBestEffort := v1.PodQOSBestEffort != v1qos.GetPodQOS(attrs.Pod) if notBestEffort { return lifecycle.PodAdmitResult{Admit: true} } // 如果对应的Pod容忍内存压力的污点，则就可以继续进行其他准入控制器的检查 if v1helper.TolerationsTolerateTaint(attrs.Pod.Spec.Tolerations, &v1.Taint{ Key: v1.TaintNodeMemoryPressure, Effect: v1.TaintEffectNoSchedule, }) { return lifecycle.PodAdmitResult{Admit: true} } } 2.2.2 predicate准入检查 predicate准入控制器中的逻辑主要是分为两个部分：1）检查对应的资源是否满足分配请求，同时会记录缺少的资源2）如果是Critical类型的Pod则会按照QOS等级来进行资源的抢占，满足这些高优先的Pod这里的Critical类型的Pod主要包含如下三类：静态Pod、镜像Pod、高优先Pod(优先级高于2000000000) func (w *predicateAdmitHandler) Admit(attrs *PodAdmitAttributes) PodAdmitResult { node, err := w.getNodeAnyWayFunc() // 踢出扩展资源，只进行内存和CPU资源的检查 podWithoutMissingExtendedResources := removeMissingExtendedResources(admitPod, nodeInfo) // 进行预选算法筛选， 筛选出那些资源不足的资源 fit, reasons, err := predicates.GeneralPredicates(podWithoutMissingExtendedResources, nil, nodeInfo) if !fit { // 如果预选失败，则尝试进行抢占 fit, reasons, err = w.admissionFailureHandler.HandleAdmissionFailure(admitPod, reasons) } } 2.3 探针管理 k8s里面的探针主要分为三类：startup、readiness、liveness，在Pod通过准入控制检查后，会根据Pod的探针配置创建对应的探针，但是这里的探针并不会真正的进行探测，因为当前还无法感知到对应的pod的状态 kl.probeManager.AddPod(pod) 2.4 分发事件 在kubelet中会为每个Pod都创建一个对应的goroutine和事件管道，后续新的事件也都通过管道发送给对应的goroutine func (p *podWorkers) UpdatePod(options *UpdatePodOptions) { // 获取pod信息 pod := options.Pod uid := pod.UID var podUpdates chan UpdatePodOptions var exists bool p.podLock.Lock() defer p.podLock.Unlock() // kubelet会为每个pod创建一个goroutine, 并且通过管道来进行通信 if podUpdates, exists = p.podUpdates[uid]; !exists { podUpdates = make(chan UpdatePodOptions, 1) p.podUpdates[uid] = podUpdates // 为当前pod启动一个goroutine go func() { defer runtime.HandleCrash() p.managePodLoop(podUpdates) }() } if !p.isWorking[pod.UID] { p.isWorking[pod.UID] = true // 更新Pod的事件发送到管道 podUpdates 至此一个Pod的启动的准备阶段就基本完成了，检查运行环境、拉取对应的cofnigMap和secret资源、创建探针、启动负责Pod状态维护的线程，至此准备阶段完成 3.配置阶段 在kubelet最终的状态同步都是由syncPod来完成，该函数会根据传递进来的目标状态和Pod的当前状态来进行决策，从而满足目标状态，因为内部逻辑的复杂，会分为：配置阶段、清理阶段、构建运行阶段，这里先看下配置阶段 配置阶段主要是获取当前的Pod状态、应用CGOUP配置、Pod数据目录构建、等待VOlume挂载、获取镜像拉取的secret等 3.1 计算Pod的状态 Pod的状态数据主要包含当前阶段、Conditions(容器Condition、初始化容器Condition、PodReadyCondition),而这些状态则需要根据当前的PodStatus里面的状态计算，还有probeManager里面探测的数据两部分共同完成 func (kl *Kubelet) generateAPIPodStatus(pod *v1.Pod, podStatus *kubecontainer.PodStatus) v1.PodStatus { allStatus := append(append([]v1.ContainerStatus{}, s.ContainerStatuses...), s.InitContainerStatuses...) // 根据Pod的容器状态，设定当前的的阶段 s.Phase = getPhase(spec, allStatus) kl.probeManager.UpdatePodStatus(pod.UID, s) s.Conditions = append(s.Conditions, status.GeneratePodInitializedCondition(spec, s.InitContainerStatuses, s.Phase)) s.Conditions = append(s.Conditions, status.GeneratePodReadyCondition(spec, s.Conditions, s.ContainerStatuses, s.Phase)) s.Conditions = append(s.Conditions, status.GenerateContainersReadyCondition(spec, s.ContainerStatuses, s.Phase)) return *s } 3.2 运行环境准入检查 该运行环境是指的一些软件状态的，这里主要涉及到Appmor、特权模式、proc挂载，实现机制就是检测对应的Pod是否需要对应的操作，并且SecurityContext中是否允许对应的操作，从而确定Pod是否能够进行运行 func (kl *Kubelet) canRunPod(pod *v1.Pod) lifecycle.PodAdmitResult { // 准入控制插件 for _, handler := range kl.softAdmitHandlers { if result := handler.Admit(attrs); !result.Admit { return result } } return lifecycle.PodAdmitResult{Admit: true} } 3.3 更新状态 更新状态主要是为了probeManager来进行状态检查的，如果probeManager无法获取到对应的状态，就不会执行对应的健康探针的检查，这里的状态就是根据之前的各种计算在kubelet上对应Pod的当前状态 kl.statusManager.SetPodStatus(pod, apiPodStatus) 3.4 网络运行时检查 if err := kl.runtimeState.networkErrors(); err != nil && !kubecontainer.IsHostNetworkPod(pod) { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.NetworkNotReady, \"%s: %v\", NetworkNotReadyErrorMsg, err) return fmt.Errorf(\"%s: %v\", NetworkNotReadyErrorMsg, err) } 3.5 CGroup配置 Cgroup的配置主要是按照QOS等级来进行cgroup目录的构建，并且更新当前Pod的配置 pcm := kl.containerManager.NewPodContainerManager() // cgroup应用cgroup if !kl.podIsTerminated(pod) { podKilled := false if !pcm.Exists(pod) && !firstSync { // 如果对于的cgroup不存在，并且也不是第一次运行，就先将之前的pod沙雕 if err := kl.killPod(pod, nil, podStatus, nil); err == nil { podKilled = true } } if !(podKilled && pod.Spec.RestartPolicy == v1.RestartPolicyNever) { if !pcm.Exists(pod) { // 更新qoscgroup设置 if err := kl.containerManager.UpdateQOSCgroups(); err != nil { } // 更新podde的cgroup配置 if err := pcm.EnsureExists(pod); err != nil { } } } } 3.6 镜像Pod的检查 因为要通过镜像Pod来向apiserver传递静态Pod的状态，所以该阶段主要是为静态Pod创建对应的镜像Pod if kubetypes.IsStaticPod(pod) { // 静态pod podFullName := kubecontainer.GetPodFullName(pod) deleted := false if mirrorPod != nil { if mirrorPod.DeletionTimestamp != nil || !kl.podManager.IsMirrorPodOf(mirrorPod, pod) { deleted, err = kl.podManager.DeleteMirrorPod(podFullName, &mirrorPod.ObjectMeta.UID) } } if mirrorPod == nil || deleted { if err := kl.podManager.CreateMirrorPod(pod); err != nil { } } } } 3.7 创建Pod的数据目录 Pod的数据目录主要是包含三个部分：Pod目录、Volume目录、Plugin目录三个目录 if err := kl.makePodDataDirs(pod); err != nil { return err } 3.8 等待volume的挂载 if !kl.podIsTerminated(pod) { if err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil { } } 3.9 获取镜像拉取的secrets pullSecrets := kl.getPullSecretsForPod(pod) 3.10 调用容器的运行时进行同步 着可能是最复杂的一部分了，接下来就进入到下一个阶段：清理阶段 result := kl.containerRuntime.SyncPod(pod, podStatus, pullSecrets, kl.backOff) kl.reasonCache.Update(pod.UID, result) 4. 清理阶段 在Pod运行前可能已经有部分容器已经在运行，则此时就需要根据当前的状态，来进行一些容器的清理工作，为接下来的构建运行阶段提供一个相对干净的环境 4.1 计算Pod状态变更 在k8s中Pod的状态主要包含sandbox容器状态、初始化容器状态、临时容器状态、业务容器状态等几部分，我们依次来看下关键的实现 podContainerChanges := m.computePodActions(pod, podStatus) 沙箱状态计算：当且仅有一个Ready的沙箱并且沙箱的IP不为空的情况，沙箱的状态才不需要更改，其他情况下，都需要重新进行沙箱的构建，并且需要kill掉Pod关联的所有容器 func (m *kubeGenericRuntimeManager) podSandboxChanged(pod *v1.Pod, podStatus *kubecontainer.PodStatus) (bool, uint32, string) { if len(podStatus.SandboxStatuses) == 0 { return true, 0, \"\" } readySandboxCount := 0 for _, s := range podStatus.SandboxStatuses { if s.State == runtimeapi.PodSandboxState_SANDBOX_READY { readySandboxCount++ } } sandboxStatus := podStatus.SandboxStatuses[0] if readySandboxCount > 1 { return true, sandboxStatus.Metadata.Attempt + 1, sandboxStatus.Id } if sandboxStatus.State != runtimeapi.PodSandboxState_SANDBOX_READY { return true, sandboxStatus.Metadata.Attempt + 1, sandboxStatus.Id } if sandboxStatus.GetLinux().GetNamespaces().GetOptions().GetNetwork() != networkNamespaceForPod(pod) { return true, sandboxStatus.Metadata.Attempt + 1, \"\" } if !kubecontainer.IsHostNetworkPod(pod) && sandboxStatus.Network.Ip == \"\" { return true, sandboxStatus.Metadata.Attempt + 1, sandboxStatus.Id } return false, sandboxStatus.Metadata.Attempt, sandboxStatus.Id } 计算Pod的容器状态计算逻辑相对长一些，这里我就不贴代码了，其如要流程分为两个部分： 1.需要创建sandbox: 在该状态下，如果存在初始化容器，则会先进行初始化容器的初始化，即当前步骤只创建第一个初始化容器，如果没有初始化容器，则就将所有的业务容器加入到启动的列表里面 2.不需要创建sandbox: 该状态下会检查遍历所有的临时容器，初始化容器(如果存在失败的初始化容器，则就先启动初始化容器，不会进行业务容器的启动)，业务容器，最终会构建一个需要kill掉的容器列表，还有两个启动的容器列表 4.2 killPod全部清理 需要进行KillPod的状态有两种： sanbbox状态变更 即当sandbox状态不满足要求，则此时需要将Pod的所有容器都杀掉，然后进行重建 无需进行保留的容器 如果Pod对应的容器的hash值变更、状态为失败，则就需要重建 if podContainerChanges.KillPod { // 杀死当前所有的pod killResult := m.killPodWithSyncResult(pod, kubecontainer.ConvertPodStatusToRunningPod(m.runtimeName, podStatus), nil) if podContainerChanges.CreateSandbox { // 终止初始化运行 m.purgeInitContainers(pod, podStatus) } } 4.3 部分清理 如果容器当前的状态是正常的，并且hash没有发生变化，则就不需要进行变更，此时就只需要将当前状态不正常的容器进行清理重建即可 for containerID, containerInfo := range podContainerChanges.ContainersToKill { if err := m.killContainer(pod, containerID, containerInfo.name, containerInfo.message, nil); err != nil { return } } 清理初始化容器 在正式启动容器之前，除了上面两部分，还会进行初始化容器的清理工作 m.pruneInitContainersBeforeStart(pod, podStatus) 5.构建运行阶段 构建运行阶段，主要分为两个大的部分：创建并运行sandbox容器、运行用户容器 5.1 运行sandbox 检查需要创建sandbox,则会首先创建sandbox容器，并获取状态，然后填充当前的Pod的IP信息 // Step 4: Create a sandbox for the pod if necessary. // 创建沙箱环境 podSandboxID := podContainerChanges.SandboxID if podContainerChanges.CreateSandbox { podSandboxID, msg, err = m.createPodSandbox(pod, podContainerChanges.Attempt) podSandboxStatus, err := m.runtimeService.PodSandboxStatus(podSandboxID) if !kubecontainer.IsHostNetworkPod(pod) { podIPs = m.determinePodSandboxIPs(pod.Namespace, pod.Name, podSandboxStatus) } } 5.2 创建sandbox主流程 创建sandbox的主流程主要就三个步骤：创建配置信息、创建日志目录、调用cri运行sandbox生成配置阶段主要包含端口映射、主机名、DNS、Linux中的SecurityContext灯的配置 func (m *kubeGenericRuntimeManager) createPodSandbox(pod *v1.Pod, attempt uint32) (string, string, error) { // 获取沙箱配置 podSandboxConfig, err := m.generatePodSandboxConfig(pod, attempt) // 创建目录 err = m.osInterface.MkdirAll(podSandboxConfig.LogDirectory, 0755) runtimeHandler := \"\" if utilfeature.DefaultFeatureGate.Enabled(features.RuntimeClass) && m.runtimeClassManager != nil { // 获取当前的runtimeHandler runtimeHandler, err = m.runtimeClassManager.LookupRuntimeHandler(pod.Spec.RuntimeClassName) } // 运行Sandbox podSandBoxID, err := m.runtimeService.RunPodSandbox(podSandboxConfig, runtimeHandler) return podSandBoxID, \"\", nil } 5.3 cri中的RunSandbox sandbox的启动主要包含下面几部分：1) 拉取sanbox容器镜像 2)创建sandbox容器 3)创建sandbox的checkpoint 4)启动sandbox容器 5)为sandbox启动网络(如果不是主机网络) func (ds *dockerService) RunPodSandbox(ctx context.Context, r *runtimeapi.RunPodSandboxRequest) (*runtimeapi.RunPodSandboxResponse, error) { config := r.GetConfig() // Step 1: Pull the image for the sandbox. // 拉取sandbox沙箱 // defaultPodSandboxImageName = \"k8s.gcr.io/pause\" // defaultPodSandboxImageVersion = \"3.1\" image := defaultSandboxImage podSandboxImage := ds.podSandboxImage if len(podSandboxImage) != 0 { image = podSandboxImage } // 拉取镜像 if err := ensureSandboxImageExists(ds.client, image); err != nil { return nil, err } // 2.创建sandbox容器 if r.GetRuntimeHandler() != \"\" && r.GetRuntimeHandler() != runtimeName { return nil, fmt.Errorf(\"RuntimeHandler %q not supported\", r.GetRuntimeHandler()) } // 创建沙箱配置 createConfig, err := ds.makeSandboxDockerConfig(config, image) // 创建容器 createResp, err := ds.client.CreateContainer(*createConfig) resp := &runtimeapi.RunPodSandboxResponse{PodSandboxId: createResp.ID} ds.setNetworkReady(createResp.ID, false) defer func(e *error) { // Set networking ready depending on the error return of // the parent function if *e == nil { ds.setNetworkReady(createResp.ID, true) } }(&err) // Step 3: 创建sandbox checkpoint if err = ds.checkpointManager.CreateCheckpoint(createResp.ID, constructPodSandboxCheckpoint(config)); err != nil { return nil, err } // Step 4: Start the sandbox container. // // 4.启动sandbox容器 err = ds.client.StartContainer(createResp.ID) if err != nil { return nil, fmt.Errorf(\"failed to start sandbox container for pod %q: %v\", config.Metadata.Name, err) } //重写docker生成的resolv.conf文件。 if dnsConfig := config.GetDnsConfig(); dnsConfig != nil { containerInfo, err := ds.client.InspectContainer(createResp.ID) if err != nil { return nil, fmt.Errorf(\"failed to inspect sandbox container for pod %q: %v\", config.Metadata.Name, err) } // DNS写配置文件 if err := rewriteResolvFile(containerInfo.ResolvConfPath, dnsConfig.Servers, dnsConfig.Searches, dnsConfig.Options); err != nil { return nil, fmt.Errorf(\"rewrite resolv.conf failed for pod %q: %v\", config.Metadata.Name, err) } } // 如果处于主机网络模式，请不要调用网络插件。 if config.GetLinux().GetSecurityContext().GetNamespaceOptions().GetNetwork() == runtimeapi.NamespaceMode_NODE { return resp, nil } // Step 5: 设置sandbox容器的网络 //所有的pod网络都是由启动时发现的CNI插件设置的。 //这个插件分配pod ip，在沙盒内设置路由，创建接口等。理论上，它的管辖权以pod沙盒网络结束， // 但它也可能在主机上插入iptables规则或打开端口，以满足CNI标准尚未识别的pod规范的部分要求。 cID := kubecontainer.BuildContainerID(runtimeName, createResp.ID) networkOptions := make(map[string]string) if dnsConfig := config.GetDnsConfig(); dnsConfig != nil { // Build DNS options. dnsOption, err := json.Marshal(dnsConfig) if err != nil { return nil, fmt.Errorf(\"failed to marshal dns config for pod %q: %v\", config.Metadata.Name, err) } // 设置网络dns networkOptions[\"dns\"] = string(dnsOption) } // 网络信息 err = ds.network.SetUpPod(config.GetMetadata().Namespace, config.GetMetadata().Name, cID, config.Annotations, networkOptions) return resp, nil } 5.4 容器启动函数 容器启动函数中会通过闭包来保存上面创建的sandbox的信息，同时根据当前容器的配置，创建新的业务容器 start := func(typeName string, container *v1.Container) error { klog.V(4).Infof(\"Creating %v %+v in pod %v\", typeName, container, format.Pod(pod)) if msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, podIPs); err != nil { startContainerResult.Fail(err, msg) } return nil } 5.5 启动容器 容器的启动，主要包含四个流程：1.拉取镜像 2.创建容器&PreStart钩子回调 3) 启动容器 4）postStart启动容器 func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, container *v1.Container, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, podIPs []string) (string, error) { // 启动容器 // Step 1: pull the image. imageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets, podSandboxConfig) // Step 2: create the container. ref, err := kubecontainer.GenerateContainerRef(pod, container) // 获取容器配置， 里面会进行各种文件目录的挂载 containerConfig, cleanupAction, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, podIPs) if cleanupAction != nil { defer cleanupAction() } if err != nil { s, _ := grpcstatus.FromError(err) m.recordContainerEvent(pod, container, \"\", v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", s.Message()) return s.Message(), ErrCreateContainerConfig } // 创建容器 containerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig) // 启动容器钩子 err = m.internalLifecycle.PreStartContainer(pod, container, containerID) m.recordContainerEvent(pod, container, containerID, v1.EventTypeNormal, events.CreatedContainer, fmt.Sprintf(\"Created container %s\", container.Name)) if ref != nil { m.containerRefManager.SetRef(kubecontainer.ContainerID{ Type: m.runtimeName, ID: containerID, }, ref) } // Step 3: 启动容器 err = m.runtimeService.StartContainer(containerID) if err != nil { s, _ := grpcstatus.FromError(err) m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToStartContainer, \"Error: %v\", s.Message()) return s.Message(), kubecontainer.ErrRunContainer } containerMeta := containerConfig.GetMetadata() sandboxMeta := podSandboxConfig.GetMetadata() legacySymlink := legacyLogSymlink(containerID, containerMeta.Name, sandboxMeta.Name, sandboxMeta.Namespace) // 容器日志 containerLog := filepath.Join(podSandboxConfig.LogDirectory, containerConfig.LogPath) if _, err := m.osInterface.Stat(containerLog); !os.IsNotExist(err) { if err := m.osInterface.Symlink(containerLog, legacySymlink); err != nil { } } // Step 4: 执行postStart钩子 if container.Lifecycle != nil && container.Lifecycle.PostStart != nil { msg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart) if handlerErr != nil { if err := m.killContainer(pod, kubeContainerID, container.Name, \"FailedPostStartHook\", nil); err != nil { } } } return \"\", nil } 5.6 cri.CreateContainer CreateContainer中会首先根据k8s里面传递的配置信息，根据当前平台和对应的参数来进行docker容器运行的配置，然后调用docker接口进行容器的配置 func (ds *dockerService) CreateContainer(_ context.Context, r *runtimeapi.CreateContainerRequest) (*runtimeapi.CreateContainerResponse, error) { podSandboxID := r.PodSandboxId config := r.GetConfig() sandboxConfig := r.GetSandboxConfig() containerName := makeContainerName(sandboxConfig, config) // 创建容器配置 createConfig := dockertypes.ContainerCreateConfig{ Name: containerName, Config: &dockercontainer.Config{ // TODO: set User. Entrypoint: dockerstrslice.StrSlice(config.Command), Cmd: dockerstrslice.StrSlice(config.Args), Env: generateEnvList(config.GetEnvs()), Image: image, WorkingDir: config.WorkingDir, Labels: labels, // Interactive containers: OpenStdin: config.Stdin, StdinOnce: config.StdinOnce, Tty: config.Tty, // Disable Docker's health check until we officially support it // (https://github.com/kubernetes/kubernetes/issues/25829). Healthcheck: &dockercontainer.HealthConfig{ Test: []string{\"NONE\"}, }, }, HostConfig: &dockercontainer.HostConfig{ Binds: generateMountBindings(config.GetMounts()), RestartPolicy: dockercontainer.RestartPolicy{ Name: \"no\", }, }, } hc := createConfig.HostConfig err = ds.updateCreateConfig(&createConfig, config, sandboxConfig, podSandboxID, securityOptSeparator, apiVersion) if err != nil { return nil, fmt.Errorf(\"failed to update container create config: %v\", err) } // 设置容器devices devices := make([]dockercontainer.DeviceMapping, len(config.Devices)) for i, device := range config.Devices { devices[i] = dockercontainer.DeviceMapping{ PathOnHost: device.HostPath, PathInContainer: device.ContainerPath, CgroupPermissions: device.Permissions, } } hc.Resources.Devices = devices securityOpts, err := ds.getSecurityOpts(config.GetLinux().GetSecurityContext().GetSeccompProfilePath(), securityOptSeparator) if err != nil { return nil, fmt.Errorf(\"failed to generate security options for container %q: %v\", config.Metadata.Name, err) } hc.SecurityOpt = append(hc.SecurityOpt, securityOpts...) cleanupInfo, err := ds.applyPlatformSpecificDockerConfig(r, &createConfig) if err != nil { return nil, err } createResp, createErr := ds.client.CreateContainer(createConfig) if createErr != nil { createResp, createErr = recoverFromCreationConflictIfNeeded(ds.client, createConfig, createErr) } if createResp != nil { containerID := createResp.ID if cleanupInfo != nil { // we don't perform the clean up just yet at that could destroy information // needed for the container to start (e.g. Windows credentials stored in // registry keys); instead, we'll clean up when the container gets removed ds.containerCleanupInfos[containerID] = cleanupInfo } return &runtimeapi.CreateContainerResponse{ContainerId: containerID}, nil } return nil, createErr } 更新容器配置 func (ds *dockerService) updateCreateConfig( createConfig *dockertypes.ContainerCreateConfig, config *runtimeapi.ContainerConfig, sandboxConfig *runtimeapi.PodSandboxConfig, podSandboxID string, securityOptSep rune, apiVersion *semver.Version) error { if lc := config.GetLinux(); lc != nil { rOpts := lc.GetResources() if rOpts != nil { // 更新资源配置信息 createConfig.HostConfig.Resources = dockercontainer.Resources{ Memory: rOpts.MemoryLimitInBytes, MemorySwap: rOpts.MemoryLimitInBytes, CPUShares: rOpts.CpuShares, CPUQuota: rOpts.CpuQuota, CPUPeriod: rOpts.CpuPeriod, } createConfig.HostConfig.OomScoreAdj = int(rOpts.OomScoreAdj) } // 应用SecurityContext if err := applyContainerSecurityContext(lc, podSandboxID, createConfig.Config, createConfig.HostConfig, securityOptSep); err != nil { return fmt.Errorf(\"failed to apply container security context for container %q: %v\", config.Metadata.Name, err) } } // 应用cgroup配置 if lc := sandboxConfig.GetLinux(); lc != nil { // Apply Cgroup options. cgroupParent, err := ds.GenerateExpectedCgroupParent(lc.CgroupParent) createConfig.HostConfig.CgroupParent = cgroupParent } return nil } 5.7 cri.StartContainer 其实就直接掉Docker的接口启动容器即可 func (ds *dockerService) StartContainer(_ context.Context, r *runtimeapi.StartContainerRequest) (*runtimeapi.StartContainerResponse, error) { err := ds.client.StartContainer(r.ContainerId) return &runtimeapi.StartContainerResponse{}, nil } 6. 总结 Pod启动的核心流程大概就这些，里面会有一些笔认购具体参数数据的构建，没有写明，但是如果对代码感兴趣的，可以顺着这个核心流程基本可以读下来，如果对代码不感兴趣，则后面这张图可以算作一个精简版的，面试可用的Pod创建流程图 kubernetes学习笔记地址: https://www.yuque.com/baxiaoshi/tyado3 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/helm/helm2.16.3安装.html":{"url":"linux/k8s/helm/helm2.16.3安装.html","title":"helm2.16.安装记录","keywords":"","body":"[toc] k8s集群安装helm安装 说明 在k8s集群安装helm v2.16.3，kubeadm安装k8s v1.17.4版本 helm发行版本github地址 本文严重抄袭至互联网 一、helm简介 很多人都使用过Ubuntu下的ap-get或者CentOS下的yum, 这两者都是Linux系统下的包管理工具。采用apt-get/yum,应用开发者可以管理应用包之间的依赖关系，发布应用；用户则可以以简单的方式查找、安装、升级、卸载应用程序。 我们可以将Helm看作Kubernetes下的apt-get/yum。Helm是Deis (https://deis.com/) 开发的一个用于kubernetes的包管理器。每个包称为一个Chart，一个Chart是一个目录（一般情况下会将目录进行打包压缩，形成name-version.tgz格式的单一文件，方便传输和存储）。 对于应用发布者而言，可以通过Helm打包应用，管理应用依赖关系，管理应用版本并发布应用到软件仓库。 对于使用者而言，使用Helm后不用需要了解Kubernetes的Yaml语法并编写应用部署文件，可以通过Helm下载并在kubernetes上安装需要的应用。 除此以外，Helm还提供了kubernetes上的软件部署，删除，升级，回滚应用的强大功能。 Helm 组件及相关术语 Helm Helm 是一个命令行下的客户端工具。主要用于 Kubernetes 应用程序 Chart 的创建、打包、发布以及创建和管理本地和远程的 Chart 仓库。 Tiller Tiller 是 Helm 的服务端，部署在 Kubernetes 集群中。Tiller 用于接收 Helm 的请求，并根据 Chart 生成 Kubernetes 的部署文件（ Helm 称为 Release ），然后提交给 Kubernetes 创建应用。Tiller 还提供了 Release 的升级、删除、回滚等一系列功能。 Chart Helm 的软件包，采用 TAR 格式。类似于 APT 的 DEB 包或者 YUM 的 RPM 包，其包含了一组定义 Kubernetes 资源相关的 YAML 文件。 Repoistory Helm 的软件仓库，Repository 本质上是一个 Web 服务器，该服务器保存了一系列的 Chart 软件包以供用户下载，并且提供了一个该 Repository 的 Chart 包的清单文件以供查询。Helm 可以同时管理多个不同的 Repository。 Release 使用 helm install 命令在 Kubernetes 集群中部署的 Chart 称为 Release。 注：需要注意的是：Helm 中提到的 Release 和我们通常概念中的版本有所不同，这里的 Release 可以理解为 Helm 使用 Chart 包部署的一个应用实例。 Helm工作原理 Chart Install 过程： Helm从指定的目录或者tgz文件中解析出Chart结构信息 Helm将指定的Chart结构和Values信息通过gRPC传递给Tiller Tiller根据Chart和Values生成一个Release Tiller将Release发送给Kubernetes用于生成Release Chart Update过程： Helm从指定的目录或者tgz文件中解析出Chart结构信息 Helm将要更新的Release的名称和Chart结构，Values信息传递给Tiller Tiller生成Release并更新指定名称的Release的History Tiller将Release发送给Kubernetes用于更新Release Chart Rollback过程： Helm将要回滚的Release的名称传递给Tiller Tiller根据Release的名称查找History Tiller从History中获取上一个Release Tiller将上一个Release发送给Kubernetes用于替换当前Release 二、helm安装 客户端helm安装 1.下载helm客户端 wget https://get.helm.sh/helm-v2.16.3-linux-amd64.tar.gz 2.解压缩并拷贝helm二进制文件 tar xf helm-v2.16.3-linux-amd64.tar.gz cp linux-amd64/helm /usr/local/bin 服务端tiller安装 1.集群每个节点安装socat 否则会报错Error: cannot connect to Tiller yum install -y socat 2.初始化helm，部署tiller Tiller 是以 Deployment 方式部署在 Kubernetes 集群中的，只需执行helm init命令便可简单的完成安装，但是Helm默认会去 storage.googleapis.com 拉取镜像。。。。。。这里需要使用阿里云的仓库完成安装 #添加阿里云的仓库 helm init --client-only --stable-repo-url https://aliacs-app-catalog.oss-cn-hangzhou.aliyuncs.com/charts/ helm repo add incubator https://aliacs-app-catalog.oss-cn-hangzhou.aliyuncs.com/charts-incubator/ helm repo update #创建服务端 helm init --service-account tiller --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.16.3 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts #创建TLS认证服务端，参考地址：https://github.com/gjmzj/kubeasz/raw/branch/branch/master/docs/guide/helm.md helm init --service-account tiller --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.16.3 --tiller-tls-cert /etc/kubernetes/ssl/tiller001.pem --tiller-tls-key /etc/kubernetes/ssl/tiller001-key.pem --tls-ca-cert /etc/kubernetes/ssl/ca.pem --tiller-namespace kube-system --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts 3.给tiller授权 因为 Helm 的服务端 Tiller 是一个部署在 Kubernetes 中 kube-system namespace下的deployment，它会去连接 kube-api在Kubernetes里创建和删除应用。 而从Kubernetes1.6版本开始，API Server 启用了RBAC授权。目前的Tiller部署时默认没有定义授权的ServiceAccount，这会导致访问API Server时被拒绝。所以我们需要明确为Tiller部署添加授权。 创建 Kubernetes 的服务帐号和绑定角色 kubectl create serviceaccount --namespace kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller 为tiller设置帐号 #使用kubectl patch更新API对象 kubectl patch deploy --namespace kube-system tiller-deploy -p '{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\":\"tiller\"}}}}' #验证是否授权成功 kubectl get deploy --namespace kube-system tiller-deploy --output yaml|grep serviceAccount serviceAccount: tiller serviceAccountName: tiller 4.验证tiller是否安装成功 kubectl -n kube-system get pods|grep tiller tiller-deploy-6d8dfbb696-4cbcz 1/1 Running 0 88s 输入命令 helm version 显示结果以下既为成功 Client: &version.Version{SemVer:\"v2.16.3\", GitCommit:\"1ee0254c86d4ed6887327dabed7aa7da29d7eb0d\", GitTreeState:\"clean\"} Server: &version.Version{SemVer:\"v2.16.3\", GitCommit:\"1ee0254c86d4ed6887327dabed7aa7da29d7eb0d\", GitTreeState:\"clean\"} 5.卸载helm服务端tiller $ helm reset 或 $ helm reset -f 强制删除 三、helm使用 1.更换仓库 若遇到Unable to get an update from the “stable” chart repository (https://kubernetes-charts.storage.googleapis.com) 错误 手动更换stable 存储库为阿里云的存储库 #先移除原先的仓库 helm repo remove stable #添加新的仓库地址 helm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts #更新仓库 helm repo update 2.查看在存储库中可用的所有 Helm charts： $ helm search |head -5 NAME CHART VERSION APP VERSION DESCRIPTION incubator/ack-advanced-horizontal-pod-autoscaler 0.1.0 1.0 A Helm chart for Kubernetes incubator/ack-ahas-pilot 1.7.2 1.7.2 A cloud service that aims to improve the high availabilit... incubator/ack-ahas-sentinel-pilot 0.1.1 0.1.1 Ahas sentinel Pilot - Webhook Admission Controller incubator/ack-ahas-springcloud-gateway 0.1.1 0.1.1 Spring Cloud Gateway with AHAS Sentinel integration Helm ... #更新charts列表 helm repo update 3.安装ingress-nginx Monocular是一个开源软件，用于管理kubernetes上以Helm Charts形式创建的服务，可以通过它的web页面来安装helm Charts 创建rbac cat > helm.rbac.yaml kubectl apply -f helm.rbac.yaml 安装Nginx Ingress controller，安装的k8s集群启用了RBAC，则一定要加rbac.create=true参数 helm install stable/nginx-ingress \\ -n nginx-ingress \\ --namespace kube-system \\ --set controller.hostNetwork=true,rbac.create=true \\ --set controller.replicaCount=1 结果报错如下 Error: validation failed: unable to recognize \"\": no matches for kind \"Deployment\" in version \"extensions/v1beta1\" 解决方法 将Deployment和StatefuSet apiVersion更改为apiVersion: apps/v1 1.先search 执行 helm search nginx-ingress 2.如果有兼容性问题，需要手动先fetch helm fetch stable/nginx-ingress 然后会下载nginx-ingress-0.9.5.tgz 解压缩这个包，然后搜索extensions/v1beta1 [root@k8s-master ~]# grep -R \"extensions/v1beta1\" nginx-ingress nginx-ingress/templates/NOTES.txt: apiVersion: extensions/v1beta1 nginx-ingress/templates/controller-daemonset.yaml:apiVersion: extensions/v1beta1 nginx-ingress/templates/controller-deployment.yaml:apiVersion: extensions/v1beta1 nginx-ingress/templates/default-backend-deployment.yaml:apiVersion: extensions/v1beta1 把以上这些文件中的v1beta1修改为v1 grep -irl \"extensions/v1beta1\" nginx-ingress | grep deploy | xargs sed -i 's#extensions/v1beta1#apps/v1#g' 修改完后应用本地的nginx-ingress包，但是不会操作！！！ 创建helm-rbac.yaml文件后手动删除了，结果报错如下 Error: the server has asked for the client to provide credentials 把运行的tiller的pod干掉，让他自动重启 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/k8s/kubesphere/在k8s集群安装kubesphere2.1.html":{"url":"linux/k8s/kubesphere/在k8s集群安装kubesphere2.1.html","title":"kubesphere","keywords":"","body":"[toc] 在 Kubernetes集群中 安装 KubeSphere2.1 安装介绍 完整安装、高可用安装、升级、可插拔功能组件安装等等全在kubesphere官网了，写的非常详细，如果有一些解决不了的问题就在 kubesphere论坛 发帖(发帖前最好自己尝试解决一下，尤其是持久化存储这一块！！！) 文档说明 本文将使用kubeadm安装k8s1.16.9版本，并且在k8s集群中安装kubesphere2.1 官方文档中明确说明了如果要完全安装kubesphere，集群机器配置至少是8c16g，而最小化安装机器配置最少为2g内存，所以这里仅仅是最小化安装体验 使用 kubeadm 搭建 v1.16.9 版本 Kubernetes 集群 一、环境准备 1.1实验环境 角色 IP地址 主机名 docker版本 硬件配置 系统 内核 master 192.168.9.10 k8s-master 18.09.9 2c4g CentOS7.7 3.10.0-1062.el7.x86_64 node1 192.168.9.13 k8s-node1 18.09.9 2c6g CentOS7.7 3.10.0-1062.el7.x86_64 node2 192.168.9.14 k8s-node2 18.09.9 2c6g CentOS7.7 3.10.0-1062.el7.x86_64 1.2每个节点配置host信息 cat >> /etc/hosts 1.3禁用防火墙和selinux //禁用防火墙 systemctl stop firewalld && systemctl disable firewalld //禁用selinux #临时修改 setenforce 0 #永久修改，重启服务器后生效 sed -i '7s/enforcing/disabled/' /etc/selinux/config 1.4创建/etc/sysctl.d/k8s.conf文件，添加如下内容 //向文件中写入以下内容 cat >/etc/sysctl.d/k8s.conf 1.5安装ipvs 脚本创建了的/etc/sysconfig/modules/ipvs.modules文件，保证在节点重启后能自动加载所需模块。使用lsmod | grep -e ip_vs -e nf_conntrack_ipv4命令查看是否已经正确加载所需的内核模块 //向文件中写入以下内容 cat > /etc/sysconfig/modules/ipvs.modules 安装ipset和ipvsadm(便于查看 ipvs 的代理规则) yum -y install ipset ipvsadm 1.6同步服务器时间 //安装chrony yum -y install chrony //修改同步服务器地址为阿里云 sed -i.bak '3,6d' /etc/chrony.conf && sed -i '3cserver ntp1.aliyun.com iburst' \\ /etc/chrony.conf //启动chronyd及加入开机自启 systemctl start chronyd && systemctl enable chronyd //查看同步结果 chronyc sources 210 Number of sources = 1 MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== ^* 120.25.115.20 2 6 37 13 +194us[+6131us] +/- 33ms 1.7关闭swap分区 修改/etc/fstab文件，注释掉 SWAP 的自动挂载，使用free -m确认 swap 已经关闭 //手动关闭swap swapoff -a //修改fstab文件，注释swap自动挂载 sed -i '/^\\/dev\\/mapper\\/centos-swap/c#/dev/mapper/centos-swap swap swap defaults 0 0' /etc/fstab //查看swap是否关闭 free -m total used free shared buff/cache available Mem: 1994 682 612 9 699 1086 Swap: 0 0 0 swappiness 参数调整，修改/etc/sysctl.d/k8s.conf添加下面一行 cat >>/etc/sysctl.d/k8s.conf 1.8安装docker18.09.9 1.添加阿里云yum源 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 2.查看可用版本 yum list docker-ce --showduplicates | sort -r 已加载插件：fastestmirror, langpacks 可安装的软件包 * updates: mirrors.aliyun.com Loading mirror speeds from cached hostfile * extras: mirrors.aliyun.com docker-ce.x86_64 3:19.03.5-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.4-3.el7 docker-ce-stable 。。。。。。 docker-ce.x86_64 3:18.09.9-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.8-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.7-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.6-3.el7 docker-ce-stable 。。。。。。 3.安装docker18.09.9 yum -y install docker-ce-18.09.9-3.el7 docker-ce-cli-18.09.9 4.启动docker并设置开机自启 systemctl enable docker && systemctl start docker 5.配置阿里云docker镜像加速 cat > /etc/docker/daemon.json 1.9修改docker Cgroup Driver为systemd #修改docker Cgroup Driver为systemd 将/usr/lib/systemd/system/docker.service文件中的这一行 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock 修改为 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd 如果不修改，在添加 worker 节点时可能会碰到如下错误 [WARNING IsDockerSystemdCheck]: detected \"cgroupfs\" as the Docker cgroup driver. The recommended driver is \"systemd\". Please follow the guide at https://kubernetes.io/docs/setup/cri/ //使用如下命令修改 sed -i.bak \"s#^ExecStart=/usr/bin/dockerd.*#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd#g\" /usr/lib/systemd/system/docker.service //重启docker systemctl daemon-reload && systemctl restart docker 1.10安装Kubeadm 需要科学上网 cat >/etc/yum.repos.d/kubernetes.repo 使用阿里云yum源 cat >/etc/yum.repos.d/kubernetes.repo 安装 kubeadm、kubelet、kubectl(阿里云yum源会随官方更新最新版，因此指定版本) //安装1.16.9版本 yum -y install kubelet-1.16.9 kubeadm-1.16.9 kubectl-1.16.9 //查看版本 kubeadm version kubeadm version: &version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.9\", GitCommit:\"a17149e1a189050796ced469dbd78d380f2ed5ef\", GitTreeState:\"clean\", BuildDate:\"2020-04-16T11:42:30Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"} 设置kubelet开机自启 systemctl enable kubelet 设置k8s命令自动补全 yum -y install bash-completion source /usr/share/bash-completion/bash_completion source > ~/.bashrc 到此，基本环境安装完成!!! 二、初始化集群 2.1master节点操作，配置 kubeadm 初始化文件 cat ./kubeadm-config.yaml apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration kubernetesVersion: v1.16.9 imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers #master地址 controlPlaneEndpoint: \"192.168.9.10:6443\" networking: serviceSubnet: \"10.96.0.0/16\" #k8s容器组所在的网段 podSubnet: \"10.20.0.1/16\" dnsDomain: \"cluster.local\" EOF 2.2初始化master ⚠️如果想要重新初始化，需要执行命令kubeadm reset -f #kubeadm init --config=kubeadm-config.yaml --upload-certs 完整输出结果 kubeadm init --config=kubeadm-config.yaml [init] Using Kubernetes version: v1.16.9 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using 'kubeadm config images pull' [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Activating the kubelet service [certs] Using certificateDir folder \"/etc/kubernetes/pki\" [certs] Generating \"ca\" certificate and key [certs] Generating \"apiserver\" certificate and key [certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.9.10 192.168.9.10] [certs] Generating \"apiserver-kubelet-client\" certificate and key [certs] Generating \"front-proxy-ca\" certificate and key [certs] Generating \"front-proxy-client\" certificate and key [certs] Generating \"etcd/ca\" certificate and key [certs] Generating \"etcd/server\" certificate and key [certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.9.10 127.0.0.1 ::1] [certs] Generating \"etcd/peer\" certificate and key [certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.9.10 127.0.0.1 ::1] [certs] Generating \"etcd/healthcheck-client\" certificate and key [certs] Generating \"apiserver-etcd-client\" certificate and key [certs] Generating \"sa\" key and public key [kubeconfig] Using kubeconfig folder \"/etc/kubernetes\" [kubeconfig] Writing \"admin.conf\" kubeconfig file [kubeconfig] Writing \"kubelet.conf\" kubeconfig file [kubeconfig] Writing \"controller-manager.conf\" kubeconfig file [kubeconfig] Writing \"scheduler.conf\" kubeconfig file [control-plane] Using manifest folder \"/etc/kubernetes/manifests\" [control-plane] Creating static Pod manifest for \"kube-apiserver\" [control-plane] Creating static Pod manifest for \"kube-controller-manager\" [control-plane] Creating static Pod manifest for \"kube-scheduler\" [etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\" [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s [apiclient] All control plane components are healthy after 16.501777 seconds [upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [kubelet] Creating a ConfigMap \"kubelet-config-1.16\" in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --upload-certs [mark-control-plane] Marking the node k8s-master as control-plane by adding the label \"node-role.kubernetes.io/master=''\" [mark-control-plane] Marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [bootstrap-token] Using token: px979r.mphk9ee5ya8fgy44 [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of control-plane nodes by copying certificate authorities and service account keys on each node and then running the following as root: kubeadm join 192.168.9.10:6443 --token px979r.mphk9ee5ya8fgy44 \\ --discovery-token-ca-cert-hash sha256:5e7c7cd1cc1f86c0761e54b9380de22968b6b221cb98939c14ab2942223f6f51 \\ --control-plane Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.9.10:6443 --token px979r.mphk9ee5ya8fgy44 \\ --discovery-token-ca-cert-hash sha256:5e7c7cd1cc1f86c0761e54b9380de22968b6b221cb98939c14ab2942223f6f51 拷贝 kubeconfig 文件 //这里的路径为/root mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 2.3master添加节点 node1和node2相同操作 将master节点上的$HOME/.kube/config 文件拷贝到node节点对应的文件中 1.创建目录，这里的路径为/root mkdir -p $HOME/.kube 2.把master节点上的config文件拷贝到node1和node2的$HOME/.kube scp k8s-master1:~/.kube/config $HOME/.kube 3.修改权限 chown $(id -u):$(id -g) $HOME/.kube/config 将node1和node2加入到集群中 这里需要用到2.2中初始化master最后生成的token和sha256值 #kubeadm join 192.168.9.10:6443 --token px979r.mphk9ee5ya8fgy44 --discovery-token-ca-cert-hash sha256:5e7c7cd1cc1f86c0761e54b9380de22968b6b221cb98939c14ab2942223f6f51 输出结果 [preflight] Running pre-flight checks [WARNING IsDockerSystemdCheck]: detected \"cgroupfs\" as the Docker cgroup driver. The recommended driver is \"systemd\". Please follow the guide at https://kubernetes.io/docs/setup/cri/ [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [kubelet-start] Downloading configuration for the kubelet from the \"kubelet-config-1.16\" ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Activating the kubelet service [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster. 如果忘记了token和sha256值，可以在master节点使用如下命令查看 //查看token #kubeadm token list TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS px979r.mphk9ee5ya8fgy44 20h 2020-03-18T13:49:48+08:00 authentication,signing system:bootstrappers:kubeadm:default-node-token //查看sha256 #openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' 5e7c7cd1cc1f86c0761e54b9380de22968b6b221cb98939c14ab2942223f6f51 //同时查看token和sha256 #kubeadm token create --print-join-command kubeadm join 192.168.9.10:6443 --token 9b28zg.oyt0kvvpmtrem4bg --discovery-token-ca-cert-hash sha256:5e7c7cd1cc1f86c0761e54b9380de22968b6b221cb98939c14ab2942223f6f51 master节点查看node，发现状态都是NotReady，因为还没有安装网络插件，这里我们安装calio官方插件文档 kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master NotReady master 19m v1.16.9 k8s-node1 NotReady 4m10s v1.16.9 k8s-node2 NotReady 4m3s v1.16.9 2.4master节点安装网络插件calio //下载文件 wget https://docs.projectcalico.org/v3.8/manifests/calico.yaml 将文件中的620行改为如下，因为在上边kubeadm-config.yaml配置文件中指定了容器组IP 620行 value: \"10.20.0.1/16\" //修改完成后安装calico网络插件 kubectl apply -f calico.yaml //安装完成后稍等一会查看pods状态 kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-dc6cb64cb-8sh59 1/1 Running 0 6m22s calico-node-89s9k 1/1 Running 0 6m22s calico-node-dkt7w 1/1 Running 0 6m22s calico-node-tgg2h 1/1 Running 0 6m22s coredns-667f964f9b-7hrj9 1/1 Running 0 33m coredns-667f964f9b-8q7sh 1/1 Running 0 33m etcd-k8s-master 1/1 Running 0 33m kube-apiserver-k8s-master 1/1 Running 0 32m kube-controller-manager-k8s-master 1/1 Running 0 33m kube-proxy-b2r5d 1/1 Running 0 12m kube-proxy-nd982 1/1 Running 0 11m kube-proxy-zh6cz 1/1 Running 0 33m kube-scheduler-k8s-master 1/1 Running 0 32m //查看node状态 [root@k8s-master ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready master 31m v1.16.9 k8s-node1 Ready 9m46s v1.16.9 k8s-node2 Ready 9m22s v1.16.9 2.5安装Dashboard(可选) 下载文件及修改内容 这里查看dashboard对应的k8s版本 //下载文件 v2.0.0-rc3是中文版本，beta8是英文版本 wget https://raw/branch.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml wget https://raw/branch.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc3/aio/deploy/recommended.yaml //修改Service为NodePort类型 42行下增加一行 nodePort: 30001 44行下增加一行 type: NodePort //原先内容 spec: ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard //修改后内容 spec: ports: - port: 443 targetPort: 8443 nodePort: 30001 #增加，指定nodeport端口 selector: k8s-app: kubernetes-dashboard type: NodePort #增加，修改类型为nodeport 部署dashboard kubectl apply -f recommended.yaml 查看dashboard的运行状态及外网访问端口 //查看dashboard运行状态 #kubectl get pods -n kubernetes-dashboard -l k8s-app=kubernetes-dashboard NAME READY STATUS RESTARTS AGE kubernetes-dashboard-5996555fd8-2ppc5 1/1 Running 0 8m16s //查看dashboard外网访问端口，命名空间为kubernetes-dashboard #kubectl get svc -n kubernetes-dashboard -l k8s-app=kubernetes-dashboard kubectl get svc -n kubernetes-dashboard -l k8s-app=kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard NodePort 10.96.142.172 443:30001/TCP 8m37s 通过上边的30001端口访问dashboard，注意是https ⚠️k8s1.16.9这个版本中，使用的dashboard版本是2.0.0-beta8，只有火狐浏览器可以访问，其余浏览器都不能访问，会报错如下 使用火狐浏览器访问，由于 dashboard 默认是自建的 https 证书，该证书是不受浏览器信任的，所以我们需要强制跳转就可以了 然后创建一个具有全局所有权限的用户来登录Dashboard //编辑admin.yaml文件 cat > admin.yaml 2.6安装k8s切换命名空间工具 git clone https://github.com.cnpmjs.org/ahmetb/kubectx cp kubectx/kubens /usr/local/bin #查看所有命名空间 kubens #切换到kube-system命名空间 kubens kube-system Context \"kubernetes-admin@kubernetes\" modified. Active namespace is \"kube-system\". 到此，使用kubeadm安装k8s 1.16.3完成！！！ 三、在k8s集群中安装kubesphere 官网中安装kubesphere的前提条件如下 Kubernetes版本： 1.15.x ≤ K8s version ≤ 1.17.x； Helm版本： 2.10.0 ≤ Helm Version ＜ 3.0.0（不支持 helm 2.16.0 #6894），且已安装了 Tiller，参考 如何安装与配置 Helm （预计 3.0 支持 Helm v3）； 集群已有默认的存储类型（StorageClass），若还没有准备存储请参考 安装 OpenEBS 创建 LocalPV 存储类型 用作开发测试环境。 集群能够访问外网，若无外网请参考 在 Kubernetes 离线安装 KubeSphere。 3.1安装helm2.16.3 客户端helm安装 3.1.1下载helm客户端 wget https://get.helm.sh/helm-v2.16.3-linux-amd64.tar.gz 3.1.2解压缩并拷贝helm二进制文件 tar xf helm-v2.16.3-linux-amd64.tar.gz cp linux-amd64/helm /usr/local/bin 服务端tiller安装 3.1.3集群每个节点安装socat 否则会报错Error: cannot connect to Tiller yum install -y socat 3.1.4初始化helm，部署tiller Tiller 是以 Deployment 方式部署在 Kubernetes 集群中的，只需执行helm init命令便可简单的完成安装，但是Helm默认会去 storage.googleapis.com 拉取镜像。。。。。。这里需要使用阿里云的仓库完成安装 #添加阿里云的仓库 helm init --client-only --stable-repo-url https://aliacs-app-catalog.oss-cn-hangzhou.aliyuncs.com/charts/ helm repo add incubator https://aliacs-app-catalog.oss-cn-hangzhou.aliyuncs.com/charts-incubator/ helm repo update #创建服务端 使用-i指定阿里云仓库 helm init --service-account tiller --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.16.3 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts #创建TLS认证服务端，参考地址：#https://github.com/gjmzj/kubeasz/raw/branch/branch/master/docs/guide/helm.md helm init --service-account tiller --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.16.3 --tiller-tls-cert /etc/kubernetes/ssl/tiller001.pem --tiller-tls-key /etc/kubernetes/ssl/tiller001-key.pem --tls-ca-cert /etc/kubernetes/ssl/ca.pem --tiller-namespace kube-system --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts 3.1.5给tiller授权 因为 Helm 的服务端 Tiller 是一个部署在 Kubernetes 中 kube-system namespace下的deployment，它会去连接 kube-api在Kubernetes里创建和删除应用。 而从Kubernetes1.6版本开始，API Server 启用了RBAC授权。目前的Tiller部署时默认没有定义授权的ServiceAccount，这会导致访问API Server时被拒绝。所以我们需要明确为Tiller部署添加授权。 创建 Kubernetes 的服务帐号和绑定角色 #创建serviceaccount kubectl create serviceaccount --namespace kube-system tiller #创建角色绑定 kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller 为tiller设置帐号 #使用kubectl patch更新API对象 kubectl patch deploy --namespace kube-system tiller-deploy -p '{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\":\"tiller\"}}}}' #验证是否授权成功 kubectl get deploy --namespace kube-system tiller-deploy --output yaml|grep serviceAccount serviceAccount: tiller serviceAccountName: tiller 3.1.6验证tiller是否安装成功 kubectl -n kube-system get pods|grep tiller tiller-deploy-6d8dfbb696-4cbcz 1/1 Running 0 88s 输入命令 helm version 显示结果以下既为成功 Client: &version.Version{SemVer:\"v2.16.3\", GitCommit:\"1ee0254c86d4ed6887327dabed7aa7da29d7eb0d\", GitTreeState:\"clean\"} Server: &version.Version{SemVer:\"v2.16.3\", GitCommit:\"1ee0254c86d4ed6887327dabed7aa7da29d7eb0d\", GitTreeState:\"clean\"} 3.1.7卸载helm服务端tiller $ helm reset 或 $ helm reset -f 强制删除 3.2安装nfs存储 官方提供的openebs存储貌似不太好使，反正我是安装完后pod的状态一直是pending nfs存储比较简单，适合实验环境 也可以使用别的持久化存储 安装nfs参考文章 nfs这里选择在master安装，上边的参考文章中说nfs server安装在master节点会有问题，但是我这里没有 3.2.1安装配置nfs client端，这里为两个node节点 yum -y install nfs-utils server端，master节点 1.安装包 yum -y install nfs-utils rpcbind 2.编辑配置文件 ⚠️配置文件中的*是允许所有网段，根据自己实际情况写明网段 cat >/etc/exports 配置storageclass，注意修改nfs服务端IP和共享目录 cat >storageclass.yaml 创建storageclass kubectl apply -f storageclass.yaml 设置默认strorageclass kubectl patch storageclass nfs-storage -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' 检查nfs-client pod状态 #这里是在default命名空间下创建的 kubectl get pods NAME READY STATUS RESTARTS AGE nfs-client-provisioner-7b9746695c-nrz4n 1/1 Running 0 2m38s 检查默认存储 #这里是在default命名空间下创建的 kubectl get sc NAME PROVISIONER AGE nfs-storage (default) fuseim.pri/ifs 7m22s 3.3部署kubesphere 官方文档 3.3.1最小化安装 KubeSphere kubectl apply -f https://raw/branch.githubusercontent.com/kubesphere/ks-installer/master/kubesphere-minimal.yaml 3.3.2查看安装日志 #使用如下命令查看安装日志 kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath='{.items[0].metadata.name}') -f 当日志最后提示如下即表明安装完成，但是还是要等待一些pod完全运行起来才可以 Start installing monitoring ************************************************** task monitoring status is successful total: 1 completed:1 ************************************************** ##################################################### ### Welcome to KubeSphere! ### ##################################################### Console: http://192.168.9.10:30880 Account: admin Password: P@88w0rd NOTES： 1. After logging into the console, please check the monitoring status of service components in the \"Cluster Status\". If the service is not ready, please wait patiently. You can start to use when all components are ready. 2. Please modify the default password after login. ##################################################### 安装日志中会有一个报错如下，但是没有影响 TASK [ks-core/ks-core : KubeSphere | Delete Ingress-controller configmap] ****** fatal: [localhost]: FAILED! => {\"changed\": true, \"cmd\": \"/usr/local/bin/kubectl delete cm -n kubesphere-system ks-router-config\\n\", \"delta\": \"0:00:00.562513\", \"end\": \"2020-04-28 07:18:28.772284\", \"msg\": \"non-zero return code\", \"rc\": 1, \"start\": \"2020-04-28 07:18:28.209771\", \"stderr\": \"Error from server (NotFound): configmaps \\\"ks-router-config\\\" not found\", \"stderr_lines\": [\"Error from server (NotFound): configmaps \\\"ks-router-config\\\" not found\"], \"stdout\": \"\", \"stdout_lines\": []} ...ignoring 检查所有pod状态，都为running才可以 kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE default nfs-client-provisioner-7b9746695c-nrz4n 1/1 Running 0 18m kube-system calico-kube-controllers-bc44d789c-ksgnt 1/1 Running 0 39h kube-system calico-node-2t4gr 1/1 Running 0 39h kube-system calico-node-5bzjl 1/1 Running 0 39h kube-system calico-node-fjdll 1/1 Running 0 39h kube-system coredns-58cc8c89f4-8jrlt 1/1 Running 0 39h kube-system coredns-58cc8c89f4-nt5z5 1/1 Running 0 39h kube-system etcd-k8s-master1 1/1 Running 0 39h kube-system kube-apiserver-k8s-master1 1/1 Running 0 39h kube-system kube-controller-manager-k8s-master1 1/1 Running 0 39h kube-system kube-proxy-b7vj4 1/1 Running 0 39h kube-system kube-proxy-bghx7 1/1 Running 0 39h kube-system kube-proxy-ntrxx 1/1 Running 0 39h kube-system kube-scheduler-k8s-master1 1/1 Running 0 39h kube-system kuboard-756d46c4d4-dwzwt 1/1 Running 0 39h kube-system metrics-server-78cff478b7-lwcfl 1/1 Running 0 39h kube-system tiller-deploy-6d8dfbb696-ldpjd 1/1 Running 0 40m kubernetes-dashboard dashboard-metrics-scraper-b68468655-t2wgd 1/1 Running 0 39h kubernetes-dashboard kubernetes-dashboard-64999dbccd-zwnn5 1/1 Running 1 39h kubesphere-controls-system default-http-backend-5d464dd566-5hlzs 1/1 Running 0 6m9s kubesphere-controls-system kubectl-admin-6c664db975-kp6r5 1/1 Running 0 3m10s kubesphere-monitoring-system kube-state-metrics-566cdbcb48-cc4fv 4/4 Running 0 5m32s kubesphere-monitoring-system node-exporter-5lvpx 2/2 Running 0 5m32s kubesphere-monitoring-system node-exporter-hlfbh 2/2 Running 0 5m32s kubesphere-monitoring-system node-exporter-qxkm6 2/2 Running 0 5m32s kubesphere-monitoring-system prometheus-k8s-0 3/3 Running 1 4m32s kubesphere-monitoring-system prometheus-k8s-system-0 3/3 Running 1 4m32s kubesphere-monitoring-system prometheus-operator-6b97679cfd-6dztx 1/1 Running 0 5m32s kubesphere-system ks-account-596657f8c6-kzx9w 1/1 Running 0 5m56s kubesphere-system ks-apigateway-78bcdc8ffc-2rvbg 1/1 Running 0 5m58s kubesphere-system ks-apiserver-5b548d7c5c-dxqt7 1/1 Running 0 5m57s kubesphere-system ks-console-78bcf96dbf-kdh7q 1/1 Running 0 5m53s kubesphere-system ks-controller-manager-696986f8d9-fklzv 1/1 Running 0 5m55s kubesphere-system ks-installer-75b8d89dff-zm6fl 1/1 Running 0 7m49s kubesphere-system openldap-0 1/1 Running 0 6m21s kubesphere-system redis-6fd6c6d6f9-dqh2s 1/1 Running 0 6m25s 访问kubesphere:30880 用户名：admin 密码：P@88w0rd 登陆后首界面 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/kvm/安装/kvm安装.html":{"url":"linux/kvm/安装/kvm安装.html","title":"kvm安装","keywords":"","body":"kvm安装 1.安装软件包 yum -y install libvirt* virt-* qemu-kvm* 软件包名 作用 libvirt 虚拟机的管理软件 virt、virt-install、virt-clone 虚拟机的安装和克隆 qemu-kvm、qemu-img 复制管理虚拟机的磁盘 2.启动服务并设置开机自启 systemctl start libvirtd && systemctl enable libvirtd 3.安装图形化管理工具 virt-manager yum -y install virt-manager 安装完成后在 Applications -> System Tools -> Virtual Machine Manager(kvm图形化管理工具) 打开后是这样的 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/kvm/操作/kvm虚拟机基本操作.html":{"url":"linux/kvm/操作/kvm虚拟机基本操作.html","title":"kvm虚拟机基本操作","keywords":"","body":"kvm虚拟机基本操作 1.创建、删除、修改名称 1.1 创建虚拟机 1.1.1 图形化安装 virt-install \\ --virt-type kvm \\ --os-variant centos7.0 \\ --name abc \\ --memory 1024 \\ --vcpus 1 \\ --disk /data/KVM_imgs/abc.qcow2,format=qcow2,size=10 \\ --cdrom /data/ISO/CentOS-7-x86_64-DVD-2009.iso \\ --network bridge=br0 \\ --graphics vnc,listen=0.0.0.0,password=1,port=10086 \\ --noautoconsole 以下为命令输出 Starting install... Allocating 'abc.qcow2' | 10 GB 00:00:00 Domain installation still in progress. You can reconnect to the console to complete the installation process. 参数说明 参数 说明 --virt-type 虚拟机类型，可选有 kvm 、qemu 、xen --os-variant 发型版本，例如 CentOS 、 Ubuntu，可执行 osinfo-query os 查看支持的版本 --name 虚拟机名称 --memory 虚拟机内存，单位 MB --vcpus 虚拟机cpu --disk 虚拟机磁盘文件信息，包括大小(单位GB)、路径、格式 --cdrom 镜像文件 --network 虚拟机网络类型 --graphics 指定图形界面，可选有vnc、spice 1.1.2 命令行安装 1.2 删除虚拟机 ⚠️删除虚拟机之前虚拟机必须为关闭状态 virsh undefine 虚拟机名称 virsh undefine linux-templet-mini-clone 1.3 修改名称 virsh domrename 源虚拟机名称 新虚拟名称 virsh domrename linux-new linux-new-xxx 2.查看、启动、停止、重启 2.1 查看 virsh list --all 2.2 启动 virsh start 虚拟机名称 virsh start linux-templet-mini-clone 2.3 停止 2.3.1 正常停止 virsh shutdown 虚拟机名称 virsh shutdown linux-templet-mini-clone 2.3.2 强制停止(相当于拔电源) virsh destroy 虚拟机名称 virsh destroy linux-templet-mini-clone 2.4 重启 virsh reboot 虚拟机名称 virsh reboot linux-templet-mini-clone 3.备份、恢复 3.1 备份 ⚠️备份虚拟机主要就是备份虚拟机磁盘文件和配置文件 virsh dumpxml 虚拟机名称 # 备份配置文件 virsh dumpxml linux-templet-mini-clone > /opt/bak/linux-templet-mini-clone.xml # 备份磁盘文件 cp /data/KVM_imgs/linux-templet-mini-clone.qcow2 /opt/bak 3.2 恢复 ⚠️ 恢复虚拟机，磁盘文件和配置文件必须在相同目录 virsh define 虚拟机备份文件 virsh define linux-templet-mini-clone.xml 4.克隆 4.1 完整克隆 ⚠️克隆虚拟机之前虚拟机必须为关闭状态 virt-clone --auto-clone -o 源虚拟机名称 -n 新虚拟机名称 virt-clone \\ --auto-clone \\ -o linux-templet-mini \\ -n linux-templet-mini-clone 以下为命令输出 WARNING Setting the graphics device port to autoport, in order to avoid conflicting. Allocating 'linux-templet-mini-clone.qcow2' | 10 GB 00:00:02 Clone 'linux-templet-mini-clone' created successfully. 4.2 链接克隆 4.2.1 需要修改配置文件 创建链接克隆磁盘文件 qemu-img create -f qcow2 -b 原磁盘文件名 克隆后磁盘文件名 qemu-img create -f qcow2 -b linux-templet-mini.qcow2 linux-new.qcow2 查看链接克隆 $ qemu-img info linux-new.qcow2 image: linux-new.qcow2 file format: qcow2 virtual size: 10G (10737418240 bytes) disk size: 196K cluster_size: 65536 backing file: linux-templet-mini.qcow2 Format specific information: compat: 1.1 lazy refcounts: false 拷贝配置文件 cd /etc/libvirt/qemu/ cp linux-templet-mini.xml linux-new.xml 修改配置文件 第一处修改：修改name linux-templet-mini 第二处修改：删除UUID bee80325-5cf8-4824-ab74-125640b8ab73 第三处修改：磁盘文件名，搜索disk找到 第四处修改：删除mac地址，搜索mac addr找到 依据文件，恢复虚拟机 virsh define linux-new.xml 启动虚拟机 virsh start linux-new 查看克隆 $ virsh list --all |grep linux-new 73 linux-new running 4.2.2 不需要修改配置文件 创建链接克隆磁盘文件 qemu-img create -f qcow2 -b linux-templet-mini.qcow2 linux-new.qcow2 依据链接磁盘文件创建链接克隆虚拟机 --boot hd 表示从硬盘启动 --disk 直接指定上边创建的链接磁盘文件，而不需要指定ISO镜像 virt-install \\ --virt-type kvm \\ --os-variant centos7.0 \\ --name linux-new \\ --memory 1024 \\ --vcpus 1 \\ --disk /data/KVM_imgs/linux-new.qcow2,format=qcow2,size=10 \\ --boot hd \\ --network bridge=br0 \\ --graphics vnc,listen=0.0.0.0,password=1,port=10086 \\ --noautoconsole 5.挂起、恢复挂起 查看虚拟机 $ virsh list --all |grep linux-new 74 linux-new running 挂起虚拟机 virsh suspend linux-new 再次查看，可以看到虚拟机状态变为了 paused 挂起状态的虚拟机并不是出于关机状态，同时不能做任何操作 $ virsh list --all |grep linux-new 74 linux-new paused 恢复挂起 virsh resume linux-new 6.查看虚拟机vnc端口号 virsh vncdisplay 虚拟机名称 $ virsh vncdisplay --domain linux-new-xxx :4186 7.磁盘格式转换 7.1 kvm虚拟机磁盘格式 磁盘格式 说明 raw/branch 裸格式，不支持快照，性能好，磁盘存储是连续性的 qcow2 支持快照，性能比raw/branch差一点，磁盘存储是不连续的 7.2 虚拟磁盘操作 7.2.1 查看虚拟磁盘信息 qemu-img info 磁盘名 $ qemu-img info linux-new.qcow2 image: linux-new.qcow2 file format: qcow2 virtual size: 10G (10737418240 bytes) disk size: 24M cluster_size: 65536 backing file: linux-templet-mini.qcow2 Format specific information: compat: 1.1 lazy refcounts: false 7.2.2 创建虚拟磁盘 创建 raw/branch 格式虚拟磁盘 # 不指定格式默认就是 `raw/branch` 格式 $ qemu-img create /opt/test.raw/branch 2G Formatting '/opt/test.raw/branch', fmt=raw/branch size=2147483648 # 查看虚拟磁盘，此时是空的 $ qemu-img info /opt/test.raw/branch image: /opt/test.raw/branch file format: raw/branch virtual size: 2.0G (2147483648 bytes) disk size: 0 $ du -sh /opt/test.raw/branch 0 /opt/test.raw/branch 创建 qcow2 虚拟磁盘 # 使用 -f 指定磁盘类型 $ qemu-img create -f qcow2 /opt/test.qcow2 2G Formatting '/opt/test.qcow2', fmt=qcow2 size=2147483648 encryption=off cluster_size=65536 lazy_refcounts=off # 查看虚拟磁盘 $ qemu-img info /opt/test.qcow2 image: /opt/test.qcow2 file format: qcow2 virtual size: 2.0G (2147483648 bytes) disk size: 196K cluster_size: 65536 Format specific information: compat: 1.1 lazy refcounts: false $ du -sh /opt/test.qcow2 196K /opt/test.qcow2 7.2.3 调整虚拟磁盘容量 # 查看虚拟磁盘大小 $ qemu-img info /opt/test.raw/branch image: /opt/test.raw/branch file format: raw/branch virtual size: 2.0G (2147483648 bytes) disk size: 0 # 调整磁盘大小，使用 n 直接指定大小，使用 +nG 指定增加多少 $ qemu-img resize /opt/test.raw/branch +1G Image resized. # 再次查看虚拟磁盘大小，可以看到大小已经增加 $ qemu-img info /opt/test.raw/branch image: /opt/test.raw/branch file format: raw/branch virtual size: 3.0G (3221225472 bytes) disk size: 0 7.2.4 转换磁盘格式 qemu-img convert -f 源磁盘类型 -O 目标磁盘类型 源文件 目标文件 raw/branch 格式转换为 qcow2 格式 # 转换 $ qemu-img convert -f raw/branch -O qcow2 /opt/test.raw/branch /opt/newtest.qcow2 # 查看 $ qemu-img info /opt/newtest.qcow2 image: /opt/newtest.qcow2 file format: qcow2 virtual size: 3.0G (3221225472 bytes) disk size: 196K cluster_size: 65536 Format specific information: compat: 1.1 lazy refcounts: false qcow2 格式转换为 raw/branch 格式 # 转换 $ qemu-img convert -f qcow2 -O raw/branch /opt/test.qcow2 /opt/newtest.raw/branch # 查看 $ qemu-img info /opt/newtest.raw/branch image: /opt/newtest.raw/branch file format: raw/branch virtual size: 2.0G (2147483648 bytes) disk size: 0 8.快照 ⚠️raw/branch格式不支持快照 快照默认存放路径是 /var/lib/libvirt/qemu/snapshot 8.1 查看快照 virsh snapshot-list 虚拟机名称 查看快照 $ virsh snapshot-list linux-new-xxx Name Creation Time State ------------------------------------------------------------ 8.2 创建快照 virsh snapshot-create 虚拟机名称 8.2.1 指定快照名称 $ virsh snapshot-create-as --domain linux-new-xxx hehe Domain snapshot hehe created 8.2.2 不指定快照名称 不指定快照名称默认会以时间戳名称，1646575935是时间戳，用的是格林威治时间，意思是从1970年1月1日到现在经过了多少秒 $ virsh snapshot-create linux-new-xxx Domain snapshot 1646575935 created 8.3 删除快照 # 删除当前快照，默认会删除最近一次快照 $ virsh snapshot-delete --current linux-new-xxx Domain snapshot hehe deleted # 删除指定快照 $ virsh snapshot-delete linux-new-xxx 1646575935 Domain snapshot 1646575935 deleted 8.4 恢复快照 virsh snapshot-revert linux-new-xxx --snapshotname hehe 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/kvm/知识点/kvm虚拟机扩容硬盘.html":{"url":"linux/kvm/知识点/kvm虚拟机扩容硬盘.html","title":"kvm虚拟机扩容硬盘","keywords":"","body":"kvm虚拟机扩容硬盘 1.宿主机查看kvm虚拟机磁盘文件大小 此时文件大小为50G $ qemu-img info abc.qcow2 image: abc.qcow2 file format: qcow2 virtual size: 50G (53687091200 bytes) disk size: 2.5G cluster_size: 65536 Format specific information: compat: 1.1 lazy refcounts: true 2.宿主机扩容kvm磁盘文件大小 ⚠️执行扩容命令前一定要先关闭虚拟机 ⚠️执行扩容命令前一定要先关闭虚拟机 ⚠️执行扩容命令前一定要先关闭虚拟机 $ qemu-img resize abc.qcow2 500G Image resized. 再次查看kvm虚拟机磁盘文件大小，此时已经增加 $ qemu-img info abc.qcow2 image: abc.qcow2 file format: qcow2 virtual size: 500G (536870912000 bytes) disk size: 2.5G cluster_size: 65536 Format specific information: compat: 1.1 lazy refcounts: true 3.查看虚拟机硬盘大小 可以看到当前硬盘 /dev/sda 大小为500G 使用命令 df -h 查看，此时硬盘还未扩容 4.开始扩容硬盘 4.1 新建一个分区 4.2 修改分区格式 修改分区格式为8e，即lvm格式 保存后执行一下 partprobe 命令 4.3 再次查看虚拟机硬盘 可以看到新建的分区/dev/sda3，并且硬盘Id为8e 4.4 查看新增的分区 4.5 使用lvm开始扩容 4.5.1 创建pv $ pvcreate /dev/sda3 Physical volume \"/dev/sda3\" successfully created. 4.5.2 将创建的pv加入vg 执行命令 vgdisplay 查看vg信息 $ vgdisplay --- Volume group --- VG Name centos System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 3 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 2 Max PV 0 Cur PV 1 Act PV 1 VG Size 将创建的pv加入vg 这里vg名称为 centos $ vgextend centos /dev/sda3 Volume group \"centos\" successfully extended 再次查看vg信息，可以看到新增的450G $ vgdisplay --- Volume group --- VG Name centos System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 4 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 2 Max PV 0 Cur PV 2 Act PV 2 VG Size 498.99 GiB PE Size 4.00 MiB Total PE 127742 Alloc PE / Size 12542 / 48.99 GiB Free PE / Size 115200 / 450.00 GiB VG UUID 3uV4Hk-GNAr-fIet-W5He-filb-pzd1-1cC4Os 4.5.3 扩容lv 执行命令 lvdisplay 查看lv信息 $ lvdisplay --- Logical volume --- LV Path /dev/centos/swap LV Name swap VG Name centos LV UUID K5Vifj-QpYD-MABK-NOAE-K8os-9BqR-8M620U LV Write Access read/write LV Creation host, time templet, 2021-11-15 15:59:43 +0800 LV Status available # open 2 LV Size 2.00 GiB Current LE 512 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 8192 Block device 253:1 --- Logical volume --- LV Path /dev/centos/root LV Name root VG Name centos LV UUID uMARyx-VRJh-79dj-v15L-yr7M-XzR7-SKRzMW LV Write Access read/write LV Creation host, time templet, 2021-11-15 15:59:44 +0800 LV Status available # open 1 LV Size 46.99 GiB Current LE 12030 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 8192 Block device 253:0 扩容lv $ lvextend -L +450G /dev/centos/root Size of logical volume centos/root changed from 46.99 GiB (12030 extents) to 496.99 GiB (127230 extents). Logical volume centos/root successfully resized. 4.5.4 扩容硬盘 如果硬盘分区类型是ext4，则使用 resize2fs 命令 如果硬盘分区类型是xfs，则使用 xfs_growfs 命令 这里硬盘分区类型是xfs $ xfs_growfs /dev/mapper/centos-root meta-data=/dev/mapper/centos-root isize=512 agcount=4, agsize=3079680 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0 spinodes=0 data = bsize=4096 blocks=12318720, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal bsize=4096 blocks=6015, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 data blocks changed from 12318720 to 130283520 查看磁盘，扩容完成 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/kvm/知识点/使用 virt-manager 安装虚拟机.html":{"url":"linux/kvm/知识点/使用 virt-manager 安装虚拟机.html","title":"使用 virt-manager 安装虚拟机","keywords":"","body":"使用 virt-manager 安装虚拟机 1.安装图形化管理工具 virt-manager yum -y install virt-manager 安装完成后在 Applications -> System Tools -> Virtual Machine Manager(kvm图形化管理工具) 打开后是这样的 关于 virt-manager 的配套工具 virt-install 是一个命令行工具，它提供了一种将操作系统配置到虚拟机中的简单方法。 virt-viewer 是一个轻量级的 UI 界面，用于与虚拟化来宾操作系统的图形显示进行交互。它可以显示 VNC 或 SPICE，并使用 libvirt 查找图形连接详细信息。 virt-clone 是一个命令行工具，用于克隆现有的非活动来宾。它复制磁盘映像，并使用指向复制磁盘的新名称、UUID 和 MAC 地址定义配置。 virt-xml 是一个命令行工具，用于使用 virt-install 的命令行选项轻松编辑 libvirt 域 XML。 virt-bootstrap 是一个命令行工具，提供了一种简单的方法来为基于 libvirt 的容器设置根文件系统。 2.安装虚拟机 2.1 配置 storage pool storage pool 是宿主机上的一块存储空间，可以是一个目录(文件系统)或者vg 默认虚拟机磁盘文件存储路径为 /var/lib/libvirt/images ，是由 /etc/libvirt/storage/default.xml 文件中决定的 创建一个名为 kvm_storage 的存储池 路径设置为 /data/kvm/iso 创建完成后的存储池 2.2 点击电脑图标然后选择 Local install media (ISO image or CDROM) 2.3 选择 Use ISO image 2.4 选择本地镜像 2.5 设置虚拟机的cpu和内存 2.6 设置虚拟机的硬盘大小 2.7 设置虚拟机名称 2.8 开始安装 安装界面，后续就是系统安装了 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/kvm/知识点/vncserver安装.html":{"url":"linux/kvm/知识点/vncserver安装.html","title":"vncserver安装","keywords":"","body":"vncserver安装 1.安装包 yum -y install tigervnc tigervnc-server 2.配置systemd管理vncserver 安装完包后会生产一个文件 /usr/lib/systemd/system/vncserver@.service 文件中已经说的非常清楚了 # Quick HowTo: # 1. Copy this file to /etc/systemd/system/vncserver@.service # 2. Replace with the actual user name and edit vncserver # parameters in the wrapper script located in /usr/bin/vncserver_wrapper # 3. Run `systemctl daemon-reload` # 4. Run `systemctl enable vncserver@:.service` 按照说明配置一下就可以 第一步、拷贝文件 cp /usr/lib/systemd/system/vncserver@.service /etc/systemd/system/ 第二步、替换文件中的 为具体的用户 sed -i 's//root/' /etc/systemd/system/vncserver@.service 第三步、运行命令 systemctl daemon-reload systemctl daemon-reload 第四步、设置vnc密码 ⚠️必须在启动vnc前设置密码，否则启动会失败 # 输入2次密码 $ vncpasswd Password: Verify: Would you like to enter a view-only password (y/n)? n A view-only password is not used 第四步、启动vncserver 使用命令 systemctl enable vncserver@:.service 把想要开启的vnc窗口加入开机自启，其中 为具体的窗口号 systemctl start vncserver@:1.service systemctl enable vncserver@:1.service 查看启动 默认的启动脚本路径 /root/.vnc/xstartup 默认的配置文件路径 /root/.vnc/config 默认的日志文件路径 /root/.vnc/devops01:1.log 其中 devops01 是主机名0 $ systemctl status vncserver@:1 ● vncserver@:1.service - Remote desktop service (VNC) Loaded: loaded (/etc/systemd/system/vncserver@.service; enabled; vendor preset: disabled) Active: active (running) since Wed 2021-12-01 10:37:10 CST; 23s ago Process: 9480 ExecStartPre=/bin/sh -c /usr/bin/vncserver -kill %i > /dev/null 2>&1 || : (code=exited, status=0/SUCCESS) Main PID: 9483 (vncserver_wrapp) CGroup: /system.slice/system-vncserver.slice/vncserver@:1.service ├─9483 /bin/sh /usr/bin/vncserver_wrapper root :1 └─9675 sleep 5 Dec 01 10:37:10 devops01 systemd[1]: Starting Remote desktop service (VNC)... Dec 01 10:37:10 devops01 systemd[1]: Started Remote desktop service (VNC). Dec 01 10:37:10 devops01 vncserver_wrapper[9483]: xauth: file /root/.Xauthority does not exist Dec 01 10:37:13 devops01 vncserver_wrapper[9483]: New 'devops01:1 (root)' desktop is devops01:1 Dec 01 10:37:13 devops01 vncserver_wrapper[9483]: Creating default startup script /root/.vnc/xstartup Dec 01 10:37:13 devops01 vncserver_wrapper[9483]: Creating default config /root/.vnc/config Dec 01 10:37:13 devops01 vncserver_wrapper[9483]: Starting applications specified in /root/.vnc/xstartup Dec 01 10:37:13 devops01 vncserver_wrapper[9483]: Log file is /root/.vnc/devops01:1.log Dec 01 10:37:19 devops01 vncserver_wrapper[9483]: 'vncserver :1' has PID 9544, waiting until it exits ... 查看进程 $ ps aux|grep [X]vnc root 9544 0.0 2.3 194016 24140 ? Sl 10:37 0:00 /usr/bin/Xvnc :1 -auth /root/.Xauthority -desktop devops01:1 (root) -fp catalogue:/etc/X11/fontpath.d -geometry 1024x768 -pn -rfbauth /root/.vnc/passwd -rfbport 5901 -rfbwait 30000 3.vncserver一些操作说明 3.1 启动、关闭 启动 方式一： systemd管理 systemctl start vncserver@:1.service 方式二：vncserver命令 直接运行 vncserver :数字 启动 $ vncserver :1 New 'devops01:1 (root)' desktop is devops01:1 Starting applications specified in /root/.vnc/xstartup Log file is /root/.vnc/devops01:1.log 关闭 方式一：systemd管理 systemctl stop vncserver@:1.service 方式二：vncserver命令 $ vncserver -kill :1 Killing Xvnc process ID 16654 3.2 修改vncserver窗口分辨率 通过查看vnc进程可以看到，vncserver默认的窗口分辨率是 1024x768 $ ps aux|grep [X]vnc root 9544 0.0 2.3 194016 24140 ? Sl 10:37 0:00 /usr/bin/Xvnc :1 -auth /root/.Xauthority -desktop devops01:1 (root) -fp catalogue:/etc/X11/fontpath.d -geometry 1024x768 -pn -rfbauth /root/.vnc/passwd -rfbport 5901 -rfbwait 30000 如果想要修改vncserver启动窗口的默认分辨率，需要修改 .vnc/config 文件中 geometry= 参数，默认是注释的 $ cat ~/.vnc/config ## Supported server options to pass to vncserver upon invocation can be listed ## in this file. See the following manpages for more: vncserver(1) Xvnc(1). ## Several common ones are shown below. Uncomment and modify to your liking. ## # securitytypes=vncauth,tlsvnc # desktop=sandbox # geometry=2000x1200 # localhost # alwaysshared 修改为想要的分辨率，然后重启进程 ⚠️重启vncserver进程不能使用system命令，重启会不生效并且报错 使用 systemctl 命令重启vncserver systemctl restart vncserver@:1 然后查看进程 $ ps aux|grep [X]vnc root 16693 0.0 2.2 192504 22640 pts/0 Sl 18:44 0:00 /usr/bin/Xvnc :1 -auth /root/.Xauthority -desktop devops01:1 (root) -fp catalogue:/etc/X11/fontpath.d -geometry 1024x768 -pn -rfbauth /root/.vnc/passwd -rfbport 5901 -rfbwait 30000 查看进程状态，发现报错 A VNC server is already running as :1 并且分辨率没有修改 $ systemctl status vncserver@:1 ● vncserver@:1.service - Remote desktop service (VNC) Loaded: loaded (/etc/systemd/system/vncserver@.service; enabled; vendor preset: disabled) Active: failed (Result: exit-code) since Wed 2021-12-01 18:57:41 CST; 12s ago Process: 16631 ExecStop=/bin/sh -c /usr/bin/vncserver -kill %i > /dev/null 2>&1 || : (code=exited, status=0/SUCCESS) Process: 16765 ExecStart=/usr/bin/vncserver_wrapper root %i (code=exited, status=2) Process: 16762 ExecStartPre=/bin/sh -c /usr/bin/vncserver -kill %i > /dev/null 2>&1 || : (code=exited, status=0/SUCCESS) Main PID: 16765 (code=exited, status=2) Dec 01 18:57:41 devops01 systemd[1]: Starting Remote desktop service (VNC)... Dec 01 18:57:41 devops01 systemd[1]: Started Remote desktop service (VNC). Dec 01 18:57:41 devops01 vncserver_wrapper[16765]: A VNC server is already running as :1 Dec 01 18:57:41 devops01 vncserver_wrapper[16765]: FATAL: 'runuser -l root' failed! Dec 01 18:57:41 devops01 systemd[1]: vncserver@:1.service: main process exited, code=exited, status=2/INVALIDARGUMENT Dec 01 18:57:41 devops01 systemd[1]: Unit vncserver@:1.service entered failed state. Dec 01 18:57:41 devops01 systemd[1]: vncserver@:1.service failed. 使用 vncserver -kill :1 kill掉进程 $ vncserver -kill :1 Killing Xvnc process ID 16693 [root@devops01 ~]# vncserver :1 New 'devops01:1 (root)' desktop is devops01:1 Starting applications specified in /root/.vnc/xstartup Log file is /root/.vnc/devops01:1.log 然后再次查看进程，可以看到分辨率已经修改 $ ps aux|grep [X]vnc root 16842 0.3 2.8 198808 28936 pts/0 Sl 18:58 0:00 /usr/bin/Xvnc :1 -geometry 2000x1200 -auth /root/.Xauthority -desktop devops01:1 (root) -fp catalogue:/etc/X11/fontpath.d -pn -rfbauth /root/.vnc/passwd -rfbport 5901 -rfbwait 30000 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/kvm/知识点/kvm虚拟机网卡改动无法启动问题.html":{"url":"linux/kvm/知识点/kvm虚拟机网卡改动无法启动问题.html","title":"kvm虚拟机网卡改动无法启动问题","keywords":"","body":"kvm虚拟机网卡改动无法启动问题 1.背景说明 centos7安装kvm，kvm创建的虚拟机由于无法ping通宿主机，因此选择增加了一块网卡并设置模式为NAT，并且IP段为 192.168.122.0/24，这样的话就可以通过这个192的地址去和宿主机的 virbr0 通信了，但是实际上这种做法是错误的，因为这个192段完全是虚拟的，外界无法连通这个段，比如连接公司内网vpn后就无法连接，并且在虚拟机中是无法ping通宿主机的 kvm安装完成后会创建一块名为 virbr0 IP为 192.168.122.1 的虚拟网卡 这块网卡就是虚拟机中的eth0，模式为桥接 这块网卡就是虚拟机中的eth1，模式为NAT 2.解决方法 2.1 虚拟网桥说明 参考链接 KVM 客户机网络连接有两种方式： 用户网络（User Networking）：让虚拟机访问主机、互联网或本地网络上的资源的简单方法，但是不能从网络或其他的客户机访问客户机，性能上也需要大的调整。NAT方式。 虚拟网桥（Virtual Bridge）：这种方式要比用户网络复杂一些，但是设置好后客户机与互联网，客户机与主机之间的通信都很容易。Bridge方式。 设置kvm虚拟机网络连接为虚拟网桥方式，这样就能解决使用一块网卡让kvm虚拟机与宿主机互通以及虚拟机间互通 如图所示，网桥的基本原理就是创建一个桥接接口br0，在屋里网卡和虚拟机网络接口之间传递数据 2.2 配置虚拟网桥 2.2.1 新增网桥设备br0 cat > /etc/sysconfig/network-scripts/ifcfg-br0 2.2.2 修改eth0网卡 注释以下内容 IPADDR=10.0.0.111 PREFIX=24 GATEWAY=10.0.0.1 DNS1=223.5.5.5 DNS2=114.114.114.114 新增以下内容 NM_CONTROLLED=no 表示关闭服务 NetworkManager BRIDGE=br0 NM_CONTROLLED=no 2.2.3 重启网络服务 systemctl restart network 2.3 配置虚拟机 2.3.1 图形化方式 Network Source 选择 Specify shared device name ， Bridge name 填写新增加的网卡 br0 ，Device model 选择 virtio ，最后点击 Apply 配置完成后如下图所示 2.3.2 修改配置文件方式 修改虚拟机配置文件 /etc/libvirt/qemu/xxx.xml 找到 interface 处的配置，因为原先有2块网卡，所以会有2处配置 修改为如下内容 需要修改 type='virtio' 的mac地址和pci 3.遇到的问题 修改完宿主机使用br0虚拟桥接网卡后，kvm中的虚拟机是无法启动的，点击启动会报错如下，原因就是没有做2.3步骤中的操作，修改虚拟机的网卡配置使用br0后就可以了 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/kvm/知识点/kvm虚拟机开机自启.html":{"url":"linux/kvm/知识点/kvm虚拟机开机自启.html","title":"kvm虚拟机开机自启","keywords":"","body":"kvm虚拟机开机自启 为了防止宿主机因某些原因重启后而导致kvm虚拟机不可用，我们需要对kvm虚拟机设置开机自启 设置虚拟机开机自启 设置开机自启 virsh autostart 虚拟机名称 virsh autostart test 查看虚拟机状态，此时为关闭状态 $ virsh list --all Id Name State ---------------------------------------------------- - test shut off 重启libvirtd服务 systemctl restart libvirtd 重启 libvirtd 服务后查看虚拟机状态，已经启动 $ virsh list --all Id Name State ---------------------------------------------------- 1 test running kvm虚拟机开机自启原理 /etc/libvirt/qemu 多了一个 autostart 目录，并且会有一个虚拟机名称的软连接存在 $ ll /etc/libvirt/qemu/autostart total 0 lrwxrwxrwx 1 root root 26 Feb 27 17:19 test.xml -> /etc/libvirt/qemu/test.xml 删除 /etc/libvirt/qemu/autostart 目录 rm -rf /etc/libvirt/qemu/autostart 关闭虚拟机 virsh shutdown test 此时虚拟机为关闭状态 $ virsh list --all Id Name State ---------------------------------------------------- - test shut off 重启 libvirtd systemctl restart libvirtd 再次查看虚拟机状态，开机自启失败 $ virsh list --all Id Name State ---------------------------------------------------- - test shut off 创建 autostart 目录并修改权限为700 mkdir /etc/libvirt/qemu/autostart && chmod 700 /etc/libvirt/qemu/autostart 创建软连接，想要让哪些虚拟机开机启动就做软连接即可 ln -s /etc/libvirt/qemu/test.xml /etc/libvirt/qemu/autostart/ 重启 libvirtd ，验证开机自启 systemctl restart libvirtd 再次查看虚拟机状态，已经开机自启 $ virsh list --all Id Name State ---------------------------------------------------- 1 test running 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/kvm/知识点/kvm虚拟机console登录.html":{"url":"linux/kvm/知识点/kvm虚拟机console登录.html","title":"kvm虚拟机console登录","keywords":"","body":"kvm虚拟机console登录 在宿主机执行命令 virsh console 虚拟机名称，此时是无法console连接的，因为没有做配置， ctrl+] 退出 $ virsh console linux-new-xxx Connected to domain linux-new-xxx Escape character is ^] ​ 连接虚拟机，备份虚拟机内核文件 cp /boot/grub2/grub.cfg{,.bak} 修改内核参数 grubby --update-kernel=ALL --args=\"console=ttyS0,115200n8\" 参数 说明 --update-kernel=ALL 指定内核信息，ALL代表全部 console=ttyS0 可以通过ttys0终端登陆虚拟机 115200n8 频率参数，波特率，不加也可以，登陆虚拟机重启后可以看到重启过程 重启机器后就可以使用console登录了 $ virsh console linux-new-xxx Connected to domain linux-new-xxx Escape character is ^] # 按下回车就会出现登录界面，ctrl+] 退出 CentOS Linux 7 (Core) Kernel 3.10.0-1160.el7.x86_64 on an x86_64 linux-templet-mini login: 登录方式 ctrl+alt+f2 切换终端 登录方式 对应标识 说明 宿主机登陆到虚拟机 ttyS0 VNC登陆虚拟机 tty1 终端 终端工具登陆虚拟机(xshell、item等) pts/0 虚拟终端 # 宿主机登陆到虚拟机 $ who root ttyS0 2022-03-06 21:32 # VNC登陆虚拟机 $ who root tty1 2022-03-06 21:32 # 终端工具登陆虚拟机 $ who root pts/0 2022-03-06 21:37 (10.0.19.22) 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/openstack/mitaka/1.centos7.8搭建openstack Mitaka版.html":{"url":"linux/openstack/mitaka/1.centos7.8搭建openstack Mitaka版.html","title":"centos7.8搭建openstack Mitaka版","keywords":"","body":"[toc] centos7.8搭建openstack Mitaka版 openstack中文文档 mitaka版官方文档 mitaka版密码说明 密码名称 描述 数据库密码(不能使用变量) 数据库的root密码 ADMIN_PASS admin 用户密码 CEILOMETER_DBPASS Telemetry 服务的数据库密码 CEILOMETER_PASS Telemetry 服务的 ceilometer 用户密码 CINDER_DBPASS 块设备存储服务的数据库密码 CINDER_PASS 块设备存储服务的 cinder 密码 DASH_DBPASS Database password for the dashboard DEMO_PASS demo 用户的密码 GLANCE_DBPASS 镜像服务的数据库密码 GLANCE_PASS 镜像服务的 glance 用户密码 HEAT_DBPASS Orchestration服务的数据库密码 HEAT_DOMAIN_PASS Orchestration 域的密码 HEAT_PASS Orchestration 服务中heat用户的密码 KEYSTONE_DBPASS 认证服务的数据库密码 NEUTRON_DBPASS 网络服务的数据库密码 NEUTRON_PASS 网络服务的 neutron 用户密码 NOVA_DBPASS 计算服务的数据库密码 NOVA_PASS 计算服务中nova用户的密码 RABBIT_PASS RabbitMQ的guest用户密码 SWIFT_PASS 对象存储服务用户swift的密码 实验环境 角色 IP 主机名 默认网关 硬件环境 虚拟化 防火墙 selinux 控制节点 10.0.0.11/24 controller 10.0.0.1 4G内存，50G硬盘 开启 关闭 关闭 计算节点 10.0.0.31/24 compute1 10.0.0.1 4G内存，50G硬盘 开启 关闭 关闭 一、基础环境配置 1.1 关闭防火墙和selinux //禁用防火墙 systemctl stop firewalld && systemctl disable firewalld //禁用selinux #临时修改 setenforce 0 #永久修改，重启服务器后生效 sed -i '7s/enforcing/disabled/' /etc/selinux/config 1.2 配置hosts解析 #控制节点和计算节点相同操作 cat >> /etc/hosts 1.3 配置NTP服务，要保证控制节点和计算节点时间一致 控制节点 1.安装chrony yum -y install chrony 2.编辑chrony配置文件/etc/chrony.conf /删除以下4行，使用阿里云NTP服务器 server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst 修改为 server ntp1.aliyun.com iburst /允许连接控制节点的网段，24行增加以下一行 allow 10.0.0.0/24 #用以下命令修改 sed -i '3,6d' /etc/chrony.conf && sed -i '3cserver ntp1.aliyun.com iburst' \\ /etc/chrony.conf && sed -i '23callow 10.0.0.0/24' /etc/chrony.conf 3.启动NTP服务并设置开机自启 systemctl enable chronyd && systemctl start chronyd 4.检查端口，监听udp323端口 netstat -nupl|grep chronyd udp 0 0 127.0.0.1:323 0.0.0.0:* 29356/chronyd udp 0 0 0.0.0.0:123 0.0.0.0:* 29356/chronyd udp6 0 0 ::1:323 :::* 29356/chronyd 5.验证 chronyc sources 210 Number of sources = 1 MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== ^* 8.8.8.8 2 6 37 29 +43us[ -830us] +/- 22ms 计算节点 1.安装chrony yum -y install chrony 2.编辑chrony配置文件/etc/chrony.conf /删除以下4行，指定控制节点为NTP服务器 server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst 修改为 server controller iburst #用以下命令修改 sed -i '3,6d' /etc/chrony.conf && sed -i '3cserver controller iburst' /etc/chrony.conf 3.启动NTP服务并设置开机自启 systemctl enable chronyd && systemctl start chronyd 4.检查端口，监听udp323端口 netstat -nupl|grep chronyd udp 0 0 127.0.0.1:323 0.0.0.0:* 1327/chronyd udp6 0 0 ::1:323 :::* 1327/chronyd 5.验证，计算节点显示的是控制节点 chronyc sources 210 Number of sources = 1 MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== ^? controller 3 6 200 50 +1319ms[+1319ms] +/- 14.4s 1.4下载openstack官方yum源安装openstack客户端 ⚠️Mitaka版官方文档中直接安装centos-release-openstack-mitaka会提示没有可用包(使用的是阿里云的yum源)，得先下载一个包才可以继续安装！！！ 下载官方yum源提示无包可用解决方法 关于m版安装的坑，提示无安装包 具体解决方法 控制节点和计算节点相同操作 #下载yum源并安装openstack客户端 wget https://cbs.centos.org/kojifiles/packages/centos-release-openstack-mitaka/1/1.el7/noarch/centos-release-openstack-mitaka-1-1.el7.noarch.rpm yum -y localinstall centos-release-openstack-mitaka-1-1.el7.noarch.rpm yum -y install python-openstackclient 到此，控制节点和计算节点操作完成！！！ 二、控制节点环境安装 2.1 安装mariadb数据库 1.安装mariadb数据库 yum -y install mariadb mariadb-server python2-PyMySQL 2.创建并编辑 /etc/my.cnf.d/openstack.cnf 在[mysqld]中，设置“bind-address”值为控制节点的管理网络IP地址以使得其他节点可以通过管理网络访问访问数据库。设置其他关键字来设置一些有用的选项和UTF-8编码 cat > /etc/my.cnf.d/openstack.cnf 2.2 安装MongoDB数据库 Telemetry 服务使用 NoSQL 数据库来存储信息，典型地，这个数据库运行在控制节点上。向导中使用MongoDB。 mongodb监听tcp/27017 1.安装MongoDB数据库 yum -y install mongodb-server mongodb 2.编辑文件/etc/mongod.conf 配置 bind_ip 使用控制节点管理网卡的IP地址 修改第6行为 bind_ip = 10.0.0.11 默认情况下，MongoDB会在``/var/lib/mongodb/journal`` 目录下创建几个1GB大小的日志文件。如果你想将每个日志文件大小减小到128MB并且限制日志文件占用的总空间为512MB，配置 smallfiles 的值 取消第113行注释 smallfiles = true #用以下命令修改 sed -i.bak '/^bind_ip/cbind_ip = 10.0.0.11' /etc/mongod.conf \\ && sed -i 's/#smallfiles = true/smallfiles = true/' /etc/mongod.conf 3.启动MongoDB并设置为开机自启 systemctl enable mongod && systemctl start mongod 2.3 安装消息队列rabbitmq OpenStack 使用 message queue 协调操作和各服务的状态信息。消息队列服务一般运行在控制节点上 rabbitmq会启动2个端口 tcp/5672 rabbitmq服务端口 tcp/25672 多个rabbitmq通信用到的端口 1.安装rabbitmq yum -y install rabbitmq-server 2.启动消息队列rabbitmq并设置为开机自启 systemctl enable rabbitmq-server && systemctl start rabbitmq-server 3.添加openstack用户 rabbitmqctl add_user openstack RABBIT_PASS Creating user \"openstack\" 4.给openstack用户设置读和写权限 3个.*分别是 可读、可写、可配置 rabbitmqctl set_permissions openstack \".*\" \".*\" \".*\" Setting permissions for user \"openstack\" in vhost \"/\" 5.启动rabbitmq一个插件，启动之后会监听tcp/15672，是一个web管理界面，默认用户名密码guest rabbitmq-plugins enable rabbitmq_management The following plugins have been enabled: amqp_client cowlib cowboy rabbitmq_web_dispatch rabbitmq_management_agent rabbitmq_management Applying plugin configuration to rabbit@controller... started 6 plugins. 2.4 安装memcached 认证服务认证缓存使用Memcached缓存令牌。缓存服务memecached运行在控制节点。在生产部署中，我们推荐联合启用防火墙、认证和加密保证它的安全。 memcache监听 tcp/udp 11211端口 1.安装软件包 yum -y install memcached python-memcached 2.修改配置文件，设置memcache监听端口为控制节点，默认监听127.0.0.1 sed -i.bak 's#127.0.0.1#10.0.0.11#g' /etc/sysconfig/memcached 3.启动memcached并设置为开机自启 systemctl enable memcached && systemctl start memcached 到此，控制节点环境安装完成！！！ 三、控制节点认证服务keystone安装 keystone认证服务功能：认证管理、授权管理、服务目录 认证：用户名和密码 授权：授权管理，例如一些技术网站(掘金、csdn)可以授权微信、QQ登陆 服务目录：相当于通讯录，即要访问openstack的镜像、网络、存储等服务，只需要找到keystone即可，而不需要再单独记住各个服务的访问地址 后续每安装一个服务都需要在keystone上注册 3.1 创建keystone数据库并授权 #用以下命令操作 mysql -e \"CREATE DATABASE keystone;\" mysql -e \"GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' \\ IDENTIFIED BY 'KEYSTONE_DBPASS';\" mysql -e \"GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' \\ IDENTIFIED BY 'KEYSTONE_DBPASS';\" 3.2 配置keystron 3.2.1 安装和配置keystron keystone借助apache访问 mod_wsgi是帮助apache连接python程序 监听端口 5000(普通用户访问) 35357(管理员用户访问)，apache做了2个多端口的站点 1.安装相关包 yum -y install openstack-keystone httpd mod_wsgi openstack-utils.noarch 2.编辑文件 /etc/keystone/keystone.conf 并完成如下动作： 在 [database] 部分，配置数据库访问： [root@controller ~]# vim /etc/keystone/keystone.conf [database] connection = mysql+pymysql://keystone:KEYSTONE_DBPASS@controller/keystone 在[token]部分，配置Fernet UUID令牌的提供者 [token] provider = fernet #用以下命令修改 \\cp /etc/keystone/keystone.conf{,.bak} grep -Ev '^$|#' /etc/keystone/keystone.conf.bak >/etc/keystone/keystone.conf openstack-config --set /etc/keystone/keystone.conf DEFAULT admin_token ADMIN_TOKEN openstack-config --set /etc/keystone/keystone.conf database connection mysql+pymysql://keystone:KEYSTONE_DBPASS@controller/keystone openstack-config --set /etc/keystone/keystone.conf token provider fernet MD5值 md5sum /etc/keystone/keystone.conf d5acb3db852fe3f247f4f872b051b7a9 /etc/keystone/keystone.conf 3.初始化身份认证服务的数据库（切换到keystone用户，使用的shell是/bin/sh，执行 -c后的命令） su -s /bin/sh -c \"keystone-manage db_sync\" keystone 上一步操作为导入表，以下命令执行返回有表即为正确 mysql keystone -e \"show tables;\"|wc -l 38 4.初始化Fernet key keystone-manage fernet_setup --keystone-user keystone \\ --keystone-group keystone 5.配置Apache服务器 5.1编辑/etc/httpd/conf/httpd.conf`文件，配置``ServerName`` 选项为控制节点 96行下入以下一行 ServerName controller #用以下命令修改 sed -i.bak '96cServerName controller' /etc/httpd/conf/httpd.conf MD5值 md5sum /etc/httpd/conf/httpd.conf eaf0e2ae3fea84bac3e5a842f64bdfdb /etc/httpd/conf/httpd.conf 5.2创建文件/etc/httpd/conf.d/wsgi-keystone.conf cat > /etc/httpd/conf.d/wsgi-keystone.conf WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone group=keystone display-name=%{GROUP} WSGIProcessGroup keystone-public WSGIScriptAlias / /usr/bin/keystone-wsgi-public WSGIApplicationGroup %{GLOBAL} WSGIPassAuthorization On ErrorLogFormat \"%{cu}t %M\" ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined Require all granted WSGIDaemonProcess keystone-admin processes=5 threads=1 user=keystone group=keystone display-name=%{GROUP} WSGIProcessGroup keystone-admin WSGIScriptAlias / /usr/bin/keystone-wsgi-admin WSGIApplicationGroup %{GLOBAL} WSGIPassAuthorization On ErrorLogFormat \"%{cu}t %M\" ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined Require all granted EOF MD5值 md5sum /etc/httpd/conf.d/wsgi-keystone.conf 8f051eb53577f67356ed03e4550315c2 /etc/httpd/conf.d/wsgi-keystone.conf 6.启动apache并设置为开机自启 systemctl enable httpd && systemctl start httpd 3.2.2 创建服务实体和API端点 API端点有3个 public 公共 internal 内部 admin 管理员 1.先决条件 #配置身份验证令牌 export OS_TOKEN=ADMIN_TOKEN #配置端点URL export OS_URL=http://controller:35357/v3 #配置Identity API版本 export OS_IDENTITY_API_VERSION=3 2.创建服务实体和API端点 2.1 Identity服务管理OpenStack环境中的服务目录。服务使用此目录来确定您的环境中可用的其他服务。 为Identity服务创建服务实体 openstack service create --name \\ keystone --description \"OpenStack Identity\" identity +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Identity | | enabled | True | | id | c7c0d1e96d7e4f809c2957099eb8a0d2 | | name | keystone | | type | identity | +-------------+----------------------------------+ 2.2 Identity服务管理与OpenStack环境中的服务关联的API端点的目录。服务使用此目录来确定如何与环境中的其他服务进行通信。 OpenStack为每项服务使用三种API端点变体：admin，internal和public。管理API端点允许默认修改用户和租户，而公共和内部API不允许这些操作。在生产环境中，出于安全原因，变体可能驻留在为不同类型的用户提供服务的单独网络上。例如，公共API网络可能从Internet上可见，因此客户可以管理他们的云。管理API网络可能仅限于管理云基础架构的组织内的运营商。内部API网络可能仅限于包含OpenStack服务的主机。此外，OpenStack支持多个区域以实现可伸缩性。为简单起见，本指南将管理网络用于所有端点变体和默认值 RegionOne地区。 创建Identity Service API端点： #公共普通用户使用5000端口 openstack endpoint create --region RegionOne \\ identity public http://controller:5000/v3 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | f63e9c4450254214947ac75cddd394c1 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | c7c0d1e96d7e4f809c2957099eb8a0d2 | | service_name | keystone | | service_type | identity | | url | http://controller:5000/v3 | +--------------+----------------------------------+ #keystone内部通信使用5000端口 openstack endpoint create --region RegionOne \\ identity internal http://controller:5000/v3 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 9b6a5be720ea46a4a38f403c47ad8b8f | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | c7c0d1e96d7e4f809c2957099eb8a0d2 | | service_name | keystone | | service_type | identity | | url | http://controller:5000/v3 | +--------------+----------------------------------+ #管理员使用35357端口 openstack endpoint create --region RegionOne \\ identity admin http://controller:35357/v3 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 53d4bdc5bea041a0abfb9ea89dff65d6 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | c7c0d1e96d7e4f809c2957099eb8a0d2 | | service_name | keystone | | service_type | identity | | url | http://controller:35357/v3 | +--------------+----------------------------------+ 创建完API端点后使用命令openstack endpoint list验证是否创建成功 openstack endpoint list +----------------------------------+-----------+--------------+--------------+---------+-----------+----------------------------+ | ID | Region | Service Name | Service Type | Enabled | Interface | URL | +----------------------------------+-----------+--------------+--------------+---------+-----------+----------------------------+ | 2f551bb367c045379a8042cdcb7287eb | RegionOne | keystone | identity | True | public | http://controller:5000/v3 | | 77b014a9b8d44d038cb5d608ff6b9d56 | RegionOne | keystone | identity | True | internal | http://controller:5000/v3 | | c2fcc9c1ee0244acb2860124a1575fd0 | RegionOne | keystone | identity | True | admin | http://controller:35357/v3 | +----------------------------------+-----------+--------------+--------------+---------+-----------+----------------------------+ 删除API端点使用openstack delete 3.3 创建域、项目、用户和角色 3.3.1 创建默认域 openstack domain create --description \"Default Domain\" default +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Default Domain | | enabled | True | | id | fad8700e172044e6ac4869c9eed6d2c3 | | name | default | +-------------+----------------------------------+ 3.3.2 为环境中的管理操作创建管理项目，用户和角色 1.创建管理项目 openstack project create --domain default \\ --description \"Admin Project\" admin +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Admin Project | | domain_id | fad8700e172044e6ac4869c9eed6d2c3 | | enabled | True | | id | 9c1b0cb2b3914507b429f3f7b0c6b5e4 | | is_domain | False | | name | admin | | parent_id | fad8700e172044e6ac4869c9eed6d2c3 | +-------------+----------------------------------+ 2.创建管理员用户，密码设置为ADMIN_PASS //这里交互式创建密码和非交互式选择其中一种 #非交互式设置密码 openstack user create --domain default \\ --password ADMIN_PASS admin +-----------+----------------------------------+ | Field | Value | +-----------+----------------------------------+ | domain_id | 984cb3d5f3054e16b029676de97b6ca6 | | enabled | True | | id | 273c94d5f389418b83ee6738376a6bdf | | name | admin | +-----------+----------------------------------+ #交互式设置密码 openstack user create --domain default \\ --password-prompt admin User Password: Repeat User Password: +-----------+----------------------------------+ | Field | Value | +-----------+----------------------------------+ | domain_id | fad8700e172044e6ac4869c9eed6d2c3 | | enabled | True | | id | cc0f0af9f5c1492aa8919bf936c1c19b | | name | admin | +-----------+----------------------------------+ 3.创建管理员角色 openstack role create admin +-----------+----------------------------------+ | Field | Value | +-----------+----------------------------------+ | domain_id | None | | id | 921e36b9338141479f52f7c46c04f9ef | | name | admin | +-----------+----------------------------------+ 4.将admin角色添加到admin项目和用户 openstack role add --project admin --user admin admin 3.3.3 本指南使用的服务项目包含您添加到环境中的每项服务的唯一用户 创建服务项目 #service，后期用于关联openstack系统用户glance、nova、neutron openstack project create --domain default \\ --description \"Service Project\" service +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Service Project | | domain_id | fad8700e172044e6ac4869c9eed6d2c3 | | enabled | True | | id | ae31639e04be474cbabcad502be62cac | | is_domain | False | | name | service | | parent_id | fad8700e172044e6ac4869c9eed6d2c3 | +-------------+----------------------------------+ 3.3.4 常规（非管理员）任务应该使用非特权项目和用户。例如，本指南创建了演示项目和用户 1.创建演示项目 openstack project create --domain default \\ --description \"Demo Project\" demo +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Demo Project | | domain_id | fad8700e172044e6ac4869c9eed6d2c3 | | enabled | True | | id | 6244aa0291104859b255990cef3eacd6 | | is_domain | False | | name | demo | | parent_id | fad8700e172044e6ac4869c9eed6d2c3 | +-------------+----------------------------------+ 2.创建演示用户，密码设置为123456 //这里交互式创建密码和非交互式选择其中一种 #非交互式创建密码 openstack user create --domain default \\ --password 123456 demo +-----------+----------------------------------+ | Field | Value | +-----------+----------------------------------+ | domain_id | af51b1180eb14a66b0380e4cd134df90 | | enabled | True | | id | c75ad14657d0497190cb479ba50f531b | | name | demo | +-----------+----------------------------------+ #交互式创建密码 openstack user create --domain default \\ --password-prompt demo User Password: Repeat User Password: +-----------+----------------------------------+ | Field | Value | +-----------+----------------------------------+ | domain_id | fad8700e172044e6ac4869c9eed6d2c3 | | enabled | True | | id | 4639e42215a946b4be7e588d36979c64 | | name | demo | +-----------+----------------------------------+ 3.创建用户角色 openstack role create user +-----------+----------------------------------+ | Field | Value | +-----------+----------------------------------+ | domain_id | None | | id | a35f989b04d1403e92a19895cae21c9d | | name | user | +-----------+----------------------------------+ 4.将用户角色添加到演示项目和用户 openstack role add --project demo --user demo user 3.4 验证操作 3.4.1 出于安全原因，请禁用临时身份验证令牌机制： 编辑文件/etc/keystone/keystone-paste.ini并且移除admin_token_auth 从[pipeline:public_api], [pipeline:admin_api],和[pipeline:api_v3] 部分 这一步操作可能会造成后续keystone认证失败！！！，实验的时候可以不执行！！！ sed -i.bak '51,64d' /etc/keystone/keystone-paste.ini 3.4.2 取消设置临时OS_TOKEN和OS_URL环境变量 unset OS_TOKEN OS_URL 3.4.3 作为admin用户，请求身份验证令牌，密码为ADMIN_PASS openstack --os-auth-url http://controller:35357/v3 \\ --os-project-domain-name default --os-user-domain-name default \\ --os-project-name admin --os-username admin token issue Password: +------------+----------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +------------+----------------------------------------------------------------------------------------------------------------------------------------------+ | expires | 2019-01-31T05:46:45.000000Z | | id | gAAAAABcUn21ftl6WYlhMsqIffRDo9Pg6Ei35hUlg8D_kzw1Azy- | | | 4Ly1DeL0s3YbMOlz88jVFWnMyg2gaxFoVsS2pZYnRhVlnclg1yofFFHOENz39XHsuCUYICuDq4XqOLEbKWyS9IfZuNbWtKjEQa-jQaoe4PCk0fyFG0B6nE3vn9gNkOvXiTA | | project_id | 9c1b0cb2b3914507b429f3f7b0c6b5e4 | | user_id | cc0f0af9f5c1492aa8919bf936c1c19b | +------------+----------------------------------------------------------------------------------------------------------------------------------------------+ 3.4.4 作为演示用户，请求身份验证令牌 ⚠️执行3.4.1就会有问题，会报错HTTP 500 openstack --os-auth-url http://controller:5000/v3 \\ --os-project-domain-name default --os-user-domain-name default \\ --os-project-name demo --os-username demo token issue Password: Discovering versions from the identity service failed when creating the password plugin. Attempting to determine version from URL. Internal Server Error (HTTP 500) 不执行3.4.1，验证就没有问题 openstack --os-auth-url http://controller:5000/v3 \\ --os-project-domain-name default --os-user-domain-name default \\ --os-project-name demo --os-username demo token issue +------------+-----------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +------------+-----------------------------------------------------------------------------------------------------------------------------------------+ | expires | 2019-01-31T10:30:43.000000Z | | id | gAAAAABeym2jDfXll4iZ2JcCP1XY1mHbu8Ovgaf8BMWe1FsoBp9XkaEqsnphx_BIuY0RFC-goS-JVZJ0xbiOajLnob7nWYKz5zlPlGkybvtDWd6L3jRMGD20RE- | | | H5gRz5oBXPPRUt9e5Kxbc-5_WXu_nfjw3ASXPIu25inoeeXsvd1aeg9FzgBE | | project_id | d08b00aa3c6944afa7095c280319acb9 | | user_id | ec75d657d09c4899894d40364011f552 | +------------+-----------------------------------------------------------------------------------------------------------------------------------------+ 3.5 创建OpenStack客户端环境脚本 3.5.1 创建脚本 编辑admin-openrc文件并添加以下内容,这里放在/opt下 cat > /opt/admin-openrc 3.5.2 使用脚本 1.加载admin-openrc文件，使用Identity服务的位置以及admin项目和用户凭据填充环境变量 source /opt/admin-openrc 2.请求身份验证令牌（注意expires中是UTC时间，落后中国8个小时，我国是东八区，使用timedatectl查看时间及时区，默认过期时间1小时） openstack token issue +------------+----------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +------------+----------------------------------------------------------------------------------------------------------------------------------------------+ | expires | 2019-01-31T10:49:41.000000Z | | id | gAAAAABcUsS1O_B3QETf0hx8KWiuUyTBz23e2E70mY6DeWPvZreQrX58bEyJcMVgLGazsrKrqaJw0gSK75JHT0WNHf7V6VxNR5-uYLJKsGIuaUzNe9RMdTys_CcK680L- | | | NU9VdSDllR6GQvbu4EqejSm_1d5iarR2cQD8n8kG1PcV_SNijApskk | | project_id | e33e3feaef784a5bb45bd9c766bc0f46 | | user_id | aaa8bfce5b5d451b956bb76dee235b9e | +------------+----------------------------------------------------------------------------------------------------------------------------------------------+ 到此，控制节点认证服务keystone安装完成！！！ 四、控制节点镜像服务glance安装 OpenStack镜像服务包括以下组件： glance-api 接收镜像API的调用，诸如镜像发现、恢复、存储 glance-registry 存储、处理和恢复镜像的元数据(属性)，元数据包括项诸如大小和类型 glance服务监听两个端口 glance-api 9292 glance-registry 9191 4.1 创建glance数据库并授权 #用以下命令修改 mysql -e \"CREATE DATABASE glance;\" mysql -e \"GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' \\ IDENTIFIED BY 'GLANCE_DBPASS';\" mysql -e \"GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' \\ IDENTIFIED BY 'GLANCE_DBPASS';\" 4.2 获取管理员凭据以获取对仅管理员CLI命令的访问 source /opt/admin-openrc 4.3 创建服务凭据 4.3.1 创建glance用户，密码设置为GLANCE_PASS //这里交互式创建密码和非交互式选择其中一种 #非交互式设置密码 openstack user create --domain default --password GLANCE_PASS glance +-----------+----------------------------------+ | Field | Value | +-----------+----------------------------------+ | domain_id | af51b1180eb14a66b0380e4cd134df90 | | enabled | True | | id | 593894e4dabc411ebecf8cbe8f3f1109 | | name | glance | +-----------+----------------------------------+ #交互式设置密码 openstack user create --domain default --password-prompt glance User Password: Repeat User Password: +-----------+----------------------------------+ | Field | Value | +-----------+----------------------------------+ | domain_id | b245194b1e8749d0b3c51a78e05d7734 | | enabled | True | | id | ff135a9cce5e4a55842dd2beaffa67e2 | | name | glance | +-----------+----------------------------------+ 4.3.2 将管理角色添加到glance用户和服务项目中 openstack role add --project service --user glance admin 4.3.3 创建glance服务实体 openstack service create --name glance \\ --description \"OpenStack Image\" image +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Image | | enabled | True | | id | de2e6d60f6234918a96f768516a36e9a | | name | glance | | type | image | +-------------+----------------------------------+ 删除服务实体使用命令openstack service delete 使用命令openstack service list查看service-id然后根据id删除 4.3.4 创建Image服务API端点 openstack endpoint create --region RegionOne \\ image public http://controller:9292 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 7aede44313aa4f98971c513fb6aa37b9 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | de2e6d60f6234918a96f768516a36e9a | | service_name | glance | | service_type | image | | url | http://controller:9292 | +--------------+----------------------------------+ openstack endpoint create --region RegionOne \\ image internal http://controller:9292 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 8edf9fd9452347d99d1a419b5f631f2c | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | de2e6d60f6234918a96f768516a36e9a | | service_name | glance | | service_type | image | | url | http://controller:9292 | +--------------+----------------------------------+ openstack endpoint create --region RegionOne \\ image admin http://controller:9292 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 89ced10fcf444d5a95c9ad5fd9381040 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | de2e6d60f6234918a96f768516a36e9a | | service_name | glance | | service_type | image | | url | http://controller:9292 | +--------------+----------------------------------+ 删除API端点使用命令openstack endpoint delete 使用命令openstack endpoint list查看endpoint-id然后根据id删除 4.4 安装和配置组件 4.4.1 安装包 yum -y install openstack-glance 4.4.2 编辑/etc/glance/glance-api.conf文件并完成以下操作 1.在[database]部分中，配置数据库访问 [database] ... connection = mysql+pymysql://glance:GLANCE_DBPASS@controller/glance 2.在[keystone_authtoken]和[paste_deploy]部分中，配置身份服务访问 [keystone_authtoken] ... auth_uri = http://controller:5000 auth_url = http://controller:35357 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = glance password = GLANCE_PASS [paste_deploy] ... flavor = keystone 3.在[glance_store]部分中，配置本地文件系统存储和映像文件的位置 [glance_store] ... stores = file,http default_store = file filesystem_store_datadir = /var/lib/glance/images/ #用以下命令修改 \\cp /etc/glance/glance-api.conf{,.bak} grep '^[a-Z\\[]' /etc/glance/glance-api.conf.bak >/etc/glance/glance-api.conf openstack-config --set /etc/glance/glance-api.conf database connection mysql+pymysql://glance:GLANCE_DBPASS@controller/glance openstack-config --set /etc/glance/glance-api.conf glance_store stores file,http openstack-config --set /etc/glance/glance-api.conf glance_store default_store file openstack-config --set /etc/glance/glance-api.conf glance_store filesystem_store_datadir /var/lib/glance/images/ openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_uri http://controller:5000 openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_url http://controller:35357 openstack-config --set /etc/glance/glance-api.conf keystone_authtoken memcached_servers controller:11211 openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_type password openstack-config --set /etc/glance/glance-api.conf keystone_authtoken project_domain_name default openstack-config --set /etc/glance/glance-api.conf keystone_authtoken user_domain_name default openstack-config --set /etc/glance/glance-api.conf keystone_authtoken project_name service openstack-config --set /etc/glance/glance-api.conf keystone_authtoken username glance openstack-config --set /etc/glance/glance-api.conf keystone_authtoken password GLANCE_PASS openstack-config --set /etc/glance/glance-api.conf paste_deploy flavor keystone MD5值 md5sum /etc/glance/glance-api.conf 3e1a4234c133eda11b413788e001cba3 /etc/glance/glance-api.conf 4.4.3 编辑/etc/glance/glance-registry.conf文件并完成以下操作 1.在[database]部分中，配置数据库访问： [database] ... connection = mysql+pymysql://glance:GLANCE_DBPASS@controller/glance 2.在[keystone_authtoken]和[paste_deploy]部分中，配置身份服务访问： ... auth_uri = http://controller:5000 auth_url = http://controller:35357 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = glance password = GLANCE_PASS [paste_deploy] ... flavor = keystone #用以下命令修改 \\cp /etc/glance/glance-registry.conf{,.bak} grep '^[a-Z\\[]' /etc/glance/glance-registry.conf.bak > /etc/glance/glance-registry.conf openstack-config --set /etc/glance/glance-registry.conf database connection mysql+pymysql://glance:GLANCE_DBPASS@controller/glance openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_uri http://controller:5000 openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_url http://controller:35357 openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken memcached_servers controller:11211 openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_type password openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken project_domain_name default openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken user_domain_name default openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken project_name service openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken username glance openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken password GLANCE_PASS openstack-config --set /etc/glance/glance-registry.conf paste_deploy flavor keystone MD5值 md5sum /etc/glance/glance-registry.conf 46acabd81a65b924256f56fe34d90b8f /etc/glance/glance-registry.conf 4.4.4 同步数据库 注意：忽略此输出中的任何弃用消息 su -s /bin/sh -c \"glance-manage db_sync\" glance Option \"verbose\" from group \"DEFAULT\" is deprecated for removal. Its value may be silently ignored in the future. /usr/lib/python2.7/site-packages/oslo_db/sqlalchemy/enginefacade.py:1056: OsloDBDeprecationWarning: EngineFacade is deprecated; please use oslo_db.sqlalchemy.enginefacade expire_on_commit=expire_on_commit, _conf=conf) /usr/lib/python2.7/site-packages/pymysql/cursors.py:170: Warning: (1831, u'Duplicate index `ix_image_properties_image_id_name`. This is deprecated and will be disallowed in a future release.') result = self._query(query) #有输出即为正确 mysql glance -e \"show tables;\" | wc -l 21 4.4.5 启动glance服务并设置为开机自启（glance-api和glance-registry） systemctl enable openstack-glance-api openstack-glance-registry && \\ systemctl start openstack-glance-api openstack-glance-registry 4.4.6 验证操作 1.获取管理员凭据以获取对仅管理员CLI命令的访问权限 source /opt/admin-openrc 2.下载源镜像 wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img 3.使用QCOW2磁盘格式，裸容器格式和公共可见性将映像上载到映像服务 ，以便所有项目都可以访问它 注意：这一步一定要看执行后输出结果中size大小，如果为0则说明镜像上载有问题 openstack image create \"cirros\" \\ --file cirros-0.3.4-x86_64-disk.img \\ --disk-format qcow2 --container-format bare \\ --public +------------------+------------------------------------------------------+ | Field | Value | +------------------+------------------------------------------------------+ | checksum | ee1eca47dc88f4879d8a229cc70a07c6 | | container_format | bare | | created_at | 2019-01-31T12:26:32Z | | disk_format | qcow2 | | file | /v2/images/ac21b17b-e910-4ca4-b743-914b8fbd0e55/file | | id | ac21b17b-e910-4ca4-b743-914b8fbd0e55 | | min_disk | 0 | | min_ram | 0 | | name | cirros | | owner | e33e3feaef784a5bb45bd9c766bc0f46 | | protected | False | | schema | /v2/schemas/image | | size | 13287936 | //一定要注意这里的大小，为0有错误 | status | active | | tags | | | updated_at | 2019-01-31T12:26:34Z | | virtual_size | None | | visibility | public | +------------------+------------------------------------------------------+ 镜像上传位置 [root@controller images]# pwd /var/lib/glance/images [root@controller images]# ls 6a143876-39c6-4b4a-8056-c3d7fbe0ce75 删除镜像使用命令glance image-delete 镜像id 4.4.7 确认上传图像并验证属性 openstack image list +--------------------------------------+--------+--------+ | ID | Name | Status | +--------------------------------------+--------+--------+ | ac21b17b-e910-4ca4-b743-914b8fbd0e55 | cirros | active | +--------------------------------------+--------+--------+ 到此，控制节点镜像服务glance安装完成！！！ 五、控制节点和计算节点计算服务nova安装 nova相关服务 服务名称 作用 nova-api 接受并响应所有的计算服务请求，管理虚拟机(云主机)生命周期 nova-api-metadata 接受来自虚拟机发送的元数据请求 nova-compute（多个） 真正管理虚拟机 nova-scheduler nova调度器（挑选出最合适的nova-compute来创建虚机） nova-conductor 帮助nova-compute代理修改数据库中虚拟机的状态 nova-network 早期openstack版本管理虚拟机的网络（已弃用，neutron） nova-consoleauth和nova-novncproxy web版的vnc来直接操作云主机 novncproxy web版 vnc客户端 安装和配置控制节点 5.1 创建nova和nova-api数据库并授权 #用以下命令 mysql -e \"CREATE DATABASE nova;\" mysql -e \"CREATE DATABASE nova_api;\" mysql -e \"GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' \\ IDENTIFIED BY 'NOVA_DBPASS';\" mysql -e \"GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' \\ IDENTIFIED BY 'NOVA_DBPASS';\" mysql -e \"GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' \\ IDENTIFIED BY 'NOVA_DBPASS';\" mysql -e \"GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' \\ IDENTIFIED BY 'NOVA_DBPASS';\" 5.2 获取管理员凭据以获取对仅管理员CLI命令的访问权限 source /opt/admin-openrc 5.3 创建服务凭据 1.创建nova用户,密码设置为NOVA_PASS //这里交互式创建密码和非交互式选择其中一种 #非交互式创建密码 openstack user create --domain default \\ --password NOVA_PASS nova +-----------+----------------------------------+ | Field | Value | +-----------+----------------------------------+ | domain_id | af51b1180eb14a66b0380e4cd134df90 | | enabled | True | | id | 1ad918dc1de84c279999e89bb7c312bc | | name | nova | +-----------+----------------------------------+ #交互式创建密码 openstack user create --domain default \\ --password-prompt nova User Password: Repeat User Password: +-----------+----------------------------------+ | Field | Value | +-----------+----------------------------------+ | domain_id | b245194b1e8749d0b3c51a78e05d7734 | | enabled | True | | id | b7cd769660c64b96bed91baebb229d54 | | name | nova | +-----------+----------------------------------+ 2.将admin角色添加到nova用户 openstack role add --project service --user nova admin 3.创建nova服务实体 openstack service create --name nova \\ --description \"OpenStack Compute\" compute +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Compute | | enabled | True | | id | 3db049ab4b334d6d979a9ee9a6aea5d5 | | name | nova | | type | compute | +-------------+----------------------------------+ 5.4 创建Compute服务API端点 openstack endpoint create --region RegionOne \\ compute public http://controller:8774/v2.1/%\\(tenant_id\\)s +--------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | 0e1da405775b4a238f4142d8df6b8b58 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 3db049ab4b334d6d979a9ee9a6aea5d5 | | service_name | nova | | service_type | compute | | url | http://controller:8774/v2.1/%(tenant_id)s | +--------------+-------------------------------------------+ openstack endpoint create --region RegionOne \\ compute internal http://controller:8774/v2.1/%\\(tenant_id\\)s +--------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | 36775f0fcbf24ce1888ff714442aea04 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 3db049ab4b334d6d979a9ee9a6aea5d5 | | service_name | nova | | service_type | compute | | url | http://controller:8774/v2.1/%(tenant_id)s | +--------------+-------------------------------------------+ openstack endpoint create --region RegionOne \\ compute admin http://controller:8774/v2.1/%\\(tenant_id\\)s +--------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | 8d575d9584df4c0cb3d903c50688175f | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 3db049ab4b334d6d979a9ee9a6aea5d5 | | service_name | nova | | service_type | compute | | url | http://controller:8774/v2.1/%(tenant_id)s | +--------------+-------------------------------------------+ 5.5 安装和配置组件 5.5.1 安装包 yum -y install openstack-nova-api openstack-nova-conductor \\ openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler 5.5.2 编辑/etc/nova/nova.conf文件并完成以下操作 1.在[DEFAULT]部分中，仅启用计算和元数据API [DEFAULT] ... enabled_apis = osapi_compute,metadata 2.在[api_database]和[database]部分中，配置数据库访问 [api_database] ... connection = mysql+pymysql://nova:NOVA_DBPASS@controller/nova_api [database] ... connection = mysql+pymysql://nova:NOVA_DBPASS@controller/nova 3.在[DEFAULT]和[oslo_messaging_rabbit]部分中，配置RabbitMQ消息队列访问 [DEFAULT] ... rpc_backend = rabbit [oslo_messaging_rabbit] ... rabbit_host = controller rabbit_userid = openstack rabbit_password = RABBIT_PASS 4.在[DEFAULT]和[keystone_authtoken]部分中，配置身份服务访问 [DEFAULT] ... auth_strategy = keystone [keystone_authtoken] ... auth_uri = http://controller:5000 auth_url = http://controller:35357 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = nova password = NOVA_PASS 5.在[DEFAULT]部分中，配置my_ip选项以使用控制器节点的管理接口IP地址 [DEFAULT] ... my_ip = 10.0.0.11 6.在[DEFAULT]部分中，启用对Networking服务的支持 [DEFAULT] ... use_neutron = True firewall_driver = nova.virt.firewall.NoopFirewallDriver 7.在[vnc]部分中，配置VNC代理以使用控制器节点的管理接口IP地址 [vnc] ... vncserver_listen = $my_ip vncserver_proxyclient_address = $my_ip 8.在[glance]部分中，配置Image服务API的位置 [glance] ... api_servers = http://controller:9292 9.在[oslo_concurrency]部分中，配置锁定路径 [oslo_concurrency] ... lock_path = /var/lib/nova/tmp #用以下命令修改，分开复制，一次性复制无法都执行 cp /etc/nova/nova.conf{,.bak} grep '^[a-Z\\[]' /etc/nova/nova.conf.bak >/etc/nova/nova.conf openstack-config --set /etc/nova/nova.conf DEFAULT enabled_apis osapi_compute,metadata openstack-config --set /etc/nova/nova.conf DEFAULT rpc_backend rabbit openstack-config --set /etc/nova/nova.conf DEFAULT auth_strategy keystone openstack-config --set /etc/nova/nova.conf DEFAULT my_ip 10.0.0.11 openstack-config --set /etc/nova/nova.conf DEFAULT use_neutron True openstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriver openstack-config --set /etc/nova/nova.conf api_database connection mysql+pymysql://nova:NOVA_DBPASS@controller/nova_api openstack-config --set /etc/nova/nova.conf database connection mysql+pymysql://nova:NOVA_DBPASS@controller/nova openstack-config --set /etc/nova/nova.conf glance api_servers http://controller:9292 openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_uri http://controller:5000 openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_url http://controller:35357 openstack-config --set /etc/nova/nova.conf keystone_authtoken memcached_servers controller:11211 openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_type password openstack-config --set /etc/nova/nova.conf keystone_authtoken project_domain_name default openstack-config --set /etc/nova/nova.conf keystone_authtoken user_domain_name default openstack-config --set /etc/nova/nova.conf keystone_authtoken project_name service openstack-config --set /etc/nova/nova.conf keystone_authtoken username nova openstack-config --set /etc/nova/nova.conf keystone_authtoken password NOVA_PASS openstack-config --set /etc/nova/nova.conf oslo_concurrency lock_path /var/lib/nova/tmp openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_host controller openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_userid openstack openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_password RABBIT_PASS openstack-config --set /etc/nova/nova.conf vnc vncserver_listen '$my_ip' openstack-config --set /etc/nova/nova.conf vnc vncserver_proxyclient_address '$my_ip' MD5值 md5sum /etc/nova/nova.conf 47ded61fdd1a79ab91bdb37ce59ef192 /etc/nova/nova.conf 5.5.3 同步数据库，忽略输出 su -s /bin/sh -c \"nova-manage api_db sync\" nova su -s /bin/sh -c \"nova-manage db sync\" nova /usr/lib/python2.7/site-packages/pymysql/cursors.py:170: Warning: (1831, u'Duplicate index `block_device_mapping_instance_uuid_virtual_name_device_name_idx`. This is deprecated and will be disallowed in a future release.') result = self._query(query) /usr/lib/python2.7/site-packages/pymysql/cursors.py:170: Warning: (1831, u'Duplicate index `uniq_instances0uuid`. This is deprecated and will be disallowed in a future release.') result = self._query(query) #查看数据库，有输出即为正确 mysql nova_api -e \"show tables;\"|wc -l 10 mysql nova -e \"show tables;\"|wc -l 110 5.5.4 启动Compute服务并将其配置为在系统引导时启动 systemctl enable openstack-nova-api.service \\ openstack-nova-consoleauth.service openstack-nova-scheduler.service \\ openstack-nova-conductor.service openstack-nova-novncproxy.service systemctl start openstack-nova-api.service \\ openstack-nova-consoleauth.service openstack-nova-scheduler.service \\ openstack-nova-conductor.service openstack-nova-novncproxy.service 安装完成后会有no VNC 10.0.0.11:6080 安装和配置计算节点 5.6 安装和配置组件 5.6.1 安装包 yum -y install openstack-nova-compute openstack-utils 5.6.2 编辑/etc/nova/nova.conf文件并完成以下操作 1.在[DEFAULT]和[oslo_messaging_rabbit]部分中，配置RabbitMQ消息队列访问 [DEFAULT] ... rpc_backend = rabbit [oslo_messaging_rabbit] ... rabbit_host = controller rabbit_userid = openstack rabbit_password = RABBIT_PASS 2.在[DEFAULT]和[keystone_authtoken]部分中，配置身份服务访问 [DEFAULT] ... auth_strategy = keystone [keystone_authtoken] ... auth_uri = http://controller:5000 auth_url = http://controller:35357 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = nova password = NOVA_PASS 3.在[DEFAULT]部分中，配置my_ip选项，将MANAGEMENT_INTERFACE_IP_ADDRESS替换为计算节点上管理网络接口的IP地址，对于示例体系结构中的第一个节点，通常为10.0.0.31 [DEFAULT] ... my_ip = MANAGEMENT_INTERFACE_IP_ADDRESS 4.在[DEFAULT]部分中，启用对Networking服务的支持 [DEFAULT] ... use_neutron = True firewall_driver = nova.virt.firewall.NoopFirewallDriver 5.在[vnc]部分中，启用并配置远程控制台访问 [vnc] ... enabled = True vncserver_listen = 0.0.0.0 vncserver_proxyclient_address = $my_ip novncproxy_base_url = http://controller:6080/vnc_auto.html 6.在[glance]部分中，配置Image服务API的位置 [glance] ... api_servers = http://controller:9292 7.在[oslo_concurrency]部分中，配置锁定路径 [oslo_concurrency] ... lock_path = /var/lib/nova/tmp #用以下命令修改 cp /etc/nova/nova.conf{,.bak} grep '^[a-Z\\[]' /etc/nova/nova.conf.bak >/etc/nova/nova.conf openstack-config --set /etc/nova/nova.conf DEFAULT rpc_backend rabbit openstack-config --set /etc/nova/nova.conf DEFAULT auth_strategy keystone openstack-config --set /etc/nova/nova.conf DEFAULT my_ip 10.0.0.31 openstack-config --set /etc/nova/nova.conf DEFAULT use_neutron True openstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriver openstack-config --set /etc/nova/nova.conf glance api_servers http://controller:9292 openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_uri http://controller:5000 openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_url http://controller:35357 openstack-config --set /etc/nova/nova.conf keystone_authtoken memcached_servers controller:11211 openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_type password openstack-config --set /etc/nova/nova.conf keystone_authtoken project_domain_name default openstack-config --set /etc/nova/nova.conf keystone_authtoken user_domain_name default openstack-config --set /etc/nova/nova.conf keystone_authtoken project_name service openstack-config --set /etc/nova/nova.conf keystone_authtoken username nova openstack-config --set /etc/nova/nova.conf keystone_authtoken password NOVA_PASS openstack-config --set /etc/nova/nova.conf oslo_concurrency lock_path /var/lib/nova/tmp openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_host controller openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_userid openstack openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_password RABBIT_PASS openstack-config --set /etc/nova/nova.conf vnc enabled True openstack-config --set /etc/nova/nova.conf vnc vncserver_listen 0.0.0.0 openstack-config --set /etc/nova/nova.conf vnc vncserver_proxyclient_address '$my_ip' openstack-config --set /etc/nova/nova.conf vnc novncproxy_base_url http://controller:6080/vnc_auto.html MD5值 md5sum /etc/nova/nova.conf 2f53f4e0848bc5927493925a4ea61f63 /etc/nova/nova.conf 5.6.3 确定您的计算节点是否支持虚拟机的硬件加速 egrep -c '(vmx|svm)' /proc/cpuinfo 1 #说明 如果此命令返回值1或更大，则计算节点支持硬件加速，通常不需要其他配置。 如果此命令返回零值，则计算节点不支持硬件加速，您必须将libvirt配置为使用QEMU而不是KVM。 编辑/etc/nova/nova.conf文件中的[libvirt]部分， 如下所示： [libvirt] ...... virt_type = qemu 5.6.4 启动Compute服务及其依赖项，并将它们配置为在系统引导时自动启动 systemctl enable libvirtd.service openstack-nova-compute.service && \\ systemctl start libvirtd.service openstack-nova-compute.service 验证操作，在控制节点执行 5.7 验证Compute服务的运行 5.7.1 获取管理员凭据以获取对仅管理员CLI命令的访问权限 source /opt/admin-openrc 5.7.2 列出服务组件以验证每个进程的成功启动和注册 此输出应指示控制器节点上启用的三个服务组件以及计算节点上启用的一个服务组件 openstack compute service list +----+------------------+------------+----------+---------+-------+----------------------------+ | Id | Binary | Host | Zone | Status | State | Updated At | +----+------------------+------------+----------+---------+-------+----------------------------+ | 1 | nova-conductor | controller | internal | enabled | up | 2019-02-01T14:33:13.000000 | | 2 | nova-consoleauth | controller | internal | enabled | up | 2019-02-01T14:33:18.000000 | | 3 | nova-scheduler | controller | internal | enabled | up | 2019-02-01T14:33:13.000000 | | 6 | nova-compute | compute1 | nova | enabled | up | 2019-02-01T14:33:19.000000 | +----+------------------+------------+----------+---------+-------+----------------------------+ 到此，控制节点和计算节点计算服务nova安装完成！！！ 六、控制节点、计算节点网络服务neutron安装 neutron相关服务 服务名 说明 neutron-server 端口(9696) api 接受和响应外部的网络管理请求 neutron-linuxbridge-agent 负责创建桥接网卡 neutron-dhcp-agent 负责分配IP neutron-metadata-agent 配合nova-metadata-api实现虚拟机的定制化操作 L3-agent 实现三层网络(网络层) 安装和配置控制节点 6.1 创建neutron数据库并授权 #用以下命令 mysql -e \"CREATE DATABASE neutron;\" mysql -e \"GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' \\ IDENTIFIED BY 'NEUTRON_DBPASS';\" mysql -e \"GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' \\ IDENTIFIED BY 'NEUTRON_DBPASS';\" 6.2 获取管理员凭据以获取对仅管理员CLI命令的访问权限 source /opt/admin-openrc 6.3 创建服务凭证 6.3.1 创建neutron用户，密码设置为NEUTRON_PASS //这里交互式创建密码和非交互式选择其中一种 #非交互式创建密码 openstack user create --domain default --password NEUTRON_PASS neutron +-----------+----------------------------------+ | Field | Value | +-----------+----------------------------------+ | domain_id | af51b1180eb14a66b0380e4cd134df90 | | enabled | True | | id | c0ac0eda2eca4a698eade50b060dd2ce | | name | neutron | +-----------+----------------------------------+ #交互式创建密码 openstack user create --domain default --password-prompt neutron User Password: Repeat User Password: +-----------+----------------------------------+ | Field | Value | +-----------+----------------------------------+ | domain_id | b245194b1e8749d0b3c51a78e05d7734 | | enabled | True | | id | 98f34dc4ddf346b3833de4a0320f7bc9 | | name | neutron | +-----------+----------------------------------+ 6.3.2 将admin角色添加到neutron用户 openstack role add --project service --user neutron admin 6.3.3 创建neutron服务实体 openstack service create --name neutron \\ --description \"OpenStack Networking\" network +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Networking | | enabled | True | | id | 86251b67e0c94b699489ee1b331c33a6 | | name | neutron | | type | network | +-------------+----------------------------------+ 6.4 创建网络服务API端点 openstack endpoint create --region RegionOne \\ network public http://controller:9696 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 48b0ab77e4e74a0788f88b0916e8b696 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 86251b67e0c94b699489ee1b331c33a6 | | service_name | neutron | | service_type | network | | url | http://controller:9696 | +--------------+----------------------------------+ openstack endpoint create --region RegionOne \\ network internal http://controller:9696 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 5b2be24de46f455694a77e8a096916fb | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 86251b67e0c94b699489ee1b331c33a6 | | service_name | neutron | | service_type | network | | url | http://controller:9696 | +--------------+----------------------------------+ openstack endpoint create --region RegionOne \\ network admin http://controller:9696 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 0f06a3d409bc41cb9b6651de68006102 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 86251b67e0c94b699489ee1b331c33a6 | | service_name | neutron | | service_type | network | | url | http://controller:9696 | +--------------+----------------------------------+ 6.5 配置网络选项，控制节点操作 6.5.1 安装包 yum -y install openstack-neutron openstack-neutron-ml2 \\ openstack-neutron-linuxbridge ebtables 6.5.2 配置服务器组件 编辑/etc/neutron/neutron.conf文件并完成以下操作 1.在[database]部分中，配置数据库访问： [database] ... connection = mysql+pymysql://neutron:NEUTRON_DBPASS@controller/neutron 2.在[DEFAULT]部分中，启用Modular Layer 2（ML2）插件并禁用其他插件： [DEFAULT] ... core_plugin = ml2 service_plugins = 3.在[DEFAULT]和[oslo_messaging_rabbit]部分中，配置RabbitMQ消息队列访问： [DEFAULT] ... rpc_backend = rabbit [oslo_messaging_rabbit] ... rabbit_host = controller rabbit_userid = openstack rabbit_password = RABBIT_PASS 4.在[DEFAULT]和[keystone_authtoken]部分中，配置身份服务访问： [DEFAULT] ... auth_strategy = keystone [keystone_authtoken] ... auth_uri = http://controller:5000 auth_url = http://controller:35357 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = NEUTRON_PASS 5.在[DEFAULT]和[nova]部分中，配置Networking以通知Compute网络拓扑更改： [DEFAULT] ... notify_nova_on_port_status_changes = True notify_nova_on_port_data_changes = True [nova] ... auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default region_name = RegionOne project_name = service username = nova password = NOVA_PASS 6.在[oslo_concurrency]部分中，配置锁定路径： [oslo_concurrency] ... lock_path = /var/lib/neutron/tmp #用以下命令修改 \\cp /etc/neutron/neutron.conf{,.bak} grep '^[a-Z\\[]' /etc/neutron/neutron.conf.bak >/etc/neutron/neutron.conf openstack-config --set /etc/neutron/neutron.conf DEFAULT core_plugin ml2 openstack-config --set /etc/neutron/neutron.conf DEFAULT service_plugins openstack-config --set /etc/neutron/neutron.conf DEFAULT rpc_backend rabbit openstack-config --set /etc/neutron/neutron.conf DEFAULT auth_strategy keystone openstack-config --set /etc/neutron/neutron.conf DEFAULT notify_nova_on_port_status_changes True openstack-config --set /etc/neutron/neutron.conf DEFAULT notify_nova_on_port_data_changes True openstack-config --set /etc/neutron/neutron.conf database connection mysql+pymysql://neutron:NEUTRON_DBPASS@controller/neutron openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_uri http://controller:5000 openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_url http://controller:35357 openstack-config --set /etc/neutron/neutron.conf keystone_authtoken memcached_servers controller:11211 openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_type password openstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_domain_name default openstack-config --set /etc/neutron/neutron.conf keystone_authtoken user_domain_name default openstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_name service openstack-config --set /etc/neutron/neutron.conf keystone_authtoken username neutron openstack-config --set /etc/neutron/neutron.conf keystone_authtoken password NEUTRON_PASS openstack-config --set /etc/neutron/neutron.conf nova auth_url http://controller:35357 openstack-config --set /etc/neutron/neutron.conf nova auth_type password openstack-config --set /etc/neutron/neutron.conf nova project_domain_name default openstack-config --set /etc/neutron/neutron.conf nova user_domain_name default openstack-config --set /etc/neutron/neutron.conf nova region_name RegionOne openstack-config --set /etc/neutron/neutron.conf nova project_name service openstack-config --set /etc/neutron/neutron.conf nova username nova openstack-config --set /etc/neutron/neutron.conf nova password NOVA_PASS openstack-config --set /etc/neutron/neutron.conf oslo_concurrency lock_path /var/lib/neutron/tmp openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_host controller openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_userid openstack openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_password RABBIT_PASS MD5值 md5sum /etc/neutron/neutron.conf e399b7958cd22f47becc6d8fd6d3521a /etc/neutron/neutron.conf 6.6 配置模块化第2层（ML2）插件 ML2插件使用Linux桥接机制为实例构建第2层（桥接和交换）虚拟网络基础架构 编辑/etc/neutron/plugins/ml2/ml2_conf.ini文件并完成以下操作 1.在[ml2]部分中，启用flat和VLAN网络： [ml2] ... type_drivers = flat,vlan 2.在[ml2]部分中，禁用自助服务网络 [ml2] ... tenant_network_types = 3.在[ml2]部分中，启用Linux桥接机制 [ml2] ... mechanism_drivers = linuxbridge 4.在[ml2]部分中，启用端口安全性扩展驱动程序 [ml2] ... extension_drivers = port_security 5.在[ml2_type_flat]部分中，将提供商虚拟网络配置为扁平网络： [ml2_type_flat] ... flat_networks = provider 6.在[securitygroup]部分中，启用ipset以提高安全组规则的效率 [securitygroup] ... enable_ipset = True #用以下命令修改 \\cp /etc/neutron/plugins/ml2/ml2_conf.ini{,.bak} grep '^[a-Z\\[]' /etc/neutron/plugins/ml2/ml2_conf.ini.bak >/etc/neutron/plugins/ml2/ml2_conf.ini openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 type_drivers flat,vlan openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 tenant_network_types openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 mechanism_drivers linuxbridge openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 extension_drivers port_security openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_flat flat_networks provider openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup enable_ipset True MD5值 md5sum /etc/neutron/plugins/ml2/ml2_conf.ini 2640b5de519fafcd675b30e1bcd3c7d5 /etc/neutron/plugins/ml2/ml2_conf.ini 6.7 配置linux桥接代理 Linux网桥代理为实例构建第2层（桥接和交换）虚拟网络基础架构并处理安全组 编辑/etc/neutron/plugins/ml2/linuxbridge_agent.ini文件并完成以下操作 1.在[linux_bridge]部分中，将提供者虚拟网络映射到提供者物理网络接口，将PROVIDER_INTERFACE_NAME替换 为基础提供程序物理网络接口的名称，这里为eth0 [linux_bridge] physical_interface_mappings = provider:PROVIDER_INTERFACE_NAME 2.在[vxlan]部分中，禁用VXLAN重叠网络 [vxlan] enable_vxlan = False 3.在[securitygroup]部分中，启用安全组并配置Linux网桥iptables防火墙驱动程序 [securitygroup] ... enable_security_group = True firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver #用以下命令修改 \\cp /etc/neutron/plugins/ml2/linuxbridge_agent.ini{,.bak} grep '^[a-Z\\[]' /etc/neutron/plugins/ml2/linuxbridge_agent.ini.bak >/etc/neutron/plugins/ml2/linuxbridge_agent.ini openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini linux_bridge physical_interface_mappings provider:eth0 openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini securitygroup enable_security_group True openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini securitygroup firewall_driver neutron.agent.linux.iptables_firewall.IptablesFirewallDriver openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini vxlan enable_vxlan False MD5值 md5sum /etc/neutron/plugins/ml2/linuxbridge_agent.ini 3f474907a7f438b34563e4d3f3c29538 /etc/neutron/plugins/ml2/linuxbridge_agent.ini 6.8 配置DHCP代理 该DHCP代理为虚拟网络提供DHCP服务 编辑/etc/neutron/dhcp_agent.ini文件并完成以下操作 1.在[DEFAULT]部分中，配置Linux桥接接口驱动程序，Dnsmasq DHCP驱动程序，并启用隔离的元数据，以便提供商网络上的实例可以通过网络访问元数据 [DEFAULT] ... interface_driver = neutron.agent.linux.interface.BridgeInterfaceDriver dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq enable_isolated_metadata = True #用以下命令修改 \\cp /etc/neutron/dhcp_agent.ini{,.bak} grep '^[a-Z\\[]' /etc/neutron/dhcp_agent.ini.bak >/etc/neutron/dhcp_agent.ini openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT interface_driver neutron.agent.linux.interface.BridgeInterfaceDriver openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT dhcp_driver neutron.agent.linux.dhcp.Dnsmasq openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT enable_isolated_metadata True MD5值 md5sum /etc/neutron/dhcp_agent.ini d39579607b2f7d92e88f8910f9213520 /etc/neutron/dhcp_agent.ini 6.9 配置元数据代理 所述元数据代理提供配置信息的诸如凭据实例。 编辑/etc/neutron/metadata_agent.ini文件并完成以下操作 在[DEFAULT]部分中，配置元数据主机和共享密钥 [DEFAULT] ... nova_metadata_ip = controller metadata_proxy_shared_secret = METADATA_SECRET #用以下领命修改 \\cp /etc/neutron/metadata_agent.ini{,.bak} grep '^[a-Z\\[]' /etc/neutron/metadata_agent.ini.bak >/etc/neutron/metadata_agent.ini openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT nova_metadata_ip controller openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT metadata_proxy_shared_secret METADATA_SECRET MD5值 md5sum /etc/neutron/metadata_agent.ini e1166b0dfcbcf4507d50860d124335d6 /etc/neutron/metadata_agent.ini 6.10 配置计算以使用网络 编辑/etc/nova/nova.conf文件并执行以下操作： 在[neutron]部分中，配置访问参数，启用元数据代理并配置密码 [neutron] ... url = http://controller:9696 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default region_name = RegionOne project_name = service username = neutron password = NEUTRON_PASS service_metadata_proxy = True metadata_proxy_shared_secret = METADATA_SECRET #用以下命令修改 openstack-config --set /etc/nova/nova.conf neutron url http://controller:9696 openstack-config --set /etc/nova/nova.conf neutron auth_url http://controller:35357 openstack-config --set /etc/nova/nova.conf neutron auth_type password openstack-config --set /etc/nova/nova.conf neutron project_domain_name default openstack-config --set /etc/nova/nova.conf neutron user_domain_name default openstack-config --set /etc/nova/nova.conf neutron region_name RegionOne openstack-config --set /etc/nova/nova.conf neutron project_name service openstack-config --set /etc/nova/nova.conf neutron username neutron openstack-config --set /etc/nova/nova.conf neutron password NEUTRON_PASS openstack-config --set /etc/nova/nova.conf neutron service_metadata_proxy True openstack-config --set /etc/nova/nova.conf neutron metadata_proxy_shared_secret METADATA_SECRET MD5值 md5sum /etc/nova/nova.conf 6334f359655efdbcf083b812ab94efc1 /etc/nova/nova.conf 6.11 完成安装 6.11.1 联网服务初始化脚本期待一个符号链接 /etc/neutron/plugin.ini指向ML2插件配置文件，/etc/neutron/plugins/ml2/ml2_conf.ini。如果此符号链接不存在，请使用以下命令创建它 ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini 6.11.2 同步数据库，最后提示OK即为正确 su -s /bin/sh -c \"neutron-db-manage --config-file /etc/neutron/neutron.conf \\ --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head\" neutron 6.11.3 重新启动Compute API服务 systemctl restart openstack-nova-api.service 6.11.4 启动网络服务并将其配置为在系统引导时启动 对于官网中的两种网络，这里选择的是第一种网络 #启动网络服务并设置开机自启 systemctl enable neutron-server.service \\ neutron-linuxbridge-agent.service neutron-dhcp-agent.service \\ neutron-metadata-agent.service systemctl start neutron-server.service \\ neutron-linuxbridge-agent.service neutron-dhcp-agent.service \\ neutron-metadata-agent.service #启动服务后提示如下即为正确，alive处都为笑脸 //注意，此处启动比较慢，需要等待几分钟 neutron agent-list +--------------------------------------+--------------------+------------+-------------------+-------+----------------+---------------------------+ | id | agent_type | host | availability_zone | alive | admin_state_up | binary | +--------------------------------------+--------------------+------------+-------------------+-------+----------------+---------------------------+ | 2a79bce3-2492-4d5b-b565-6f18aa7c8bcd | DHCP agent | controller | nova | :-) | True | neutron-dhcp-agent | | 2b3799ae-3162-47f7-82fa-6291d76e0e14 | Metadata agent | controller | | :-) | True | neutron-metadata-agent | | 82ff3326-f570-4f50-bb5c-eec057034fd1 | Linux bridge agent | controller | | :-) | True | neutron-linuxbridge-agent | +--------------------------------------+--------------------+------------+-------------------+-------+----------------+---------------------------+ 安装和配置计算节点 6.12 安装包 yum -y install openstack-neutron-linuxbridge ebtables ipset openstack-utils 6.13 配置公共组件 编辑/etc/neutron/neutron.conf文件并完成以下操作 1.在[database]部分中，注释掉任何连接选项，因为计算节点不直接访问数据库。 在[DEFAULT]和[oslo_messaging_rabbit]部分中，配置RabbitMQ消息队列访问 [DEFAULT] ... rpc_backend = rabbit [oslo_messaging_rabbit] ... rabbit_host = controller rabbit_userid = openstack rabbit_password = RABBIT_PASS 2.在[DEFAULT]和[keystone_authtoken]部分中，配置身份服务访问 [DEFAULT] ... auth_strategy = keystone [keystone_authtoken] ... auth_uri = http://controller:5000 auth_url = http://controller:35357 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = NEUTRON_PASS 3.在[oslo_concurrency]部分中，配置锁定路径 [oslo_concurrency] ... lock_path = /var/lib/neutron/tmp #用以下命令修改 \\cp /etc/neutron/neutron.conf{,.bak} grep '^[a-Z\\[]' /etc/neutron/neutron.conf.bak >/etc/neutron/neutron.conf openstack-config --set /etc/neutron/neutron.conf DEFAULT rpc_backend rabbit openstack-config --set /etc/neutron/neutron.conf DEFAULT auth_strategy keystone openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_uri http://controller:5000 openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_url http://controller:35357 openstack-config --set /etc/neutron/neutron.conf keystone_authtoken memcached_servers controller:11211 openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_type password openstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_domain_name default openstack-config --set /etc/neutron/neutron.conf keystone_authtoken user_domain_name default openstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_name service openstack-config --set /etc/neutron/neutron.conf keystone_authtoken username neutron openstack-config --set /etc/neutron/neutron.conf keystone_authtoken password NEUTRON_PASS openstack-config --set /etc/neutron/neutron.conf oslo_concurrency lock_path /var/lib/neutron/tmp openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_host controller openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_userid openstack openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_password RABBIT_PASS MD5值 md5sum /etc/neutron/neutron.conf 77ffab503797be5063c06e8b956d6ed0 /etc/neutron/neutron.conf 6.14 配置网络选项 6.14.1 配置桥接代理 Linux网桥代理为实例构建第2层（桥接和交换）虚拟网络基础架构并处理安全组。 编辑/etc/neutron/plugins/ml2/linuxbridge_agent.ini文件并完成以下操作： 1.在[linux_bridge]部分中，将提供者虚拟网络映射到提供者物理网络接口，这里为eth0 [linux_bridge] physical_interface_mappings = provider:PROVIDER_INTERFACE_NAME 2.在[vxlan]部分中，禁用VXLAN重叠网络 [vxlan] enable_vxlan = False 3.在[securitygroup]部分中，启用安全组并配置Linux网桥iptables防火墙驱动程序 [securitygroup] ... enable_security_group = True firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver #用以下命令修改 \\cp /etc/neutron/plugins/ml2/linuxbridge_agent.ini{,.bak} grep '^[a-Z\\[]' /etc/neutron/plugins/ml2/linuxbridge_agent.ini.bak >/etc/neutron/plugins/ml2/linuxbridge_agent.ini openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini linux_bridge physical_interface_mappings provider:eth0 openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini vxlan enable_vxlan False openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini securitygroup enable_security_group True openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini securitygroup firewall_driver neutron.agent.linux.iptables_firewall.IptablesFirewallDriver MD5值 md5sum /etc/neutron/plugins/ml2/linuxbridge_agent.ini 3f474907a7f438b34563e4d3f3c29538 /etc/neutron/plugins/ml2/linuxbridge_agent.ini 6.14.2 配置计算以使用网络 编辑/etc/nova/nova.conf文件并完成以下操作 在[neutron]部分中，配置访问参数 [neutron] ... url = http://controller:9696 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default region_name = RegionOne project_name = service username = neutron password = NEUTRON_PASS #使用以下命令修改 openstack-config --set /etc/nova/nova.conf neutron url http://controller:9696 && \\ openstack-config --set /etc/nova/nova.conf neutron auth_url http://controller:35357 && \\ openstack-config --set /etc/nova/nova.conf neutron auth_type password && \\ openstack-config --set /etc/nova/nova.conf neutron project_domain_name default && \\ openstack-config --set /etc/nova/nova.conf neutron user_domain_name default && \\ openstack-config --set /etc/nova/nova.conf neutron region_name RegionOne && \\ openstack-config --set /etc/nova/nova.conf neutron project_name service && \\ openstack-config --set /etc/nova/nova.conf neutron username neutron && \\ openstack-config --set /etc/nova/nova.conf neutron password NEUTRON_PASS MD5值 md5sum /etc/nova/nova.conf 8e6590c8dc3d59beb3da37fdeeadfd1d /etc/nova/nova.conf 6.15 完成安装 6.15.1 重新启动Compute服务 systemctl restart openstack-nova-compute.service 6.15.2 启动Linux网桥代理并将其配置为在系统引导时启动 systemctl enable neutron-linuxbridge-agent.service && \\ systemctl start neutron-linuxbridge-agent.service 6.16 验证操作，在控制节点操作 6.16.1 获取管理员凭据以获取对仅管理员CLI命令的访问权限 source /opt/admin-openrc 6.16.2 列出已加载的扩展以验证neutron-server进程的成功启动 neutron ext-list +---------------------------+-----------------------------------------------+ | alias | name | +---------------------------+-----------------------------------------------+ | default-subnetpools | Default Subnetpools | | availability_zone | Availability Zone | | network_availability_zone | Network Availability Zone | | auto-allocated-topology | Auto Allocated Topology Services | | binding | Port Binding | | agent | agent | | subnet_allocation | Subnet Allocation | | dhcp_agent_scheduler | DHCP Agent Scheduler | | tag | Tag support | | external-net | Neutron external network | | net-mtu | Network MTU | | network-ip-availability | Network IP Availability | | quotas | Quota management support | | provider | Provider Network | | multi-provider | Multi Provider Network | | address-scope | Address scope | | timestamp_core | Time Stamp Fields addition for core resources | | extra_dhcp_opt | Neutron Extra DHCP opts | | security-group | security-group | | rbac-policies | RBAC Policies | | standard-attr-description | standard-attr-description | | port-security | Port Security | | allowed-address-pairs | Allowed Address Pairs | +---------------------------+-----------------------------------------------+ 6.16.3 列出代理以验证成功启动neutron代理，alive处都为笑脸即为正确 neutron agent-list +--------------------------------------+--------------------+------------+-------------------+-------+----------------+---------------------------+ | id | agent_type | host | availability_zone | alive | admin_state_up | binary | +--------------------------------------+--------------------+------------+-------------------+-------+----------------+---------------------------+ | 2a79bce3-2492-4d5b-b565-6f18aa7c8bcd | DHCP agent | controller | nova | :-) | True | neutron-dhcp-agent | | 2b3799ae-3162-47f7-82fa-6291d76e0e14 | Metadata agent | controller | | :-) | True | neutron-metadata-agent | | 82ff3326-f570-4f50-bb5c-eec057034fd1 | Linux bridge agent | controller | | :-) | True | neutron-linuxbridge-agent | | ee271e44-25c3-4024-b414-40d8fa838d68 | Linux bridge agent | compute1 | | :-) | True | neutron-linuxbridge-agent | +--------------------------------------+--------------------+------------+-------------------+-------+----------------+---------------------------+ 到此，控制节点、计算节点网络服务neutron安装完成！！！ 七、计算节点horizon web界面Dashboard安装 7.1 安装包 yum -y install openstack-dashboard 7.2 编辑/etc/openstack-dashboard/local_settings文件并完成以下操作 1.配置仪表板以在控制器节点上使用OpenStack服务 158行，OPENSTACK_HOST = \"127.0.0.1\" 修改为 OPENSTACK_HOST = \"controller\" 2.允许所有主机访问仪表板 30行，ALLOWED_HOSTS = ['horizon.example.com', 'localhost'] 修改为 ALLOWED_HOSTS = ['*', ] 3.配置memcached会话存储服务 134-142行 CACHES = { 'default': { 'BACKEND': 'django.core.cache.backends.locmem.LocMemCache', }, } 修改为 SESSION_ENGINE = 'django.contrib.sessions.backends.file' CACHES = { 'default': { 'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache', 'LOCATION': 'controller:11211', } } 4.启用Identity API版本3 161行 OPENSTACK_KEYSTONE_URL = \"http://%s:5000/v2.0\" % OPENSTACK_HOST 修改为 OPENSTACK_KEYSTONE_URL = \"http://%s:5000/v3\" % OPENSTACK_HOST 5.启用对域的支持 65行 #OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = False 修改为 OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True 6.配置API版本 55-60行，取消注释 #OPENSTACK_API_VERSIONS = { # \"data-processing\": 1.1, # \"identity\": 3, # \"volume\": 2, # \"compute\": 2, #} 修改为 OPENSTACK_API_VERSIONS = { \"identity\": 3, \"image\": 2, \"volume\": 2, } 7.配置default作为您通过仪表盘创建用户的默认域 71行 #OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = 'default' 修改为 OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = \"default\" 8.将user配置为您通过仪表板创建的用户的默认角色 161行 OPENSTACK_KEYSTONE_DEFAULT_ROLE = \"_member_\" 修改为 OPENSTACK_KEYSTONE_DEFAULT_ROLE = \"user\" 9.如果选择网络选项1，请禁用对第3层网络服务的支持 261-270行 OPENSTACK_NEUTRON_NETWORK = { 'enable_router': True, 'enable_quotas': True, 'enable_ipv6': True, 'enable_distributed_router': False, 'enable_ha_router': False, 'enable_lb': True, 'enable_firewall': True, 'enable_vpn': True, 'enable_fip_topology_check': True, 修改为 OPENSTACK_NEUTRON_NETWORK = { ... 'enable_router': False, 'enable_quotas': False, 'enable_distributed_router': False, 'enable_ha_router': False, 'enable_lb': False, 'enable_firewall': False, 'enable_vpn': False, 'enable_fip_topology_check': False, 10.可选）配置时区 371行 TIME_ZONE = \"UTC\" 修改为 TIME_ZONE = \"Asia/Shanghai\" //有BUG，暂时不用 #用以下命令修改 \\cp /etc/openstack-dashboard/local_settings{,.bak} && \\ sed -i.bak '158cOPENSTACK_HOST = \"controller\"' /etc/openstack-dashboard/local_settings && \\ sed -i '30s/horizon.example.com/*/' /etc/openstack-dashboard/local_settings|sed -i '30s/\\x27localhost\\x27//' /etc/openstack-dashboard/local_settings && \\ sed -i $'137a\\t\\'LOCATION\\': \\'controller:11211\\',' /etc/openstack-dashboard/local_settings /etc/openstack-dashboard/local_settings文件内容 # -*- coding: utf-8 -*- import os from django.utils.translation import ugettext_lazy as _ from openstack_dashboard import exceptions from openstack_dashboard.settings import HORIZON_CONFIG DEBUG = False TEMPLATE_DEBUG = DEBUG # WEBROOT is the location relative to Webserver root # should end with a slash. WEBROOT = '/dashboard/' #LOGIN_URL = WEBROOT + 'auth/login/' #LOGOUT_URL = WEBROOT + 'auth/logout/' # # LOGIN_REDIRECT_URL can be used as an alternative for # HORIZON_CONFIG.user_home, if user_home is not set. # Do not set it to '/home/', as this will cause circular redirect loop #LOGIN_REDIRECT_URL = WEBROOT # If horizon is running in production (DEBUG is False), set this # with the list of host/domain names that the application can serve. # For more information see: # https://docs.djangoproject.com/en/dev/ref/settings/#allowed-hosts ALLOWED_HOSTS = ['*', ] # Set SSL proxy settings: # Pass this header from the proxy after terminating the SSL, # and don't forget to strip it from the client's request. # For more information see: # https://docs.djangoproject.com/en/1.8/ref/settings/#secure-proxy-ssl-header #SECURE_PROXY_SSL_HEADER = ('HTTP_X_FORWARDED_PROTO', 'https') # If Horizon is being served through SSL, then uncomment the following two # settings to better secure the cookies from security exploits #CSRF_COOKIE_SECURE = True #SESSION_COOKIE_SECURE = True # The absolute path to the directory where message files are collected. # The message file must have a .json file extension. When the user logins to # horizon, the message files collected are processed and displayed to the user. #MESSAGES_PATH=None # Overrides for OpenStack API versions. Use this setting to force the # OpenStack dashboard to use a specific API version for a given service API. # Versions specified here should be integers or floats, not strings. # NOTE: The version should be formatted as it appears in the URL for the # service API. For example, The identity service APIs have inconsistent # use of the decimal point, so valid options would be 2.0 or 3. OPENSTACK_API_VERSIONS = { # \"data-processing\": 1.1, \"identity\": 3, \"image\": 2, \"volume\": 2, \"compute\": 2, } # Set this to True if running on multi-domain model. When this is enabled, it # will require user to enter the Domain name in addition to username for login. OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True # Overrides the default domain used when running on single-domain model # with Keystone V3. All entities will be created in the default domain. # NOTE: This value must be the ID of the default domain, NOT the name. # Also, you will most likely have a value in the keystone policy file like this # \"cloud_admin\": \"rule:admin_required and domain_id:\" # This value must match the domain id specified there. OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = 'default' # Set this to True to enable panels that provide the ability for users to # manage Identity Providers (IdPs) and establish a set of rules to map # federation protocol attributes to Identity API attributes. # This extension requires v3.0+ of the Identity API. #OPENSTACK_KEYSTONE_FEDERATION_MANAGEMENT = False # Set Console type: # valid options are \"AUTO\"(default), \"VNC\", \"SPICE\", \"RDP\", \"SERIAL\" or None # Set to None explicitly if you want to deactivate the console. #CONSOLE_TYPE = \"AUTO\" # If provided, a \"Report Bug\" link will be displayed in the site header # which links to the value of this setting (ideally a URL containing # information on how to report issues). #HORIZON_CONFIG[\"bug_url\"] = \"http://bug-report.example.com\" # Show backdrop element outside the modal, do not close the modal # after clicking on backdrop. #HORIZON_CONFIG[\"modal_backdrop\"] = \"static\" # Specify a regular expression to validate user passwords. #HORIZON_CONFIG[\"password_validator\"] = { # \"regex\": '.*', # \"help_text\": _(\"Your password does not meet the requirements.\"), #} # Disable simplified floating IP address management for deployments with # multiple floating IP pools or complex network requirements. #HORIZON_CONFIG[\"simple_ip_management\"] = False # Turn off browser autocompletion for forms including the login form and # the database creation workflow if so desired. #HORIZON_CONFIG[\"password_autocomplete\"] = \"off\" # Setting this to True will disable the reveal button for password fields, # including on the login form. #HORIZON_CONFIG[\"disable_password_reveal\"] = False LOCAL_PATH = '/tmp' # Set custom secret key: # You can either set it to a specific value or you can let horizon generate a # default secret key that is unique on this machine, e.i. regardless of the # amount of Python WSGI workers (if used behind Apache+mod_wsgi): However, # there may be situations where you would want to set this explicitly, e.g. # when multiple dashboard instances are distributed on different machines # (usually behind a load-balancer). Either you have to make sure that a session # gets all requests routed to the same dashboard instance or you set the same # SECRET_KEY for all of them. SECRET_KEY='65941f1393ea1c265ad7' # We recommend you use memcached for development; otherwise after every reload # of the django development server, you will have to login again. To use # memcached set CACHES to something like SESSION_ENGINE = 'django.contrib.sessions.backends.file' CACHES = { 'default': { 'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache', 'LOCATION': 'controller:11211', }, } #CACHES = { # 'default': { # 'BACKEND': 'django.core.cache.backends.locmem.LocMemCache', # }, #} # Send email to the console by default EMAIL_BACKEND = 'django.core.mail.backends.console.EmailBackend' # Or send them to /dev/null #EMAIL_BACKEND = 'django.core.mail.backends.dummy.EmailBackend' # Configure these for your outgoing email host #EMAIL_HOST = 'smtp.my-company.com' #EMAIL_PORT = 25 #EMAIL_HOST_USER = 'djangomail' #EMAIL_HOST_PASSWORD = 'top-secret!' # For multiple regions uncomment this configuration, and add (endpoint, title). #AVAILABLE_REGIONS = [ # ('http://cluster1.example.com:5000/v2.0', 'cluster1'), # ('http://cluster2.example.com:5000/v2.0', 'cluster2'), #] OPENSTACK_HOST = \"controller\" OPENSTACK_KEYSTONE_URL = \"http://%s:5000/v3\" % OPENSTACK_HOST OPENSTACK_KEYSTONE_DEFAULT_ROLE = \"user\" # Enables keystone web single-sign-on if set to True. #WEBSSO_ENABLED = False # Determines which authentication choice to show as default. #WEBSSO_INITIAL_CHOICE = \"credentials\" # The list of authentication mechanisms which include keystone # federation protocols and identity provider/federation protocol # mapping keys (WEBSSO_IDP_MAPPING). Current supported protocol # IDs are 'saml2' and 'oidc' which represent SAML 2.0, OpenID # Connect respectively. # Do not remove the mandatory credentials mechanism. # Note: The last two tuples are sample mapping keys to a identity provider # and federation protocol combination (WEBSSO_IDP_MAPPING). #WEBSSO_CHOICES = ( # (\"credentials\", _(\"Keystone Credentials\")), # (\"oidc\", _(\"OpenID Connect\")), # (\"saml2\", _(\"Security Assertion Markup Language\")), # (\"acme_oidc\", \"ACME - OpenID Connect\"), # (\"acme_saml2\", \"ACME - SAML2\"), #) # A dictionary of specific identity provider and federation protocol # combinations. From the selected authentication mechanism, the value # will be looked up as keys in the dictionary. If a match is found, # it will redirect the user to a identity provider and federation protocol # specific WebSSO endpoint in keystone, otherwise it will use the value # as the protocol_id when redirecting to the WebSSO by protocol endpoint. # NOTE: The value is expected to be a tuple formatted as: (, ). #WEBSSO_IDP_MAPPING = { # \"acme_oidc\": (\"acme\", \"oidc\"), # \"acme_saml2\": (\"acme\", \"saml2\"), #} # Disable SSL certificate checks (useful for self-signed certificates): #OPENSTACK_SSL_NO_VERIFY = True # The CA certificate to use to verify SSL connections #OPENSTACK_SSL_CACERT = '/path/to/cacert.pem' # The OPENSTACK_KEYSTONE_BACKEND settings can be used to identify the # capabilities of the auth backend for Keystone. # If Keystone has been configured to use LDAP as the auth backend then set # can_edit_user to False and name to 'ldap'. # # TODO(tres): Remove these once Keystone has an API to identify auth backend. OPENSTACK_KEYSTONE_BACKEND = { 'name': 'native', 'can_edit_user': True, 'can_edit_group': True, 'can_edit_project': True, 'can_edit_domain': True, 'can_edit_role': True, } # Setting this to True, will add a new \"Retrieve Password\" action on instance, # allowing Admin session password retrieval/decryption. #OPENSTACK_ENABLE_PASSWORD_RETRIEVE = False # The Launch Instance user experience has been significantly enhanced. # You can choose whether to enable the new launch instance experience, # the legacy experience, or both. The legacy experience will be removed # in a future release, but is available as a temporary backup setting to ensure # compatibility with existing deployments. Further development will not be # done on the legacy experience. Please report any problems with the new # experience via the Launchpad tracking system. # # Toggle LAUNCH_INSTANCE_LEGACY_ENABLED and LAUNCH_INSTANCE_NG_ENABLED to # determine the experience to enable. Set them both to true to enable # both. #LAUNCH_INSTANCE_LEGACY_ENABLED = True #LAUNCH_INSTANCE_NG_ENABLED = False # A dictionary of settings which can be used to provide the default values for # properties found in the Launch Instance modal. #LAUNCH_INSTANCE_DEFAULTS = { # 'config_drive': False, #} # The Xen Hypervisor has the ability to set the mount point for volumes # attached to instances (other Hypervisors currently do not). Setting # can_set_mount_point to True will add the option to set the mount point # from the UI. OPENSTACK_HYPERVISOR_FEATURES = { 'can_set_mount_point': False, 'can_set_password': False, 'requires_keypair': False, } # The OPENSTACK_CINDER_FEATURES settings can be used to enable optional # services provided by cinder that is not exposed by its extension API. OPENSTACK_CINDER_FEATURES = { 'enable_backup': False, } # The OPENSTACK_NEUTRON_NETWORK settings can be used to enable optional # services provided by neutron. Options currently available are load # balancer service, security groups, quotas, VPN service. OPENSTACK_NEUTRON_NETWORK = { 'enable_router': False, 'enable_quotas': False, 'enable_ipv6': False, 'enable_distributed_router': False, 'enable_ha_router': False, 'enable_lb': False, 'enable_firewall': False, 'enable_vpn': False, 'enable_fip_topology_check': False, # Neutron can be configured with a default Subnet Pool to be used for IPv4 # subnet-allocation. Specify the label you wish to display in the Address # pool selector on the create subnet step if you want to use this feature. 'default_ipv4_subnet_pool_label': None, # Neutron can be configured with a default Subnet Pool to be used for IPv6 # subnet-allocation. Specify the label you wish to display in the Address # pool selector on the create subnet step if you want to use this feature. # You must set this to enable IPv6 Prefix Delegation in a PD-capable # environment. 'default_ipv6_subnet_pool_label': None, # The profile_support option is used to detect if an external router can be # configured via the dashboard. When using specific plugins the # profile_support can be turned on if needed. 'profile_support': None, #'profile_support': 'cisco', # Set which provider network types are supported. Only the network types # in this list will be available to choose from when creating a network. # Network types include local, flat, vlan, gre, and vxlan. 'supported_provider_types': ['*'], # Set which VNIC types are supported for port binding. Only the VNIC # types in this list will be available to choose from when creating a # port. # VNIC types include 'normal', 'macvtap' and 'direct'. # Set to empty list or None to disable VNIC type selection. 'supported_vnic_types': ['*'], } # The OPENSTACK_HEAT_STACK settings can be used to disable password # field required while launching the stack. OPENSTACK_HEAT_STACK = { 'enable_user_pass': True, } # The OPENSTACK_IMAGE_BACKEND settings can be used to customize features # in the OpenStack Dashboard related to the Image service, such as the list # of supported image formats. #OPENSTACK_IMAGE_BACKEND = { # 'image_formats': [ # ('', _('Select format')), # ('aki', _('AKI - Amazon Kernel Image')), # ('ami', _('AMI - Amazon Machine Image')), # ('ari', _('ARI - Amazon Ramdisk Image')), # ('docker', _('Docker')), # ('iso', _('ISO - Optical Disk Image')), # ('ova', _('OVA - Open Virtual Appliance')), # ('qcow2', _('QCOW2 - QEMU Emulator')), # ('raw', _('Raw')), # ('vdi', _('VDI - Virtual Disk Image')), # ('vhd', _('VHD - Virtual Hard Disk')), # ('vmdk', _('VMDK - Virtual Machine Disk')), # ], #} # The IMAGE_CUSTOM_PROPERTY_TITLES settings is used to customize the titles for # image custom property attributes that appear on image detail pages. IMAGE_CUSTOM_PROPERTY_TITLES = { \"architecture\": _(\"Architecture\"), \"kernel_id\": _(\"Kernel ID\"), \"ramdisk_id\": _(\"Ramdisk ID\"), \"image_state\": _(\"Euca2ools state\"), \"project_id\": _(\"Project ID\"), \"image_type\": _(\"Image Type\"), } # The IMAGE_RESERVED_CUSTOM_PROPERTIES setting is used to specify which image # custom properties should not be displayed in the Image Custom Properties # table. IMAGE_RESERVED_CUSTOM_PROPERTIES = [] # OPENSTACK_ENDPOINT_TYPE specifies the endpoint type to use for the endpoints # in the Keystone service catalog. Use this setting when Horizon is running # external to the OpenStack environment. The default is 'publicURL'. #OPENSTACK_ENDPOINT_TYPE = \"publicURL\" # SECONDARY_ENDPOINT_TYPE specifies the fallback endpoint type to use in the # case that OPENSTACK_ENDPOINT_TYPE is not present in the endpoints # in the Keystone service catalog. Use this setting when Horizon is running # external to the OpenStack environment. The default is None. This # value should differ from OPENSTACK_ENDPOINT_TYPE if used. #SECONDARY_ENDPOINT_TYPE = \"publicURL\" # The number of objects (Swift containers/objects or images) to display # on a single page before providing a paging element (a \"more\" link) # to paginate results. API_RESULT_LIMIT = 1000 API_RESULT_PAGE_SIZE = 20 # The size of chunk in bytes for downloading objects from Swift SWIFT_FILE_TRANSFER_CHUNK_SIZE = 512 * 1024 # Specify a maximum number of items to display in a dropdown. DROPDOWN_MAX_ITEMS = 30 # The timezone of the server. This should correspond with the timezone # of your entire OpenStack installation, and hopefully be in UTC. TIME_ZONE = \"Asia/Shanghai\" # When launching an instance, the menu of available flavors is # sorted by RAM usage, ascending. If you would like a different sort order, # you can provide another flavor attribute as sorting key. Alternatively, you # can provide a custom callback method to use for sorting. You can also provide # a flag for reverse sort. For more info, see # http://docs.python.org/2/library/functions.html#sorted #CREATE_INSTANCE_FLAVOR_SORT = { # 'key': 'name', # # or # 'key': my_awesome_callback_method, # 'reverse': False, #} # Set this to True to display an 'Admin Password' field on the Change Password # form to verify that it is indeed the admin logged-in who wants to change # the password. #ENFORCE_PASSWORD_CHECK = False # Modules that provide /auth routes that can be used to handle different types # of user authentication. Add auth plugins that require extra route handling to # this list. #AUTHENTICATION_URLS = [ # 'openstack_auth.urls', #] # The Horizon Policy Enforcement engine uses these values to load per service # policy rule files. The content of these files should match the files the # OpenStack services are using to determine role based access control in the # target installation. # Path to directory containing policy.json files POLICY_FILES_PATH = '/etc/openstack-dashboard' # Map of local copy of service policy files. # Please insure that your identity policy file matches the one being used on # your keystone servers. There is an alternate policy file that may be used # in the Keystone v3 multi-domain case, policy.v3cloudsample.json. # This file is not included in the Horizon repository by default but can be # found at # http://git.openstack.org/cgit/openstack/keystone/tree/etc/ \\ # policy.v3cloudsample.json # Having matching policy files on the Horizon and Keystone servers is essential # for normal operation. This holds true for all services and their policy files. #POLICY_FILES = { # 'identity': 'keystone_policy.json', # 'compute': 'nova_policy.json', # 'volume': 'cinder_policy.json', # 'image': 'glance_policy.json', # 'orchestration': 'heat_policy.json', # 'network': 'neutron_policy.json', # 'telemetry': 'ceilometer_policy.json', #} # TODO: (david-lyle) remove when plugins support adding settings. # Note: Only used when trove-dashboard plugin is configured to be used by # Horizon. # Trove user and database extension support. By default support for # creating users and databases on database instances is turned on. # To disable these extensions set the permission here to something # unusable such as [\"!\"]. #TROVE_ADD_USER_PERMS = [] #TROVE_ADD_DATABASE_PERMS = [] # Change this patch to the appropriate list of tuples containing # a key, label and static directory containing two files: # _variables.scss and _styles.scss #AVAILABLE_THEMES = [ # ('default', 'Default', 'themes/default'), # ('material', 'Material', 'themes/material'), #] LOGGING = { 'version': 1, # When set to True this will disable all logging except # for loggers specified in this configuration dictionary. Note that # if nothing is specified here and disable_existing_loggers is True, # django.db.backends will still log unless it is disabled explicitly. 'disable_existing_loggers': False, 'handlers': { 'null': { 'level': 'DEBUG', 'class': 'logging.NullHandler', }, 'console': { # Set the level to \"DEBUG\" for verbose output logging. 'level': 'INFO', 'class': 'logging.StreamHandler', }, }, 'loggers': { # Logging from django.db.backends is VERY verbose, send to null # by default. 'django.db.backends': { 'handlers': ['null'], 'propagate': False, }, 'requests': { 'handlers': ['null'], 'propagate': False, }, 'horizon': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'openstack_dashboard': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'novaclient': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'cinderclient': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'keystoneclient': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'glanceclient': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'neutronclient': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'heatclient': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'ceilometerclient': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'swiftclient': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'openstack_auth': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'nose.plugins.manager': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'django': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'iso8601': { 'handlers': ['null'], 'propagate': False, }, 'scss': { 'handlers': ['null'], 'propagate': False, }, }, } # 'direction' should not be specified for all_tcp/udp/icmp. # It is specified in the form. SECURITY_GROUP_RULES = { 'all_tcp': { 'name': _('All TCP'), 'ip_protocol': 'tcp', 'from_port': '1', 'to_port': '65535', }, 'all_udp': { 'name': _('All UDP'), 'ip_protocol': 'udp', 'from_port': '1', 'to_port': '65535', }, 'all_icmp': { 'name': _('All ICMP'), 'ip_protocol': 'icmp', 'from_port': '-1', 'to_port': '-1', }, 'ssh': { 'name': 'SSH', 'ip_protocol': 'tcp', 'from_port': '22', 'to_port': '22', }, 'smtp': { 'name': 'SMTP', 'ip_protocol': 'tcp', 'from_port': '25', 'to_port': '25', }, 'dns': { 'name': 'DNS', 'ip_protocol': 'tcp', 'from_port': '53', 'to_port': '53', }, 'http': { 'name': 'HTTP', 'ip_protocol': 'tcp', 'from_port': '80', 'to_port': '80', }, 'pop3': { 'name': 'POP3', 'ip_protocol': 'tcp', 'from_port': '110', 'to_port': '110', }, 'imap': { 'name': 'IMAP', 'ip_protocol': 'tcp', 'from_port': '143', 'to_port': '143', }, 'ldap': { 'name': 'LDAP', 'ip_protocol': 'tcp', 'from_port': '389', 'to_port': '389', }, 'https': { 'name': 'HTTPS', 'ip_protocol': 'tcp', 'from_port': '443', 'to_port': '443', }, 'smtps': { 'name': 'SMTPS', 'ip_protocol': 'tcp', 'from_port': '465', 'to_port': '465', }, 'imaps': { 'name': 'IMAPS', 'ip_protocol': 'tcp', 'from_port': '993', 'to_port': '993', }, 'pop3s': { 'name': 'POP3S', 'ip_protocol': 'tcp', 'from_port': '995', 'to_port': '995', }, 'ms_sql': { 'name': 'MS SQL', 'ip_protocol': 'tcp', 'from_port': '1433', 'to_port': '1433', }, 'mysql': { 'name': 'MYSQL', 'ip_protocol': 'tcp', 'from_port': '3306', 'to_port': '3306', }, 'rdp': { 'name': 'RDP', 'ip_protocol': 'tcp', 'from_port': '3389', 'to_port': '3389', }, } # Deprecation Notice: # # The setting FLAVOR_EXTRA_KEYS has been deprecated. # Please load extra spec metadata into the Glance Metadata Definition Catalog. # # The sample quota definitions can be found in: # /etc/metadefs/compute-quota.json # # The metadata definition catalog supports CLI and API: # $glance --os-image-api-version 2 help md-namespace-import # $glance-manage db_load_metadefs # # See Metadata Definitions on: http://docs.openstack.org/developer/glance/ # TODO: (david-lyle) remove when plugins support settings natively # Note: This is only used when the Sahara plugin is configured and enabled # for use in Horizon. # Indicate to the Sahara data processing service whether or not # automatic floating IP allocation is in effect. If it is not # in effect, the user will be prompted to choose a floating IP # pool for use in their cluster. False by default. You would want # to set this to True if you were running Nova Networking with # auto_assign_floating_ip = True. #SAHARA_AUTO_IP_ALLOCATION_ENABLED = False # The hash algorithm to use for authentication tokens. This must # match the hash algorithm that the identity server and the # auth_token middleware are using. Allowed values are the # algorithms supported by Python's hashlib library. #OPENSTACK_TOKEN_HASH_ALGORITHM = 'md5' # Hashing tokens from Keystone keeps the Horizon session data smaller, but it # doesn't work in some cases when using PKI tokens. Uncomment this value and # set it to False if using PKI tokens and there are 401 errors due to token # hashing. #OPENSTACK_TOKEN_HASH_ENABLED = True # AngularJS requires some settings to be made available to # the client side. Some settings are required by in-tree / built-in horizon # features. These settings must be added to REST_API_REQUIRED_SETTINGS in the # form of ['SETTING_1','SETTING_2'], etc. # # You may remove settings from this list for security purposes, but do so at # the risk of breaking a built-in horizon feature. These settings are required # for horizon to function properly. Only remove them if you know what you # are doing. These settings may in the future be moved to be defined within # the enabled panel configuration. # You should not add settings to this list for out of tree extensions. # See: https://wiki.openstack.org/wiki/Horizon/RESTAPI REST_API_REQUIRED_SETTINGS = ['OPENSTACK_HYPERVISOR_FEATURES', 'LAUNCH_INSTANCE_DEFAULTS'] # Additional settings can be made available to the client side for # extensibility by specifying them in REST_API_ADDITIONAL_SETTINGS # !! Please use extreme caution as the settings are transferred via HTTP/S # and are not encrypted on the browser. This is an experimental API and # may be deprecated in the future without notice. #REST_API_ADDITIONAL_SETTINGS = [] # DISALLOW_IFRAME_EMBED can be used to prevent Horizon from being embedded # within an iframe. Legacy browsers are still vulnerable to a Cross-Frame # Scripting (XFS) vulnerability, so this option allows extra security hardening # where iframes are not used in deployment. Default setting is True. # For more information see: # http://tinyurl.com/anticlickjack #DISALLOW_IFRAME_EMBED = True 7.3 验证操作 7.3.1 修改配置文件，否则后续访问dashboard会报500错误 sed -i.bak '3aWSGIApplicationGroup %{GLOBAL}' \\ /etc/httpd/conf.d/openstack-dashboard.conf #重启httpd systemctl enable httpd && systemctl restart httpd 7.3.2 登录dashboard 10.0.0.31/dashboard 域：default 用户名：admin 密码：ADMIN_PASS 登陆后首界面 到此，计算节点Dashboard安装完成！！！ 如果遇到如下错误 解决方法 安装dashboard节点做以下操作 1.修改配置文件/etc/openstack-dashboard/local_settings 修改 SESSION_ENGINE = 'django.contrib.sessions.backends.cache' 修改为 SESSION_ENGINE = 'django.contrib.sessions.backends.file' 2.重启httpd systemctl restart httpd 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/openstack/mitaka/2.Mitaka版使用命令启动一个实例.html":{"url":"linux/openstack/mitaka/2.Mitaka版使用命令启动一个实例.html","title":"Mitaka版使用命令启动一个实例","keywords":"","body":"[toc] Mitaka版使用命令启动一个示例 1.创建网络（网络名+子网） 创建网络名 neutron net-create --shared --provider:physical_network provider \\ --provider:network_type flat pptfz net-create 创建网络 --shared 创建共享网络 --provider:physical_network 指定物理网卡名称 provider网络标签 --provider:network_type 指定网络类型 flat桥接网络 pptfz网络名称 创建子网 neutron subnet-create --name bxb \\ --allocation-pool start=10.0.0.101,end=10.0.0.250 \\ --dns-nameserver 223.5.5.5 --gateway 10.0.0.254 \\ pptfz 10.0.0.0/24 subnet-create 创建子网 --name 指定名称 bxb子网名称 --allocation-pool IP地址范围 pptfz 创建的子网关联到哪个网络 以下为操作 #创建网络 neutron net-create --shared --provider:physical_network provider \\ --provider:network_type flat pptfz Created a new network: +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | True | | availability_zone_hints | | | availability_zones | | | created_at | 2019-02-03T09:37:39 | | description | | | id | 9125ad48-6bbe-4baf-8d78-f91a7c0a8ea2 | | ipv4_address_scope | | | ipv6_address_scope | | | mtu | 1500 | | name | pptfz | | port_security_enabled | True | | provider:network_type | flat | | provider:physical_network | provider | | provider:segmentation_id | | | router:external | False | | shared | True | | status | ACTIVE | | subnets | | | tags | | | tenant_id | e33e3feaef784a5bb45bd9c766bc0f46 | | updated_at | 2019-02-03T09:37:39 | +---------------------------+--------------------------------------+ #创建子网 neutron subnet-create --name bxb \\ --allocation-pool start=10.0.0.101,end=10.0.0.250 \\ --dns-nameserver 223.5.5.5 --gateway 10.0.0.1 \\ pptfz 10.0.0.0/24 Created a new subnet: +-------------------+----------------------------------------------+ | Field | Value | +-------------------+----------------------------------------------+ | allocation_pools | {\"start\": \"10.0.0.101\", \"end\": \"10.0.0.250\"} | | cidr | 10.0.0.0/24 | | created_at | 2019-02-03T09:40:37 | | description | | | dns_nameservers | 223.5.5.5 | | enable_dhcp | True | | gateway_ip | 10.0.0.254 | | host_routes | | | id | ad21906e-166d-47e2-8634-18c907c6da3b | | ip_version | 4 | | ipv6_address_mode | | | ipv6_ra_mode | | | name | bxb | | network_id | 9125ad48-6bbe-4baf-8d78-f91a7c0a8ea2 | | subnetpool_id | | | tenant_id | e33e3feaef784a5bb45bd9c766bc0f46 | | updated_at | 2019-02-03T09:40:37 | +-------------------+----------------------------------------------+ 2.创建云主机的硬件配置方案 openstack flavor create --id 0 --vcpus 1 --ram 64 --disk 1 m1.nano +----------------------------+---------+ | Field | Value | +----------------------------+---------+ | OS-FLV-DISABLED:disabled | False | | OS-FLV-EXT-DATA:ephemeral | 0 | | disk | 1 | | id | 0 | | name | m1.nano | | os-flavor-access:is_public | True | | ram | 64 | | rxtx_factor | 1.0 | | swap | | | vcpus | 1 | +----------------------------+---------+ //参数说明 openstack flavor create --id 0 --vcpus 1 --ram 64 --disk 1 m1.nano flavor 硬件配置方案 --id 指定编号 --vcpus cpu个数 --ram 内存（单位：M） --disk 磁盘（单位：G） m1.nano 方案名称 3.创建密钥对 ssh-keygen -q -N \"\" -f ~/.ssh/id_rsa openstack keypair create --public-key ~/.ssh/id_rsa.pub mykey +-------------+-------------------------------------------------+ | Field | Value | +-------------+-------------------------------------------------+ | fingerprint | 0e:af:ab:c3:74:5f:56:1b:e8:46:7d:e5:65:4f:a8:9a | | name | mykey | | user_id | aaa8bfce5b5d451b956bb76dee235b9e | +-------------+-------------------------------------------------+ //参数说明 ssh-keygen -q -N \"\" -f ~/.ssh/id_rsa 非交互式生成密钥对 -q 安静模式 -N 指定加密密码 -f 密钥对存放位置 openstack keypair create --public-key ~/.ssh/id_rsa.pub mykey 上传密钥对 4.创建安全组规则 openstack security group rule create --proto icmp default +-----------------------+--------------------------------------+ | Field | Value | +-----------------------+--------------------------------------+ | id | cb340772-4dd8-4d33-8ae0-2be3f43f26de | | ip_protocol | icmp | | ip_range | 0.0.0.0/0 | | parent_group_id | b6f5f95a-fd52-43c1-bb4c-0625750e4369 | | port_range | | | remote_security_group | | +-----------------------+--------------------------------------+ openstack security group rule create --proto tcp --dst-port 22 default +-----------------------+--------------------------------------+ | Field | Value | +-----------------------+--------------------------------------+ | id | 01c1cea4-76c0-4181-aca9-4b3148fb0397 | | ip_protocol | tcp | | ip_range | 0.0.0.0/0 | | parent_group_id | b6f5f95a-fd52-43c1-bb4c-0625750e4369 | | port_range | 22:22 | | remote_security_group | | +-----------------------+--------------------------------------+ #默认所有端口全部禁止，上边两个命令为允许ping和ssh openstack security group rule create --proto icmp default openstack security group rule create --proto tcp --dst-port 22 default 5.启动一个实例 openstack server create --flavor m1.nano --image cirros \\ --nic net-id=`neutron net-list|awk 'NR==4{print $2}'` --security-group default \\ --key-name mykey pptfz +--------------------------------------+-----------------------------------------------+ | Field | Value | +--------------------------------------+-----------------------------------------------+ | OS-DCF:diskConfig | MANUAL | | OS-EXT-AZ:availability_zone | | | OS-EXT-SRV-ATTR:host | None | | OS-EXT-SRV-ATTR:hypervisor_hostname | None | | OS-EXT-SRV-ATTR:instance_name | instance-00000001 | | OS-EXT-STS:power_state | 0 | | OS-EXT-STS:task_state | scheduling | | OS-EXT-STS:vm_state | building | | OS-SRV-USG:launched_at | None | | OS-SRV-USG:terminated_at | None | | accessIPv4 | | | accessIPv6 | | | addresses | | | adminPass | 8KcNG34aXwuK | | config_drive | | | created | 2019-02-03T12:10:10Z | | flavor | m1.nano (0) | | hostId | | | id | d5e07f54-c70e-4657-9ec5-778edc941e99 | | image | cirros (ac21b17b-e910-4ca4-b743-914b8fbd0e55) | | key_name | mykey | | name | pptfz | | os-extended-volumes:volumes_attached | [] | | progress | 0 | | project_id | e33e3feaef784a5bb45bd9c766bc0f46 | | properties | | | security_groups | [{u'name': u'default'}] | | status | BUILD | | updated | 2019-02-03T12:10:12Z | | user_id | aaa8bfce5b5d451b956bb76dee235b9e | +--------------------------------------+-----------------------------------------------+ #创建完成后查看，状态为ACTIVE即为正确 openstack server list +--------------------------------------+-------+--------+------------------+ | ID | Name | Status | Networks | +--------------------------------------+-------+--------+------------------+ | d5e07f54-c70e-4657-9ec5-778edc941e99 | pptfz | ACTIVE | pptfz=10.0.0.102 | +--------------------------------------+-------+--------+------------------+ 启动实例后会在web界面显示 项目-->计算-->示例 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/openstack/mitaka/3.Mitaka版web界面启动一个实例.html":{"url":"linux/openstack/mitaka/3.Mitaka版web界面启动一个实例.html","title":"Mitaka版web界面启动一个实例","keywords":"","body":"[toc] Mitaka版web界面启动一个实例 1.项目-->计算--实例-->启动实例 2.填写实例名称 3.选择镜像，这里只有一个，如果有多个可以任意选择不同的镜像 4.选择实例的配置 5.启动实例 这里因为刚刚安装完成，网络、网络端口、安全组、密钥对都只有一个，所以系统直接默认选择，如果后续创建了这些配置，可以依据实际情况具体选择，这里直接默认即可 6.启动完成 7.连接实例，右边下箭头-->控制台 ⚠️点击控制台后提示找不到controller地址，因为没有做hosts解析，需要先做hosts解析 windows C:\\Windows\\System32\\drivers\\etc\\hosts 10.0.0.11 controller mac /etc/hosts 10.0.0.11 controller 做完hosts解析后刷新，但是此时又有问题，会一直卡在这个界面不动 解决方法： 1.修改计算节点/etc/nova/nova.conf 在[libvirt]下添加如下两行 cpu_mode = none virt_type = qemu 2.重启nova-compute systemctl restart openstack-nova-compute 修改完openstack-nova-compute后，硬重启（相当于拔电源）实例 硬重启实例后就可以登录了，按照提示登录用户名是cirros，密码是cubswin:) ⚠️注意，需要先在周围黑框处点击一下才能进入控制台 登陆成功后，查看ip、主机名、能够上外网即为正确 切换为root，用默认用户登录系统后，执行sudo su - root切换到root用户，然后就可以修改密码了，并且可以直接用xshell或者其他终端连接 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/openstack/rocky/1.centos7.8搭建openstack Rocky版.html":{"url":"linux/openstack/rocky/1.centos7.8搭建openstack Rocky版.html","title":"centos7.8搭建openstack Rocky版","keywords":"","body":"[toc] centos7搭建openstack Rocky版 官方文档 rocky版中的密码说明 数据库密码（未使用变量） 数据库的根密码 ADMIN_PASS 用户密码 admin CINDER_DBPASS 块存储服务的数据库密码 CINDER_PASS 块存储服务用户密码 cinder DASH_DBPASS 仪表板的数据库密码 DEMO_PASS 用户密码 demo GLANCE_DBPASS 镜像服务的数据库密码 GLANCE_PASS 图片服务用户密码 glance KEYSTONE_DBPASS 身份服务的数据库密码 METADATA_SECRET 元数据代理的秘密 NEUTRON_DBPASS 网络服务的数据库密码 NEUTRON_PASS 网络服务用户密码 neutron NOVA_DBPASS 计算服务的数据库密码 NOVA_PASS 计算服务用户密码 nova PLACEMENT_PASS 展示位置服务用户的密码 placement RABBIT_PASS RabbitMQ用户密码 openstack 实验环境 角色 IP 主机名 默认网关 硬件环境 虚拟化 防火墙、selinux 操作系统 内核版本 控制节点 172.30.100.4/16 openstack-controller 172.30.255.253 4c16g，40g+100g 开启 关闭 CentOS7.6 3.10.0-957.21.3.el7.x86_64 计算节点1 172.30.100.5/16 openstack-compute01 172.30.255.253 4c16g，40g+100g 开启 关闭 CentOS7.6 3.10.0-957.21.3.el7.x86_64 存储节点1 172.30.100.6/16 openstack-block01 172.30.255.253 4c16g，40g+100g 开启 关闭 CentOS7.6 3.10.0-957.21.3.el7.x86_64 对象节点1 172.30.100.7/16 openstack-object01 172.30.255.253 4c16g，40g+50g+50g 开启 关闭 CentOS7.6 3.10.0-957.21.3.el7.x86_64 对象节点2 172.30.100.8/16 openstack-object02 172.30.255.253 4c16g，40g+50g+50g 开启 关闭 CentOS7.6 3.10.0-957.21.3.el7.x86_64 1.把rocky版rpm包做成本地yum源 rocky版离线包已上传至 百度网盘 提取码: 4gam ⚠️由于rocky官方yum源发生变更，在centos7上使用官方yum源安装会有部分包无法安装，因此采用离线包制作本地yum源方式安装 所有节点操作 控制节点操作 1.1 安装 createrepo 命令 控制节点操作 yum -y install createrepo 1.2 制作仓库 控制节点操作 createrepo openstack-rocky 1.3 安装nginx 控制节点操作 yum -y install nginx systemctl enable nginx && systemctl start nginx 1.4 配置nginx 控制节点操作 # 编辑nginx配置文件，yum安装的nginx根目录是/usr/share/nginx/html，这里个人习惯选择启用一个虚拟主机，监听88端口 cat > /etc/nginx/conf.d/openstack-rocky.repo.conf 1.5 编辑本地yum仓库文件 所有节点操作 # 指定repo文件，把提前准备好的离线包上传到/opt下，目录名称为openstack-rocky cat >/etc/yum.repos.d/openstack-rocky.repo 1.6 生成本地缓存 所有节点操作 yum makecache 2.基础环境配置 基础环境官方文档 2.1 关闭防火墙和selinux 所有节点操作 # 关闭防火墙 systemctl stop firewalld && systemctl disable firewalld # 禁用selinux // 临时修改 setenforce 0 // 永久修改，重启服务器后生效 sed -i '7s/enforcing/disabled/' /etc/selinux/config 2.2 配置hosts解析 所有节点操作 cat >> /etc/hosts 2.3 配置chrony服务，要保证所有节点时间一致 2.3.1 安装chrony 所有节点操作 yum -y install chrony 2.3.2 修改配置文件 /etc/chrony.conf 控制节点操作 sed -i.bak '3,6d' /etc/chrony.conf && \\ sed -i '3cserver ntp1.aliyun.com iburst' /etc/chrony.conf && \\ sed -i '23callow 172.30.0.0/16' /etc/chrony.conf 计算、存储、对象节点操作 sed -i '3,6d' /etc/chrony.conf && \\ sed -i '3cserver openstack-controller iburst' /etc/chrony.conf 2.3.3 启动服务并设置开机自启 所有节点操作 systemctl enable chronyd && systemctl start chronyd 2.3.4 验证 控制节点操作 $ chronyc sources 210 Number of sources = 1 MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== ^* 120.25.115.20 2 6 37 29 +43us[ -830us] +/- 22ms 计算、存储、对象节点操作 $ chronyc sources 210 Number of sources = 1 MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== ^? openstack-controller 3 6 200 50 +1319ms[+1319ms] +/- 14.4s 2.4 下载openstack官方yum源安装openstack客户端 所有节点操作 ⚠️本文使用本地源安装，不需要安装 centos-release-openstack-rocky # yum -y install centos-release-openstack-rocky yum -y install python-openstackclient 到此，所有节点基础环境配置完成！！！ 3.控制节点基础环境安装 3.1 安装配置mariadb数据库 3.1.1 安装包 yum -y install mariadb mariadb-server python2-PyMySQL 3.1.2 编辑配置文件 cat > /etc/my.cnf.d/openstack.cnf 3.1.3 启动mariadb并设置开机自启 systemctl enable mariadb && systemctl start mariadb 3.1.4 进行数据库安全设置 $ mysql_secure_installation Enter current password for root (enter for none): # 没有密码，直接回车 Set root password? [Y/n] n # 不设置root密码 Remove anonymous users? [Y/n] y # 移除匿名用户 Disallow root login remotely? [Y/n] y # 禁止root远程登陆 Remove test database and access to it? [Y/n] y # 移除test数据库 Reload privilege tables now? [Y/n] y # 刷新权限表 3.2 安装消息队列rabbitmq OpenStack 使用 message queue 协调操作和各服务的状态信息。消息队列服务一般运行在控制节点上 rabbitmq会启动2个端口 tcp/5672 rabbitmq服务端口 tcp/25672 多个rabbitmq通信用到的端口 3.2.1 安装包 yum -y install rabbitmq-server 3.2.2 启动rabbitmq并设置为开机自启 systemctl enable rabbitmq-server && systemctl start rabbitmq-server 3.2.3 添加openstack用户 $ rabbitmqctl add_user openstack RABBIT_PASS Creating user \"openstack\" 3.2.4 给openstack用户设置读和写权限 3个.*分别是 可读、可写、可配置 $ rabbitmqctl set_permissions openstack \".*\" \".*\" \".*\" Setting permissions for user \"openstack\" in vhost \"/\" 3.2.5 启动rabbitmq一个插件，启动之后会监听tcp/15672 是一个web管理界面，默认用户名和密码都是guest $ rabbitmq-plugins enable rabbitmq_management The following plugins have been enabled: amqp_client cowlib cowboy rabbitmq_web_dispatch rabbitmq_management_agent rabbitmq_management Applying plugin configuration to rabbit@openstack-controller... started 6 plugins. 3.3 安装memcached 认证服务认证缓存使用Memcached缓存令牌。缓存服务memecached运行在控制节点。在生产部署中，我们推荐联合启用防火墙、认证和加密保证它的安全。 memcache监听 tcp/udp 11211端口 3.3.1 安装包 yum -y install memcached python-memcached 3.3.2 修改配置文件 配置服务以使用控制器节点的管理IP地址。这是为了允许其他节点通过管理网络进行访问： sed -i.bak '/OPTIONS/c OPTIONS=\"-l 127.0.0.1,::1,openstack-controller\"' /etc/sysconfig/memcached 修改后文件内容如下 $ cat /etc/sysconfig/memcached PORT=\"11211\" USER=\"memcached\" MAXCONN=\"1024\" CACHESIZE=\"64\" OPTIONS=\"-l 127.0.0.1,::1,openstack-controller\" 3.3.3 启动memcached并设置为开机自启 systemctl enable memcached && systemctl start memcached 3.4 安装etcd OpenStack服务可以使用Etcd（分布式可靠键值存储）来进行分布式键锁定，存储配置，跟踪服务活动性和其他情况。 etcd服务在控制器节点上运行。 etcd服务启动后提供给外部客户端通信的端口是2379，而etcd服务中成员间的通信端口是2380 3.4.1 安装包 yum -y install etcd 3.4.2 编辑配置文件 编辑/etc/etcd/etcd.conf文件，并设置ETCD_INITIAL_CLUSTER， ETCD_INITIAL_ADVERTISE_PEER_URLS，ETCD_ADVERTISE_CLIENT_URLS， ETCD_LISTEN_CLIENT_URLS控制器节点，以使经由管理网络通过其他节点的访问的管理IP地址： export CONTROLLER_IP=172.30.100.4 cat > /etc/etcd/etcd.conf 3.4.3 启动etcd并设置开机自启 systemctl enable etcd && systemctl restart etcd 到此，控制节点环境安装完成！！！ 4.控制节点认证服务keystone安装 rocky版认证服务keystone安装配置官方文档 keystone认证服务功能：认证管理、授权管理、服务目录 认证：用户名和密码 授权：授权管理，例如一些技术网站(掘金、csdn)可以授权微信、QQ登陆 服务目录：相当于通讯录，即要访问openstack的镜像、网络、存储等服务，只需要找到keystone即可，而不需要再单独记住各个服务的访问地址 ⚠️后续每安装一个服务都需要在keystone上注册 4.1 创建keystone数据库并授权 mysql -e \"CREATE DATABASE keystone;\" mysql -e \"GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' \\ IDENTIFIED BY 'KEYSTONE_DBPASS';\" mysql -e \"GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' \\ IDENTIFIED BY 'KEYSTONE_DBPASS';\" 4.2 安装和配置keystron keystone借助apache访问 mod_wsgi是帮助apache连接python程序 监听端口 5000 4.2.1 安装软件包 yum -y install openstack-keystone httpd mod_wsgi openstack-utils.noarch 4.2.2 修改配置文件 /etc/keystone/keystone.conf # 在 [database] 部分，配置数据库访问： [database] connection = mysql+pymysql://keystone:KEYSTONE_DBPASS@controller/keystone # 在[token]部分，配置Fernet UUID令牌的提供者 [token] provider = fernet 使用如下命令修改 \\cp /etc/keystone/keystone.conf{,.bak} grep -Ev '^$|#' /etc/keystone/keystone.conf.bak >/etc/keystone/keystone.conf openstack-config --set /etc/keystone/keystone.conf database connection mysql+pymysql://keystone:KEYSTONE_DBPASS@openstack-controller/keystone openstack-config --set /etc/keystone/keystone.conf token provider fernet 文件md5值 md5sum /etc/keystone/keystone.conf e12c017255f580f414e3693bd4ccaa1a /etc/keystone/keystone.conf 4.2.3 初始化身份认证服务的数据库 命令的含义是切换到keystone用户，使用的shell是/bin/sh，执行 -c后的命令 su -s /bin/sh -c \"keystone-manage db_sync\" keystone 上一步操作为导入表，以下命令执行返回有表即为正确 $ mysql keystone -e \"show tables;\"|wc -l 45 4.2.4 初始化Fernet key keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone keystone-manage credential_setup --keystone-user keystone --keystone-group keystone 4.2.5 引导身份服务 keystone-manage bootstrap --bootstrap-password ADMIN_PASS \\ --bootstrap-admin-url http://openstack-controller:5000/v3/ \\ --bootstrap-internal-url http://openstack-controller:5000/v3/ \\ --bootstrap-public-url http://openstack-controller:5000/v3/ \\ --bootstrap-region-id RegionOne 4.2.6 配置Apache服务器 编辑/etc/httpd/conf/httpd.conf文件，配置ServerName 选项为控制节点 4.2.6.1 修改文件 \\cp /etc/httpd/conf/httpd.conf{,.bak} sed -i.bak -e '96cServerName openstack-controller' -e '/^Listen/c Listen 8080' /etc/httpd/conf/httpd.conf 文件md5值 $ md5sum /etc/httpd/conf/httpd.conf 812165839ec4f2e87a31c1ff2ba423aa /etc/httpd/conf/httpd.conf 4.2.6.2 创建文件软连接 ln -sf /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/ 4.2.7 启动Apache并设置为开机自启 systemctl enable httpd && systemctl start httpd 4.2.8 配置管理账户 以下为创建管理员账户admin，密码为ADMIN_PASS export OS_USERNAME=admin export OS_PASSWORD=ADMIN_PASS export OS_PROJECT_NAME=admin export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_DOMAIN_NAME=Default export OS_AUTH_URL=http://openstack-controller:5000/v3 export OS_IDENTITY_API_VERSION=3 4.3 创建域、项目、用户和角色 4.3.1 创建一个域 $ openstack domain create --description \"An Example Domain\" example +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | An Example Domain | | enabled | True | | id | ab6f853144384043a5dd648c154d0efe | | name | example | | tags | [] | +-------------+----------------------------------+ 4.3.2 创建一个服务项目 # service，后期用于关联openstack系统用户glance、nova、neutron $ openstack project create --domain default \\ --description \"Service Project\" service +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Service Project | | domain_id | default | | enabled | True | | id | f6696bc9511043ae9ec72d1c31a494f3 | | is_domain | False | | name | service | | parent_id | default | | tags | [] | +-------------+----------------------------------+ 4.3.3 常规（非管理员）任务应使用无特权的项目和用户 例如，本指南创建myproject项目和myuser 用户 4.3.3.1 创建myproject项目 $ openstack project create --domain default \\ --description \"Demo Project\" myproject +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Demo Project | | domain_id | default | | enabled | True | | id | 5b9ccd294c364cc68747df85f9598c89 | | is_domain | False | | name | myproject | | parent_id | default | | tags | [] | +-------------+----------------------------------+ 4.3.3.2 创建myuser用户 密码设置为 MYUSER_PASS ⚠️交互式与非交互式设置密码选择其中一种 非交互式设置密码 $ openstack user create --domain default \\ --password MYUSER_PASS myuser +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | domain_id | default | | enabled | True | | id | f7985ae93ad24f7784a5ea3e1f22109a | | name | myuser | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ 交互式设置密码 openstack user create --domain default \\ --password-prompt myuser 4.3.3.3 创建myrole角色 $ openstack role create myrole +-----------+----------------------------------+ | Field | Value | +-----------+----------------------------------+ | domain_id | None | | id | 9cb289f07a6d4bd6898dd863d616b164 | | name | myrole | +-----------+----------------------------------+ 4.3.3.4 将myrole角色添加到myproject项目和myuser用户 openstack role add --project myproject --user myuser myrole 4.3.4 验证 4.3.4.1 取消设置临时变量 OS_AUTH_URL 和 OS_PASSWORD 环境变量 unset OS_AUTH_URL OS_PASSWORD 4.3.4.2 以admin用户身份请求身份验证令牌 密码是 ADMIN_PASS $ openstack --os-auth-url http://openstack-controller:5000/v3 \\ --os-project-domain-name Default --os-user-domain-name Default \\ --os-project-name admin --os-username admin token issue +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | expires | 2021-11-04T03:32:38+0000 | | id | gAAAAABhg0ZG2yKJ00Myq8FQ33ibR6tN3I1Fu2xXJRN17usVIVPHiVJ2eJYQviKz9HeKWKEjmH_MLaWeiDZcW3QBQGjnT_Mbe9EEKqHSXKBJxo2etnI_kPCvxRoLPGE-XbevIWW6DYmsJqCJr32TdUG5wysC12ZbSWyVp25qyX_BKl_8KGSXXyM | | project_id | a6c250532966417cae11b1dfb5f0f6cc | | user_id | 20b791e627a741ed8b21e41027638986 | +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 4.3.4.3 使用上一节中创建的用户myuser，请请求认证令牌 密码是 MYUSER_PASS $ openstack --os-auth-url http://openstack-controller:5000/v3 \\ --os-project-domain-name Default --os-user-domain-name Default \\ --os-project-name myproject --os-username myuser token issue +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | expires | 2021-11-04T03:35:19+0000 | | id | gAAAAABhg0bnYFRoP2qjzgTRmT7lojzV3WO9GkYv6qFu5Nhx9_WbhIV6EDfNBbuJa7EHjmfz5BvYAza9J6wC6ZF36_nHfVPVkq3xO4E7fHNTa914q79UKTkpikR2i5NfPNgo1FqeIa0snUQ2M2-JSqteLCZxLMYZRTa_ckdV12i9OTle5_-6wk8 | | project_id | a150277718cd41439adbf88bbac6d1fe | | user_id | d7d0fdf398d949038719c5f0c22fc379 | +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 4.4 创建OpenStack客户端环境脚本 4.4.1 创建脚本 创建和编辑 admin-openrc 文件并添加以下内容，这里放在/opt下 cat > /opt/admin-openrc 创建和编辑 demo-openrc 文件并添加以下内容 cat > /opt/demo-openrc 4.4.2 使用脚本 4.4.2.1 加载 admin-openrc 文件以使用身份服务的位置以及admin项目和用户凭据填充环境变量 source /opt/admin-openrc 4.4.2.2 请求身份验证令牌 ⚠️注意expires中是UTC时间，落后中国8个小时，我国是东八区，使用timedatectl查看时间及时区，默认过期时间1小时 $ openstack token issue +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | expires | 2021-11-04T03:47:51+0000 | | id | gAAAAABhg0nXv43sRRJ5ahS0P2z86nPoZGz7g-Y2v3jcLhW-QM5eTIj_39ncEktjGu1R1SAOM9cqMpmOHF26j8ur7L26fYJ8gyNoA-JC51ZWesc5mnr1FapD0dxqCmteL22RmA5gRtzjC5qHfbn_RjVNe-AjBNSL_OmtAEdr-kY5B2IO7kvt7ko | | project_id | a6c250532966417cae11b1dfb5f0f6cc | | user_id | 20b791e627a741ed8b21e41027638986 | +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 到此，控制节点认证服务keystone安装完成！！！ 5.控制节点镜像服务glance安装 rocky版镜像服务glance安装配置官方文档 OpenStack镜像服务包括以下组件： glance-api 接收镜像API的调用，诸如镜像发现、恢复、存储 glance-registry 存储、处理和恢复镜像的元数据(属性)，元数据包括项诸如大小和类型 glance服务监听两个端口 glance-api 9292 glance-registry 9191 5.1 创建glance数据库并授权 mysql -e \"CREATE DATABASE glance;\" mysql -e \"GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' \\ IDENTIFIED BY 'GLANCE_DBPASS';\" mysql -e \"GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' \\ IDENTIFIED BY 'GLANCE_DBPASS';\" 5.2 获取管理员凭据以获取对仅管理员CLI命令的访问 source /opt/admin-openrc 5.3 创建服务凭据 5.3.1 创建 glance 用户 密码设置为 GLANCE_PASS ⚠️交互式与非交互式设置密码选择其中一种 非交互式设置密码 $ openstack user create --domain default --password GLANCE_PASS glance +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | domain_id | default | | enabled | True | | id | 36a88bb288464126837ebc19758bead6 | | name | glance | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ 交互式设置密码 openstack user create --domain default --password-prompt glance 5.3.2 将 admin 角色添加到 glance 用户和 service 项目 openstack role add --project service --user glance admin 5.3.3 创建glance服务实体 $ openstack service create --name glance \\ --description \"OpenStack Image\" image +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Image | | enabled | True | | id | ce5a424428d640c9adec06865d211916 | | name | glance | | type | image | +-------------+----------------------------------+ 5.3.4 创建Image服务API端点 $ openstack endpoint create --region RegionOne \\ image public http://openstack-controller:9292 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | bed29b8924114eee8b427f7a83f2cd64 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | ce5a424428d640c9adec06865d211916 | | service_name | glance | | service_type | image | | url | http://controller:9292 | +--------------+----------------------------------+ $ openstack endpoint create --region RegionOne \\ image internal http://openstack-controller:9292 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 94f84d946e6f4463af82041caf2877b5 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | ce5a424428d640c9adec06865d211916 | | service_name | glance | | service_type | image | | url | http://controller:9292 | +--------------+----------------------------------+ $ openstack endpoint create --region RegionOne \\ image admin http://openstack-controller:9292 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 16e947838d7948e6a0ec7feb7910b415 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | ce5a424428d640c9adec06865d211916 | | service_name | glance | | service_type | image | | url | http://controller:9292 | +--------------+----------------------------------+ 删除API端点使用命令openstack endpoint delete 使用命令openstack endpoint list查看endpoint-id然后根据id删除 5.4 安装和配置组件 5.4.1 安装软件包 yum -y install openstack-glance 5.4.2 编辑 /etc/glance/glance-api.conf 文件并完成以下操作 1.在该[database]部分中，配置数据库访问 [database] # ... connection = mysql+pymysql://glance:GLANCE_DBPASS@openstack-controller/glance 2.在[keystone_authtoken]和[paste_deploy]部分中，配置身份服务访问 [keystone_authtoken] # ... www_authenticate_uri = http://openstack-controller:5000 auth_url = http://openstack-controller:5000 memcached_servers = openstack-controller:11211 auth_type = password project_domain_name = Default user_domain_name = Default project_name = service username = glance password = GLANCE_PASS [paste_deploy] # ... flavor = keystone 3.在该[glance_store]部分中，配置本地文件系统存储和图像文件的位置 [glance_store] # ... stores = file,http default_store = file filesystem_store_datadir = /var/lib/glance/images/ 用以下命令修改 \\cp /etc/glance/glance-api.conf{,.bak} grep '^[a-Z\\[]' /etc/glance/glance-api.conf.bak >/etc/glance/glance-api.conf openstack-config --set /etc/glance/glance-api.conf database connection mysql+pymysql://glance:GLANCE_DBPASS@openstack-controller/glance openstack-config --set /etc/glance/glance-api.conf keystone_authtoken www_authenticate_uri http://openstack-controller:5000 openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_url http://openstack-controller:5000 openstack-config --set /etc/glance/glance-api.conf keystone_authtoken memcached_servers openstack-controller:11211 openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_type password openstack-config --set /etc/glance/glance-api.conf keystone_authtoken project_domain_name Default openstack-config --set /etc/glance/glance-api.conf keystone_authtoken user_domain_name Default openstack-config --set /etc/glance/glance-api.conf keystone_authtoken project_name service openstack-config --set /etc/glance/glance-api.conf keystone_authtoken username glance openstack-config --set /etc/glance/glance-api.conf keystone_authtoken password GLANCE_PASS openstack-config --set /etc/glance/glance-api.conf paste_deploy flavor keystone openstack-config --set /etc/glance/glance-api.conf glance_store stores file,http openstack-config --set /etc/glance/glance-api.conf glance_store default_store file openstack-config --set /etc/glance/glance-api.conf glance_store filesystem_store_datadir /var/lib/glance/images/ 文件md5值 $ md5sum /etc/glance/glance-api.conf 768bce1167f1545fb55115ad7e4fe3ff /etc/glance/glance-api.conf 5.4.3 编辑 /etc/glance/glance-registry.conf 文件并完成以下操作 1.在该[database]部分中，配置数据库访问 [database] # ... connection = mysql+pymysql://glance:GLANCE_DBPASS@openstack-controller/glance 2.在[keystone_authtoken]和[paste_deploy]部分中，配置身份服务访问 [keystone_authtoken] # ... www_authenticate_uri = http://openstack-controller:5000 auth_url = http://openstack-controller:5000 memcached_servers = openstack-controller:11211 auth_type = password project_domain_name = Default user_domain_name = Default project_name = service username = glance password = GLANCE_PASS [paste_deploy] # ... flavor = keystone 用以下命令修改 \\cp /etc/glance/glance-registry.conf{,.bak} grep '^[a-Z\\[]' /etc/glance/glance-registry.conf.bak > /etc/glance/glance-registry.conf openstack-config --set /etc/glance/glance-registry.conf database connection mysql+pymysql://glance:GLANCE_DBPASS@openstack-controller/glance openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken www_authenticate_uri http://openstack-controller:5000 openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_url http://openstack-controller:5000 openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken memcached_servers openstack-controller:11211 openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_type password openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken project_domain_name Default openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken user_domain_name Default openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken project_name service openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken username glance openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken password GLANCE_PASS openstack-config --set /etc/glance/glance-registry.conf paste_deploy flavor keystone 文件md5值 $ md5sum /etc/glance/glance-registry.conf ca0383d969bf7d1e9125b836769c9a2e /etc/glance/glance-registry.conf 5.4.4 同步数据库 ⚠️忽略此输出中的任何弃用消息 $ su -s /bin/sh -c \"glance-manage db_sync\" glance /usr/lib/python2.7/site-packages/oslo_db/sqlalchemy/enginefacade.py:1352: OsloDBDeprecationWarning: EngineFacade is deprecated; please use oslo_db.sqlalchemy.enginefacade expire_on_commit=expire_on_commit, _conf=conf) INFO [alembic.runtime.migration] Context impl MySQLImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. INFO [alembic.runtime.migration] Running upgrade -> liberty, liberty initial INFO [alembic.runtime.migration] Running upgrade liberty -> mitaka01, add index on created_at and updated_at columns of 'images' table INFO [alembic.runtime.migration] Running upgrade mitaka01 -> mitaka02, update metadef os_nova_server INFO [alembic.runtime.migration] Running upgrade mitaka02 -> ocata_expand01, add visibility to images INFO [alembic.runtime.migration] Running upgrade ocata_expand01 -> pike_expand01, empty expand for symmetry with pike_contract01 INFO [alembic.runtime.migration] Running upgrade pike_expand01 -> queens_expand01 INFO [alembic.runtime.migration] Running upgrade queens_expand01 -> rocky_expand01, add os_hidden column to images table INFO [alembic.runtime.migration] Running upgrade rocky_expand01 -> rocky_expand02, add os_hash_algo and os_hash_value columns to images table INFO [alembic.runtime.migration] Context impl MySQLImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. Upgraded database to: rocky_expand02, current revision(s): rocky_expand02 INFO [alembic.runtime.migration] Context impl MySQLImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. INFO [alembic.runtime.migration] Context impl MySQLImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. Database migration is up to date. No migration needed. INFO [alembic.runtime.migration] Context impl MySQLImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. INFO [alembic.runtime.migration] Context impl MySQLImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. INFO [alembic.runtime.migration] Running upgrade mitaka02 -> ocata_contract01, remove is_public from images INFO [alembic.runtime.migration] Running upgrade ocata_contract01 -> pike_contract01, drop glare artifacts tables INFO [alembic.runtime.migration] Running upgrade pike_contract01 -> queens_contract01 INFO [alembic.runtime.migration] Running upgrade queens_contract01 -> rocky_contract01 INFO [alembic.runtime.migration] Running upgrade rocky_contract01 -> rocky_contract02 INFO [alembic.runtime.migration] Context impl MySQLImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. Upgraded database to: rocky_contract02, current revision(s): rocky_contract02 INFO [alembic.runtime.migration] Context impl MySQLImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. Database is synced successfully. 有表即为正确 $ mysql glance -e \"show tables;\" | wc -l 16 5.4.5 启动glance服务并设置为开机自启 systemctl enable openstack-glance-api openstack-glance-registry systemctl start openstack-glance-api openstack-glance-registry 5.4.6 验证操作 5.4.6.1 获取管理员凭据以获取对仅管理员CLI命令的访问权限 source /opt/admin-openrc 5.4.6.2 下载测试镜像 wget http://download.cirros-cloud.net/0.4.0/cirros-0.4.0-x86_64-disk.img 5.4.6.3 上传镜像 使用QCOW2磁盘格式，裸容器格式和公共可见性将映像上载到映像服务 ，以便所有项目都可以访问它 删除镜像使用命令 glance image-delete 镜像id ⚠️这一步一定要看执行后输出结果中size大小，如果为0则说明镜像上载有问题 $ openstack image create \"cirros\" \\ --file cirros-0.4.0-x86_64-disk.img \\ --disk-format qcow2 --container-format bare \\ --public +------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | checksum | 443b7623e27ecf03dc9e01ee93f67afe | | container_format | bare | | created_at | 2021-11-04T03:56:55Z | | disk_format | qcow2 | | file | /v2/images/ff6ea9e3-e409-41e1-a871-daf3f8ebfb9e/file | | id | ff6ea9e3-e409-41e1-a871-daf3f8ebfb9e | | min_disk | 0 | | min_ram | 0 | | name | cirros | | owner | a6c250532966417cae11b1dfb5f0f6cc | | properties | os_hash_algo='sha512', os_hash_value='6513f21e44aa3da349f248188a44bc304a3653a04122d8fb4535423c8e1d14cd6a153f735bb0982e2161b5b5186106570c17a9e58b64dd39390617cd5a350f78', os_hidden='False' | | protected | False | | schema | /v2/schemas/image | | size | 12716032 | | status | active | | tags | | | updated_at | 2021-11-04T03:56:56Z | | virtual_size | None | | visibility | public | +------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 5.4.7 确认上传图像并验证属性 $ openstack image list +--------------------------------------+--------+--------+ | ID | Name | Status | +--------------------------------------+--------+--------+ | 94c96aab-d0b3-4340-835c-9a97108d0554 | cirros | active | +--------------------------------------+--------+--------+ 到此，控制节点镜像服务glance安装完成！！！ 6.控制节点和计算节点计算服务nova安装 nova相关服务 服务名称 作用 nova-api 接受并响应最终用户的计算API调用。该服务支持OpenStack Compute API。它执行一些策略并启动大多数编排活动，例如运行实例 nova-api-metadata 接受来自实例的元数据请求。nova-api-metadata当您在nova-network 安装时以多主机模式运行时，通常会使用该服务 nova-compute 通过守护程序API创建和终止虚拟机实例的辅助程序守护程序 nova-placement-api 跟踪每个提供商的库存和使用情况 nova-scheduler 从队列中获取虚拟机实例请求，并确定它在哪台计算服务器主机上运行 nova-conductor 调解nova计算服务和数据库之间的交互。它消除了nova计算服务对云数据库的直接访问。nova导体模块水平伸缩。但是，不要在nova计算服务运行的节点上部署它 nova-consoleauth 为控制台代理提供的用户授权令牌。请参阅 nova-novncproxy和nova-xvpvncproxy。该服务必须正在运行，控制台代理才能起作用。您可以在集群配置中针对单个nova-consoleauth服务运行这两种类型的代理 rocky版不推荐使用，并且以后会删除 nova-novncproxy 提供用于通过VNC连接访问正在运行的实例的代理。支持基于浏览器的novnc客户端。 nova-spicehtml5proxy 提供用于通过SPICE连接访问正在运行的实例的代理。支持基于浏览器的HTML5客户端。 nova-xvpvncproxy 提供用于通过VNC连接访问正在运行的实例的代理。支持特定于OpenStack的Java客户端。 安装和配置控制节点 rocky版控制节点计算服务nova安装配置官方文档 6.1 创建nova、nova_api、nova_cell0、placement数据库并授权 mysql -e \"CREATE DATABASE nova_api;\" mysql -e \"CREATE DATABASE nova;\" mysql -e \"CREATE DATABASE nova_cell0;\" mysql -e \"CREATE DATABASE placement;\" mysql -e \"GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' \\ IDENTIFIED BY 'NOVA_DBPASS';\" mysql -e \"GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' \\ IDENTIFIED BY 'NOVA_DBPASS';\" mysql -e \"GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' \\ IDENTIFIED BY 'NOVA_DBPASS';\" mysql -e \"GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' \\ IDENTIFIED BY 'NOVA_DBPASS';\" mysql -e \"GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'localhost' \\ IDENTIFIED BY 'NOVA_DBPASS';\" mysql -e \"GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'%' \\ IDENTIFIED BY 'NOVA_DBPASS';\" mysql -e \"GRANT ALL PRIVILEGES ON placement.* TO 'placement'@'localhost' \\ IDENTIFIED BY 'PLACEMENT_DBPASS';\" mysql -e \"GRANT ALL PRIVILEGES ON placement.* TO 'placement'@'%' \\ IDENTIFIED BY 'PLACEMENT_DBPASS';\" 6.2 获取管理员凭据以获取对仅管理员CLI命令的访问权限 source /opt/admin-openrc 6.3 创建计算服务凭据 6.3.1 创建 nova 用户 密码设置为 NOVA_PASS ⚠️交互式与非交互式设置密码选择其中一种 非交互式设置密码 $ openstack user create --domain default \\ --password NOVA_PASS nova +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | domain_id | default | | enabled | True | | id | ebe9b1934a2e4c8ca9c177af647851b1 | | name | nova | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ 交互式设置密码 openstack user create --domain default --password-prompt nova 6.3.2 将 admin 角色添加到 nova 用户 openstack role add --project service --user nova admin 6.3.3 创建 nova 服务实体 $ openstack service create --name nova \\ --description \"OpenStack Compute\" compute +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Compute | | enabled | True | | id | 412f485718f44759b6c3cd46b1d624e6 | | name | nova | | type | compute | +-------------+----------------------------------+ 6.3.4 创建 Compute API 服务端点 $ openstack endpoint create --region RegionOne \\ compute public http://openstack-controller:8774/v2.1 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | cc0a7c21acd0450998760841dd9a11c0 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 412f485718f44759b6c3cd46b1d624e6 | | service_name | nova | | service_type | compute | | url | http://controller:8774/v2.1 | +--------------+----------------------------------+ $ openstack endpoint create --region RegionOne \\ compute internal http://openstack-controller:8774/v2.1 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 69acbdd4f0114a339f8b62d9118ce137 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 412f485718f44759b6c3cd46b1d624e6 | | service_name | nova | | service_type | compute | | url | http://controller:8774/v2.1 | +--------------+----------------------------------+ $ openstack endpoint create --region RegionOne \\ compute admin http://openstack-controller:8774/v2.1 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 5382d617406a4dba8280dc375dd53329 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 412f485718f44759b6c3cd46b1d624e6 | | service_name | nova | | service_type | compute | | url | http://controller:8774/v2.1 | +--------------+----------------------------------+ 6.3.5 创建展示位置服务用户 PLACEMENT 密码设置为PLACEMENT_PASS ⚠️交互式与非交互式设置密码选择其中一种 非交互式创建密码 $ openstack user create --domain default --password PLACEMENT_PASS placement +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | domain_id | default | | enabled | True | | id | 5ab24083149e4adf978c43439b87c982 | | name | placement | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ 交互式创建密码 openstack user create --domain default --password-prompt placement 6.3.6 使用管理员角色将 Placement 用户添加到服务项目中 openstack role add --project service --user placement admin 6.3.7 在服务目录中创建 Placement API 条目 $ openstack service create --name placement \\ --description \"Placement API\" placement +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Placement API | | enabled | True | | id | 274104c9a16f4b728bd7f484d3c54d3e | | name | placement | | type | placement | +-------------+----------------------------------+ 6.3.8 创建 Placement API 服务端点 $ openstack endpoint create --region RegionOne \\ placement public http://openstack-controller:8778 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | c3afd275f71a4406a701d16ad24aa325 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 274104c9a16f4b728bd7f484d3c54d3e | | service_name | placement | | service_type | placement | | url | http://controller:8778 | +--------------+----------------------------------+ $ openstack endpoint create --region RegionOne \\ placement internal http://openstack-controller:8778 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 2ddc86a3b46d45489ebbedbd54fc3c0c | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 274104c9a16f4b728bd7f484d3c54d3e | | service_name | placement | | service_type | placement | | url | http://controller:8778 | +--------------+----------------------------------+ $ openstack endpoint create --region RegionOne \\ placement admin http://openstack-controller:8778 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | d0714a417aa44c0180d59be843e1d40d | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 274104c9a16f4b728bd7f484d3c54d3e | | service_name | placement | | service_type | placement | | url | http://controller:8778 | +--------------+----------------------------------+ 6.4 安装和配置组件 6.4.1 安装软件包 yum -y install openstack-nova-api openstack-nova-conductor \\ openstack-nova-console openstack-nova-novncproxy \\ openstack-nova-scheduler openstack-nova-placement-api 6.4.2 编辑 /etc/nova/nova.conf 文件并完成以下操作 1.在此[DEFAULT]部分中，仅启用计算和元数据API [DEFAULT] # ... enabled_apis = osapi_compute,metadata 2.在[api_database]，[database]和[placement_database] 部分，配置数据库访问 [api_database] # ... connection = mysql+pymysql://nova:NOVA_DBPASS@controller/nova_api [database] # ... connection = mysql+pymysql://nova:NOVA_DBPASS@controller/nova [placement_database] # ... connection = mysql+pymysql://placement:PLACEMENT_DBPASS@controller/placement 3.在该[DEFAULT]部分中，配置RabbitMQ消息队列访问 [DEFAULT] # ... transport_url = rabbit://openstack:RABBIT_PASS@controller 4.在[api]和[keystone_authtoken]部分中，配置身份服务访问 [api] # ... auth_strategy = keystone [keystone_authtoken] # ... auth_url = http://controller:5000/v3 memcached_servers = controller:11211 auth_type = password project_domain_name = Default user_domain_name = Default project_name = service username = nova password = NOVA_PASS 5.在该[DEFAULT]部分中，配置my_ip选项以使用控制器节点的管理接口IP地址 [DEFAULT] # ... my_ip = 10.0.0.11 6.在本[DEFAULT]节中，启用对网络服务的支持 [DEFAULT] # ... use_neutron = true firewall_driver = nova.virt.firewall.NoopFirewallDriver 默认情况下，Compute使用内部防火墙驱动程序。由于网络服务包含防火墙驱动程序，因此必须使用nova.virt.firewall.NoopFirewallDriver防火墙驱动程序禁用计算防火墙驱动 程序 7.在该[vnc]部分中，将VNC代理配置为使用控制器节点的管理接口IP地址 [vnc] enabled = true # ... server_listen = $my_ip server_proxyclient_address = $my_ip 8.在该[glance]部分中，配置图像服务API的位置 [glance] # ... api_servers = http://controller:9292 9.在该[oslo_concurrency]部分中，配置锁定路径 [oslo_concurrency] # ... lock_path = /var/lib/nova/tmp 10.在该[placement]部分中，配置Placement API [placement] # ... region_name = RegionOne project_domain_name = Default project_name = service auth_type = password user_domain_name = Default auth_url = http://controller:5000/v3 username = placement password = PLACEMENT_PASS 用以下命令修改 \\cp /etc/nova/nova.conf{,.bak} grep '^[a-Z\\[]' /etc/nova/nova.conf.bak >/etc/nova/nova.conf openstack-config --set /etc/nova/nova.conf DEFAULT enabled_apis osapi_compute,metadata openstack-config --set /etc/nova/nova.conf DEFAULT transport_url rabbit://openstack:RABBIT_PASS@openstack-controller openstack-config --set /etc/nova/nova.conf DEFAULT my_ip 172.30.100.4 openstack-config --set /etc/nova/nova.conf DEFAULT use_neutron true openstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriver openstack-config --set /etc/nova/nova.conf api_database connection mysql+pymysql://nova:NOVA_DBPASS@openstack-controller/nova_api openstack-config --set /etc/nova/nova.conf database connection mysql+pymysql://nova:NOVA_DBPASS@openstack-controller/nova openstack-config --set /etc/nova/nova.conf placement_database connection mysql+pymysql://placement:PLACEMENT_DBPASS@openstack-controller/placement openstack-config --set /etc/nova/nova.conf api auth_strategy keystone openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_url http://openstack-controller:5000/v3 openstack-config --set /etc/nova/nova.conf keystone_authtoken memcached_servers openstack-controller:11211 openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_type password openstack-config --set /etc/nova/nova.conf keystone_authtoken project_domain_name Default openstack-config --set /etc/nova/nova.conf keystone_authtoken user_domain_name Default openstack-config --set /etc/nova/nova.conf keystone_authtoken project_name service openstack-config --set /etc/nova/nova.conf keystone_authtoken username nova openstack-config --set /etc/nova/nova.conf keystone_authtoken password NOVA_PASS openstack-config --set /etc/nova/nova.conf vnc enabled true openstack-config --set /etc/nova/nova.conf vnc server_listen '$my_ip' openstack-config --set /etc/nova/nova.conf vnc server_proxyclient_address '$my_ip' openstack-config --set /etc/nova/nova.conf glance api_servers http://openstack-controller:9292 openstack-config --set /etc/nova/nova.conf oslo_concurrency lock_path /var/lib/nova/tmp openstack-config --set /etc/nova/nova.conf placement region_name RegionOne openstack-config --set /etc/nova/nova.conf placement project_domain_name Default openstack-config --set /etc/nova/nova.conf placement project_name service openstack-config --set /etc/nova/nova.conf placement auth_type password openstack-config --set /etc/nova/nova.conf placement user_domain_name Default openstack-config --set /etc/nova/nova.conf placement auth_url http://openstack-controller:5000/v3 openstack-config --set /etc/nova/nova.conf placement username placement openstack-config --set /etc/nova/nova.conf placement password PLACEMENT_PASS 文件md5值 $ md5sum /etc/nova/nova.conf 44436fe1f334fdfdf0b5efdbf4250e94 /etc/nova/nova.conf 6.4.3 由于包装错误，您必须通过将以下配置添加到来启用对Placement API的访问 /etc/httpd/conf.d/00-nova-placement-api.conf 6.4.3.1 备份文件并追加内容 ⚠️追加内容时要添加一行空行，否则格式会有错误(这里添加了两行空行，其中第一行是为了格式正确，第二行是为了格式规范，即标签与标签之间有一行空行) \\cp /etc/httpd/conf.d/00-nova-placement-api.conf{,.bak} cat >> /etc/httpd/conf.d/00-nova-placement-api.conf = 2.4> Require all granted Order allow,deny Allow from all EOF 文件md5值 $ md5sum /etc/httpd/conf.d/00-nova-placement-api.conf 4b31341049e863449951b0c76fe17bde /etc/httpd/conf.d/00-nova-placement-api.conf 6.4.3.2 重启httpd systemctl restart httpd 6.4.3 同步数据库，忽略输出 6.4.3.1 同步数据库 # 同步nova-api和placement数据库 $ su -s /bin/sh -c \"nova-manage api_db sync\" nova # 注册cell0数据库 $ su -s /bin/sh -c \"nova-manage cell_v2 map_cell0\" nova # 创建cell1单元格 $ su -s /bin/sh -c \"nova-manage cell_v2 create_cell --name=cell1 --verbose\" nova 536383cb-03e4-48bb-bb77-4eeb1bfb9d80 # 同步nova数据库 $ su -s /bin/sh -c \"nova-manage db sync\" nova /usr/lib/python2.7/site-packages/pymysql/cursors.py:170: Warning: (1831, u'Duplicate index `block_device_mapping_instance_uuid_virtual_name_device_name_idx`. This is deprecated and will be disallowed in a future release.') result = self._query(query) /usr/lib/python2.7/site-packages/pymysql/cursors.py:170: Warning: (1831, u'Duplicate index `uniq_instances0uuid`. This is deprecated and will be disallowed in a future release.') result = self._query(query) 6.4.3.2 验证 nova cell0 和 cell1 是否正确注册 $ su -s /bin/sh -c \"nova-manage cell_v2 list_cells\" nova +-------+--------------------------------------+------------------------------------+-------------------------------------------------+----------+ | Name | UUID | Transport URL | Database Connection | Disabled | +-------+--------------------------------------+------------------------------------+-------------------------------------------------+----------+ | cell0 | 00000000-0000-0000-0000-000000000000 | none:/ | mysql+pymysql://nova:****@controller/nova_cell0 | False | | cell1 | 536383cb-03e4-48bb-bb77-4eeb1bfb9d80 | rabbit://openstack:****@controller | mysql+pymysql://nova:****@controller/nova | False | +-------+--------------------------------------+------------------------------------+-------------------------------------------------+----------+ 6.4.4 启动Compute服务并将其配置为在系统引导时启动 nova-consoleauth自18.0.0（Rocky）起不推荐使用，并将在以后的版本中删除。每个单元应部署控制台代理。如果执行全新安装（而非升级），则可能不需要安装nova-consoleauth 服务。有关workarounds.enable_consoleauth详细信息，请参见 。 systemctl enable openstack-nova-api.service \\ openstack-nova-consoleauth openstack-nova-scheduler.service \\ openstack-nova-conductor.service openstack-nova-novncproxy.service systemctl start openstack-nova-api.service \\ openstack-nova-consoleauth openstack-nova-scheduler.service \\ openstack-nova-conductor.service openstack-nova-novncproxy.service 安装完成后会有no VNC 172.30.100.4:6080 安装和配置计算节点 rocky版计算节点计算服务nova安装配置官方文档 6.5 安装和配置组件 6.5.1 安装软件包 yum -y install openstack-nova-compute openstack-utils 6.5.2 编辑/etc/nova/nova.conf文件并完成以下操作 1.在此[DEFAULT]部分中，仅启用计算和元数据API [DEFAULT] # ... enabled_apis = osapi_compute,metadata 2.在该[DEFAULT]部分中，配置RabbitMQ消息队列访问： [DEFAULT] # ... transport_url = rabbit://openstack:RABBIT_PASS@controller 3.在[api]和[keystone_authtoken]部分中，配置身份服务访问： [api] # ... auth_strategy = keystone [keystone_authtoken] # ... auth_url = http://controller:5000/v3 memcached_servers = controller:11211 auth_type = password project_domain_name = Default user_domain_name = Default project_name = service username = nova password = NOVA_PASS 4.在该[DEFAULT]部分中，配置my_ip选项：MANAGEMENT_INTERFACE_IP_ADDRESS为计算节点上管理网络接口的IP地址 my_ip = MANAGEMENT_INTERFACE_IP_ADDRESS [DEFAULT] # ... my_ip = 10.0.0.31 5.在本[DEFAULT]节中，启用对网络服务的支持： [DEFAULT] # ... use_neutron = true firewall_driver = nova.virt.firewall.NoopFirewallDriver 6.在本[DEFAULT]节中，启用对网络服务的支持： [DEFAULT] # ... use_neutron = true firewall_driver = nova.virt.firewall.NoopFirewallDriver 7.在该[vnc]部分中，启用和配置远程控制台访问： [vnc] # ... enabled = true server_listen = 0.0.0.0 server_proxyclient_address = $my_ip novncproxy_base_url = http://controller:6080/vnc_auto.html 8.在该[glance]部分中，配置图像服务API的位置： [glance] # ... api_servers = http://controller:9292 9.在该[oslo_concurrency]部分中，配置锁定路径： [oslo_concurrency] # ... lock_path = /var/lib/nova/tmp 10.在该[placement]部分中，配置Placement API： [placement] # ... region_name = RegionOne project_domain_name = Default project_name = service auth_type = password user_domain_name = Default auth_url = http://controller:5000/v3 username = placement password = PLACEMENT_PASS 用以下命令修改 \\cp /etc/nova/nova.conf{,.bak} grep '^[a-Z\\[]' /etc/nova/nova.conf.bak >/etc/nova/nova.conf openstack-config --set /etc/nova/nova.conf DEFAULT enabled_apis osapi_compute,metadata openstack-config --set /etc/nova/nova.conf DEFAULT transport_url rabbit://openstack:RABBIT_PASS@controller openstack-config --set /etc/nova/nova.conf DEFAULT my_ip 172.30.100.5 openstack-config --set /etc/nova/nova.conf DEFAULT use_neutron true openstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriver openstack-config --set /etc/nova/nova.conf api auth_strategy keystone openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_url http://controller:5000/v3 openstack-config --set /etc/nova/nova.conf keystone_authtoken memcached_servers controller:11211 openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_type password openstack-config --set /etc/nova/nova.conf keystone_authtoken project_domain_name Default openstack-config --set /etc/nova/nova.conf keystone_authtoken user_domain_name Default openstack-config --set /etc/nova/nova.conf keystone_authtoken project_name service openstack-config --set /etc/nova/nova.conf keystone_authtoken username nova openstack-config --set /etc/nova/nova.conf keystone_authtoken password NOVA_PASS openstack-config --set /etc/nova/nova.conf vnc enabled true openstack-config --set /etc/nova/nova.conf vnc server_listen 0.0.0.0 openstack-config --set /etc/nova/nova.conf vnc server_proxyclient_address '$my_ip' openstack-config --set /etc/nova/nova.conf vnc novncproxy_base_url http://controller:6080/vnc_auto.html openstack-config --set /etc/nova/nova.conf glance api_servers http://controller:9292 openstack-config --set /etc/nova/nova.conf oslo_concurrency lock_path /var/lib/nova/tmp openstack-config --set /etc/nova/nova.conf placement region_name RegionOne openstack-config --set /etc/nova/nova.conf placement project_domain_name Default openstack-config --set /etc/nova/nova.conf placement project_name service openstack-config --set /etc/nova/nova.conf placement auth_type password openstack-config --set /etc/nova/nova.conf placement user_domain_name Default openstack-config --set /etc/nova/nova.conf placement auth_url http://controller:5000/v3 openstack-config --set /etc/nova/nova.conf placement username placement openstack-config --set /etc/nova/nova.conf placement password PLACEMENT_PASS 文件md5值 $ md5sum /etc/nova/nova.conf fa0ddace12aaa14c6bcfe86b70efac24 /etc/nova/nova.conf 6.5.3 确定您的计算节点是否支持虚拟机的硬件加速 $ egrep -c '(vmx|svm)' /proc/cpuinfo 2 如果此命令返回值1或更大，则计算节点支持硬件加速，通常不需要其他配置。 如果此命令返回值为0，则计算节点不支持硬件加速，您必须将libvirt配置为使用QEMU而不是KVM。 openstack-config --set /etc/nova/nova.conf libvirt virt_type qemu 6.5.4 启动Compute服务及其依赖项，并将它们配置为在系统引导时自动启动 systemctl enable libvirtd.service openstack-nova-compute.service systemctl start libvirtd.service openstack-nova-compute.service 验证操作，在控制节点执行 6.6 验证Compute服务的运行 6.6.1 获取管理员凭据以获取对仅管理员CLI命令的访问权限 source /opt/admin-openrc 6.6.2 列出服务组件以验证每个进程的成功启动和注册 $ openstack compute service list --service nova-compute +----+--------------+---------------------+------+---------+-------+----------------------------+ | ID | Binary | Host | Zone | Status | State | Updated At | +----+--------------+---------------------+------+---------+-------+----------------------------+ | 6 | nova-compute | openstack-compute01 | nova | enabled | up | 2021-11-04T07:29:47.000000 | +----+--------------+---------------------+------+---------+-------+----------------------------+ 6.6.3 发现计算主机 $ su -s /bin/sh -c \"nova-manage cell_v2 discover_hosts --verbose\" nova Found 2 cell mappings. Skipping cell0 since it does not contain hosts. Getting computes from cell 'cell1': 536383cb-03e4-48bb-bb77-4eeb1bfb9d80 Checking host mapping for compute host 'compute1': 83452da0-a693-4860-bcd8-028743169f0f Creating host mapping for compute host 'compute1': 83452da0-a693-4860-bcd8-028743169f0f Found 1 unmapped computes in cell: 536383cb-03e4-48bb-bb77-4eeb1bfb9d80 到此，控制节点和计算节点计算服务nova安装完成！！！ 7.控制节点、计算节点网络服务neutron安装 neutron相关服务 服务名 说明 neutron-server 端口(9696) api 接受和响应外部的网络管理请求 neutron-linuxbridge-agent 负责创建桥接网卡 neutron-dhcp-agent 负责分配IP neutron-metadata-agent 配合nova-metadata-api实现虚拟机的定制化操作 L3-agent 实现三层网络(网络层) 安装和配置控制节点 rocky版控制节点网络服务neutron安装配置官方文档 7.1 创建neutron数据库并授权 mysql -e \"CREATE DATABASE neutron;\" mysql -e \"GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' \\ IDENTIFIED BY 'NEUTRON_DBPASS';\" mysql -e \"GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' \\ IDENTIFIED BY 'NEUTRON_DBPASS';\" 7.2 获取管理员凭据以获取对仅管理员CLI命令的访问权限 source /opt/admin-openrc 7.3 创建服务凭证 7.3.1 创建neutron用户 密码设置为 NEUTRON_PASS ⚠️交互式与非交互式设置密码选择其中一种 非交互式创建密码 $ openstack user create --domain default --password NEUTRON_PASS neutron +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | domain_id | default | | enabled | True | | id | 014a7629fb0548899be31c87494e1156 | | name | neutron | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ 交互式创建密码 openstack user create --domain default --password-prompt neutron 7.3.2 将 admin 角色添加到 neutron用户 openstack role add --project service --user neutron admin 7.3.3 创建 neutron 服务实体 $ openstack service create --name neutron \\ --description \"OpenStack Networking\" network +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Networking | | enabled | True | | id | 9e74ccbdaa85421894cf61c97f355dc7 | | name | neutron | | type | network | +-------------+----------------------------------+ 7.4 创建网络服务API端点 $ openstack endpoint create --region RegionOne \\ network public http://controller:9696 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | abe8c37741934ade89308da46501ea03 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 9e74ccbdaa85421894cf61c97f355dc7 | | service_name | neutron | | service_type | network | | url | http://controller:9696 | +--------------+----------------------------------+ $ openstack endpoint create --region RegionOne \\ network internal http://controller:9696 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 0c32f6cb44a74ec5b653ba79153e3d68 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 9e74ccbdaa85421894cf61c97f355dc7 | | service_name | neutron | | service_type | network | | url | http://controller:9696 | +--------------+----------------------------------+ $ openstack endpoint create --region RegionOne \\ network admin http://controller:9696 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | d2c77ff079c94591bc8ea0b4e51be936 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 9e74ccbdaa85421894cf61c97f355dc7 | | service_name | neutron | | service_type | network | | url | http://controller:9696 | +--------------+----------------------------------+ 安装和配置控制节点 7.5 配置网络选项 官网关于两种网络的说明 您可以使用选项1和2表示的两种体系结构之一来部署网络服务。 选项1部署了最简单的架构，该架构仅支持将实例附加到提供程序（外部）网络。没有自助服务（专用）网络，路由器或浮动IP地址。只有admin或其他特权用户可以管理提供商网络。 选项2通过支持将实例附加到自助服务网络的第3层服务增强了选项1。该demo非特权用户或其他非特权用户可以管理自助服务网络，包括在自助服务网络与提供商网络之间提供连接的路由器。此外，浮动IP地址使用自助服务网络从外部网络（例如Internet）提供到实例的连接。 自助服务网络通常使用覆盖网络。诸如VXLAN之类的覆盖网络协议包括其他标头，这些标头增加了开销并减少了可用于有效负载或用户数据的空间。在不了解虚拟网络基础结构的情况下，实例尝试使用默认的1500字节以太网最大传输单元（MTU）发送数据包。网络服务会通过DHCP自动为实例提供正确的MTU值。但是，某些云映像不使用DHCP或忽略DHCP MTU选项，而是需要使用元数据或脚本进行配置。 网络选项1:提供商网络 网络选项2:自助服务网络 以上两种网络任选一种完成后返回这里配置元数据代理 这里选择网路选项1 7.5.1 安装软件包 yum -y install openstack-neutron openstack-neutron-ml2 \\ openstack-neutron-linuxbridge ebtables 7.5.2 编辑 /etc/neutron/neutron.conf 文件并完成以下操作 1.在该[database]部分中，配置数据库访问 [database] # ... connection = mysql+pymysql://neutron:NEUTRON_DBPASS@controller/neutron 2.在该[DEFAULT]部分中，启用模块化第2层（ML2）插件并禁用其他插件： [DEFAULT] # ... core_plugin = ml2 service_plugins = 3.在该[DEFAULT]部分中，配置RabbitMQ 消息队列访问： [DEFAULT] # ... transport_url = rabbit://openstack:RABBIT_PASS@controller 4.在[DEFAULT]和[keystone_authtoken]部分中，配置身份服务访问 [DEFAULT] # ... auth_strategy = keystone [keystone_authtoken] # ... www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = NEUTRON_PASS 5.在[DEFAULT]和[nova]部分中，将网络配置为通知Compute网络拓扑更改 [DEFAULT] # ... notify_nova_on_port_status_changes = true notify_nova_on_port_data_changes = true [nova] # ... auth_url = http://controller:5000 auth_type = password project_domain_name = default user_domain_name = default region_name = RegionOne project_name = service username = nova password = NOVA_PASS 6.在该[oslo_concurrency]部分中，配置锁定路径 [oslo_concurrency] # ... lock_path = /var/lib/neutron/tmp 用以下命令修改 \\cp /etc/neutron/neutron.conf{,.bak} grep '^[a-Z\\[]' /etc/neutron/neutron.conf.bak >/etc/neutron/neutron.conf openstack-config --set /etc/neutron/neutron.conf database connection mysql+pymysql://neutron:NEUTRON_DBPASS@controller/neutron openstack-config --set /etc/neutron/neutron.conf DEFAULT core_plugin ml2 openstack-config --set /etc/neutron/neutron.conf DEFAULT service_plugins openstack-config --set /etc/neutron/neutron.conf DEFAULT transport_url rabbit://openstack:RABBIT_PASS@controller openstack-config --set /etc/neutron/neutron.conf DEFAULT auth_strategy keystone openstack-config --set /etc/neutron/neutron.conf DEFAULT notify_nova_on_port_status_changes true openstack-config --set /etc/neutron/neutron.conf DEFAULT notify_nova_on_port_data_changes true openstack-config --set /etc/neutron/neutron.conf keystone_authtoken www_authenticate_uri http://controller:5000 openstack-config --set /etc/neutron/neutron.conf keystone_authtoken www_authenticate_uri http://controller:5000 openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_url http://controller:5000 openstack-config --set /etc/neutron/neutron.conf keystone_authtoken memcached_servers controller:11211 openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_type password openstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_domain_name default openstack-config --set /etc/neutron/neutron.conf keystone_authtoken user_domain_name default openstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_name service openstack-config --set /etc/neutron/neutron.conf keystone_authtoken username neutron openstack-config --set /etc/neutron/neutron.conf keystone_authtoken password NEUTRON_PASS openstack-config --set /etc/neutron/neutron.conf nova auth_url http://controller:5000 openstack-config --set /etc/neutron/neutron.conf nova auth_type password openstack-config --set /etc/neutron/neutron.conf nova project_domain_name default openstack-config --set /etc/neutron/neutron.conf nova user_domain_name default openstack-config --set /etc/neutron/neutron.conf nova region_name RegionOne openstack-config --set /etc/neutron/neutron.conf nova project_name service openstack-config --set /etc/neutron/neutron.conf nova username nova openstack-config --set /etc/neutron/neutron.conf nova password NOVA_PASS openstack-config --set /etc/neutron/neutron.conf oslo_concurrency lock_path /var/lib/neutron/tmp 文件md5值 $ md5sum /etc/neutron/neutron.conf 1c4b4339f83596fa6bfdbec7a622a35e /etc/neutron/neutron.conf 7.6 编辑 /etc/neutron/plugins/ml2/ml2_conf.ini 文件并完成以下操作 ML2插件使用Linux桥接机制为实例构建第2层（桥接和交换）虚拟网络基础架构 1.在本[ml2]节中，启用平面和VLAN网络 [ml2] # ... type_drivers = flat,vlan 2.在该[ml2]部分中，禁用自助服务网络： [ml2] # ... tenant_network_types = 3.在本[ml2]节中，启用Linux桥接机制： [ml2] # ... mechanism_drivers = linuxbridge 4.在此[ml2]部分中，启用端口安全扩展驱动程序： [ml2] # ... extension_drivers = port_security 5.在本[ml2_type_flat]节中，将提供者虚拟网络配置为平面网络： [ml2_type_flat] # ... flat_networks = provider 6.在本[securitygroup]节中，启用ipset以提高安全组规则的效率： [securitygroup] # ... enable_ipset = true 用以下命令修改 \\cp /etc/neutron/plugins/ml2/ml2_conf.ini{,.bak} grep '^[a-Z\\[]' /etc/neutron/plugins/ml2/ml2_conf.ini.bak >/etc/neutron/plugins/ml2/ml2_conf.ini openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 type_drivers flat,vlan openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 tenant_network_types openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 mechanism_drivers linuxbridge openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 extension_drivers port_security openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_flat flat_networks provider openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup enable_ipset true 文件md5值 $ md5sum /etc/neutron/plugins/ml2/ml2_conf.ini eb38c10cfd26c1cc308a050c9a5d8aa1 /etc/neutron/plugins/ml2/ml2_conf.ini 7.7 配置linux桥接代理 7.7.1 编辑/etc/neutron/plugins/ml2/linuxbridge_agent.ini文件并完成以下操作 Linux网桥代理为实例构建第2层（桥接和交换）虚拟网络基础架构并处理安全组 1.在本[linux_bridge]节中，将提供者虚拟网络映射到提供者物理网络接口： [linux_bridge] physical_interface_mappings = provider:PROVIDER_INTERFACE_NAME 替换PROVIDER_INTERFACE_NAME为基础提供商物理网络接口的名称。这里是eth0 2.在该[vxlan]部分中，禁用VXLAN覆盖网络： [vxlan] enable_vxlan = false 3.在该[securitygroup]部分中，启用安全组并配置Linux网桥iptables防火墙驱动程序： [securitygroup] # ... enable_security_group = true firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver 通过验证以下所有sysctl值是否设置为确保Linux操作系统内核支持网桥过滤器1： net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables 要启用网络桥接器支持，通常br_netfilter需要加载内核模块。查看操作系统的文档，以获取有关启用此模块的其他详细信息。 用以下命令修改 \\cp /etc/neutron/plugins/ml2/linuxbridge_agent.ini{,.bak} grep '^[a-Z\\[]' /etc/neutron/plugins/ml2/linuxbridge_agent.ini.bak >/etc/neutron/plugins/ml2/linuxbridge_agent.ini openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini linux_bridge physical_interface_mappings provider:eth0 openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini vxlan enable_vxlan false openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini securitygroup enable_security_group true openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini securitygroup firewall_driver neutron.agent.linux.iptables_firewall.IptablesFirewallDriver 文件md5值 $ md5sum /etc/neutron/plugins/ml2/linuxbridge_agent.ini 794b19995c83e2fc0c3fd42f506904f1 /etc/neutron/plugins/ml2/linuxbridge_agent.ini 7.7.2 使Linux操作系统内核支持网桥过滤器1 向 /etc/sysctl.d/openstack-rocky-bridge.conf 写入以下内容 cat >> /etc/sysctl.d/openstack-rocky-bridge.conf 执行以下命令生效 $ modprobe br_netfilter && sysctl -p /etc/sysctl.d/openstack-rocky-bridge.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 7.8 配置DHCP代理 DHCP代理为虚拟网络提供DHCP服务 编辑 /etc/neutron/dhcp_agent.ini 文件并完成以下操作 在本[DEFAULT]节中，配置Linux桥接口驱动程序Dnsmasq DHCP驱动程序，并启用隔离的元数据，以便提供商网络上的实例可以通过网络访问元数据 [DEFAULT] # ... interface_driver = linuxbridge dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq enable_isolated_metadata = true 用以下命令修改 \\cp /etc/neutron/dhcp_agent.ini{,.bak} grep '^[a-Z\\[]' /etc/neutron/dhcp_agent.ini.bak >/etc/neutron/dhcp_agent.ini openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT interface_driver linuxbridge openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT dhcp_driver neutron.agent.linux.dhcp.Dnsmasq openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT enable_isolated_metadata true 文件md5值 $ md5sum /etc/neutron/dhcp_agent.ini 33a1e93e1853796070d5da0773496665 /etc/neutron/dhcp_agent.ini 7.9 配置元数据代理 所述元数据代理提供配置信息的诸如凭据实例。 编辑 /etc/neutron/metadata_agent.ini 文件并完成以下操作 在该[DEFAULT]部分中，配置元数据主机和共享机密： [DEFAULT] # ... nova_metadata_host = controller metadata_proxy_shared_secret = METADATA_SECRET 替换METADATA_SECRET为元数据代理的适当机密 用以下命令修改 \\cp /etc/neutron/metadata_agent.ini{,.bak} grep '^[a-Z\\[]' /etc/neutron/metadata_agent.ini.bak >/etc/neutron/metadata_agent.ini openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT nova_metadata_host controller openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT metadata_proxy_shared_secret METADATA_SECRET 文件md5值 $ md5sum /etc/neutron/metadata_agent.ini e8b90a011b94fece31d33edfd8bc72b6 /etc/neutron/metadata_agent.ini 7.10 配置计算以使用网络 编辑 /etc/nova/nova.conf 文件并执行以下操作 在该[neutron]部分中，配置访问参数，启用元数据代理，并配置机密： [neutron] # ... url = http://controller:9696 auth_url = http://controller:5000 auth_type = password project_domain_name = default user_domain_name = default region_name = RegionOne project_name = service username = neutron password = NEUTRON_PASS service_metadata_proxy = true metadata_proxy_shared_secret = METADATA_SECRET 用以下命令修改 openstack-config --set /etc/nova/nova.conf neutron url http://controller:9696 openstack-config --set /etc/nova/nova.conf neutron auth_url http://controller:5000 openstack-config --set /etc/nova/nova.conf neutron auth_type password openstack-config --set /etc/nova/nova.conf neutron project_domain_name default openstack-config --set /etc/nova/nova.conf neutron user_domain_name default openstack-config --set /etc/nova/nova.conf neutron region_name RegionOne openstack-config --set /etc/nova/nova.conf neutron project_name service openstack-config --set /etc/nova/nova.conf neutron username neutron openstack-config --set /etc/nova/nova.conf neutron password NEUTRON_PASS openstack-config --set /etc/nova/nova.conf neutron service_metadata_proxy true openstack-config --set /etc/nova/nova.conf neutron metadata_proxy_shared_secret METADATA_SECRET 文件md5值 $ md5sum /etc/nova/nova.conf 81feca9d18ee91397cc973d455bfa271 /etc/nova/nova.conf 7.11 完成安装 7.11.1 创建链接文件 网络服务初始化脚本需要 /etc/neutron/plugin.ini 指向ML2插件配置文件的符号链接 /etc/neutron/plugins/ml2/ml2_conf.ini。如果此符号链接不存在，请使用以下命令创建它 ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini 7.11.2 同步数据库，最后提示OK即为正确 su -s /bin/sh -c \"neutron-db-manage --config-file /etc/neutron/neutron.conf \\ --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head\" neutron 7.11.3 重新启动Compute API服务 systemctl restart openstack-nova-api.service 7.11.4 启动网络服务并将其配置为在系统引导时启动 对于官网中的两种网络，这里选择的是第一种网络 systemctl enable neutron-server.service \\ neutron-linuxbridge-agent.service neutron-dhcp-agent.service \\ neutron-metadata-agent.service systemctl start neutron-server.service \\ neutron-linuxbridge-agent.service neutron-dhcp-agent.service \\ neutron-metadata-agent.service 7.11.5 验证 # 启动服务后提示如下即为正确，alive处都为笑脸 $ neutron agent-list neutron CLI is deprecated and will be removed in the future. Use openstack CLI instead. +--------------------------------------+--------------------+------------+-------------------+-------+----------------+---------------------------+ | id | agent_type | host | availability_zone | alive | admin_state_up | binary | +--------------------------------------+--------------------+------------+-------------------+-------+----------------+---------------------------+ | beffcac6-745e-449f-bad8-7f2e4fa973f2 | Linux bridge agent | controller | | :-) | True | neutron-linuxbridge-agent | +--------------------------------------+--------------------+------------+-------------------+-------+----------------+---------------------------+ 安装和配置计算节点 rocky版计算节点网络服务neutron安装配置官方文档 7.12 安装包 yum -y install openstack-neutron-linuxbridge ebtables ipset openstack-utils 7.13 配置公共组件 编辑 /etc/neutron/neutron.conf 文件并完成以下操作 1.在该[DEFAULT]部分中，配置RabbitMQ 消息队列访问： [DEFAULT] # ... transport_url = rabbit://openstack:RABBIT_PASS@controller 2.在[DEFAULT]和[keystone_authtoken]部分中，配置身份服务访问： [DEFAULT] # ... auth_strategy = keystone [keystone_authtoken] # ... www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = NEUTRON_PASS 3.在该[oslo_concurrency]部分中，配置锁定路径： [oslo_concurrency] # ... lock_path = /var/lib/neutron/tmp #用以下命令修改 \\cp /etc/neutron/neutron.conf{,.bak} grep '^[a-Z\\[]' /etc/neutron/neutron.conf.bak >/etc/neutron/neutron.conf openstack-config --set /etc/neutron/neutron.conf DEFAULT transport_url rabbit://openstack:RABBIT_PASS@controller openstack-config --set /etc/neutron/neutron.conf DEFAULT auth_strategy keystone openstack-config --set /etc/neutron/neutron.conf keystone_authtoken www_authenticate_uri http://controller:5000 openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_url http://controller:5000 openstack-config --set /etc/neutron/neutron.conf keystone_authtoken memcached_servers controller:11211 openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_type password openstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_domain_name default openstack-config --set /etc/neutron/neutron.conf keystone_authtoken user_domain_name default openstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_name service openstack-config --set /etc/neutron/neutron.conf keystone_authtoken username neutron openstack-config --set /etc/neutron/neutron.conf keystone_authtoken password NEUTRON_PASS openstack-config --set /etc/neutron/neutron.conf oslo_concurrency lock_path /var/lib/neutron/tmp 用以下命令修改 \\cp /etc/neutron/neutron.conf{,.bak} grep '^[a-Z\\[]' /etc/neutron/neutron.conf.bak >/etc/neutron/neutron.conf openstack-config --set /etc/neutron/neutron.conf DEFAULT transport_url rabbit://openstack:RABBIT_PASS@controller openstack-config --set /etc/neutron/neutron.conf DEFAULT auth_strategy keystone openstack-config --set /etc/neutron/neutron.conf keystone_authtoken www_authenticate_uri http://controller:5000 openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_url http://controller:5000 openstack-config --set /etc/neutron/neutron.conf keystone_authtoken memcached_servers controller:11211 openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_type password openstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_domain_name default openstack-config --set /etc/neutron/neutron.conf keystone_authtoken user_domain_name default openstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_name service openstack-config --set /etc/neutron/neutron.conf keystone_authtoken username neutron openstack-config --set /etc/neutron/neutron.conf keystone_authtoken password NEUTRON_PASS openstack-config --set /etc/neutron/neutron.conf oslo_concurrency lock_path /var/lib/neutron/tmp 文件md5值 $ md5sum /etc/neutron/neutron.conf 9c47ffb59b23516b59e7de84a39bcbe8 /etc/neutron/neutron.conf 7.14 配置网络选项 7.14.1 配置桥接代理 Linux网桥代理为实例构建第2层（桥接和交换）虚拟网络基础结构并处理安全组 7.14.1.1 编辑 /etc/neutron/plugins/ml2/linuxbridge_agent.ini 文件并完成以下操作 1.在本[linux_bridge]节中，将提供者虚拟网络映射到提供者物理网络接口： [linux_bridge] physical_interface_mappings = provider:PROVIDER_INTERFACE_NAME 替换PROVIDER_INTERFACE_NAME为基础提供商物理网络接口的名称。这里为eth0 2.在该[vxlan]部分中，禁用VXLAN覆盖网络： [vxlan] enable_vxlan = false 3.在该[securitygroup]部分中，启用安全组并配置Linux网桥iptables防火墙驱动程序： [securitygroup] # ... enable_security_group = true firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver 用以下命令修改 \\cp /etc/neutron/plugins/ml2/linuxbridge_agent.ini{,.bak} grep '^[a-Z\\[]' /etc/neutron/plugins/ml2/linuxbridge_agent.ini.bak >/etc/neutron/plugins/ml2/linuxbridge_agent.ini openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini linux_bridge physical_interface_mappings provider:eth0 openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini vxlan enable_vxlan false openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini securitygroup enable_security_group true openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini securitygroup firewall_driver neutron.agent.linux.iptables_firewall.IptablesFirewallDriver 文件md5值 $ md5sum /etc/neutron/plugins/ml2/linuxbridge_agent.ini 794b19995c83e2fc0c3fd42f506904f1 /etc/neutron/plugins/ml2/linuxbridge_agent.ini 7.14.1.2 使Linux操作系统内核支持网桥过滤器1 向 /etc/sysctl.d/openstack-rocky-bridge.conf 写入以下内容 cat >> /etc/sysctl.d/openstack-rocky-bridge.conf 执行以下命令生效 $ modprobe br_netfilter && sysctl -p /etc/sysctl.d/openstack-rocky-bridge.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 7.14.2 配置计算以使用网络 编辑 /etc/nova/nova.conf 文件并完成以下操作 在该[neutron]部分中，配置访问参数： [neutron] # ... url = http://controller:9696 auth_url = http://controller:5000 auth_type = password project_domain_name = default user_domain_name = default region_name = RegionOne project_name = service username = neutron password = NEUTRON_PASS 用以下命令修改 openstack-config --set /etc/nova/nova.conf neutron url http://controller:9696 openstack-config --set /etc/nova/nova.conf neutron auth_url http://controller:5000 openstack-config --set /etc/nova/nova.conf neutron auth_type password openstack-config --set /etc/nova/nova.conf neutron project_domain_name default openstack-config --set /etc/nova/nova.conf neutron user_domain_name default openstack-config --set /etc/nova/nova.conf neutron region_name RegionOne openstack-config --set /etc/nova/nova.conf neutron project_name service openstack-config --set /etc/nova/nova.conf neutron username neutron openstack-config --set /etc/nova/nova.conf neutron password NEUTRON_PASS 文件md5值 $ md5sum /etc/nova/nova.conf 9b96b21ae709f89c96cc559018ba7058 /etc/nova/nova.conf 7.15 完成安装 7.15.1 重新启动Compute服务 systemctl restart openstack-nova-compute.service 7.15.2 启动Linux网桥代理并将其配置为在系统引导时启动 systemctl enable neutron-linuxbridge-agent.service systemctl start neutron-linuxbridge-agent.service 7.16 验证 控制节点执行 输出应指示控制器节点上的三个代理，每个计算节点上的一个代理 $ openstack network agent list +--------------------------------------+--------------------+------------+-------------------+-------+-------+---------------------------+ | ID | Agent Type | Host | Availability Zone | Alive | State | Binary | +--------------------------------------+--------------------+------------+-------------------+-------+-------+---------------------------+ | 42cfe05b-0a9c-40ce-8f99-06ba76938c50 | Linux bridge agent | controller | None | :-) | UP | neutron-linuxbridge-agent | | 749cb43f-a5db-4918-a3f5-8765e92e851c | DHCP agent | controller | nova | :-) | UP | neutron-dhcp-agent | | 856ecf5f-6018-4ac4-a66b-f6f88784db0e | Metadata agent | controller | None | :-) | UP | neutron-metadata-agent | | b5c2309c-fefc-46d0-b98e-37b05861095c | Linux bridge agent | compute1 | None | :-) | UP | neutron-linuxbridge-agent | +--------------------------------------+--------------------+------------+-------------------+-------+-------+---------------------------+ 到此，控制节点、计算节点网络服务neutron安装完成！！！ 8.控制节点horizon web界面Dashboard安装 rocky版控制节点horizon web节点dashboard安装配置官方文档 horizon插件注册表官方文档 8.1 安装包 yum -y install openstack-dashboard 8.2 编辑 /etc/openstack-dashboard/local_settings 文件并完成以下操作 ⚠️因为有一些内容是删除之后粘贴的，所以一些行数并不是很准确，但是行数误差不超过3行 1.配置仪表板以在controller节点上使用OpenStack服务 ： 184行 OPENSTACK_HOST = \"127.0.0.1\" 修改为 OPENSTACK_HOST = \"controller\" 2.允许主机访问仪表板： ALLOWED_HOSTS也可以是['*']以接受所有主机。这对于开发工作可能有用，但是可能不安全，因此不应在生产中使用。有关 更多信息，请参见 https://docs.djangoproject.com/en/dev/ref/settings/#allowed-hosts。 38行 ALLOWED_HOSTS = ['one.example.com', 'two.example.com'] 修改为 ALLOWED_HOSTS = ['*', ] 3.配置memcached会话存储服务： 161行，CACHES上加一行 SESSION_ENGINE，并且修改为如下内容 SESSION_ENGINE = 'django.contrib.sessions.backends.cache' CACHES = { 'default': { 'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache', 'LOCATION': 'controller:11211', } } 4.启用身份API版本3： 187行，不用修改 OPENSTACK_KEYSTONE_URL = \"http://%s:5000/v3\" % OPENSTACK_HOST 5.启用对域的支持： 75行 #OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = False 修改为 OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True 6.配置API版本： 64行，原先为注释， #OPENSTACK_API_VERSIONS = { # \"data-processing\": 1.1, # \"identity\": 3, # \"image\": 2, # \"volume\": 2, # \"compute\": 2, #} 修改为如下 OPENSTACK_API_VERSIONS = { \"identity\": 3, \"image\": 2, \"volume\": 2, } 7.配置Default为通过仪表板创建的用户的默认域： 95行 #OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = 'Default' 修改为 OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = \"Default\" 8.配置user为通过仪表板创建的用户的默认角色： 186行 OPENSTACK_KEYSTONE_DEFAULT_ROLE = \"_member_\" 修改为 OPENSTACK_KEYSTONE_DEFAULT_ROLE = \"user\" 9.如果选择网络选项1，请禁用对第3层网络服务的支持： 324行 OPENSTACK_NEUTRON_NETWORK = { 'enable_router': True, 'enable_quotas': True, 'enable_ipv6': True, 'enable_distributed_router': False, 'enable_ha_router': False, 'enable_fip_topology_check': True, 修改为如下 OPENSTACK_NEUTRON_NETWORK = { 'enable_router': False, 'enable_quotas': False, 'enable_distributed_router': False, 'enable_ha_router': False, 'enable_lb': False, 'enable_firewall': False, 'enable_vpn': False, 'enable_fip_topology_check': False, 10.配置时区： 467行 TIME_ZONE = \"TIME_ZONE\" 修改为 TIME_ZONE = \"Asia/Shanghai\" 备份一下文件 \\cp /etc/openstack-dashboard/local_settings{,.bak} 采用 cat EOF 方式可能会有格式问题，因为文件内容太多了，这里必须使用vi打开文件然后复制粘贴内容，不能使用vim会有格式错误 /etc/openstack-dashboard/local_settings文件内容 # -*- coding: utf-8 -*- import os from django.utils.translation import ugettext_lazy as _ from openstack_dashboard.settings import HORIZON_CONFIG DEBUG = False # This setting controls whether or not compression is enabled. Disabling # compression makes Horizon considerably slower, but makes it much easier # to debug JS and CSS changes #COMPRESS_ENABLED = not DEBUG # This setting controls whether compression happens on the fly, or offline # with `python manage.py compress` # See https://django-compressor.readthedocs.io/en/latest/usage/#offline-compression # for more information #COMPRESS_OFFLINE = not DEBUG # WEBROOT is the location relative to Webserver root # should end with a slash. WEBROOT = '/dashboard/' #LOGIN_URL = WEBROOT + 'auth/login/' #LOGOUT_URL = WEBROOT + 'auth/logout/' # # LOGIN_REDIRECT_URL can be used as an alternative for # HORIZON_CONFIG.user_home, if user_home is not set. # Do not set it to '/home/', as this will cause circular redirect loop #LOGIN_REDIRECT_URL = WEBROOT # If horizon is running in production (DEBUG is False), set this # with the list of host/domain names that the application can serve. # For more information see: # https://docs.djangoproject.com/en/dev/ref/settings/#allowed-hosts ALLOWED_HOSTS = ['*', ] # Set SSL proxy settings: # Pass this header from the proxy after terminating the SSL, # and don't forget to strip it from the client's request. # For more information see: # https://docs.djangoproject.com/en/dev/ref/settings/#secure-proxy-ssl-header #SECURE_PROXY_SSL_HEADER = ('HTTP_X_FORWARDED_PROTO', 'https') # If Horizon is being served through SSL, then uncomment the following two # settings to better secure the cookies from security exploits #CSRF_COOKIE_SECURE = True #SESSION_COOKIE_SECURE = True # The absolute path to the directory where message files are collected. # The message file must have a .json file extension. When the user logins to # horizon, the message files collected are processed and displayed to the user. #MESSAGES_PATH=None # Overrides for OpenStack API versions. Use this setting to force the # OpenStack dashboard to use a specific API version for a given service API. # Versions specified here should be integers or floats, not strings. # NOTE: The version should be formatted as it appears in the URL for the # service API. For example, The identity service APIs have inconsistent # use of the decimal point, so valid options would be 2.0 or 3. # Minimum compute version to get the instance locked status is 2.9. OPENSTACK_API_VERSIONS = { \"identity\": 3, \"image\": 2, \"volume\": 2, } # Set this to True if running on a multi-domain model. When this is enabled, it # will require the user to enter the Domain name in addition to the username # for login. OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True # Set this to True if you want available domains displayed as a dropdown menu # on the login screen. It is strongly advised NOT to enable this for public # clouds, as advertising enabled domains to unauthenticated customers # irresponsibly exposes private information. This should only be used for # private clouds where the dashboard sits behind a corporate firewall. #OPENSTACK_KEYSTONE_DOMAIN_DROPDOWN = False # If OPENSTACK_KEYSTONE_DOMAIN_DROPDOWN is enabled, this option can be used to # set the available domains to choose from. This is a list of pairs whose first # value is the domain name and the second is the display name. #OPENSTACK_KEYSTONE_DOMAIN_CHOICES = ( # ('Default', 'Default'), #) # Overrides the default domain used when running on single-domain model # with Keystone V3. All entities will be created in the default domain. # NOTE: This value must be the name of the default domain, NOT the ID. # Also, you will most likely have a value in the keystone policy file like this # \"cloud_admin\": \"rule:admin_required and domain_id:\" # This value must be the name of the domain whose ID is specified there. OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = 'Default' # Set this to True to enable panels that provide the ability for users to # manage Identity Providers (IdPs) and establish a set of rules to map # federation protocol attributes to Identity API attributes. # This extension requires v3.0+ of the Identity API. #OPENSTACK_KEYSTONE_FEDERATION_MANAGEMENT = False # Set Console type: # valid options are \"AUTO\"(default), \"VNC\", \"SPICE\", \"RDP\", \"SERIAL\", \"MKS\" # or None. Set to None explicitly if you want to deactivate the console. #CONSOLE_TYPE = \"AUTO\" # Toggle showing the openrc file for Keystone V2. # If set to false the link will be removed from the user dropdown menu # and the API Access page #SHOW_KEYSTONE_V2_RC = True # If provided, a \"Report Bug\" link will be displayed in the site header # which links to the value of this setting (ideally a URL containing # information on how to report issues). #HORIZON_CONFIG[\"bug_url\"] = \"http://bug-report.example.com\" # Show backdrop element outside the modal, do not close the modal # after clicking on backdrop. #HORIZON_CONFIG[\"modal_backdrop\"] = \"static\" # Specify a regular expression to validate user passwords. #HORIZON_CONFIG[\"password_validator\"] = { # \"regex\": '.*', # \"help_text\": _(\"Your password does not meet the requirements.\"), #} # Turn off browser autocompletion for forms including the login form and # the database creation workflow if so desired. #HORIZON_CONFIG[\"password_autocomplete\"] = \"off\" # Setting this to True will disable the reveal button for password fields, # including on the login form. #HORIZON_CONFIG[\"disable_password_reveal\"] = False LOCAL_PATH = '/tmp' # Set custom secret key: # You can either set it to a specific value or you can let horizon generate a # default secret key that is unique on this machine, e.i. regardless of the # amount of Python WSGI workers (if used behind Apache+mod_wsgi): However, # there may be situations where you would want to set this explicitly, e.g. # when multiple dashboard instances are distributed on different machines # (usually behind a load-balancer). Either you have to make sure that a session # gets all requests routed to the same dashboard instance or you set the same # SECRET_KEY for all of them. SECRET_KEY='f9ed41e34c2b04178998' # We recommend you use memcached for development; otherwise after every reload # of the django development server, you will have to login again. To use # memcached set CACHES to something like #CACHES = { # 'default': { # 'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache', # 'LOCATION': '127.0.0.1:11211', # }, #} SESSION_ENGINE = 'django.contrib.sessions.backends.cache' CACHES = { 'default': { 'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache', 'LOCATION': 'controller:11211', } } # Send email to the console by default EMAIL_BACKEND = 'django.core.mail.backends.console.EmailBackend' # Or send them to /dev/null #EMAIL_BACKEND = 'django.core.mail.backends.dummy.EmailBackend' # Configure these for your outgoing email host #EMAIL_HOST = 'smtp.my-company.com' #EMAIL_PORT = 25 #EMAIL_HOST_USER = 'djangomail' #EMAIL_HOST_PASSWORD = 'top-secret!' # For multiple regions uncomment this configuration, and add (endpoint, title). #AVAILABLE_REGIONS = [ # ('http://cluster1.example.com:5000/v3', 'cluster1'), # ('http://cluster2.example.com:5000/v3', 'cluster2'), #] OPENSTACK_HOST = \"controller\" OPENSTACK_KEYSTONE_URL = \"http://%s:5000/v3\" % OPENSTACK_HOST OPENSTACK_KEYSTONE_DEFAULT_ROLE = \"user\" # For setting the default service region on a per-endpoint basis. Note that the # default value for this setting is {}, and below is just an example of how it # should be specified. # A key of '*' is an optional global default if no other key matches. #DEFAULT_SERVICE_REGIONS = { # '*': 'RegionOne' # OPENSTACK_KEYSTONE_URL: 'RegionTwo' #} # Enables keystone web single-sign-on if set to True. #WEBSSO_ENABLED = False # Authentication mechanism to be selected as default. # The value must be a key from WEBSSO_CHOICES. #WEBSSO_INITIAL_CHOICE = \"credentials\" # The list of authentication mechanisms which include keystone # federation protocols and identity provider/federation protocol # mapping keys (WEBSSO_IDP_MAPPING). Current supported protocol # IDs are 'saml2' and 'oidc' which represent SAML 2.0, OpenID # Connect respectively. # Do not remove the mandatory credentials mechanism. # Note: The last two tuples are sample mapping keys to a identity provider # and federation protocol combination (WEBSSO_IDP_MAPPING). #WEBSSO_CHOICES = ( # (\"credentials\", _(\"Keystone Credentials\")), # (\"oidc\", _(\"OpenID Connect\")), # (\"saml2\", _(\"Security Assertion Markup Language\")), # (\"acme_oidc\", \"ACME - OpenID Connect\"), # (\"acme_saml2\", \"ACME - SAML2\"), #) # A dictionary of specific identity provider and federation protocol # combinations. From the selected authentication mechanism, the value # will be looked up as keys in the dictionary. If a match is found, # it will redirect the user to a identity provider and federation protocol # specific WebSSO endpoint in keystone, otherwise it will use the value # as the protocol_id when redirecting to the WebSSO by protocol endpoint. # NOTE: The value is expected to be a tuple formatted as: (, ). #WEBSSO_IDP_MAPPING = { # \"acme_oidc\": (\"acme\", \"oidc\"), # \"acme_saml2\": (\"acme\", \"saml2\"), #} # If set this URL will be used for web single-sign-on authentication # instead of OPENSTACK_KEYSTONE_URL. This is needed in the deployment # scenarios where network segmentation is used per security requirement. # In this case, the controllers are not reachable from public network. # Therefore, user's browser will not be able to access OPENSTACK_KEYSTONE_URL # if it is set to the internal endpoint. #WEBSSO_KEYSTONE_URL = \"http://keystone-public.example.com/v3\" # The Keystone Provider drop down uses Keystone to Keystone federation # to switch between Keystone service providers. # Set display name for Identity Provider (dropdown display name) #KEYSTONE_PROVIDER_IDP_NAME = \"Local Keystone\" # This id is used for only for comparison with the service provider IDs. This ID # should not match any service provider IDs. #KEYSTONE_PROVIDER_IDP_ID = \"localkeystone\" # Disable SSL certificate checks (useful for self-signed certificates): #OPENSTACK_SSL_NO_VERIFY = True # The CA certificate to use to verify SSL connections #OPENSTACK_SSL_CACERT = '/path/to/cacert.pem' # The OPENSTACK_KEYSTONE_BACKEND settings can be used to identify the # capabilities of the auth backend for Keystone. # If Keystone has been configured to use LDAP as the auth backend then set # can_edit_user to False and name to 'ldap'. # # TODO(tres): Remove these once Keystone has an API to identify auth backend. OPENSTACK_KEYSTONE_BACKEND = { 'name': 'native', 'can_edit_user': True, 'can_edit_group': True, 'can_edit_project': True, 'can_edit_domain': True, 'can_edit_role': True, } # Setting this to True, will add a new \"Retrieve Password\" action on instance, # allowing Admin session password retrieval/decryption. #OPENSTACK_ENABLE_PASSWORD_RETRIEVE = False # The Launch Instance user experience has been significantly enhanced. # You can choose whether to enable the new launch instance experience, # the legacy experience, or both. The legacy experience will be removed # in a future release, but is available as a temporary backup setting to ensure # compatibility with existing deployments. Further development will not be # done on the legacy experience. Please report any problems with the new # experience via the Launchpad tracking system. # # Toggle LAUNCH_INSTANCE_LEGACY_ENABLED and LAUNCH_INSTANCE_NG_ENABLED to # determine the experience to enable. Set them both to true to enable # both. #LAUNCH_INSTANCE_LEGACY_ENABLED = True #LAUNCH_INSTANCE_NG_ENABLED = False # A dictionary of settings which can be used to provide the default values for # properties found in the Launch Instance modal. #LAUNCH_INSTANCE_DEFAULTS = { # 'config_drive': False, # 'enable_scheduler_hints': True, # 'disable_image': False, # 'disable_instance_snapshot': False, # 'disable_volume': False, # 'disable_volume_snapshot': False, # 'create_volume': True, #} # The Xen Hypervisor has the ability to set the mount point for volumes # attached to instances (other Hypervisors currently do not). Setting # can_set_mount_point to True will add the option to set the mount point # from the UI. OPENSTACK_HYPERVISOR_FEATURES = { 'can_set_mount_point': False, 'can_set_password': False, 'requires_keypair': False, 'enable_quotas': True } # This settings controls whether IP addresses of servers are retrieved from # neutron in the project instance table. Setting this to ``False`` may mitigate # a performance issue in the project instance table in large deployments. #OPENSTACK_INSTANCE_RETRIEVE_IP_ADDRESSES = True # The OPENSTACK_CINDER_FEATURES settings can be used to enable optional # services provided by cinder that is not exposed by its extension API. OPENSTACK_CINDER_FEATURES = { 'enable_backup': False, } # The OPENSTACK_NEUTRON_NETWORK settings can be used to enable optional # services provided by neutron. Options currently available are load # balancer service, security groups, quotas, VPN service. OPENSTACK_NEUTRON_NETWORK = { 'enable_router': False, 'enable_quotas': False, 'enable_distributed_router': False, 'enable_ha_router': False, 'enable_lb': False, 'enable_firewall': False, 'enable_vpn': False, 'enable_fip_topology_check': False, # Default dns servers you would like to use when a subnet is # created. This is only a default, users can still choose a different # list of dns servers when creating a new subnet. # The entries below are examples only, and are not appropriate for # real deployments # 'default_dns_nameservers': [\"8.8.8.8\", \"8.8.4.4\", \"208.67.222.222\"], # Set which provider network types are supported. Only the network types # in this list will be available to choose from when creating a network. # Network types include local, flat, vlan, gre, vxlan and geneve. # 'supported_provider_types': ['*'], # You can configure available segmentation ID range per network type # in your deployment. # 'segmentation_id_range': { # 'vlan': [1024, 2048], # 'vxlan': [4094, 65536], # }, # You can define additional provider network types here. # 'extra_provider_types': { # 'awesome_type': { # 'display_name': 'Awesome New Type', # 'require_physical_network': False, # 'require_segmentation_id': True, # } # }, # Set which VNIC types are supported for port binding. Only the VNIC # types in this list will be available to choose from when creating a # port. # VNIC types include 'normal', 'direct', 'direct-physical', 'macvtap', # 'baremetal' and 'virtio-forwarder' # Set to empty list or None to disable VNIC type selection. 'supported_vnic_types': ['*'], # Set list of available physical networks to be selected in the physical # network field on the admin create network modal. If it's set to an empty # list, the field will be a regular input field. # e.g. ['default', 'test'] 'physical_networks': [], } # The OPENSTACK_HEAT_STACK settings can be used to disable password # field required while launching the stack. OPENSTACK_HEAT_STACK = { 'enable_user_pass': True, } # The OPENSTACK_IMAGE_BACKEND settings can be used to customize features # in the OpenStack Dashboard related to the Image service, such as the list # of supported image formats. #OPENSTACK_IMAGE_BACKEND = { # 'image_formats': [ # ('', _('Select format')), # ('aki', _('AKI - Amazon Kernel Image')), # ('ami', _('AMI - Amazon Machine Image')), # ('ari', _('ARI - Amazon Ramdisk Image')), # ('docker', _('Docker')), # ('iso', _('ISO - Optical Disk Image')), # ('ova', _('OVA - Open Virtual Appliance')), # ('qcow2', _('QCOW2 - QEMU Emulator')), # ('raw', _('Raw')), # ('vdi', _('VDI - Virtual Disk Image')), # ('vhd', _('VHD - Virtual Hard Disk')), # ('vhdx', _('VHDX - Large Virtual Hard Disk')), # ('vmdk', _('VMDK - Virtual Machine Disk')), # ], #} # The IMAGE_CUSTOM_PROPERTY_TITLES settings is used to customize the titles for # image custom property attributes that appear on image detail pages. IMAGE_CUSTOM_PROPERTY_TITLES = { \"architecture\": _(\"Architecture\"), \"kernel_id\": _(\"Kernel ID\"), \"ramdisk_id\": _(\"Ramdisk ID\"), \"image_state\": _(\"Euca2ools state\"), \"project_id\": _(\"Project ID\"), \"image_type\": _(\"Image Type\"), } # The IMAGE_RESERVED_CUSTOM_PROPERTIES setting is used to specify which image # custom properties should not be displayed in the Image Custom Properties # table. IMAGE_RESERVED_CUSTOM_PROPERTIES = [] # Set to 'legacy' or 'direct' to allow users to upload images to glance via # Horizon server. When enabled, a file form field will appear on the create # image form. If set to 'off', there will be no file form field on the create # image form. See documentation for deployment considerations. #HORIZON_IMAGES_UPLOAD_MODE = 'legacy' # Allow a location to be set when creating or updating Glance images. # If using Glance V2, this value should be False unless the Glance # configuration and policies allow setting locations. #IMAGES_ALLOW_LOCATION = False # A dictionary of default settings for create image modal. #CREATE_IMAGE_DEFAULTS = { # 'image_visibility': \"public\", #} # OPENSTACK_ENDPOINT_TYPE specifies the endpoint type to use for the endpoints # in the Keystone service catalog. Use this setting when Horizon is running # external to the OpenStack environment. The default is 'publicURL'. #OPENSTACK_ENDPOINT_TYPE = \"publicURL\" # SECONDARY_ENDPOINT_TYPE specifies the fallback endpoint type to use in the # case that OPENSTACK_ENDPOINT_TYPE is not present in the endpoints # in the Keystone service catalog. Use this setting when Horizon is running # external to the OpenStack environment. The default is None. This # value should differ from OPENSTACK_ENDPOINT_TYPE if used. #SECONDARY_ENDPOINT_TYPE = None # The number of objects (Swift containers/objects or images) to display # on a single page before providing a paging element (a \"more\" link) # to paginate results. API_RESULT_LIMIT = 1000 API_RESULT_PAGE_SIZE = 20 # The size of chunk in bytes for downloading objects from Swift SWIFT_FILE_TRANSFER_CHUNK_SIZE = 512 * 1024 # The default number of lines displayed for instance console log. INSTANCE_LOG_LENGTH = 35 # Specify a maximum number of items to display in a dropdown. DROPDOWN_MAX_ITEMS = 30 # The timezone of the server. This should correspond with the timezone # of your entire OpenStack installation, and hopefully be in UTC. TIME_ZONE = \"Asia/Shanghai\" # When launching an instance, the menu of available flavors is # sorted by RAM usage, ascending. If you would like a different sort order, # you can provide another flavor attribute as sorting key. Alternatively, you # can provide a custom callback method to use for sorting. You can also provide # a flag for reverse sort. For more info, see # http://docs.python.org/2/library/functions.html#sorted #CREATE_INSTANCE_FLAVOR_SORT = { # 'key': 'name', # # or # 'key': my_awesome_callback_method, # 'reverse': False, #} # Set this to True to display an 'Admin Password' field on the Change Password # form to verify that it is indeed the admin logged-in who wants to change # the password. #ENFORCE_PASSWORD_CHECK = False # Modules that provide /auth routes that can be used to handle different types # of user authentication. Add auth plugins that require extra route handling to # this list. #AUTHENTICATION_URLS = [ # 'openstack_auth.urls', #] # The Horizon Policy Enforcement engine uses these values to load per service # policy rule files. The content of these files should match the files the # OpenStack services are using to determine role based access control in the # target installation. # Path to directory containing policy.json files POLICY_FILES_PATH = '/etc/openstack-dashboard' # Map of local copy of service policy files. # Please insure that your identity policy file matches the one being used on # your keystone servers. There is an alternate policy file that may be used # in the Keystone v3 multi-domain case, policy.v3cloudsample.json. # This file is not included in the Horizon repository by default but can be # found at # http://git.openstack.org/cgit/openstack/keystone/tree/etc/ \\ # policy.v3cloudsample.json # Having matching policy files on the Horizon and Keystone servers is essential # for normal operation. This holds true for all services and their policy files. #POLICY_FILES = { # 'identity': 'keystone_policy.json', # 'compute': 'nova_policy.json', # 'volume': 'cinder_policy.json', # 'image': 'glance_policy.json', # 'network': 'neutron_policy.json', #} # Change this patch to the appropriate list of tuples containing # a key, label and static directory containing two files: # _variables.scss and _styles.scss #AVAILABLE_THEMES = [ # ('default', 'Default', 'themes/default'), # ('material', 'Material', 'themes/material'), #] LOGGING = { 'version': 1, # When set to True this will disable all logging except # for loggers specified in this configuration dictionary. Note that # if nothing is specified here and disable_existing_loggers is True, # django.db.backends will still log unless it is disabled explicitly. 'disable_existing_loggers': False, # If apache2 mod_wsgi is used to deploy OpenStack dashboard # timestamp is output by mod_wsgi. If WSGI framework you use does not # output timestamp for logging, add %(asctime)s in the following # format definitions. 'formatters': { 'console': { 'format': '%(levelname)s %(name)s %(message)s' }, 'operation': { # The format of \"%(message)s\" is defined by # OPERATION_LOG_OPTIONS['format'] 'format': '%(message)s' }, }, 'handlers': { 'null': { 'level': 'DEBUG', 'class': 'logging.NullHandler', }, 'console': { # Set the level to \"DEBUG\" for verbose output logging. 'level': 'INFO', 'class': 'logging.StreamHandler', 'formatter': 'console', }, 'operation': { 'level': 'INFO', 'class': 'logging.StreamHandler', 'formatter': 'operation', }, }, 'loggers': { 'horizon': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'horizon.operation_log': { 'handlers': ['operation'], 'level': 'INFO', 'propagate': False, }, 'openstack_dashboard': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'novaclient': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'cinderclient': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'keystoneauth': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'keystoneclient': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'glanceclient': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'neutronclient': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'swiftclient': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'oslo_policy': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'openstack_auth': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, 'django': { 'handlers': ['console'], 'level': 'DEBUG', 'propagate': False, }, # Logging from django.db.backends is VERY verbose, send to null # by default. 'django.db.backends': { 'handlers': ['null'], 'propagate': False, }, 'requests': { 'handlers': ['null'], 'propagate': False, }, 'urllib3': { 'handlers': ['null'], 'propagate': False, }, 'chardet.charsetprober': { 'handlers': ['null'], 'propagate': False, }, 'iso8601': { 'handlers': ['null'], 'propagate': False, }, 'scss': { 'handlers': ['null'], 'propagate': False, }, }, } # 'direction' should not be specified for all_tcp/udp/icmp. # It is specified in the form. SECURITY_GROUP_RULES = { 'all_tcp': { 'name': _('All TCP'), 'ip_protocol': 'tcp', 'from_port': '1', 'to_port': '65535', }, 'all_udp': { 'name': _('All UDP'), 'ip_protocol': 'udp', 'from_port': '1', 'to_port': '65535', }, 'all_icmp': { 'name': _('All ICMP'), 'ip_protocol': 'icmp', 'from_port': '-1', 'to_port': '-1', }, 'ssh': { 'name': 'SSH', 'ip_protocol': 'tcp', 'from_port': '22', 'to_port': '22', }, 'smtp': { 'name': 'SMTP', 'ip_protocol': 'tcp', 'from_port': '25', 'to_port': '25', }, 'dns': { 'name': 'DNS', 'ip_protocol': 'tcp', 'from_port': '53', 'to_port': '53', }, 'http': { 'name': 'HTTP', 'ip_protocol': 'tcp', 'from_port': '80', 'to_port': '80', }, 'pop3': { 'name': 'POP3', 'ip_protocol': 'tcp', 'from_port': '110', 'to_port': '110', }, 'imap': { 'name': 'IMAP', 'ip_protocol': 'tcp', 'from_port': '143', 'to_port': '143', }, 'ldap': { 'name': 'LDAP', 'ip_protocol': 'tcp', 'from_port': '389', 'to_port': '389', }, 'https': { 'name': 'HTTPS', 'ip_protocol': 'tcp', 'from_port': '443', 'to_port': '443', }, 'smtps': { 'name': 'SMTPS', 'ip_protocol': 'tcp', 'from_port': '465', 'to_port': '465', }, 'imaps': { 'name': 'IMAPS', 'ip_protocol': 'tcp', 'from_port': '993', 'to_port': '993', }, 'pop3s': { 'name': 'POP3S', 'ip_protocol': 'tcp', 'from_port': '995', 'to_port': '995', }, 'ms_sql': { 'name': 'MS SQL', 'ip_protocol': 'tcp', 'from_port': '1433', 'to_port': '1433', }, 'mysql': { 'name': 'MYSQL', 'ip_protocol': 'tcp', 'from_port': '3306', 'to_port': '3306', }, 'rdp': { 'name': 'RDP', 'ip_protocol': 'tcp', 'from_port': '3389', 'to_port': '3389', }, } # Deprecation Notice: # # The setting FLAVOR_EXTRA_KEYS has been deprecated. # Please load extra spec metadata into the Glance Metadata Definition Catalog. # # The sample quota definitions can be found in: # /etc/metadefs/compute-quota.json # # The metadata definition catalog supports CLI and API: # $glance --os-image-api-version 2 help md-namespace-import # $glance-manage db_load_metadefs # # See Metadata Definitions on: # https://docs.openstack.org/glance/latest/user/glancemetadefcatalogapi.html # The hash algorithm to use for authentication tokens. This must # match the hash algorithm that the identity server and the # auth_token middleware are using. Allowed values are the # algorithms supported by Python's hashlib library. #OPENSTACK_TOKEN_HASH_ALGORITHM = 'md5' # AngularJS requires some settings to be made available to # the client side. Some settings are required by in-tree / built-in horizon # features. These settings must be added to REST_API_REQUIRED_SETTINGS in the # form of ['SETTING_1','SETTING_2'], etc. # # You may remove settings from this list for security purposes, but do so at # the risk of breaking a built-in horizon feature. These settings are required # for horizon to function properly. Only remove them if you know what you # are doing. These settings may in the future be moved to be defined within # the enabled panel configuration. # You should not add settings to this list for out of tree extensions. # See: https://wiki.openstack.org/wiki/Horizon/RESTAPI REST_API_REQUIRED_SETTINGS = ['OPENSTACK_HYPERVISOR_FEATURES', 'LAUNCH_INSTANCE_DEFAULTS', 'OPENSTACK_IMAGE_FORMATS', 'OPENSTACK_KEYSTONE_BACKEND', 'OPENSTACK_KEYSTONE_DEFAULT_DOMAIN', 'CREATE_IMAGE_DEFAULTS', 'ENFORCE_PASSWORD_CHECK'] # Additional settings can be made available to the client side for # extensibility by specifying them in REST_API_ADDITIONAL_SETTINGS # !! Please use extreme caution as the settings are transferred via HTTP/S # and are not encrypted on the browser. This is an experimental API and # may be deprecated in the future without notice. #REST_API_ADDITIONAL_SETTINGS = [] # DISALLOW_IFRAME_EMBED can be used to prevent Horizon from being embedded # within an iframe. Legacy browsers are still vulnerable to a Cross-Frame # Scripting (XFS) vulnerability, so this option allows extra security hardening # where iframes are not used in deployment. Default setting is True. # For more information see: # http://tinyurl.com/anticlickjack #DISALLOW_IFRAME_EMBED = True # Help URL can be made available for the client. To provide a help URL, edit the # following attribute to the URL of your choice. #HORIZON_CONFIG[\"help_url\"] = \"http://openstack.mycompany.org\" # Settings for OperationLogMiddleware # OPERATION_LOG_ENABLED is flag to use the function to log an operation on # Horizon. # mask_targets is arrangement for appointing a target to mask. # method_targets is arrangement of HTTP method to output log. # format is the log contents. #OPERATION_LOG_ENABLED = False #OPERATION_LOG_OPTIONS = { # 'mask_fields': ['password'], # 'target_methods': ['POST'], # 'ignored_urls': ['/js/', '/static/', '^/api/'], # 'format': (\"[%(client_ip)s] [%(domain_name)s]\" # \" [%(domain_id)s] [%(project_name)s]\" # \" [%(project_id)s] [%(user_name)s] [%(user_id)s] [%(request_scheme)s]\" # \" [%(referer_url)s] [%(request_url)s] [%(message)s] [%(method)s]\" # \" [%(http_status)s] [%(param)s]\"), #} # The default date range in the Overview panel meters - either minus N # days (if the value is integer N), or from the beginning of the current month # until today (if set to None). This setting should be used to limit the amount # of data fetched by default when rendering the Overview panel. #OVERVIEW_DAYS_RANGE = 1 # To allow operators to require users provide a search criteria first # before loading any data into the views, set the following dict # attributes to True in each one of the panels you want to enable this feature. # Follow the convention . #FILTER_DATA_FIRST = { # 'admin.instances': False, # 'admin.images': False, # 'admin.networks': False, # 'admin.routers': False, # 'admin.volumes': False, # 'identity.users': False, # 'identity.projects': False, # 'identity.groups': False, # 'identity.roles': False #} # Dict used to restrict user private subnet cidr range. # An empty list means that user input will not be restricted # for a corresponding IP version. By default, there is # no restriction for IPv4 or IPv6. To restrict # user private subnet cidr range set ALLOWED_PRIVATE_SUBNET_CIDR # to something like #ALLOWED_PRIVATE_SUBNET_CIDR = { # 'ipv4': ['10.0.0.0/8', '192.168.0.0/16'], # 'ipv6': ['fc00::/7'] #} ALLOWED_PRIVATE_SUBNET_CIDR = {'ipv4': [], 'ipv6': []} # Projects and users can have extra attributes as defined by keystone v3. # Horizon has the ability to display these extra attributes via this setting. # If you'd like to display extra data in the project or user tables, set the # corresponding dict key to the attribute name, followed by the display name. # For more information, see horizon's customization # (https://docs.openstack.org/horizon/latest/configuration/customizing.html#horizon-customization-module-overrides) #PROJECT_TABLE_EXTRA_INFO = { # 'phone_num': _('Phone Number'), #} #USER_TABLE_EXTRA_INFO = { # 'phone_num': _('Phone Number'), #} # Password will have an expiration date when using keystone v3 and enabling the # feature. # This setting allows you to set the number of days that the user will be alerted # prior to the password expiration. # Once the password expires keystone will deny the access and users must # contact an admin to change their password. #PASSWORD_EXPIRES_WARNING_THRESHOLD_DAYS = 0 文件md5值 $ md5sum /etc/openstack-dashboard/local_settings 0e53f197affdd94c9e25a4f6f7fdf14b /etc/openstack-dashboard/local_settings 8.3 修改配置文件，否则后续访问dashboard会报500错误 8.3.1 编辑 /etc/httpd/conf.d/openstack-dashboard.conf sed -i.bak '3aWSGIApplicationGroup %{GLOBAL}' /etc/httpd/conf.d/openstack-dashboard.conf 8.3.2 重启httpd和memcache systemctl restart httpd.service memcached.service 8.4 登陆dashboard 172.30.100.4:8080/dashboard 域：default 用户名：admin 密码：ADMIN_PASS 登陆后首界面 如果登陆报错如下 解决方法 在安装dashboard节点做以下操作 1.修改配置文件 /etc/openstack-dashboard/local_settings 修改 SESSION_ENGINE = 'django.contrib.sessions.backends.cache' 修改为 SESSION_ENGINE = 'django.contrib.sessions.backends.file' 2.重启httpd systemctl restart httpd 9.控制节点和存储节点块存储服务cinder安装 rocky版块存储cinder安装配置官方文档 官方对cidner服务的说明 本节介绍如何为块存储服务安装和配置存储节点。为简单起见，此配置引用一个具有空本地块存储设备的存储节点。指令使用/dev/sdb，但是您可以将特定节点替换为其他值。 该服务使用LVM驱动程序在该设备上置备逻辑卷， 并通过iSCSI传输将其提供给实例。您可以对这些说明进行少量修改，以通过其他存储节点水平扩展您的环境。 cinder相关服务 服务名 说明 cinder-volume 提供存储空间，包括lvm、nfs、glusterfs、ceph等等存储 cinder-api 接收外部的api请求 cinder-scheduler 调度器，决定由哪一个cinder-volume提供存储空间 cinder-backup 备份创建的卷 安装和配置控制节点 rocky版控制节点块存储服务cinder安装配置官方文档 9.1 创建cinder数据库并授权 mysql -e \"CREATE DATABASE cinder;\" mysql -e \"GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'localhost' \\ IDENTIFIED BY 'CINDER_DBPASS';\" mysql -e \"GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'%' \\ IDENTIFIED BY 'CINDER_DBPASS';\" 9.2 获取管理员凭据以获取对仅管理员CLI命令的访问 source /opt/admin-openrc 9.3 创建服务凭证 9.3.1 创建 cinder用户 密码设置为 CIDNER_PASS ⚠️交互式与非交互式设置密码选择其中一种 非交互式设置密码 $ openstack user create --domain default --password CINDER_PASS cinder +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | domain_id | default | | enabled | True | | id | 65b5343859e6409994d007f2de30570b | | name | cinder | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ 交互式设置密码 openstack user create --domain default --password-prompt cinder 9.3.2 将admin角色添加到cinder用户 openstack role add --project service --user cinder admin 9.3.3 创建 cinderv2 和 cinderv3 服务实体 ⚠️块存储服务需要两个服务实体 $ openstack service create --name cinderv2 \\ --description \"OpenStack Block Storage\" volumev2 +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Block Storage | | enabled | True | | id | efcdff53520142d2ac6b0953cf532340 | | name | cinderv2 | | type | volumev2 | +-------------+----------------------------------+ $ openstack service create --name cinderv3 \\ --description \"OpenStack Block Storage\" volumev3 +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Block Storage | | enabled | True | | id | 910b223e07244359ae0480d579a0231a | | name | cinderv3 | | type | volumev3 | +-------------+----------------------------------+ 9.4 创建块存储服务API端点 创建cinderv2 $ openstack endpoint create --region RegionOne \\ volumev2 public http://controller:8776/v2/%\\(project_id\\)s +--------------+------------------------------------------+ | Field | Value | +--------------+------------------------------------------+ | enabled | True | | id | dcd7a491205f4c8a8a1af94fbd95d452 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | efcdff53520142d2ac6b0953cf532340 | | service_name | cinderv2 | | service_type | volumev2 | | url | http://controller:8776/v2/%(project_id)s | +--------------+------------------------------------------+ $ openstack endpoint create --region RegionOne \\ volumev2 internal http://controller:8776/v2/%\\(project_id\\)s +--------------+------------------------------------------+ | Field | Value | +--------------+------------------------------------------+ | enabled | True | | id | e044f276dd624336b5c4bb51aa343a55 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | efcdff53520142d2ac6b0953cf532340 | | service_name | cinderv2 | | service_type | volumev2 | | url | http://controller:8776/v2/%(project_id)s | +--------------+------------------------------------------+ $ openstack endpoint create --region RegionOne \\ volumev2 admin http://controller:8776/v2/%\\(project_id\\)s +--------------+------------------------------------------+ | Field | Value | +--------------+------------------------------------------+ | enabled | True | | id | e9c888a00e354a2cab3035b70b9e6c30 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | efcdff53520142d2ac6b0953cf532340 | | service_name | cinderv2 | | service_type | volumev2 | | url | http://controller:8776/v2/%(project_id)s | +--------------+------------------------------------------+ 创建cinderv3 $ openstack endpoint create --region RegionOne \\ volumev3 public http://controller:8776/v3/%\\(project_id\\)s +--------------+------------------------------------------+ | Field | Value | +--------------+------------------------------------------+ | enabled | True | | id | 4f818560bac745dfa493dc53b3106cc3 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 910b223e07244359ae0480d579a0231a | | service_name | cinderv3 | | service_type | volumev3 | | url | http://controller:8776/v3/%(project_id)s | +--------------+------------------------------------------+ $ openstack endpoint create --region RegionOne \\ volumev3 internal http://controller:8776/v3/%\\(project_id\\)s +--------------+------------------------------------------+ | Field | Value | +--------------+------------------------------------------+ | enabled | True | | id | e102e7b28f5f47409e4e231af0af4776 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 910b223e07244359ae0480d579a0231a | | service_name | cinderv3 | | service_type | volumev3 | | url | http://controller:8776/v3/%(project_id)s | +--------------+------------------------------------------+ $ openstack endpoint create --region RegionOne \\ volumev3 admin http://controller:8776/v3/%\\(project_id\\)s +--------------+------------------------------------------+ | Field | Value | +--------------+------------------------------------------+ | enabled | True | | id | 4585dc9397b041b7a1a45d063d515dcb | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 910b223e07244359ae0480d579a0231a | | service_name | cinderv3 | | service_type | volumev3 | | url | http://controller:8776/v3/%(project_id)s | +--------------+------------------------------------------+ 9.5 安装和配置组件 9.5.1 安装软件包 yum -y install openstack-cinder 9.5.2 编辑 /etc/cinder/cinder.conf 文件并完成以下操作 1.在该[database]部分中，配置数据库访问： [database] # ... connection = mysql+pymysql://cinder:CINDER_DBPASS@controller/cinder 2.在该[DEFAULT]部分中，配置RabbitMQ 消息队列访问： [DEFAULT] # ... transport_url = rabbit://openstack:RABBIT_PASS@controller 3.在[DEFAULT]和[keystone_authtoken]部分中，配置身份服务访问： [DEFAULT] # ... auth_strategy = keystone [keystone_authtoken] # ... www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_id = default user_domain_id = default project_name = service username = cinder password = CINDER_PASS 4.在该[DEFAULT]部分中，配置my_ip选项以使用控制器节点的管理接口IP地址： [DEFAULT] # ... my_ip = 10.0.0.11 5.在该[oslo_concurrency]部分中，配置锁定路径： [oslo_concurrency] # ... lock_path = /var/lib/cinder/tmp 用以下命令修改 \\cp /etc/cinder/cinder.conf{,.bak} grep '^[a-Z\\[]' /etc/cinder/cinder.conf.bak > /etc/cinder/cinder.conf openstack-config --set /etc/cinder/cinder.conf database connection mysql+pymysql://cinder:CINDER_DBPASS@controller/cinder openstack-config --set /etc/cinder/cinder.conf DEFAULT transport_url rabbit://openstack:RABBIT_PASS@controller openstack-config --set /etc/cinder/cinder.conf DEFAULT auth_strategy keystone openstack-config --set /etc/cinder/cinder.conf DEFAULT my_ip 172.30.100.4 openstack-config --set /etc/cinder/cinder.conf keystone_authtoken www_authenticate_uri http://controller:5000 openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_url http://controller:5000 openstack-config --set /etc/cinder/cinder.conf keystone_authtoken memcached_servers controller:11211 openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_type password openstack-config --set /etc/cinder/cinder.conf keystone_authtoken project_domain_id default openstack-config --set /etc/cinder/cinder.conf keystone_authtoken user_domain_id default openstack-config --set /etc/cinder/cinder.conf keystone_authtoken project_name service openstack-config --set /etc/cinder/cinder.conf keystone_authtoken username cinder openstack-config --set /etc/cinder/cinder.conf keystone_authtoken password CINDER_PASS openstack-config --set /etc/cinder/cinder.conf oslo_concurrency lock_path /var/lib/cinder/tmp 文件md5值 $ md5sum /etc/cinder/cinder.conf a5023d5b6df47ce8d186d5d32623c076 /etc/cinder/cinder.conf 9.5.3 同步数据库 $ su -s /bin/sh -c \"cinder-manage db sync\" cinder Deprecated: Option \"logdir\" from group \"DEFAULT\" is deprecated. Use option \"log-dir\" from group \"DEFAULT\" 有表即为正确 $ mysql cinder -e \"show tables\"|wc -l 36 9.6 配置计算已使用块存储 编辑 /etc/nova/nova.conf 文件并向其中添加以下内容 openstack-config --set /etc/nova/nova.conf cinder os_region_name RegionOne 文件md5值 $ md5sum /etc/nova/nova.conf 606a18d1be80cd7e0a57150ca0e5040f /etc/nova/nova.conf 9.7 完成安装 9.7.1 重启Compute API服务 systemctl restart openstack-nova-api.service 9.7.2 启动块存储服务，并将其配置为在系统启动时启动 systemctl enable openstack-cinder-api.service openstack-cinder-scheduler.service systemctl start openstack-cinder-api.service openstack-cinder-scheduler.service 9.8 验证 返回的结果是 cinder-api 提供的，并且 cinder-scheduler 的状态是up $ cinder service-list +------------------+------------+------+---------+-------+----------------------------+-----------------+ | Binary | Host | Zone | Status | State | Updated_at | Disabled Reason | +------------------+------------+------+---------+-------+----------------------------+-----------------+ | cinder-scheduler | controller | nova | enabled | up | 2020-05-29T10:58:08.000000 | - | +------------------+------------+------+---------+-------+----------------------------+-----------------+ 安装和配置块存储节点 rocky版块存储节点块存储服务cinder安装配置官方文档 9.9 前提条件 9.9.1 安装包 yum -y install lvm2 device-mapper-persistent-data openstack-utils.noarch 9.9.2 启动LVM元数据服务，并将其配置为在系统引导时启动 systemctl enable lvm2-lvmetad.service systemctl start lvm2-lvmetad.service 9.9.3 创建LVM物理卷 /dev/vdb ⚠️如果是在虚拟机中，需要添加一块数据盘，使用命令 echo \"- - -\" >/sys/class/scsi_host/host0/scan 实现热加载（不一定为host0，也可能是其他数字，比如1、2、3等） 查看添加的磁盘 $ fdisk -l /dev/vdb Disk /dev/vdb: 107.4 GB, 107374182400 bytes, 209715200 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes 创建物理卷pv $ pvcreate /dev/vdb Physical volume \"/dev/vdb\" successfully created. 9.9.4 创建LVM卷组cinder-volumes $ vgcreate cinder-volumes /dev/vdb Volume group \"cinder-volumes\" successfully created 9.9.5 关于lvm的问题 只有实例可以访问块存储卷。但是，底层操作系统管理与卷关联的设备。默认情况下，LVM卷扫描工具会在 /dev 目录中扫描包含卷的块存储设备。如果项目在其卷上使用LVM，则扫描工具会检测到这些卷并尝试对其进行缓存，这可能导致基础操作系统卷和项目卷出现各种问题。您必须将LVM重新配置为仅扫描包含 cinder-volumes 卷组的设备 ⚠️⚠️⚠️ 如果存储节点在操作系统磁盘上使用LVM，则还必须将关联的设备添加到过滤器中。例如，如果 /dev/vda 设备包含操作系统： filter = [ \"a/vda/\", \"a/vdb/\", \"r/.*/\"] 同样，如果您的计算节点在操作系统磁盘上使用LVM，则还必须 /etc/lvm/lvm.conf 在这些节点上的文件中修改过滤器， 使其仅包括操作系统磁盘。例如，如果 /dev/vda 设备包含操作系统： filter = [ \"a/vda/\", \"r/.*/\"] 因为使用的虚拟机在安装的时候是采用的lvm，所以配置文件中应当把系统盘/dev/vda也添加 编辑 /etc/lvm/lvm.conf 文件并完成以下操作 在该devices部分中，添加一个接受/dev/sdb设备并拒绝所有其他设备的过滤 器： devices { ... filter = [ \"a/vdb/\", \"r/.*/\"] 滤波器阵列中的每个项目开始于a用于接受或 r用于拒绝，并且包括用于所述装置名称的正则表达式。该阵列必须r/.*/以拒绝任何剩余的设备结尾。您可以使用vgs -vvvv命令测试过滤器 用以下命令修改 \\cp /etc/lvm/lvm.conf{,.bak} egrep -v '^$|#' /etc/lvm/lvm.conf.bak > /etc/lvm/lvm.conf sed -i '/^devices/a\\\\tfilter = [ \"a/vda/\", \"a/vdb/\", \"r/.*/\"]' /etc/lvm/lvm.conf 文件md5值 $ md5sum /etc/lvm/lvm.conf 572157ddd9d8b095ac37e89f4d1e603a /etc/lvm/lvm.conf 9.10 安装和配置组件 9.10.1 安装软件包 targetcli是iscsi的包 yum -y install openstack-cinder targetcli python-keystone 9.10.2 编辑 /etc/cinder/cinder.conf 文件并完成以下操作 ⚠️在配置文件中的 [DEFAULT] 区域，默认官方定义的后端lvm卷名称就是lvm，这个名称是任意的，并且DEFAULT区域的enabled_backends定义的名称是有单独的一个区域，例如 # default区域内容如下 [DEFAULT] enabled_backends = lvm # 则如下区域内容和DEFAULT区域定义的名称一一对应的 [lvm] volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver # 驱动 volume_group = cinder-volumes # 卷组名 iscsi_protocol = iscsi iscsi_helper = lioadm 如果有多个lvm卷，则DEFAULT区域可以写成如下，并且最后要写上一一对应的内容 [DEFAULT] enabled_backends = lvm,lvm2,lvm3 # 后段名称任意，这里我们定义为普通磁盘sata，固态硬盘ssd [DEFAULT] enabled_backends = sata,ssd [sata] volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volume_group = cinder-volumes iscsi_protocol = iscsi iscsi_helper = lioadm volume_backend_name = sata [ssd] volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volume_group = cinder-ssd iscsi_protocol = iscsi iscsi_helper = lioadm volume_backend_name = ssd 1.在该[database]部分中，配置数据库访问： [database] # ... connection = mysql+pymysql://cinder:CINDER_DBPASS@controller/cinder 2.在该[DEFAULT]部分中，配置RabbitMQ 消息队列访问： [DEFAULT] # ... transport_url = rabbit://openstack:RABBIT_PASS@controller 3.在[DEFAULT]和[keystone_authtoken]部分中，配置身份服务访问： [DEFAULT] # ... auth_strategy = keystone [keystone_authtoken] # ... www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_id = default user_domain_id = default project_name = service username = cinder password = CINDER_PASS 4.在该[DEFAULT]部分中，配置my_ip选项：替换MANAGEMENT_INTERFACE_IP_ADDRESS为存储节点上管理网络接口的IP地址，对于示例体系结构中的第一个节点，通常为10.0.0.41 [DEFAULT] # ... my_ip = MANAGEMENT_INTERFACE_IP_ADDRESS 5.在该[lvm]部分中，为LVM后端配置LVM驱动程序，cinder-volumes卷组，iSCSI协议和适当的iSCSI服务。如果该[lvm]部分不存在，请创建它： [lvm] volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volume_group = cinder-volumes iscsi_protocol = iscsi iscsi_helper = lioadm 6.在该[DEFAULT]部分中，启用LVM后端： 后端名称是任意的。例如，本指南使用驱动程序的名称作为后端的名称。 [DEFAULT] # ... enabled_backends = lvm 7.在该[DEFAULT]部分中，配置图像服务API的位置： [DEFAULT] # ... glance_api_servers = http://controller:9292 8.在该[oslo_concurrency]部分中，配置锁定路径： [oslo_concurrency] # ... lock_path = /var/lib/cinder/tmp 用以下命令修改 \\cp /etc/cinder/cinder.conf{,.bak} grep '^[a-Z\\[]' /etc/cinder/cinder.conf.bak > /etc/cinder/cinder.conf openstack-config --set /etc/cinder/cinder.conf database connection mysql+pymysql://cinder:CINDER_DBPASS@controller/cinder openstack-config --set /etc/cinder/cinder.conf DEFAULT transport_url rabbit://openstack:RABBIT_PASS@controller openstack-config --set /etc/cinder/cinder.conf DEFAULT auth_strategy keystone openstack-config --set /etc/cinder/cinder.conf DEFAULT my_ip 172.30.100.6 openstack-config --set /etc/cinder/cinder.conf DEFAULT enabled_backends lvm openstack-config --set /etc/cinder/cinder.conf DEFAULT glance_api_servers http://controller:9292 openstack-config --set /etc/cinder/cinder.conf keystone_authtoken www_authenticate_uri http://controller:5000 openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_url http://controller:5000 openstack-config --set /etc/cinder/cinder.conf keystone_authtoken memcached_servers controller:11211 openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_type password openstack-config --set /etc/cinder/cinder.conf keystone_authtoken project_domain_id default openstack-config --set /etc/cinder/cinder.conf keystone_authtoken user_domain_id default openstack-config --set /etc/cinder/cinder.conf keystone_authtoken project_name service openstack-config --set /etc/cinder/cinder.conf keystone_authtoken username cinder openstack-config --set /etc/cinder/cinder.conf keystone_authtoken password CINDER_PASS openstack-config --set /etc/cinder/cinder.conf lvm volume_driver cinder.volume.drivers.lvm.LVMVolumeDriver openstack-config --set /etc/cinder/cinder.conf lvm volume_group cinder-volumes openstack-config --set /etc/cinder/cinder.conf lvm iscsi_protocol iscsi openstack-config --set /etc/cinder/cinder.conf lvm iscsi_helper lioadm openstack-config --set /etc/cinder/cinder.conf oslo_concurrency lock_path /var/lib/cinder/tmp 文件md5值 $ md5sum /etc/cinder/cinder.conf c88237e48f728cbe389b57c75b2be155 /etc/cinder/cinder.conf 9.10.3 启动块存储卷服务及其相关性，并将其配置为在系统启动时启动 systemctl enable openstack-cinder-volume.service target.service systemctl start openstack-cinder-volume.service target.service 到此，控制节点和块存储节点块存储服务cinder安装完成！！！ 10.控制节点和对象节点对象存储服务swift安装 OpenStack对象存储是一个多租户对象存储系统。它具有高度的可扩展性，并且可以通过RESTful HTTP API以低成本管理大量非结构化数据。 swift相关组件 名称 说明 代理服务器（swift-proxy-server） 接受OpenStack对象存储API和原始HTTP请求，以上传文件，修改元数据和创建容器。它还向网络浏览器提供文件或容器列表。为了提高性能，代理服务器可以使用通常与memcache一起部署的可选缓存。 帐户服务器（swift-account-server） 管理使用对象存储定义的帐户 容器服务器（swift容器服务器） 在对象存储中管理容器或文件夹的映射 对象服务器（swift-object-server） 管理存储节点上的实际对象，例如文件 各种周期性过程 在大型数据存储上执行内务处理任务。复制服务可确保整个群集的一致性和可用性。其他定期过程包括审核员，更新者和收割者 WSGI中间件 处理身份验证，通常是OpenStack身份 swift client 使用户能够通过授权为管理员用户，代理商用户或快速用户的命令行客户端将命令提交到REST API swift-init 脚本初始化环文件的构建，以守护程序名称作为参数并提供命令。记录在 https://docs.openstack.org/swift/latest/admin_guide.html#managing-services中 swift-recon cli工具，用于检索swift-recon中间件已收集的有关群集的各种度量和遥测信息 swift-ring-builder 存储环构建和重新平衡实用程序。在 https://docs.openstack.org/swift/latest/admin_guide.html#managing-the-rings中记录 安装和配置控制节点 rocky版控制节点对象存储服务swift安装配置官方文档 10.1 创建身份服务凭据 10.1.1 获取管理员凭据以获取对仅管理员CLI命令的访问 source /opt/admin-openrc 10.1.2 创建 swift 用户 密码设置为 SWIFT_PASS ⚠️交互式与非交互式设置密码选择其中一种 非交互式设置密码 $ openstack user create --domain default --password SWIFT_PASS swift +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | domain_id | default | | enabled | True | | id | 312ab4f320434d30b76c9486463e2dea | | name | swift | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ 交互式设置密码 openstack user create --domain default --password-prompt swift 10.1.3 将 admin 角色添加到 swift 用户 openstack role add --project service --user swift admin 10.1.4 创建 swift 服务实体 $ openstack service create --name swift \\ --description \"OpenStack Object Storage\" object-store +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Object Storage | | enabled | True | | id | a41ace8ca3bb42ec92e27a29503828e7 | | name | swift | | type | object-store | +-------------+----------------------------------+ 10.2 创建对象存储服务API端点 $ openstack endpoint create --region RegionOne \\ object-store public http://controller:8080/v1/AUTH_%\\(project_id\\)s +--------------+-----------------------------------------------+ | Field | Value | +--------------+-----------------------------------------------+ | enabled | True | | id | 25063822e1c947f38189af370d97a0c2 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | a41ace8ca3bb42ec92e27a29503828e7 | | service_name | swift | | service_type | object-store | | url | http://controller:8080/v1/AUTH_%(project_id)s | +--------------+-----------------------------------------------+ $ openstack endpoint create --region RegionOne \\ object-store internal http://controller:8080/v1/AUTH_%\\(project_id\\)s +--------------+-----------------------------------------------+ | Field | Value | +--------------+-----------------------------------------------+ | enabled | True | | id | 6f36da03e9344d84906f98a31a09ebe9 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | a41ace8ca3bb42ec92e27a29503828e7 | | service_name | swift | | service_type | object-store | | url | http://controller:8080/v1/AUTH_%(project_id)s | +--------------+-----------------------------------------------+ $ openstack endpoint create --region RegionOne \\ object-store admin http://controller:8080/v1 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | ffe6a96aeb224d8abe3e0c3de6c9e072 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | a41ace8ca3bb42ec92e27a29503828e7 | | service_name | swift | | service_type | object-store | | url | http://controller:8080/v1 | +--------------+----------------------------------+ 10.3 安装和配置组件 10.3.1 安装软件包 yum -y install openstack-swift-proxy python-swiftclient \\ python-keystoneclient python-keystonemiddleware memcached openstack-utils.noarch 10.3.2 从对象存储源存储库获取代理服务配置文件 curl -o /etc/swift/proxy-server.conf https://opendev.org/openstack/swift/raw/branch/stable/rocky/etc/proxy-server.conf-sample 10.3.3 编辑 /etc/swift/proxy-server.conf 文件并完成以下操作 ⚠️⚠️⚠️这里官方文档有坑，q版之后就不用35357端口了，但是文档中还是写着auth_url = http://controller:35357，这里应该改成5000端口，否则后续验证swift会报错500 1.在该[DEFAULT]部分中，配置绑定端口，用户和配置目录： [DEFAULT] ... bind_port = 8080 user = swift swift_dir = /etc/swift 2.在该[pipeline:main]部分中，删除tempurl和 tempauth模块，然后添加authtoken和keystoneauth 模块 请勿更改模块的顺序！！！ 有关启用其他功能的其他模块的更多信息，请参考如下地址 https://docs.openstack.org/swift/latest/deployment_guide.html [pipeline:main] pipeline = catch_errors gatekeeper healthcheck proxy-logging cache container_sync bulk ratelimit authtoken keystoneauth container-quotas account-quotas slo dlo versioned_writes proxy-logging proxy-server 3.在该[app:proxy-server]部分中，启用自动帐户创建： [app:proxy-server] use = egg:swift#proxy ... account_autocreate = True 4.在该[filter:keystoneauth]部分中，配置操作员角色： [filter:keystoneauth] use = egg:swift#keystoneauth ... operator_roles = admin,user 5.在该[filter:authtoken]部分中，配置身份服务访问： [filter:authtoken] paste.filter_factory = keystonemiddleware.auth_token:filter_factory ... www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_id = default user_domain_id = default project_name = service username = swift password = SWIFT_PASS delay_auth_decision = True 6.在该[filter:cache]部分中，配置memcached位置： [filter:cache] use = egg:swift#memcache ... memcache_servers = controller:11211 用以下命令修改 \\cp /etc/swift/proxy-server.conf{,.bak} grep '^[a-Z\\[]' /etc/swift/proxy-server.conf.bak > /etc/swift/proxy-server.conf openstack-config --set /etc/swift/proxy-server.conf DEFAULT bind_port 8080 openstack-config --set /etc/swift/proxy-server.conf DEFAULT user swift openstack-config --set /etc/swift/proxy-server.conf DEFAULT swift_dir /etc/swift openstack-config --set /etc/swift/proxy-server.conf app:proxy-server use egg:swift#proxy openstack-config --set /etc/swift/proxy-server.conf app:proxy-server account_autocreate True openstack-config --set /etc/swift/proxy-server.conf filter:keystoneauth use egg:swift#keystoneauth openstack-config --set /etc/swift/proxy-server.conf filter:keystoneauth operator_roles admin,user openstack-config --set /etc/swift/proxy-server.conf filter:authtoken paste.filter_factory keystonemiddleware.auth_token:filter_factory openstack-config --set /etc/swift/proxy-server.conf filter:authtoken www_authenticate_uri http://controller:5000 openstack-config --set /etc/swift/proxy-server.conf filter:authtoken auth_url http://controller:5000 openstack-config --set /etc/swift/proxy-server.conf filter:authtoken memcached_servers controller:11211 openstack-config --set /etc/swift/proxy-server.conf filter:authtoken auth_type password openstack-config --set /etc/swift/proxy-server.conf filter:authtoken project_domain_id default openstack-config --set /etc/swift/proxy-server.conf filter:authtoken user_domain_id default openstack-config --set /etc/swift/proxy-server.conf filter:authtoken project_name service openstack-config --set /etc/swift/proxy-server.conf filter:authtoken username swift openstack-config --set /etc/swift/proxy-server.conf filter:authtoken password SWIFT_PASS openstack-config --set /etc/swift/proxy-server.conf filter:authtoken delay_auth_decision True openstack-config --set /etc/swift/proxy-server.conf filter:cache use egg:swift#memcache openstack-config --set /etc/swift/proxy-server.conf filter:cache memcache_servers controller:11211 sed -i '/^pipeline/c pipeline = catch_errors gatekeeper healthcheck proxy-logging cache container_sync bulk ratelimit authtoken keystoneauth container-quotas account-quotas slo dlo versioned_writes proxy-logging proxy-server' /etc/swift/proxy-server.conf 安装和配置对象节点 rocky版存储节点对象存储服务swift安装配置官方文档 本节介绍如何安装和配置用于操作帐户，容器和对象服务的存储节点。为简单起见，此配置引用两个存储节点，每个存储节点包含两个空的本地块存储设备。指令使用 /dev/sdb 和 /dev/sdc，但是您可以为特定节点替换不同的值。 尽管对象存储支持具有扩展属性（xattr）的任何文件系统，但是测试和基准测试表明XFS具有最佳性能和可靠性。有关水平扩展环境的更多信息，请参阅《 部署指南》。 本部分适用于Red Hat Enterprise Linux 7和CentOS 7 10.4 前提条件 10.4.1 安装软件包 yum -y install xfsprogs rsync 10.4.2 将/dev/sdb和/dev/sdc设备格式化为XFS mkfs.xfs /dev/sdb mkfs.xfs /dev/sdc 10.4.3 创建安装点目录结构 mkdir -p /srv/node/sdb mkdir -p /srv/node/sdc 10.4.4 编辑 /etc/fstab 文件并向其中添加以下内容 cp /etc/fstab{,.bak} cat >> /etc/fstab 10.4.5 挂载目录 mount /srv/node/sdb mount /srv/node/sdc 10.4.6 创建或编辑 /etc/rsyncd.conf 文件以包含以下内容 该 rsync 服务不需要身份验证，因此请考虑在生产环境中的专用网络上运行它。 替换 MANAGEMENT_INTERFACE_IP_ADDRESS 为存储节点上管理网络的IP地址 object01操作 \\cp /etc/rsyncd{,.bak} cat > /etc/rsyncd.conf object02操作 \\cp /etc/rsyncd{,.bak} cat > /etc/rsyncd.conf 10.5 安装和配置组件 ⚠️在每个对象节点上执行以下步骤 10.5.1 安装软件包 yum -y install openstack-swift-account openstack-swift-container \\ openstack-swift-object openstack-utils.noarch 10.5.2 从对象存储源存储库中获取计费，容器和对象服务配置文件 curl -o /etc/swift/account-server.conf https://opendev.org/openstack/swift/raw/branch/stable/rocky/etc/account-server.conf-sample curl -o /etc/swift/container-server.conf https://opendev.org/openstack/swift/raw/branch/stable/rocky/etc/container-server.conf-sample curl -o /etc/swift/object-server.conf https://opendev.org/openstack/swift/raw/branch/stable/rocky/etc/object-server.conf-sample 10.5.3 编辑 /etc/swift/account-server.conf 文件并完成以下操作 1.在此[DEFAULT]部分中，配置绑定IP地址，绑定端口，用户，配置目录和安装点目录：替换MANAGEMENT_INTERFACE_IP_ADDRESS为存储节点上管理网络的IP地址。 [DEFAULT] ... bind_ip = MANAGEMENT_INTERFACE_IP_ADDRESS bind_port = 6202 user = swift swift_dir = /etc/swift devices = /srv/node mount_check = True 2.在该[pipeline:main]部分中，启用适当的模块： 有关启用其他功能的其他模块的更多信息，参考如下地址 https://docs.openstack.org/swift/latest/deployment_guide.html [pipeline:main] pipeline = healthcheck recon account-server 3.在该[filter:recon]部分中，配置侦察（计量）缓存目录： [filter:recon] use = egg:swift#recon ... recon_cache_path = /var/cache/swift 用以下命令操作 export IP=`ip a s eth0| awk -F '[ /]+' 'NR==3{print $3}'` \\cp /etc/swift/account-server.conf{,.bak} grep '^[a-Z\\[]' /etc/swift/account-server.conf.bak > /etc/swift/account-server.conf openstack-config --set /etc/swift/account-server.conf DEFAULT bind_ip ${IP} openstack-config --set /etc/swift/account-server.conf DEFAULT bind_port 6202 openstack-config --set /etc/swift/account-server.conf DEFAULT user swift openstack-config --set /etc/swift/account-server.conf DEFAULT swift_dir /etc/swift openstack-config --set /etc/swift/account-server.conf DEFAULT devices /srv/node openstack-config --set /etc/swift/account-server.conf DEFAULT mount_check True openstack-config --set /etc/swift/account-server.conf filter:recon use egg:swift#recon openstack-config --set /etc/swift/account-server.conf filter:recon recon_cache_path /var/cache/swift 10.5.4 编辑 /etc/swift/container-server.conf 文件并完成以下操作 1.在此[DEFAULT]部分中，配置绑定IP地址，绑定端口，用户，配置目录和安装点目录： 替换MANAGEMENT_INTERFACE_IP_ADDRESS为存储节点上管理网络的IP地址。 [DEFAULT] ... bind_ip = MANAGEMENT_INTERFACE_IP_ADDRESS bind_port = 6201 user = swift swift_dir = /etc/swift devices = /srv/node mount_check = True 2.在该[pipeline:main]部分中，启用适当的模块： 有关启用其他功能的其他模块的更多信息，请参考如下地址 https://docs.openstack.org/swift/latest/deployment_guide.html [pipeline:main] pipeline = healthcheck recon container-server 3.在该[filter:recon]部分中，配置侦察（计量）缓存目录： [filter:recon] use = egg:swift#recon ... recon_cache_path = /var/cache/swift 用以下命令修改 export IP=`ip a s eth0| awk -F '[ /]+' 'NR==3{print $3}'` \\cp /etc/swift/container-server.conf{,.bak} grep '^[a-Z\\[]' /etc/swift/container-server.conf.bak > /etc/swift/container-server.conf openstack-config --set /etc/swift/container-server.conf DEFAULT bind_ip ${IP} openstack-config --set /etc/swift/container-server.conf DEFAULT bind_port 6201 openstack-config --set /etc/swift/container-server.conf DEFAULT user swift openstack-config --set /etc/swift/container-server.conf DEFAULT swift_dir /etc/swift openstack-config --set /etc/swift/container-server.conf DEFAULT devices /srv/node openstack-config --set /etc/swift/container-server.conf DEFAULT mount_check True openstack-config --set /etc/swift/container-server.conf filter:recon use egg:swift#recon openstack-config --set /etc/swift/container-server.conf filter:recon recon_cache_path /var/cache/swift 10.5.5 编辑/etc/swift/object-server.conf文件并完成以下操作 1.在此[DEFAULT]部分中，配置绑定IP地址，绑定端口，用户，配置目录和安装点目录： 替换MANAGEMENT_INTERFACE_IP_ADDRESS为存储节点上管理网络的IP地址。 [DEFAULT] ... bind_ip = MANAGEMENT_INTERFACE_IP_ADDRESS bind_port = 6200 user = swift swift_dir = /etc/swift devices = /srv/node mount_check = True 2.在该[pipeline:main]部分中，启用适当的模块： 有关启用其他功能的其他模块的更多信息，请参考如下地址 https://docs.openstack.org/swift/latest/deployment_guide.html [pipeline:main] pipeline = healthcheck recon object-server 3.在此[filter:recon]部分中，配置侦察（仪表）缓存和锁定目录： [filter:recon] use = egg:swift#recon ... recon_cache_path = /var/cache/swift recon_lock_path = /var/lock 用以下命令修改 export IP=`ip a s eth0| awk -F '[ /]+' 'NR==3{print $3}'` \\cp /etc/swift/object-server.conf{,.bak} grep '^[a-Z\\[]' /etc/swift/object-server.conf.bak > /etc/swift/object-server.conf openstack-config --set /etc/swift/object-server.conf DEFAULT bind_ip ${IP} openstack-config --set /etc/swift/object-server.conf DEFAULT bind_port 6200 openstack-config --set /etc/swift/object-server.conf DEFAULT user swift openstack-config --set /etc/swift/object-server.conf DEFAULT swift_dir /etc/swift openstack-config --set /etc/swift/object-server.conf DEFAULT devices /srv/node openstack-config --set /etc/swift/object-server.conf DEFAULT mount_check True openstack-config --set /etc/swift/object-server.conf filter:recon use egg:swift#recon openstack-config --set /etc/swift/object-server.conf filter:recon recon_cache_path /var/cache/swift openstack-config --set /etc/swift/object-server.conf filter:recon recon_lock_path /var/lock 10.5.6 确保对安装点目录结构拥有适当的所有权 chown -R swift:swift /srv/node 10.5.7 创建 recon 目录并确保对其拥有适当的所有权 mkdir -p /var/cache/swift chown -R root:swift /var/cache/swift chmod -R 775 /var/cache/swift 10.6 创建和分发初始环 ⚠️在控制节点执行以下步骤 在启动对象存储服务之前，必须创建初始帐户，容器和对象环。环形构建器创建配置文件，每个节点都使用该配置文件来确定和部署存储体系结构。为简单起见，本指南使用一个区域和两个区域，最大分区为2 ^ 10（1024）个，每个对象3个副本，两次移动一个分区之间的最少间隔时间为1小时。对于对象存储，分区表示存储设备上的目录，而不是常规分区表。有关更多信息，请参见《 部署指南》。 10.7 创建帐户环 帐户服务器使用帐户环维护容器列表 10.7.1 ⚠️要切换到/etc/swift目录 cd /etc/swift 10.7.2 创建基础account.builder文件 swift-ring-builder account.builder create 10 3 1 10.7.3 将每个存储节点添加到环 官网示例 swift-ring-builder account.builder \\ add --region 1 --zone 1 --ip STORAGE_NODE_MANAGEMENT_INTERFACE_IP_ADDRESS --port 6202 \\ --device DEVICE_NAME --weight DEVICE_WEIGHT 替换STORAGE_NODE_MANAGEMENT_INTERFACE_IP_ADDRESS为存储节点上管理网络的IP地址。用DEVICE_NAME同一存储节点上的存储设备名称替换。例如，使用“ 安装”中的第一个存储节点并使用/dev/sdb存储设备和权重100 配置存储节点： 对每个存储节点上的每个存储设备重复此命令。在示例体系结构中，使用以下四个变体中的命令 $ swift-ring-builder account.builder add \\ --region 1 --zone 1 --ip 172.30.100.7 --port 6202 --device vdb --weight 100 Device d0r1z1-172.30.100.7:6202R172.30.100.7:6202/vdb_\"\" with 100.0 weight got id 0 $ swift-ring-builder account.builder add \\ --region 1 --zone 1 --ip 172.30.100.7 --port 6202 --device vdc --weight 100 Device d1r1z1-172.30.100.7:6202R172.30.100.7:6202/vdc_\"\" with 100.0 weight got id 1 $ swift-ring-builder account.builder add \\ --region 1 --zone 2 --ip 172.30.100.8 --port 6202 --device vdb --weight 100 Device d2r1z2-172.30.100.8:6202R172.30.100.8:6202/vdb_\"\" with 100.0 weight got id 2 $ swift-ring-builder account.builder add \\ --region 1 --zone 2 --ip 172.30.100.8 --port 6202 --device vdc --weight 100 Device d3r1z2-172.30.100.8:6202R172.30.100.8:6202/vdc_\"\" with 100.0 weight got id 3 10.7.4 验证 $ swift-ring-builder account.builder account.builder, build version 4, id 2738119c3e3c47b199313d7ad28f17cd 1024 partitions, 3.000000 replicas, 1 regions, 2 zones, 4 devices, 100.00 balance, 0.00 dispersion The minimum number of hours before a partition can be reassigned is 1 (0:00:00 remaining) The overload factor is 0.00% (0.000000) Ring file account.ring.gz not found, probably it hasn't been written yet Devices: id region zone ip address:port replication ip:port name weight partitions balance flags meta 0 1 1 172.30.100.7:6202 172.30.100.7:6202 vdb 100.00 0 -100.00 1 1 1 172.30.100.7:6202 172.30.100.7:6202 vdc 100.00 0 -100.00 2 1 2 172.30.100.8:6202 172.30.100.8:6202 vdb 100.00 0 -100.00 3 1 2 172.30.100.8:6202 172.30.100.8:6202 vdc 100.00 0 -100.00 10.7.5 重新平衡权重 $ swift-ring-builder account.builder rebalance Reassigned 3072 (300.00%) partitions. Balance is now 0.00. Dispersion is now 0.00 10.8 创建容器环 容器服务器使用容器环维护对象列表。但是，它不跟踪对象位置 10.8.1 转到 /etc/swift 目录 cd /etc/swift 10.8.2 创建基础container.builder文件 swift-ring-builder container.builder create 10 3 1 10.8.3 将每个存储节点添加到环 官网示例 swift-ring-builder container.builder \\ add --region 1 --zone 1 --ip STORAGE_NODE_MANAGEMENT_INTERFACE_IP_ADDRESS --port 6201 --device DEVICE_NAME --weight DEVICE_WEIGHT 替换STORAGE_NODE_MANAGEMENT_INTERFACE_IP_ADDRESS为存储节点上管理网络的IP地址。用DEVICE_NAME同一存储节点上的存储设备名称替换。例如，使用“ 安装”中的第一个存储节点并使用/dev/sdb 存储设备和权重100 配置存储节点 对每个存储节点上的每个存储设备重复此命令。在示例体系结构中，使用以下四个变体中的命令 $ swift-ring-builder container.builder add \\ --region 1 --zone 1 --ip 172.30.100.7 --port 6201 --device vdb --weight 100 Device d0r1z1-172.30.100.7:6201R172.30.100.7:6201/vdb_\"\" with 100.0 weight got id 0 $ swift-ring-builder container.builder add \\ --region 1 --zone 1 --ip 172.30.100.7 --port 6201 --device vdc --weight 100 Device d1r1z1-172.30.100.7:6201R172.30.100.7:6201/vdc_\"\" with 100.0 weight got id 1 $ swift-ring-builder container.builder add \\ --region 1 --zone 2 --ip 172.30.100.8 --port 6201 --device vdb --weight 100 Device d2r1z2-172.30.100.8:6201R172.30.100.8:6201/vdb_\"\" with 100.0 weight got id 2 $ swift-ring-builder container.builder add \\ --region 1 --zone 2 --ip 172.30.100.8 --port 6201 --device vdc --weight 100 Device d3r1z2-172.30.100.8:6201R172.30.100.8:6201/vdc_\"\" with 100.0 weight got id 3 10.8.4 验证 $ swift-ring-builder container.builder container.builder, build version 4, id c50b252ff09548ba8e9f1639516cd6b1 1024 partitions, 3.000000 replicas, 1 regions, 2 zones, 4 devices, 100.00 balance, 0.00 dispersion The minimum number of hours before a partition can be reassigned is 1 (0:00:00 remaining) The overload factor is 0.00% (0.000000) Ring file container.ring.gz not found, probably it hasn't been written yet Devices: id region zone ip address:port replication ip:port name weight partitions balance flags meta 0 1 1 172.30.100.7:6201 172.30.100.7:6201 vdb 100.00 0 -100.00 1 1 1 172.30.100.7:6201 172.30.100.7:6201 vdc 100.00 0 -100.00 2 1 2 172.30.100.8:6201 172.30.100.8:6201 vdb 100.00 0 -100.00 3 1 2 172.30.100.8:6201 172.30.100.8:6201 vdc 100.00 0 -100.00 10.8.5 重新平衡权重 $ swift-ring-builder container.builder rebalance Reassigned 3072 (300.00%) partitions. Balance is now 0.00. Dispersion is now 0.00 10.9 创建对象环 对象服务器使用对象环维护本地设备上对象位置的列表 10.9.1 转到 /etc/swift 目录 cd /etc/dwift 10.9.2 创建基础 object.builder 文件 swift-ring-builder object.builder create 10 3 1 10.9.3 将每个存储节点添加到环 官方示例 swift-ring-builder object.builder \\ add --region 1 --zone 1 --ip STORAGE_NODE_MANAGEMENT_INTERFACE_IP_ADDRESS --port 6200 --device DEVICE_NAME --weight DEVICE_WEIGHT 替换STORAGE_NODE_MANAGEMENT_INTERFACE_IP_ADDRESS为存储节点上管理网络的IP地址。用DEVICE_NAME同一存储节点上的存储设备名称替换。例如，使用“ 安装”中的第一个存储节点并使用/dev/sdb存储设备和权重100 配置存储节点 对每个存储节点上的每个存储设备重复此命令。在示例体系结构中，使用以下四个变体中的命令 $ swift-ring-builder object.builder add \\ --region 1 --zone 1 --ip 172.30.100.7 --port 6200 --device vdb --weight 100 Device d0r1z1-172.30.100.7:6200R172.30.100.7:6200/vdb_\"\" with 100.0 weight got id 0 $ swift-ring-builder object.builder add \\ --region 1 --zone 1 --ip 172.30.100.7 --port 6200 --device vdc --weight 100 Device d1r1z1-172.30.100.7:6200R172.30.100.7:6200/vdc_\"\" with 100.0 weight got id 1 $ swift-ring-builder object.builder add \\ --region 1 --zone 2 --ip 172.30.100.8 --port 6200 --device vdb --weight 100 Device d2r1z2-172.30.100.8:6200R172.30.100.8:6200/vdb_\"\" with 100.0 weight got id 2 $ swift-ring-builder object.builder add \\ --region 1 --zone 2 --ip 172.30.100.8 --port 6200 --device sdc --weight 100 Device d3r1z2-172.30.100.8:6200R172.30.100.8:6200/sdc_\"\" with 100.0 weight got id 3 10.9.4 验证 $ swift-ring-builder object.builder object.builder, build version 4, id 0a4d3c0a65aa4d3d9a0d9407c99312c6 1024 partitions, 3.000000 replicas, 1 regions, 2 zones, 4 devices, 100.00 balance, 0.00 dispersion The minimum number of hours before a partition can be reassigned is 1 (0:00:00 remaining) The overload factor is 0.00% (0.000000) Ring file object.ring.gz not found, probably it hasn't been written yet Devices: id region zone ip address:port replication ip:port name weight partitions balance flags meta 0 1 1 172.30.100.7:6200 172.30.100.7:6200 vdb 100.00 0 -100.00 1 1 1 172.30.100.7:6200 172.30.100.7:6200 vdc 100.00 0 -100.00 3 1 2 172.30.100.8:6200 172.30.100.8:6200 sdc 100.00 0 -100.00 2 1 2 172.30.100.8:6200 172.30.100.8:6200 vdb 100.00 0 -100.00 10.9.5 重新平衡权重 $ swift-ring-builder object.builder rebalance Reassigned 3072 (300.00%) partitions. Balance is now 0.00. Dispersion is now 0.00 10.10 分发环网配置文件 拷贝 account.ring.gz，container.ring.gz 以及 object.ring.gz 文件复制到 /etc/swift 每个存储节点和运行代理服务的任何其他节点上目录 scp /etc/swift/{account.ring.gz,container.ring.gz,object.ring.gz} object01:/etc/swift scp /etc/swift/{account.ring.gz,container.ring.gz,object.ring.gz} object02:/etc/swift 10.11 完成安装 rocky版对象存储服务swift完成安装官方文档 10.11.1 /etc/swift/swift.conf从对象存储源存储库中获取文件 ⚠️这一步在控制节点执行 /etc/swift/swift.conf 原先内容 [swift-hash] swift_hash_path_suffix = %SWIFT_HASH_PATH_SUFFIX% 执行命令 curl -o /etc/swift/swift.conf \\ https://opendev.org/openstack/swift/raw/branch/stable/rocky/etc/swift.conf-sample 10.11.2 编辑 /etc/swift/swift.conf 文件并完成以下操作 ⚠️这一步在控制节点执行 1.在该[swift-hash]部分中，为您的环境配置哈希路径前缀和后缀。用唯一值替换HASH_PATH_PREFIX和HASH_PATH_SUFFIX，请将这些值保密，不要更改或丢失它们 [swift-hash] ... swift_hash_path_suffix = HASH_PATH_SUFFIX swift_hash_path_prefix = HASH_PATH_PREFIX 2.在该[storage-policy:0]部分中，配置默认存储策略： [storage-policy:0] ... name = Policy-0 default = yes 用以下命令修改 \\cp /etc/swift/swift.conf{,.bak} grep '^[a-Z\\[]' /etc/swift/swift.conf.bak > /etc/swift/swift.conf openstack-config --set /etc/swift/swift.conf swift-hash swift_hash_path_suffix HASH_PATH_SUFFIX openstack-config --set /etc/swift/swift.conf swift-hash swift_hash_path_prefix HASH_PATH_PREFIX openstack-config --set /etc/swift/swift.conf storage-policy:0 name Policy-0 openstack-config --set /etc/swift/swift.conf storage-policy:0 default yes 10.11.3 将 swift.conf 文件复制到 /etc/swift 每个存储节点以及运行代理服务的所有其他节点上的目录中 ⚠️这一步在控制节点执行 scp /etc/swift/swift.conf object01:/etc/swift scp /etc/swift/swift.conf object02:/etc/swift 10.11.4 在所有节点上，确保对配置目录拥有适当的所有权 在控制节点和两个对象存储节点执行 chown -R root:swift /etc/swift 10.11.5 在控制器节点和运行代理服务的任何其他节点上，启动对象存储代理服务及其相关性，并将它们配置为在系统启动时启动 ⚠️这一步在控制节点执行 systemctl enable openstack-swift-proxy.service memcached.service systemctl start openstack-swift-proxy.service memcached.service 10.11.6 在存储节点上，启动对象存储服务并将其配置为在系统引导时启动 两个对象存储节点执行 systemctl enable openstack-swift-account \\ openstack-swift-account-auditor \\ openstack-swift-account-reaper \\ openstack-swift-account-replicator \\ openstack-swift-container \\ openstack-swift-container-auditor \\ openstack-swift-container-replicator \\ openstack-swift-container-updater \\ openstack-swift-object \\ openstack-swift-object-auditor \\ openstack-swift-object-replicator \\ openstack-swift-object-updater systemctl start openstack-swift-account \\ openstack-swift-account-auditor \\ openstack-swift-account-reaper \\ openstack-swift-account-replicator \\ openstack-swift-container \\ openstack-swift-container-auditor \\ openstack-swift-container-replicator \\ openstack-swift-container-updater \\ openstack-swift-object \\ openstack-swift-object-auditor \\ openstack-swift-object-replicator \\ openstack-swift-object-updater 10.12 验证操作 rocky版对象存储服务swift验证操作官方文档 在控制器上执行以下步骤 ⚠️⚠️⚠️ 如果您使用的是Red Hat Enterprise Linux 7或CentOS 7，并且其中一个或多个步骤不起作用，请检查该/var/log/audit/audit.log文件中是否有SELinux消息，表明该swift进程拒绝采取措施。如果存在，请将/srv/node目录的安全上下文更改为swift_data_t类型，object_r 角色和system_u用户的最低安全级别（s0）： chcon -R system_u:object_r:swift_data_t:s0 /srv/node 9.12.1 获取demo凭证 ⚠️⚠️⚠️这里不知道是什么原因(也肯能是我自己哪里出错了)，如果按照官方文档中加载demo凭证，那么在执行swift stat命令时会报错403权限拒绝，所以这里加载了admin凭证 source /opt/admin-openrc 9.12.2 显示服务状态 $ swift stat Account: AUTH_a8cb8e52e5a44288b2ac1a216195ee10 Containers: 0 Objects: 0 Bytes: 0 X-Put-Timestamp: 1590683616.82847 X-Timestamp: 1590683616.82847 X-Trans-Id: tx85844a798ed34d998b692-005ecfe7e0 Content-Type: text/plain; charset=utf-8 X-Openstack-Request-Id: tx85844a798ed34d998b692-005ecfe7e0 9.12.3 创建container1容器 openstack container create container1 +---------------------------------------+------------+------------------------------------+ | account | container | x-trans-id | +---------------------------------------+------------+------------------------------------+ | AUTH_a8cb8e52e5a44288b2ac1a216195ee10 | container1 | tx3e166ae00ded4264a0dbe-005ecfea29 | +---------------------------------------+------------+------------------------------------+ 9.12.4 将测试文件上传到container1容器 #需要创建一个测试文件 echo test >/tmp/test.txt $ openstack object create container1 /tmp/test.txt +---------------+------------+----------------------------------+ | object | container | etag | +---------------+------------+----------------------------------+ | /tmp/test.txt | container1 | d8e8fca2dc0f896fd7cb4cb0031ba249 | +---------------+------------+----------------------------------+ 9.12.5 列出container1容器中的文件 openstack object list container1 +---------------+ | Name | +---------------+ | /tmp/test.txt | +---------------+ 9.12.6 从container1容器下载测试文件 #先把本地的测试文件/tmp/test.txt删除 rm /tmp/test.txt #然后下载测试文件，能下载并且文件内容不变即为正确 openstack object save container1 /tmp/test.txt 到此，对象存储服务swift安装完成 rocky版更多服务参考这个官方文档 十、安装和配置备份服务(可选) 安装和配置备份服务。为简单起见，此配置使用“块存储”节点和“对象存储”（驱动程序）驱动程序，因此取决于“ 对象存储”服务。 在安装和配置备份服务之前，必须先安装和配置存储节点。 在块存储节点上执行以下步骤 10.1 安装软件包 yum -y install openstack-cinder 10.2 编辑/etc/cinder/cinder.conf文件并完成以下操作 在该[DEFAULT]部分中，配置备份选项： [DEFAULT] # ... backup_driver = cinder.backup.drivers.swift backup_swift_url = SWIFT_URL #用以下命令修改 openstack-config --set /etc/cinder/cinder.conf DEFAULT backup_driver cinder.backup.drivers.swift openstack-config --set /etc/cinder/cinder.conf DEFAULT backup_swift_url http://controller:8080/v1/AUTH_ee435c972a7a476cadbd2c9ad782c6f0 public的url每次都是不一样的 替换SWIFT_URL为对象存储服务的URL。可以通过显示对象库API端点来找到URL，在控制节点上执行命令openstack catalog show object-store openstack catalog show object-store +-----------+-----------------------------------------------------------------------------+ | Field | Value | +-----------+-----------------------------------------------------------------------------+ | endpoints | RegionOne | | | admin: http://controller:8080/v1 | | | RegionOne | | | public: http://controller:8080/v1/AUTH_a8cb8e52e5a44288b2ac1a216195ee10 | | | RegionOne | | | internal: http://controller:8080/v1/AUTH_a8cb8e52e5a44288b2ac1a216195ee10 | | | | | id | b5169189845a4fb1b80fe1ab06584ffc | | name | swift | | type | object-store | +-----------+-----------------------------------------------------------------------------+ 10.3 启动块存储备份服务，并将其配置为在系统启动时启动 systemctl enable openstack-cinder-backup.service systemctl start openstack-cinder-backup.service 到此块存储节点块存储备份服务安装完成！！！ 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/openstack/rocky/2.rocky版使用命令启动一个实例.html":{"url":"linux/openstack/rocky/2.rocky版使用命令启动一个实例.html","title":"rocky版使用命令启动一个实例","keywords":"","body":"[toc] rocky版使用命令启动一个实例 rocky版启动实例官方文档 因为选择的是提供商网络，即网络1，所以参考这个官方文档 1.创建网络 openstack network create 创建网络 --shared 创建共享网络 --provider-physical-network 指定物理网卡名称 provider网络标签 --provider-network-type 指定网络类型 flat是桥接网络 pptfz是网络名称 openstack network create --share --external \\ --provider-physical-network provider \\ --provider-network-type flat pptfz +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | UP | | availability_zone_hints | | | availability_zones | | | created_at | 2020-05-26T12:41:05Z | | description | | | dns_domain | None | | id | 26fa223a-231f-419a-a387-750d6eabf3fe | | ipv4_address_scope | None | | ipv6_address_scope | None | | is_default | None | | is_vlan_transparent | None | | mtu | 1500 | | name | pptfz | | port_security_enabled | True | | project_id | 108d3fecb61840e3818f694c69c3ec4a | | provider:network_type | flat | | provider:physical_network | provider | | provider:segmentation_id | None | | qos_policy_id | None | | revision_number | 1 | | router:external | External | | segments | None | | shared | True | | status | ACTIVE | | subnets | | | tags | | | updated_at | 2020-05-26T12:41:05Z | +---------------------------+--------------------------------------+ 2.创建一个子网 创建一个名为pptfz的子网，依据的网络是第一步中创建的网络pptfz openstack subnet create --network pptfz \\ --allocation-pool start=10.0.0.101,end=10.0.0.250 \\ --dns-nameserver 223.5.5.5 --gateway 10.0.0.1 \\ --subnet-range 10.0.0.0/24 pptfz +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | allocation_pools | 10.0.0.101-10.0.0.250 | | cidr | 10.0.0.0/24 | | created_at | 2020-05-26T12:45:13Z | | description | | | dns_nameservers | 223.5.5.5 | | enable_dhcp | True | | gateway_ip | 10.0.0.1 | | host_routes | | | id | ad3d939a-866f-4b2b-9321-29e98fe64f26 | | ip_version | 4 | | ipv6_address_mode | None | | ipv6_ra_mode | None | | name | pptfz | | network_id | 26fa223a-231f-419a-a387-750d6eabf3fe | | project_id | 108d3fecb61840e3818f694c69c3ec4a | | revision_number | 0 | | segment_id | None | | service_types | | | subnetpool_id | None | | tags | | | updated_at | 2020-05-26T12:45:13Z | +-------------------+--------------------------------------+ 3.创建云主机的硬件配置方案 openstack flavor create --id 0 --vcpus 1 --ram 64 --disk 1 m1.nano +----------------------------+---------+ | Field | Value | +----------------------------+---------+ | OS-FLV-DISABLED:disabled | False | | OS-FLV-EXT-DATA:ephemeral | 0 | | disk | 1 | | id | 0 | | name | m1.nano | | os-flavor-access:is_public | True | | properties | | | ram | 64 | | rxtx_factor | 1.0 | | swap | | | vcpus | 1 | +----------------------------+---------+ #参数说明 flavor 硬件配置方案 --id 指定编号 --vcpus cpu个数 --ram 内存（单位：M） --disk 磁盘（单位：G） m1.nano 方案名称 #创建其余配置 openstack flavor create --id 1 --vcpus 1 --ram 512 --disk 5 m1.tiny1 openstack flavor create --id 2 --vcpus 1 --ram 1024 --disk 5 m1.tiny2 openstack flavor create --id 3 --vcpus 1 --ram 2048 --disk 10 m1.small openstack flavor create --id 4 --vcpus 2 --ram 4096 --disk 20 m1.medium 4.创建密钥对 ssh-keygen -q -N \"\" -f ~/.ssh/id_rsa && \\ openstack keypair create --public-key ~/.ssh/id_rsa.pub mykey +-------------+-------------------------------------------------+ | Field | Value | +-------------+-------------------------------------------------+ | fingerprint | 38:66:ea:65:c2:4d:50:3c:ee:b1:86:a2:2a:af:70:03 | | name | mykey | | user_id | a0d3db84d1984a24ac6ba213525fe382 | +-------------+-------------------------------------------------+ //参数说明 ssh-keygen -q -N \"\" -f ~/.ssh/id_rsa 非交互式生成密钥对 -q 安静模式 -N 指定加密密码 -f 密钥对存放位置 openstack keypair create --public-key ~/.ssh/id_rsa.pub mykey 上传密钥对 #验证密钥对的添加 openstack keypair list +-------+-------------------------------------------------+ | Name | Fingerprint | +-------+-------------------------------------------------+ | mykey | 38:66:ea:65:c2:4d:50:3c:ee:b1:86:a2:2a:af:70:03 | +-------+-------------------------------------------------+ 5.创建安全组规则 **默认情况下，default安全组适用于所有实例，并包括拒绝对实例进行远程访问的防火墙规则。 许可ICMP（ping） openstack security group rule create --proto icmp default +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | created_at | 2020-05-26T12:53:35Z | | description | | | direction | ingress | | ether_type | IPv4 | | id | 838f15e5-7b23-42e3-aadc-a16887830efc | | name | None | | port_range_max | None | | port_range_min | None | | project_id | 108d3fecb61840e3818f694c69c3ec4a | | protocol | icmp | | remote_group_id | None | | remote_ip_prefix | 0.0.0.0/0 | | revision_number | 0 | | security_group_id | 04ea403e-40c7-4881-a8d7-e62825e6509c | | updated_at | 2020-05-26T12:53:35Z | +-------------------+--------------------------------------+ 允许安全外壳（SSH）访问 openstack security group rule create --proto tcp --dst-port 22 default +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | created_at | 2020-05-26T12:53:49Z | | description | | | direction | ingress | | ether_type | IPv4 | | id | 5e7a6e16-14c7-4204-9651-52e0559a2a92 | | name | None | | port_range_max | 22 | | port_range_min | 22 | | project_id | 108d3fecb61840e3818f694c69c3ec4a | | protocol | tcp | | remote_group_id | None | | remote_ip_prefix | 0.0.0.0/0 | | revision_number | 0 | | security_group_id | 04ea403e-40c7-4881-a8d7-e62825e6509c | | updated_at | 2020-05-26T12:53:49Z | +-------------------+--------------------------------------+ 6.启动一个实例 6.1 确定实例的选项 6.1.1 在控制器节点上，demo获取凭据以访问仅用户的CLI命令 source /opt/demo-openrc 6.1.2 指定虚拟资源分配配置文件，其中包括处理器，内存和存储，列出可用的规格 openstack flavor list +----+-----------+------+------+-----------+-------+-----------+ | ID | Name | RAM | Disk | Ephemeral | VCPUs | Is Public | +----+-----------+------+------+-----------+-------+-----------+ | 0 | m1.nano | 64 | 1 | 0 | 1 | True | | 1 | m1.tiny1 | 512 | 5 | 0 | 1 | True | | 2 | m1.tiny2 | 1024 | 5 | 0 | 1 | True | | 3 | m1.small | 2048 | 10 | 0 | 1 | True | | 4 | m1.medium | 4096 | 20 | 0 | 2 | True | +----+-----------+------+------+-----------+-------+-----------+ 6.1.3 列出可用的镜像 openstack image list +--------------------------------------+--------+--------+ | ID | Name | Status | +--------------------------------------+--------+--------+ | 94c96aab-d0b3-4340-835c-9a97108d0554 | cirros | active | +--------------------------------------+--------+--------+ 6.1.4 列出可用的网络 openstack network list +--------------------------------------+-------+--------------------------------------+ | ID | Name | Subnets | +--------------------------------------+-------+--------------------------------------+ | 26fa223a-231f-419a-a387-750d6eabf3fe | pptfz | ad3d939a-866f-4b2b-9321-29e98fe64f26 | +--------------------------------------+-------+--------------------------------------+ 6.1.5 列出可用的安全组 openstack security group list +--------------------------------------+---------+------------------------+----------------------------------+------+ | ID | Name | Description | Project | Tags | +--------------------------------------+---------+------------------------+----------------------------------+------+ | e65b8ec1-3544-414c-94d0-2c87dd6eadbf | default | Default security group | 5b9ccd294c364cc68747df85f9598c89 | [] | +--------------------------------------+---------+------------------------+----------------------------------+------+ 6.2 启动一个实例 官网启动示例 openstack server create --flavor m1.nano --image cirros \\ --nic net-id=PROVIDER_NET_ID --security-group default \\ --key-name mykey provider-instance 启动实例过程中需要用到net-id，因此使用neutron net-list|awk 'NR==4{print $2}'获取 openstack server create --flavor m1.nano --image cirros \\ --nic net-id=`neutron net-list|awk 'NR==4{print $2}'` --security-group default \\ --key-name mykey pptfz neutron CLI is deprecated and will be removed in the future. Use openstack CLI instead. +-----------------------------+-----------------------------------------------+ | Field | Value | +-----------------------------+-----------------------------------------------+ | OS-DCF:diskConfig | MANUAL | | OS-EXT-AZ:availability_zone | | | OS-EXT-STS:power_state | NOSTATE | | OS-EXT-STS:task_state | scheduling | | OS-EXT-STS:vm_state | building | | OS-SRV-USG:launched_at | None | | OS-SRV-USG:terminated_at | None | | accessIPv4 | | | accessIPv6 | | | addresses | | | adminPass | jVR6on2EN7AX | | config_drive | | | created | 2020-05-26T13:08:14Z | | flavor | m1.nano (0) | | hostId | | | id | da4a32c7-e51e-4de0-b8a9-c0533db7e6fd | | image | cirros (94c96aab-d0b3-4340-835c-9a97108d0554) | | key_name | mykey | | name | pptfz | | progress | 0 | | project_id | 5b9ccd294c364cc68747df85f9598c89 | | properties | | | security_groups | name='e65b8ec1-3544-414c-94d0-2c87dd6eadbf' | | status | BUILD | | updated | 2020-05-26T13:08:14Z | | user_id | 82d945a092b44988af8d6e02ba2cc15c | | volumes_attached | | +-----------------------------+-----------------------------------------------+ 查看示例启动状态 openstack server list +--------------------------------------+-------+--------+------------------+--------+---------+ | ID | Name | Status | Networks | Image | Flavor | +--------------------------------------+-------+--------+------------------+--------+---------+ | 3191793e-055d-4417-bab5-df6c6574aaed | pptfz | ACTIVE | pptfz=10.0.0.113 | cirros | m1.nano | +--------------------------------------+-------+--------+------------------+--------+---------+ 创建成功后会在web界面展示 点击控制台登陆虚拟机 ⚠️点击控制台后提示找不到controller地址，因为没有做hosts解析，需要先做hosts解析 windows C:\\Windows\\System32\\drivers\\etc\\hosts 10.0.0.11 controller mac /etc/hosts 10.0.0.11 controller 解析完后刷新浏览器，会看到默认用户是cirros，密码是gocubsgo 查看主机名、IP地址、检查是否能联网 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/openstack/rocky/3.openstack rocky版手动增加一个计算节点.html":{"url":"linux/openstack/rocky/3.openstack rocky版手动增加一个计算节点.html","title":"openstack rocky版手动增加一个计算节点","keywords":"","body":"[toc] openstack rocky版手动增加一个计算节点 1.配置host解析 cat /etc/hosts 2.配置yum源 yum -y install centos-release-openstack-rocky yum -y install python-openstackclient 3.时间同步 1.安装chrony yum -y install chrony 2.编辑chrony配置文件/etc/chrony.conf /删除以下4行，指定控制节点为NTP服务器 server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst 修改为 server controller iburst #用以下命令修改 sed -i '3,6d' /etc/chrony.conf && sed -i '3cserver controller iburst' /etc/chrony.conf 3.启动NTP服务并设置开机自启 systemctl enable chronyd && systemctl start chronyd 4.检查端口，监听udp323端口 $ netstat -nupl|grep chronyd udp 0 0 127.0.0.1:323 0.0.0.0:* 1327/chronyd udp6 0 0 ::1:323 :::* 1327/chronyd 5.验证，计算节点显示的是控制节点 $ chronyc sources 210 Number of sources = 1 MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== ^? controller 3 6 200 50 +1319ms[+1319ms] +/- 14.4s 4.安装软件包 yum -y install openstack-nova-compute openstack-utils 5.配置计算服务nova 5.1 编辑/etc/nova/nova.conf文件并完成以下操作 ⚠️compute2的IP地址为10.0.0.32 1.在此[DEFAULT]部分中，仅启用计算和元数据API [DEFAULT] # ... enabled_apis = osapi_compute,metadata 2.在该[DEFAULT]部分中，配置RabbitMQ消息队列访问： [DEFAULT] # ... transport_url = rabbit://openstack:RABBIT_PASS@controller 3.在[api]和[keystone_authtoken]部分中，配置身份服务访问： [api] # ... auth_strategy = keystone [keystone_authtoken] # ... auth_url = http://controller:5000/v3 memcached_servers = controller:11211 auth_type = password project_domain_name = Default user_domain_name = Default project_name = service username = nova password = NOVA_PASS 4.在该[DEFAULT]部分中，配置my_ip选项：MANAGEMENT_INTERFACE_IP_ADDRESS为计算节点上管理网络接口的IP地址 my_ip = MANAGEMENT_INTERFACE_IP_ADDRESS [DEFAULT] # ... my_ip = 10.0.0.32 5.在本[DEFAULT]节中，启用对网络服务的支持： [DEFAULT] # ... use_neutron = true firewall_driver = nova.virt.firewall.NoopFirewallDriver 6.在本[DEFAULT]节中，启用对网络服务的支持： [DEFAULT] # ... use_neutron = true firewall_driver = nova.virt.firewall.NoopFirewallDriver 7.在该[vnc]部分中，启用和配置远程控制台访问： [vnc] # ... enabled = true server_listen = 0.0.0.0 server_proxyclient_address = $my_ip novncproxy_base_url = http://controller:6080/vnc_auto.html 8.在该[glance]部分中，配置图像服务API的位置： [glance] # ... api_servers = http://controller:9292 9.在该[oslo_concurrency]部分中，配置锁定路径： [oslo_concurrency] # ... lock_path = /var/lib/nova/tmp 10.在该[placement]部分中，配置Placement API： [placement] # ... region_name = RegionOne project_domain_name = Default project_name = service auth_type = password user_domain_name = Default auth_url = http://controller:5000/v3 username = placement password = PLACEMENT_PASS #用以下命令修改 \\cp /etc/nova/nova.conf{,.bak} grep '^[a-Z\\[]' /etc/nova/nova.conf.bak >/etc/nova/nova.conf openstack-config --set /etc/nova/nova.conf DEFAULT enabled_apis osapi_compute,metadata openstack-config --set /etc/nova/nova.conf DEFAULT transport_url rabbit://openstack:RABBIT_PASS@controller openstack-config --set /etc/nova/nova.conf DEFAULT my_ip 10.0.0.32 openstack-config --set /etc/nova/nova.conf DEFAULT use_neutron true openstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriver openstack-config --set /etc/nova/nova.conf api auth_strategy keystone openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_url http://controller:5000/v3 openstack-config --set /etc/nova/nova.conf keystone_authtoken memcached_servers controller:11211 openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_type password openstack-config --set /etc/nova/nova.conf keystone_authtoken project_domain_name Default openstack-config --set /etc/nova/nova.conf keystone_authtoken user_domain_name Default openstack-config --set /etc/nova/nova.conf keystone_authtoken project_name service openstack-config --set /etc/nova/nova.conf keystone_authtoken username nova openstack-config --set /etc/nova/nova.conf keystone_authtoken password NOVA_PASS openstack-config --set /etc/nova/nova.conf vnc enabled true openstack-config --set /etc/nova/nova.conf vnc server_listen 0.0.0.0 openstack-config --set /etc/nova/nova.conf vnc server_proxyclient_address '$my_ip' openstack-config --set /etc/nova/nova.conf vnc novncproxy_base_url http://controller:6080/vnc_auto.html openstack-config --set /etc/nova/nova.conf glance api_servers http://controller:9292 openstack-config --set /etc/nova/nova.conf oslo_concurrency lock_path /var/lib/nova/tmp openstack-config --set /etc/nova/nova.conf placement region_name RegionOne openstack-config --set /etc/nova/nova.conf placement project_domain_name Default openstack-config --set /etc/nova/nova.conf placement project_name service openstack-config --set /etc/nova/nova.conf placement auth_type password openstack-config --set /etc/nova/nova.conf placement user_domain_name Default openstack-config --set /etc/nova/nova.conf placement auth_url http://controller:5000/v3 openstack-config --set /etc/nova/nova.conf placement username placement openstack-config --set /etc/nova/nova.conf placement password PLACEMENT_PASS MD5值 md5sum /etc/nova/nova.conf c348d4b98cb5dc08fb329050ef592d86 /etc/nova/nova.conf 5.2 启动Compute服务及其依赖项，并将它们配置为在系统引导时自动启动 systemctl enable libvirtd.service openstack-nova-compute.service && \\ systemctl start libvirtd.service openstack-nova-compute.service 6.配置网络服务neutron 6.1 安装包 yum -y install openstack-neutron-linuxbridge ebtables ipset openstack-utils 6.2 配置公共组件 编辑/etc/neutron/neutron.conf文件并完成以下操作 1.在该[DEFAULT]部分中，配置RabbitMQ 消息队列访问： [DEFAULT] # ... transport_url = rabbit://openstack:RABBIT_PASS@controller 2.在[DEFAULT]和[keystone_authtoken]部分中，配置身份服务访问： [DEFAULT] # ... auth_strategy = keystone [keystone_authtoken] # ... www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = NEUTRON_PASS 3.在该[oslo_concurrency]部分中，配置锁定路径： [oslo_concurrency] # ... lock_path = /var/lib/neutron/tmp #用以下命令修改 \\cp /etc/neutron/neutron.conf{,.bak} grep '^[a-Z\\[]' /etc/neutron/neutron.conf.bak >/etc/neutron/neutron.conf openstack-config --set /etc/neutron/neutron.conf DEFAULT transport_url rabbit://openstack:RABBIT_PASS@controller openstack-config --set /etc/neutron/neutron.conf DEFAULT auth_strategy keystone openstack-config --set /etc/neutron/neutron.conf keystone_authtoken www_authenticate_uri http://controller:5000 openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_url http://controller:5000 openstack-config --set /etc/neutron/neutron.conf keystone_authtoken memcached_servers controller:11211 openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_type password openstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_domain_name default openstack-config --set /etc/neutron/neutron.conf keystone_authtoken user_domain_name default openstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_name service openstack-config --set /etc/neutron/neutron.conf keystone_authtoken username neutron openstack-config --set /etc/neutron/neutron.conf keystone_authtoken password NEUTRON_PASS openstack-config --set /etc/neutron/neutron.conf oslo_concurrency lock_path /var/lib/neutron/tmp MD5值 md5sum /etc/neutron/neutron.conf 9c47ffb59b23516b59e7de84a39bcbe8 /etc/neutron/neutron.conf 6.3 配置网络选项 6.3.1 配置桥接代理 Linux网桥代理为实例构建第2层（桥接和交换）虚拟网络基础结构并处理安全组 编辑/etc/neutron/plugins/ml2/linuxbridge_agent.ini文件并完成以下操作 1.在本[linux_bridge]节中，将提供者虚拟网络映射到提供者物理网络接口： [linux_bridge] physical_interface_mappings = provider:PROVIDER_INTERFACE_NAME 替换PROVIDER_INTERFACE_NAME为基础提供商物理网络接口的名称。这里为eth0 2.在该[vxlan]部分中，禁用VXLAN覆盖网络： [vxlan] enable_vxlan = false 3.在该[securitygroup]部分中，启用安全组并配置Linux网桥iptables防火墙驱动程序： [securitygroup] # ... enable_security_group = true firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver #用以下命令修改 \\cp /etc/neutron/plugins/ml2/linuxbridge_agent.ini{,.bak} grep '^[a-Z\\[]' /etc/neutron/plugins/ml2/linuxbridge_agent.ini.bak >/etc/neutron/plugins/ml2/linuxbridge_agent.ini openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini linux_bridge physical_interface_mappings provider:eth0 openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini vxlan enable_vxlan false openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini securitygroup enable_security_group true openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini securitygroup firewall_driver neutron.agent.linux.iptables_firewall.IptablesFirewallDriver MD5值 md5sum /etc/neutron/plugins/ml2/linuxbridge_agent.ini 794b19995c83e2fc0c3fd42f506904f1 /etc/neutron/plugins/ml2/linuxbridge_agent.ini 使Linux操作系统内核支持网桥过滤器1 #向/etc/sysctl.d/openstack-rocky-bridge.conf写入以下内容 cat >> /etc/sysctl.d/openstack-rocky-bridge.conf 6.4 配置计算以使用网络 编辑/etc/nova/nova.conf文件并完成以下操作 在该[neutron]部分中，配置访问参数： [neutron] # ... url = http://controller:9696 auth_url = http://controller:5000 auth_type = password project_domain_name = default user_domain_name = default region_name = RegionOne project_name = service username = neutron password = NEUTRON_PASS #用以下命令修改 openstack-config --set /etc/nova/nova.conf neutron url http://controller:9696 openstack-config --set /etc/nova/nova.conf neutron auth_url http://controller:5000 openstack-config --set /etc/nova/nova.conf neutron auth_type password openstack-config --set /etc/nova/nova.conf neutron project_domain_name default openstack-config --set /etc/nova/nova.conf neutron user_domain_name default openstack-config --set /etc/nova/nova.conf neutron region_name RegionOne openstack-config --set /etc/nova/nova.conf neutron project_name service openstack-config --set /etc/nova/nova.conf neutron username neutron openstack-config --set /etc/nova/nova.conf neutron password NEUTRON_PASS MD5值 md5sum /etc/nova/nova.conf 6ba9c6b28eb8145e4bbec4d9942d1774 /etc/nova/nova.conf 6.5 完成安装 6.5.1 重新启动Compute服务 systemctl restart openstack-nova-compute.service 6.5.2 启动Linux网桥代理并将其配置为在系统引导时启动 systemctl enable neutron-linuxbridge-agent.service && \\ systemctl start neutron-linuxbridge-agent.service 6.6 验证 控制节点执行 有compute2输出并且状态为up即为正确 openstack network agent list +--------------------------------------+--------------------+------------+-------------------+-------+-------+---------------------------+ | ID | Agent Type | Host | Availability Zone | Alive | State | Binary | +--------------------------------------+--------------------+------------+-------------------+-------+-------+---------------------------+ | 6f3e4e38-0d60-4fd7-b545-b6accccc728e | Linux bridge agent | compute2 | None | :-) | UP | neutron-linuxbridge-agent | | b627a998-e6d2-4cea-b6f1-773f0a294823 | DHCP agent | controller | nova | :-) | UP | neutron-dhcp-agent | | d2999370-36db-43c1-9fa8-dbdf9afcd118 | Linux bridge agent | compute1 | None | :-) | UP | neutron-linuxbridge-agent | | f6e6488d-2be8-425d-99ea-97974450cedf | Linux bridge agent | controller | None | :-) | UP | neutron-linuxbridge-agent | | fc7271f1-f214-4a17-bda7-ba340a61e0f9 | Metadata agent | controller | None | :-) | UP | neutron-metadata-agent | +--------------------------------------+--------------------+------------+-------------------+-------+-------+---------------------------+ 7.验证计算节点是否可用 手动创建虚拟机并且指定调度到compute2 需要先创建分组，分组在dashboard中叫主机聚合 查看主机聚合 管理员-->计算-->主机聚合 创建主机聚合 创建一个名称为compute2的主机聚合，可用域也定义为compute2 点击管理聚合内的主机，选择compute2，然后创建主机聚合 创建完成之后可用域中就会多出一个compute2域 然后web界面创建虚拟机 项目-->计算-->实例-->创建实例 实例名称为compute2，可用域选择compute2 选择测试镜像ciiros 选择实例规格，然后点击创建实例 成功创建后的实例 进入实例控制台，使用用户cirros、密码gocubsgo登陆，然后sudo su -切换到root用户，设置root用户密码 接下来就可以使用远程连接工具连接虚拟机了 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/openstack/rocky/4.openstack 域、用户、项目、角色之间的关系.html":{"url":"linux/openstack/rocky/4.openstack 域、用户、项目、角色之间的关系.html","title":"openstack 域、用户、项目、角色之间的关系","keywords":"","body":"openstack 域、用户、项目、角色之间的关系 创建默认域 openstack domain create --description \"Default Domain\" default 创建管理项目 openstack project create --domain default \\ --description \"Admin Project\" admin 创建管理员用户 openstack user create --domain default \\ --password ADMIN_PASS admin 创建管理员角色 openstack role create admin 将admin角色添加到admin项目和用户 openstack role add --project admin --user admin admin 示意图 项目、用户、角色关联关系 在xx项目中给xx用户授予xx角色 openstack中会有一个default默认域，也可以创建其他的域 default域中会有admin项目、service项目，也可以创建自定义项目 default域中会有admin用户、服务用户，也可以创建自定义用户 默认的角色是管理员admin角色和普通角色`user角色` 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/openstack/rocky/5.glance镜像服务迁移.html":{"url":"linux/openstack/rocky/5.glance镜像服务迁移.html","title":"glance镜像服务迁移","keywords":"","body":"[toc] glance镜像服务迁移 1.迁移步骤 1、glance数据库迁移 2、在新机器上安装glance服务 3、迁移之前已有的镜像 4、在keystone上，修改glance服务的api地址 5、修改控制节点和计算节点nova配置文件中glance的api地址以及块存储节点中cinder配置文件中glance的api地址 6、上传新的镜像测试glance服务是否正常 7、启动一台新的虚拟机完成测试 2.迁移过程 2.1 控制节点导出glance数据库并拷贝到新机器 新机器的IP地址是10.0.0.10 #备份glance数据库 mysqldump -B glance >glance-db.sql #拷贝备份文件到新机器 scp -p glance-db.sql 10.0.0.10:~ 2.2 新机器配置hosts解析 cat >> /etc/hosts 2.3 新机器安装mariadb并设置开启自启 yum -y install mariadb mariadb-server python2-PyMySQL systemctl enable mariadb && systemctl start mariadb 2.4 新机器导入数据库glance-db.sql mysql 2.5 新机器给glance数据库授权 #用以下命令修改 mysql -e \"GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' \\ IDENTIFIED BY 'GLANCE_DBPASS';\" mysql -e \"GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' \\ IDENTIFIED BY 'GLANCE_DBPASS';\" 2.6 控制节点停止glance服务 systemctl stop openstack-glance-api openstack-glance-registry systemctl disable openstack-glance-api openstack-glance-registry 2.7 新机器安装glance yum -y install openstack-glance openstack-utils 2.8 新机器拷贝控制节点glance配置文件 scp -rp 10.0.0.11:/etc/glance/glance-api.conf /etc/glance scp -rp 10.0.0.11:/etc/glance/glance-registry.conf /etc/glance 2.9 新机器编辑glance配置文件/etc/glance/glance-api.conf，修改数据库连接 openstack-config --set /etc/glance/glance-api.conf database connection mysql+pymysql://glance:GLANCE_DBPASS@localhost/glance MD5值 md5sum /etc/glance/glance-api.conf 76d2be813471725c008245a9d135ea92 /etc/glance/glance-api.conf 2.10 新机器编辑glance配置文件/etc/glance/glance-registry.conf openstack-config --set /etc/glance/glance-registry.conf database connection mysql+pymysql://glance:GLANCE_DBPASS@localhost/glance md5sum /etc/glance/glance-registry.conf 7fefe761789d2d4d2afa3409b0e22bb0 /etc/glance/glance-registry.conf 2.11 新机器启动glance服务 glance-api 监听tcp/9292 glance-registry 监听tcp/9191 systemctl start openstack-glance-api openstack-glance-registry systemctl enable openstack-glance-api openstack-glance-registry 2.12 新机器拷贝控制节点/var/lib/glance/images下的镜像到本地的/var/lib/glance/images目录下 #如果使用scp的话，拷贝过来的镜像还需要手动修改镜像文件所有者为glance，拷贝过来的文件默认所有者是root rsync -avz 10.0.0.11:/var/lib/glance/images/* /var/lib/glance/images/ 2.13 控制节点上修改keystone中glance api的地址 keystone数据库中的endpoint表记录了各个api地址 2.13.1 备份keystone数据库中的endpoint表 mysqldump keystone endpoint > keystone-endpoint.sql 2.13.2 keystone数据库中的endpoint表记录了各api的地址，可以先查询一下endpoint表中有9292端口的记录 mysql keystone -e \"select * from endpoint where url like '%9292'\" +----------------------------------+--------------------+-----------+----------------------------------+------------------------+-------+---------+-----------+ | id | legacy_endpoint_id | interface | service_id | url | extra | enabled | region_id | +----------------------------------+--------------------+-----------+----------------------------------+------------------------+-------+---------+-----------+ | 0db3f8b44aff4501a3c866ba01bd546d | NULL | public | eb33b46815ba43a982fc39a9737efa7f | http://controller:9292 | {} | 1 | RegionOne | | 3119eb7c9e6e4bf98cf9b9738d53703d | NULL | admin | eb33b46815ba43a982fc39a9737efa7f | http://controller:9292 | {} | 1 | RegionOne | | 6382b60ec0a94472b6e68f8ecc1d13fe | NULL | internal | eb33b46815ba43a982fc39a9737efa7f | http://controller:9292 | {} | 1 | RegionOne | +----------------------------------+--------------------+-----------+----------------------------------+------------------------+-------+---------+-----------+ 2.13.3 然后修改这3条记录中的url mysql keystone -e \"update endpoint set url='http://10.0.0.10:9292' where url like '%9292'\" 2.13.4 再次查看9292相关记录 url中的地址修改为了10.0.0.10 mysql keystone -e \"select * from endpoint where url like '%9292'\" +----------------------------------+--------------------+-----------+----------------------------------+-----------------------+-------+---------+-----------+ | id | legacy_endpoint_id | interface | service_id | url | extra | enabled | region_id | +----------------------------------+--------------------+-----------+----------------------------------+-----------------------+-------+---------+-----------+ | 0db3f8b44aff4501a3c866ba01bd546d | NULL | public | eb33b46815ba43a982fc39a9737efa7f | http://10.0.0.10:9292 | {} | 1 | RegionOne | | 3119eb7c9e6e4bf98cf9b9738d53703d | NULL | admin | eb33b46815ba43a982fc39a9737efa7f | http://10.0.0.10:9292 | {} | 1 | RegionOne | | 6382b60ec0a94472b6e68f8ecc1d13fe | NULL | internal | eb33b46815ba43a982fc39a9737efa7f | http://10.0.0.10:9292 | {} | 1 | RegionOne | +----------------------------------+--------------------+-----------+----------------------------------+-----------------------+-------+---------+-----------+ 2.14 控制节点验证修改是否成功 openstack endpoint list|grep glance | 0db3f8b44aff4501a3c866ba01bd546d | RegionOne | glance | image | True | public | http://10.0.0.10:9292 | | 3119eb7c9e6e4bf98cf9b9738d53703d | RegionOne | glance | image | True | admin | http://10.0.0.10:9292 | | 6382b60ec0a94472b6e68f8ecc1d13fe | RegionOne | glance | image | True | internal | http://10.0.0.10:9292 2.15 控制节点、计算节点修改/etc/nova/nova.conf中glance的地址 1.修改nova配置文件 openstack-config --set /etc/nova/nova.conf glance api_servers http://10.0.0.10:9292 2.控制节点重启nova-api systemctl restart openstack-nova-api 3.计算节点重启nova-compute systemctl restart openstack-nova-compute 2.16 块存储节点修改配置文件/etc/cinder/cinder.conf中的glance api的地址 1.修改cinder配置文件中glance api地址 openstack-config --set /etc/cinder/cinder.conf DEFAULT glance_api_servers http://10.0.0.10:9292 2.重启cinder服务 systemctl restart openstack-cinder-volume.service target.service 2.17 最后web界面上传镜像并启动虚拟机测试 能成功上传镜像并能成功启动虚拟机即为成功 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/openstack/rocky/6.openstack中的卷.html":{"url":"linux/openstack/rocky/6.openstack中的卷.html","title":"openstack中的卷","keywords":"","body":"[toc] openstack中的卷 openstack中卷可以转换成镜像，镜像也可以转换成卷 在块存储节点上的配置文件/etc/lvm/lvm.conf中定义了使用lvm作为后端存储 a/sdb/代表一个lvm，a/sdc/代表另一个lvm，可以有多个磁盘做成lvm 在该devices部分中，添加一个接受/dev/sdb设备并拒绝所有其他设备的过滤 器： devices { ... filter = [ \"a/sdb/\", \"a/sdc/\", \"r/.*/\"] /etc/cinder/cinder.conf文件中关于后端存储的定义 enabled_backends = sata,ssd表示的是后端存储的名称，名称任意，这里使用/dev/sdb和/dev/sdc做了两个lvm，分别模拟普通的sata盘和较快速的固态硬盘(ssd) enabled_backends后边的任意名称会在配置文件中作为单独的一个区域存在，这个区域中的volume_backend_name会在后续创建卷的时候指定磁盘的类型，其中volume_backend_name是键，后边的sata和ssd是值，这样的话就能把普通磁盘和ssd磁盘区分开来并且给实例挂载不同的磁盘 #后段名称任意，这里我们定义为普通磁盘sata，固态硬盘ssd [DEFAULT] enabled_backends = sata,ssd [sata] volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volume_group = cinder-volumes iscsi_protocol = iscsi iscsi_helper = lioadm volume_backend_name = sata [ssd] volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volume_group = cinder-ssd iscsi_protocol = iscsi iscsi_helper = lioadm volume_backend_name = ssd 创建卷(没有卷) 项目-->卷-->卷-->创建卷 创建一个名为sata的卷 创建一个名为ssd的卷 创建后的卷 创建卷 管理员-->卷-->卷-->创建卷 创建sata和ssd卷 创建后的卷 创建卷扩展规格 点击创建后的卷-->查看扩展规格 点击已创建 创建卷扩展规格，写入配置文件中的键和值，这里示例了ssd，sata是同样的操作 键就是/etc/cinder/cinder.conf中enabled_backends对应的后端存储名称单独区域中的volume_backend_name #后段名称任意，这里我们定义为普通磁盘sata，固态硬盘ssd [DEFAULT] enabled_backends = sata,ssd [sata] volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volume_group = cinder-volumes iscsi_protocol = iscsi iscsi_helper = lioadm volume_backend_name = sata [ssd] volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volume_group = cinder-ssd iscsi_protocol = iscsi iscsi_helper = lioadm volume_backend_name = ssd 创建后的卷扩展规格 ssd sata 创建卷(有卷) 删除之前创建的没有卷的卷 创建sata类型的卷和ssd类型的卷 创建后的卷 给虚拟机关联卷 虚拟机中的初始磁盘，默认是1G大小 选择要连接卷的虚拟机，点击连接卷 选择一个卷连接 虚拟机中查看磁盘，发现会多了一个10G的磁盘 格式化后挂载 关于卷的安全性问题 上一步中把新关联的卷挂载到了/mnt，现在向新关联的卷写入一些内容 $ echo 'test ssd' > /mnt/test.txt $ cat /mnt/test.txt test ssd 然后在web界面查看卷的ID 项目-->卷-->卷-->点击ssd 在块存储节点上查看lv，可以看到volume-c1b20eb9-4e05-4fd6-816b-87d93c7cdcfb是对应的ssd卷 这个ssd的具体路径是/dev/mapper/cinder--ssd-volume--c1b20eb9--4e05--4fd6--816b--87d93c7cdcfb 当尝试挂载这个ssd时是不被允许的 [root@block1 ~]# mount /dev/mapper/cinder--ssd-volume--c1b20eb9--4e05--4fd6--816b--87d93c7cdcfb /opt mount: /dev/mapper/cinder--ssd-volume--c1b20eb9--4e05--4fd6--816b--87d93c7cdcfb is already mounted or /opt busy 可以采用先拷贝这个文件然后再挂载(cp、dd都可以) #拷贝目录 cp /dev/mapper/cinder--ssd-volume--c1b20eb9--4e05--4fd6--816b--87d93c7cdcfb /root #查看大小 [root@block1 ~]# ll -h total 10G -rw-r----- 1 root root 10G Jun 2 08:18 cinder--ssd-volume--c1b20eb9--4e05--4fd6--816b--87d93c7cdcfb #挂载 loop的意思是用来把一个文件当成硬盘分区mount到目录 mount -o loop cinder--ssd-volume--c1b20eb9--4e05--4fd6--816b--87d93c7cdcfb /mnt #进入/mnt目录，查看文件内容，此时是可以看见的 [root@block1 ~]# cd /mnt [root@block1 mnt]# ls lost+found test.txt [root@block1 mnt]# cat test.txt test ssd 计算节点上虚拟机中的磁盘是可以被拷贝到计算节点的，并且内容可见 把虚拟机启动在卷上 openstack中的虚拟机默认是启动在计算节点上的 点击实例名称，查看示例id 虚拟机的存放位置默认是计算节点中的/var/lib/nova/instances 选择创建新卷、删除实例时删除卷，并且指定卷大小 源中指定了卷大小为5G，所以这里要选择和卷大小相同规格的根磁盘 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/openstack/rocky/7.增加一个flat网络.html":{"url":"linux/openstack/rocky/7.增加一个flat网络.html","title":"增加flat网络","keywords":"","body":"[toc] 增加一个flat网络 控制节点和计算节点增加一块网卡，配置另外一个网段 控制节点 编辑/etc/neutron/plugins/ml2/ml2_conf.ini #官方文档中默认只配置一个网段 [ml2_type_flat] flat_networks = provider #现在再增加一个网段，名称为net_172_16_1 [ml2_type_flat] flat_networks = provider,net_172_16_1 编辑/etc/neutron/plugins/ml2/linuxbridge_agent.ini #原先配置 [linux_bridge] physical_interface_mappings = provider:eth0 #现在再增加一个网段 [linux_bridge] physical_interface_mappings = provider:eth0,net_172_16_1:eth1 重启服务 systemctl restart neutron-server.service neutron-linuxbridge-agent.service neutron-dhcp-agent.service 计算节点 编辑/etc/neutron/plugins/ml2/linuxbridge_agent.ini #原先配置 [linux_bridge] physical_interface_mappings = provider:eth0 #改为如下 [linux_bridge] physical_interface_mappings = provider:eth0,net_172_16_1:eth1 重启服务 systemctl restart neutron-linuxbridge-agent.service 控制节点检查neutron服务是否正常 neutron agent-list neutron CLI is deprecated and will be removed in the future. Use openstack CLI instead. +--------------------------------------+--------------------+------------+-------------------+-------+----------------+---------------------------+ | id | agent_type | host | availability_zone | alive | admin_state_up | binary | +--------------------------------------+--------------------+------------+-------------------+-------+----------------+---------------------------+ | b627a998-e6d2-4cea-b6f1-773f0a294823 | DHCP agent | controller | nova | :-) | True | neutron-dhcp-agent | | d2999370-36db-43c1-9fa8-dbdf9afcd118 | Linux bridge agent | compute1 | | :-) | True | neutron-linuxbridge-agent | | f6e6488d-2be8-425d-99ea-97974450cedf | Linux bridge agent | controller | | :-) | True | neutron-linuxbridge-agent | | fc7271f1-f214-4a17-bda7-ba340a61e0f9 | Metadata agent | controller | | :-) | True | neutron-metadata-agent | +--------------------------------------+--------------------+------------+-------------------+-------+----------------+---------------------------+ 创建网络 openstack network create --share --external \\ --provider-physical-network net_172_16_1 \\ --provider-network-type flat net_172_16_1 创建子网 一个网络可以对应多个子网 openstack subnet create --network net_172_16_1 \\ --allocation-pool start=172.16.1.10,end=172.16.1.250 \\ --dns-nameserver 223.5.5.5 --gateway 172.16.1.1 \\ --subnet-range 172.16.1.0/24 net_172_16_1 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/nginx/1.centos7安装nginx.html":{"url":"linux/linux服务/nginx/1.centos7安装nginx.html","title":"nginx安装","keywords":"","body":"[toc] centos7安装nginx nginx官网 nginx github地址 一、源码编译安装 nginx官网总下载地址 nginx官方源码下载地址 nginx github下载地址 nginx官方源码安装文档 1.1 下载nginx源码包 export NG_VERSION=1.21.0 wget https://nginx.org/download/nginx-${NG_VERSION}.tar.gz 1.2 编译安装nginx 1.2.1 创建www用户 useradd www -s /sbin/nologin -M 1.2.2 安装依赖包 yum -y install gcc gcc-c++ zlib zlib-devel openssl openssl-devel pcre-devel httpd-tools 1.2.3 解压缩包 tar xf nginx-${NG_VERSION}.tar.gz && cd nginx-${NG_VERSION} 1.2.4 编译安装 ./configure \\ --prefix=/usr/local/nginx \\ --user=www \\ --group=www \\ --sbin-path=/usr/sbin/nginx \\ --pid-path=/var/run/nginx.pid \\ --lock-path=/var/run/nginx.lock \\ --error-log-path=/var/log/nginx/error.log \\ --http-log-path=/var/log/nginx/access.log \\ --with-http_gzip_static_module \\ --with-http_stub_status_module \\ --with-http_ssl_module \\ --with-http_realip_module \\ --with-pcre \\ --with-file-aio make -j `nproc` && make install 编译参数说明 更多参数请使用./configure --help查看或者查看官方编译安装说明文档 参数 说明 --prefix nginx安装路径 --user nginx运行用户 --group nginx运行用户组 --sbin-path nginx可执行文件路径 --conf-path nginx配置文件路径 --pid-path nginx pid文件路径 --lock-path nginx锁文件路径 --error-log-path nginx错误日志路径 --http-log-path nginx访问日志路径 --with-http_gzip_static_module 支持.gz压缩文件 --with-http_stub_status_module 支持nginx基本状态信息访问 --with-http_ssl_module 支持nginx https --with-pcre 强制使用pcre库 --with-file-aio 支持异步IO --with-http_realip_module 将客户端地址更改为在指定的头字段中发送的地址 --without-http_scgi_module 禁止将请求传递到SCGI服务器 --without-http_uwsgi_module 禁止将请求传递到uwsgi服务器 --without-http_fastcgi_module 禁止将请求传递到FastCGI服务器 我们可以在nginx的配置文件中加入 server_tokens off 参数来隐藏nginx版本，如下图所示 如果想要把浏览器显示的 nginx 也修改，需要修改nginx源码中的4个文件，这里把 nginx 修改为 come baby 参考链接 文件一 src/core/nginx.h 修改前 #define NGINX_VER \"nginx/\" NGINX_VERSION #define NGINX_VAR \"NGINX\" 修改后 修改命令 sed -i -e \"s/nginx/$NGX/\" -e \"s/\\\"NGINX\\\"/\\\"$NGX\\\"/\" src/core/nginx.h #define NGINX_VER \"come baby/\" NGINX_VERSION #define NGINX_VAR \"come baby\" 文件二 src/http/ngx_http_header_filter_module.c 修改前 49行 static u_char ngx_http_server_string[] = \"Server: nginx\" CRLF; 修改后 修改命令 sed -i \"s/: nginx/: $NGX/\" src/http/ngx_http_header_filter_module.c static u_char ngx_http_server_string[] = \"Server: come baby\" CRLF; 文件三 src/http/ngx_http_special_response.c 修改前 36行 \"nginx\" CRLF 修改后 修改命令 sed -i \"s/>nginx/>$NGX/\" src/http/ngx_http_special_response.c \"come baby\" CRLF 文件四 src/http/v2/ngx_http_v2_filter_module.c 修改前 480行 ngx_log_debug0(NGX_LOG_DEBUG_HTTP, fc->log, 0, 480 \"http2 output header: \\\"server: nginx\\\"\"); 481 } 修改后 修改命令 sed -i \"s/: nginx/: $NGX/\" src/http/v2/ngx_http_v2_filter_module.c ngx_log_debug0(NGX_LOG_DEBUG_HTTP, fc->log, 0, 480 \"http2 output header: \\\"server: come baby\\\"\"); 481 } 使用如下命令修改 src/core/nginx.h src/http/ngx_http_header_filter_module.c src/http/ngx_http_special_response.c src/http/v2/ngx_http_v2_filter_module.c #export NGX='What The Fuck ?' export NGX='Are you fucking kidding me ?' sed -i.bak -e \"s/nginx\\//$NGX/\" -e \"s/\\\"NGINX\\\"/\\\"$NGX\\\"/\" src/core/nginx.h sed -i.bak \"s/: nginx/: $NGX/\" src/http/ngx_http_header_filter_module.c sed -i.bak \"s/>nginx/>$NGX/\" src/http/ngx_http_special_response.c sed -i.bak \"s/: nginx/: $NGX/\" src/http/v2/ngx_http_v2_filter_module.c 验证 $ curl -I spug.test.com HTTP/1.1 502 Bad Gateway Server: come baby Date: Wed, 03 Feb 2021 03:54:36 GMT Content-Type: text/html Content-Length: 154 Connection: keep-alive 二、通过yum源安装 nginx yum源官方地址 nginx centos yum源官方地址 2.1 添加nginx官方yum源 cat > /etc/yum.repos.d/nginx.repo 2.2 安装nginx 2.3.1 安装最新版 yum -y install nginx 2.3.2 安装指定版本 查看可用版本 yum list nginx --showduplicates|sort -r 安装指定版本 yum -y install nginx-1.12.2 三、rpm包安装 3.1 下载rpm包 nginx rpm包官方下载地址 nginx centos rpm包官方下载地址 wget https://nginx.org/packages/rhel/7/x86_64/RPMS/nginx-1.18.0-1.el7.ngx.x86_64.rpm 3.2 安装 yum -y localinstall nginx-1.18.0-1.el7.ngx.x86_64.rpm 四、nginx配置文件主要模块 nginx主配置文件/etc/nginx/nginx.conf是一个纯文本类型的文件，整个配置文件是以区块的形式组织的。一般，每个区块以一对大括号{}来表示开始与结束。 模块分类 1.CoreModule 核心模块 2.EventModule 事件驱动模块 3.HttpCoreModule http内核模块 CoreModule层下可以有Event、HTTP HTTP模块层允许有多个Server层, Server主要用于配置多个网站 Server层又允许有多个Location, Location主要用于定义网站访问路径 CoreModule核心模块 user www; #Nginx进程所使用的用户 worker_processes 1; #启动的work进程数(CPU数量一致或auto) error_log /log/nginx/error.log #错误日志 pid /var/run/nginx.pid #Nginx服务启动后产生的pid进程号 events事件模块 events { worker_connections #每个worker进程支持的最大连接数 use #事件驱动模型, epoll默认 } http内核模块 #公共的配置定义在http{} http { #使用Server配置网站, 每个Server{}代表一个网站(简称虚拟主机) server { listen 80; #监听端口, 默认80 server_name localhost; #提供服务的域名或主机名 access_log host.access.log; #访问日志 #控制网站访问路径 location / { root /usr/share/nginx/html; #存放网站代码路径 index index.html index.htm; #服务器返回的默认页面文件 } #指定错误代码, 统一定义错误页面, 错误代码重定向到新的Locaiton error_page 500 502 503 504 /50x.html; } #第二个虚拟主机配置 server { } include /etc/nginx/conf.d/*.conf; #包含/etc/nginx/conf.d/目录下所有以.conf结尾的文件 } 五、nginx工作原理简介 Nginx WEB服务器最主要就是各种模块的工作，模块从结构上分为核心模块、基础模块和第三方模块，其中三类模块分别如下： 核心模块：HTTP模块、EVENT模块和MAIL模块等； 基础模块：HTTP Access模块、HTTP FastCGI模块、HTTP Proxy模块和HTTP Rewrite模块； 第三方模块：HTTP Upstream Request Hash模块、Notice模块和HTTP Access Key模块、Limit_req模块等； Nginx的模块从功能上分为如下三类。 Handlers（处理器模块）：此类模块直接处理请求，并进行输出内容和修改headers信息等操作，Handlers处理器模块一般只能有一个； Filters （过滤器模块）：此类模块主要对其他处理器模块输出的内容进行修改操作，最后由Nginx输出； Proxies （代理类模块）：此类模块是Nginx的HTTP Upstream之类的模块，这些模块主要与后端一些服务比如FastCGI等进行交互，实现服务代理和负载均衡等功能。 Nginx由内核和模块组成，其中内核的设计非常微小和简洁，完成的工作也非常简单，仅仅通过查找配置文件将客户端的请求映射到一个location block，而location是Nginx配置中的一个指令，用于访问的URL匹配，而在这个location中所配置的每个指令将会启动不同的模块去完成相应的工作，如图所示： Nginx的高并发得益于其采用了epoll模型，与传统的服务器程序架构不同，epoll是Linux内核2.6以后才出现的，Nginx采用epoll模型，异步非阻塞，而apache采用的是select模型： Select特点：select 选择句柄的时候，是遍历所有句柄，也就是说句柄有事件响应时，select需要遍历所有句柄才能获取到哪些句柄有事件通知，因此效率是非常低。 epoll的特点：epoll对于句柄事件的选择不是遍历的，是事件响应的，就是句柄上事件来就马上选择出来，不需要遍历整个句柄链表，因此效率非常高。 Nginx默认以80端口监听在服务器上，并且启动一个master进程，同时有master进程生成多个工作进程，当浏览器发起一个HTTP连接请求，每个进程都有可能处理这个连接，怎么做到的呢？怎么保证同一时刻一个HTTP请求被一个工作进程处理呢。 首先每个worker进程都是从Master进程fork出来，在Master进程里面，建立好需要listen的socket（listenfd）之后，会fork出多个worker进程。 所有worker进程的listenfd会在新连接到来时变得可读，为保证只有一个进程处理该连接，所有worker进程在注册listenfd读事件前抢accept_mutex，抢到互斥锁的那个进程注册listenfd读事件，在读事件里调用accept接受该连接。 当一个worker进程在accept这个连接之后，就开始读取请求、解析请求、处理请求，产生数据后，再返回给客户端，最后才断开连接，这样形成一个完整的请求流程。 管理员---->发送信号--->master进程---->worker进程客户端 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/nginx/2.http简要记录.html":{"url":"linux/linux服务/nginx/2.http简要记录.html","title":"http简要记录","keywords":"","body":"[toc] http简要记录 一、http（超文本传输协议） 由html文件 ->包含各种各样的元素（URL链接）->web页面 二、URL结构 http:// www.qq.com:80 /news/index.html 协议 域名: 端口 具体的文件名下面的某个页面 三、http的工作原理 1.客户端发起dns请求 2.获取到域名对应的IP地址 3.浏览器发起TCP的连接 4.基于TCP的连接，传输http的请求（一次TCP的连接，可以建立多次的http请求） 5.浏览器请求/index.html 6.服务器响应/index.html至浏览器 7.浏览器翻译index.html中的内容为人类可读 8.断开TCP连接-->四次挥手 http的短连接：建立一次tcp的连接，发起一次http的请求，结束，tcp断开 http的长连接：建立一次tcp的连接，发起多次http的请求，结束，tcp断开 四、http的请求方法 方法 描述 请求 响应 GET 用来请求访问已被URI识别的资源 指定的资源经服务器端解析后返回响应内容 GET /index.html HTTP/1.1 Host: www.baidu.com 返回index.html的页面资源 POST 用来传输实体的主体 虽然用GET方法也可以传输实体的主体，但一般不用GET方法进行传输，而是用POST方法 虽说POST的功能与GET相似，但POST的主要目的并不是获取响应的主体内容 POST /submit.cgi HTTP/1.1 Host: www.baidu.com Content-Length: 1500(1500字节的数据) 返回submit.cgi接收数据的处理结果 PUT 用来传输文件 就像FTP协议的文件上传一样，要求在请求报文的主体中包含文件的内容，然后保存到请求URI指定的位置 PUT /example.html HTTP/1.1 Host: www.baidu.com Content-Type: text/html Content-Length: 1560(1560字节的数据) 响应返回状态吗 204 No Content(比如：该html已存在于服务器上) HEAD 用于确认URI的有效性及资源更新的日期时间等，和GET方法一样，只是不返回报文主体部分 HEAD /index.html HTTP/1.1 Host: www.baidu.com 返回index.html有关的响应头部 DELETE 用来删除文件，是与PUT 相反的方法 按请求URI删除指定的资源 DELETE /example.html HTTP/1.1 Host: www.baidu.com 响应返回状态吗 204 No Content(比如：该html已存在于服务器上) OPTIONS 用来查询针对请求URI指定的资源支持的方法 OPTIONS * HTTP/1.1 Host: www.baidu.com HTTP/1.1 200 OK Allow: GET,POST,HEAD,OPTIONS(返回服务器支持的方法) TRACE 让web服务器端将之前的请求通信环回给客户端的方法 发送请求时，在Max-Forwards首部字段中填入数值，每经过一个服务端就将该数字减1，当数值刚好剑到0时就停止持续传输，最后接收到请求的服务端则返回状态码 200 OK 的响应 TRACE /HTTP/1.1 Host: www.baidu.com Max-Forwards: 2 HTTP/1.1 200 Ok Content-Type: message/http Content-Length: 1024 TRACE / HTTP/1.1 Host: www.baidu.com Max-Forwards: 2(返回响应包含请求内容) CONNECT 要求在与代理服务器通信时建立隧道，实现用隧道协议进行TCP通信，主要使用SSL和TLS协议把通信内容加密后经网络隧道传输 CONNECT proxy.sample.com:8000 HTTP/1.1 Host: proxy.sample.com HTTP/1.1 200 OK(之后进入网络隧道) 五、常见http的响应状态码（以3位数字组成） 状态码 说明 200 成功 301 永久重定向 302 临时重定向 304 浏览器缓存 401 权限认证失败 403 请求不到首页，权限被拒绝 404 资源找不到 500 服务器内部错误 502 错误的网关，找不到后端资源 504 请求超时 六、用户访问网站携带的参数，以及服务端返回的参数 概况 字段 说明 Request URL: http://www.abc.com/index.html 请求的URL地址 Request Method: GET 请求的方法（获取） Status Code: 304 Not Modified 返回的状态 Remote Address: www.abc.com:80 请求的地址 客户端请求的头部信息 字段 说明 Accept: text/html 请求的类型 Accept-Encoding: gzip, deflate 是否进行压缩 Accept-Language: zh-CN,zh;q=0.9 请求的语言 Cache-Control: max-age=0 缓存 Connection: keep-alive TCP长连接 Host: www.abc.com 请求的域名 If-Modified-Since: Fri, 05 May 2019 09:33:22 GMT 修改的时间 If-None-Match: \"a49-56b5ce607fe00\" 标记 Upgrade-Insecure-Requests:1 在http和https之间起的一个过渡作用 User-Agent: Mozilla/5.0 用户的浏览器 请求一个空行 服务端响应的头部信息 字段 说明 HTTP/1.1 304 Not Modified 返回服务器的http协议，状态码 Date: Fri, 15 Sep 2018 09:15:28 GMT 返回服务器的时间 Server: Apache/2.4.6 (CentOS) PHP/5.4.16 返回服务器使用的软件（Apache php） Connection: Keep-Alive TCP长连接 Keep-Alive: timeout=5, max=100 长连接的超时时间 ETag: \"a49-56b5ce607fe00\" 验证客户端标记 返回一个空行 xxx 返回页面内容 七、PV、UV、IP PV：页面浏览量 UV：独立的客户 IP：独立IP 示例说明 公司有100人，每个人有一台电脑一个手机，上网都是通过nat转换出口，每个人点击网站2次(假设点击一次返回的pv是1) PV：400 UV：200 IP：1个 八、用户访问网站的大体流程 1.客户端输入域名以及请求的页面 2.解析域名对应的dns 3.最终客户端浏览器获取到dns的IP地址 4.客户端会与服务端发起TCP的三次握手（长连接） 5.客户端发起http请求，请求会先抵达前端的防火墙 6.防火墙识别用户身份，正常的请求通过内部交换机通过TCP连接前端的负载均衡，然后传递用户的http请求 7.负载接收到请求，会根据请求的内容进行下发任务，通过TCP连接后端的web，然后下发用户的http请求 8.web接收到用户的http请求后，会根据用户请求的内容进行解析，解析分为如下两步： ​ 静态请求:由web服务器向nfs建立TCP连接，获取对应的图片，最后返回给负载均衡（负载均衡->防火墙->用户） ​ 动态请求:由web向后端的动态程序建立TCP连接，将用户的动态http请求传递给动态程序->由动态程序进行解析 9.动态程序在解析的过程中，如果碰到查询数据库的请求，则优先和缓存建立TCP的连接，然后缓存服务发起http的查询 10.如果缓存没有对应的数据，动态程序再次向数据库建立TCP的连接，然后发起查询操作 11.由数据库返回->动态程序->缓存->web服务->负载均衡->防火墙->用户 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/nginx/3.nginx基础应用.html":{"url":"linux/linux服务/nginx/3.nginx基础应用.html","title":"nginx基础应用","keywords":"","body":"[toc] nginx基础应用 一、nginx目录索引 加参数charset utf-8,gbk;解决中文乱码问题 autoindex 是否列出网站根目录(nginx默认是不允许列出整个目录浏览下载的，即autoindex off) 默认为off，禁止列出网站根目录内容 修改为on，列出网站根目录内容 编辑nginx配置文件 cat > /etc/nginx/conf.d/www.abc.com 检测nginx语法并重载nginx nginx -t nginx -s reload 创建网站根目录并创建文件 mkdir /website && cd /website touch {1..3}.txt 本机hosts解析，然后浏览器访问域名www.abc.com windows c:\\windows\\system32\\drivers\\etc mac /etc/hosts 当autoindex为off的时候是拒绝访问网站根目录的 当autoindex为on的时候是可以访问网站根目录的 autoindex_exact_size 是否显示文件的确切大小 默认为on， 显示出文件的确切大小，单位是bytes。 修改为off，显示出文件的大概大小，单位是kB或者MB或者GB 当autoindex_exact_size为on的时候，显示文件确切大小，单位是字节，显示如下 当autoindex_exact_size为off的时候，显示是文件的大概大小，单位是KB/MB/GB，显示如下 autoindex_localtime 显示文件修改时间或文件服务器时间 默认为off，显示的文件时间为GMT时间。 修改为on， 显示的文件时间为文件的服务器时间。 北京时间=GMT时间+8小时 当autoindex_localtime为off的时候，显示如下 当autoindex_localtime为on的时候，显示如下 ⚠️上传的文件显示的时间是文件的修改时间，与服务器时间没有关系，在服务器中创建的文件才是服务器的时间 配置站点目录浏览功能 在nginx配置文件中开启以下参数即可 location / { root /xxx; autoindex on; #列出根目录，默认off autoindex_localtime on; #显示文件时间为当前服务器时间，默认off autoindex_exact_size off; #显示文件确切大小，以人类易读的方式显示，默认on } 二、nginx状态监控 nginx中ngx_http_stub_status_module用于展示nginx连接状态信息, 需要--with-http_stub_status_module模块支持 2.1 检测nginx是否支持stub_status模块 使用命令nginx -V &>nginx.txt把nginx支持的模块信息放入到文件中，然后在文件中过滤--with-http_stub_status模块(rpm包或者yum安装的nginx都支持)，注意一定要写成&>，只写>不会有内容 nginx -V &> nginx.txt 2.2 配置nginx status location /nginx_status { stub_status; access_log off; } 2.3 浏览器访问域名/nginx_status 返回结果如下 各参数含义 Active connections:1 #当前活动的连接数 server accepts handled requests 21 21 27 21 #总的tcp连接数connection 21 #成功tcp连接数connection(失败连接=(总连接数-成功连接数)) 27 #总共处理的http请求数requests #keepalive_timeout 0; 每次连接都会产生一次请求(短连接) #keepalive_timeout 60; 在60s以内的请求建立在一个连接基础之上(长连接) Reading:0 Writing:1 Waiting: 0 Reading #请求 Writing #响应 Waiting #等待的请求数，开启了keepalive 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/nginx/4.nginx访问控制.html":{"url":"linux/linux服务/nginx/4.nginx访问控制.html","title":"nginx访问控制","keywords":"","body":"[toc] nginx访问控制 访问控制类别 基于IP的访问控制 http_access_module 基于用户登陆认证的访问控制 http_auth_basic_module 1.1 基于IP的访问控制 语法： allow|deny address 1.1.1 访问控制配置示例1， 拒绝指定的IP，其他全部允许 location /nginx_status { stub_status; access_log off; deny 10.0.0.51; allow all; } 10.0.0.51访问，权限拒绝 $ curl www.abc.com/nginx_status 403 Forbidden 403 Forbidden nginx/1.18.0 10.0.0.52访问，可以访问 $ curl www.abc.com/nginx_status Active connections: 1 server accepts handled requests 26 26 31 Reading: 0 Writing: 1 Waiting: 0 1.1.2 访问控制配置示例2，只允许谁能访问，其它全部拒绝 location / { root /website; index index.php index.html index.htm; allow 192.168.9.0/24; allow 10.0.0.51; deny all; } 10.0.0.51访问，可以访问 $ curl www.abc.com www.abc.com 的网站根目录 10.0.0.52访问，权限拒绝 $ curl www.abc.com 403 Forbidden 403 Forbidden nginx/1.18.0 http_access_module局限性 当客户端通过代理服务器访问真实的后端服务器时，通过remote_addr能获取到代理服务器的IP地址，但是无法获取客户端的IP地址 在nginx主配置文件/etc/nginx/nginx.conf中访问日志格式有如下定义 log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; nginx访问日志/var/log/nginx/access.log中如下，可以看到使用$remote_addr可以获取直接访问后端真实web的代理服务器的IP，但是最后边的\"-\"表明无法获取真实客户端的IP 10.0.0.51 - - [16/Jun/2020:00:27:30 +0800] \"GET / HTTP/1.0\" 200 10 \"-\" \"curl/7.29.0\" \"-\" 现在在代理服务器和后端真实web中的nginx头文件proxy_params中加上参数X-Forwarded-For以获取客户端真实IP，然后模拟访问过程如下 客户端10.0.0.10-->通过代理服务器10.0.0.51-->访问后端真实web 10.0.0.52 代理服务器10.0.0.51 nginx配置 #编辑nginx反向代理配置文件 cat > /etc/nginx/conf.d/www.abc.com.conf /etc/nginx/proxy_params 后端真实web 10.0.0.52操作 #编辑nginx配置文件 cat > /etc/nginx/conf.d/www.abc.com.conf /etc/nginx/proxy_params /website/index.html 加上X-Forwarded-For参数后，nginx的访问日志中就可以获取到客户端的真实IP地址了 10.0.0.51 - - [16/Jun/2020:10:35:12 +0800] \"GET / HTTP/1.0\" 200 10 \"-\" \"curl/7.29.0\" \"10.0.0.10\" 1.2 基于用户登陆认证的访问控制 可在http、server、location下添加如下内容即可 auth_basic \"Auth access down Input your Passwd!\"; auth_basic_user_file /etc/nginx/auth_file; 配置示例 #安装包 yum -y install httpd-tools #创建一个用户名为test密码为123的登陆认证用户，同时将密码存放于/etc/nginx/auth_conf htpasswd -b -c /etc/nginx/auth_file test 123 #编辑nginx配置文件 location / { root /website; auth_basic \"Auth access down Input your Passwd!\"; auth_basic_user_file /etc/nginx/auth_file; autoindex on; autoindex_exact_size off; autoindex_localtime on; charset utf-8,gbk; index index.php index.html index.htm; } 浏览器访问提示需要输入用户名和密码 1.3 基于配置参数的访问控制 限制对http资源访问的官方文档 ngx_http_limit_conn_module模块可以根据定义的key来限制每个键值的连接数 因为一次tcp连接可以建立多次http请求连接，因此http请求连接要比tcp连接限制更准确 limit_conn_module 连接频率限制，TCP连接 limit_req_module 请求频率限制，http请求连接 http协议的连接与请求 HTTP是建立在TCP连接之上, 在完成HTTP请求需要先建立TCP三次握手（称为TCP连接）,在连接的基础上再发起HTTP请求。 HTTP请求建立在一次TCP连接基础上，一次TCP请求至少产生一次HTTP请求 1.3.1 nginx连接限制配置 limit_conn_module 配置示例 # 在nginx主配置文件nginx.con中http模块下加入 limit_conn_zone http { # http段配置连接限制, 同一时刻只允许一个客户端IP连接 # 定义一个名为 conn_zone 的limit_conn用来存储session，大小是10M内存，以$binary_remote_addr为key limit_conn_zone $binary_remote_addr zone=conn_zone:10m; } # 在xxx.conf中加入limit_conn server { location / { # 同一时刻只允许10个客户端IP连接 limit_conn conn_zone 10; } } 1.3.2 nginx请求限制配置 limit_req_module 如何设置能限制某个IP某一时间段的访问次数是一个让人头疼的问题，特别面对恶意的ddos攻击的时候。其中CC攻击（Challenge Collapsar）是DDOS（分布式拒绝服务）的一种，也是一种常见的网站攻击方法，攻击者通过代理服务器或者肉鸡向向受害主机不停地发大量数据包，造成对方服务器资源耗尽，一直到宕机崩溃。 cc攻击一般就是使用有限的ip数对服务器频繁发送数据来达到攻击的目的，nginx可以通过HttpLimitReqModul 和 HttpLimitZoneModule 配置来限制ip在同一时间段的访问次数来防cc攻击。 HttpLimitReqModul 用来限制连单位时间内连接数的模块，使用 limit_req_zone 和limit_req 指令配合使用来达到限制。一旦并发连接超过指定数量，就会返回503错误。 HttpLimitConnModul 用来限制单个ip的并发连接数，使用 limit_zone 和 limit_conn 指令 这两个模块的区别前一个是对一段时间内的连接数限制，后者是对同一时刻的连接数限制 配置示例 # 在nginx主配置文件nginx.con中http模块下加入 limit_req_module http { # http段配置连接限制, 1r/s只接收一个请求,其余请求拒绝处理并返回错误码给客户端 # 定义一个名为req_zone的limit_req_zone用来存储session，大小是10M内存，以$binary_remote_addr为key，限制平均每秒的请求数为20个 # 1M能存储16000个状态，rate的值必须为整数 # 如果限制2秒一个请求，可以设置成30r/m limit_req_zone $binary_remote_addr zone=req_zone:10m rate=20r/s; } # 在xxx.conf中加入 limit_req server { location / { # 限制ip每秒不超过20个请求，漏桶数burst为5，burst的意思就是，如果第1秒、2、3、4秒请求为19个，第5秒的请求为25个是被允许的，但是如果第1秒就有25个请求，则第2秒超过20的请求返回503 # nodelay，如果不设置该选项，则严格使用平均速率限制请求数，第1秒为25个请求，5个请求会被放到第2秒执行；如果设置了nodelay，则25个请求在第1秒执行 limit_req zone=req_zone burst=5 nodelay; } } 在终端中快速重复访问，就可以看到有503的报错 [root@test1 ~]# curl www.abc.com www.abc.com 的网站根目录 [root@test1 ~]# curl www.abc.com www.abc.com 的网站根目录 [root@test1 ~]# curl www.abc.com 503 Service Temporarily Unavailable 503 Service Temporarily Unavailable nginx/1.18.0 www.abc.com 的网站根目录 也可以用ab简单压测一下 -n 指定数量 -c 指定并发数 可以看到请求数是50，失败的数量是46，因为配置文件中请求限制为1r/s，burst=3，因此成功数量是4 $ ab -n 50 -c 10 www.abc.com/index.html This is ApacheBench, Version 2.3 Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking www.abc.com (be patient).....done Server Software: nginx/1.18.0 Server Hostname: www.abc.com Server Port: 80 Document Path: /index.html Document Length: 31 bytes Concurrency Level: 10 Time taken for tests: 0.005 seconds Complete requests: 50 Failed requests: 46 (Connect: 0, Receive: 0, Length: 46, Exceptions: 0) Write errors: 0 Non-2xx responses: 46 Total transferred: 18972 bytes HTML transferred: 9186 bytes Requests per second: 10066.44 [#/sec] (mean) Time per request: 0.993 [ms] (mean) Time per request: 0.099 [ms] (mean, across all concurrent requests) Transfer rate: 3730.09 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.1 0 0 Processing: 0 1 0.1 1 1 Waiting: 0 0 0.1 0 1 Total: 1 1 0.1 1 1 Percentage of the requests served within a certain time (ms) 50% 1 66% 1 75% 1 80% 1 90% 1 95% 1 98% 1 99% 1 100% 1 (longest request) 连接限制没有请求限制有效? http多个请求可以建立在一次TCP连接之上, 那么我们对请求的精度限制，要比对一个连接的限制会更加的有效。 因为同一时刻只允许一个连接请求进入。 但是同一时刻多个请求可以通过一个连接进入。 所以请求限制才是比较优的解决方案。 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/nginx/5.nginx日志.html":{"url":"linux/linux服务/nginx/5.nginx日志.html","title":"nginx日志","keywords":"","body":"[toc] nginx日志 nginx日志官方文档 access_log指令 字段 说明 Syntax: access_log path [format [buffer=size] [gzip[=level]] [flush=time] [if=condition]];access_log off; Default: access_log logs/access.log combined; Context: http, server, location, if in location, limit_except 设置缓冲日志写入的路径、格式和配置。可以在同一级别指定多个日志。可以通过在第一个参数中指定“syslog:”前缀来配置登录到syslog。特殊值off取消当前级别上的所有access_log指令。如果未指定格式，则使用预定义的“组合”格式。 如果使用buffer或gzip(1.3.10, 1.2.7)参数，写入日志将被缓冲。 缓冲区的大小不能超过对磁盘文件的原子写入的大小。对于FreeBSD，这个大小是无限制的。 当启用缓冲时，数据将写入文件: 如果缓冲区无法容纳下一个日志行; 如果缓冲数据比刷新参数(1.3.10,1.2.7)指定的早; 工作进程重新打开日志文件或关闭日志文件时。 如果使用了gzip参数，那么缓冲数据将在写入文件之前被压缩。压缩级别可以设置在1(最快，较少压缩)和9(最慢，最好压缩)之间。默认情况下，缓冲区大小为64K字节，压缩级别设置为1。由于数据是在原子块中压缩的，所以“zcat”可以在任何时候解压或读取日志文件。 示例: access_log /path/to/log.gz combined gzip flush=5m; 为了使gzip压缩正常工作，必须使用zlib库构建nginx。 文件路径可以包含变量(0.7.6+)，但是这样的日志有一些约束: 工作进程使用其凭证的用户应该有权限在具有此类日志的目录中创建文件; 缓冲写不工作; 每次写日志时，都会打开和关闭该文件。但是，由于常用文件的描述符可以存储在缓存中，所以可以在open_log_file_cache指令的有效参数指定的时间内继续写入旧文件 在每个日志写期间，会检查请求的根目录是否存在，如果它不存在，就不会创建日志。因此，在同一个级别上同时指定 root 和access_log是一个好主意: server { root /spool/vhost/data/$host; access_log /spool/vhost/logs/$host; ... } if参数(1.7.0)支持条件日志记录。如果条件的计算结果为“0”或空字符串，则不会记录请求。在下面的示例中，响应码为2xx和3xx的请求将不被记录: map $status $loggable { ~^[23] 0; default 1; } access_log /path/to/access.log combined if=$loggable; log_format指令 字段 说明 Syntax: log_format name [escape=default Default: log_format combined \"...\"; Context: http 指定日志格式。 转义参数(1.11.8)允许在变量中设置json或默认转义字符，默认情况下，使用默认转义。none值(1.13.10)禁止转义。 对于默认转义，字符\" \" \" \"，\" \\ \"和值小于32(0.7.0)或大于126(1.1.6)的字符被转义为\" \\xXX \"。如果没有找到变量值，将记录一个连字符(\" - \")。 对于json转义，json字符串中不允许的所有字符都将进行转义:字符“”和“\\”将转义为“\\”和“\\”，值小于32的字符将转义为“\\n”、“\\r”、“\\t”、“\\b”、“\\f”或“\\u00XX”。 日志格式可以包含常见的变量，以及只在写日志时存在的变量: # 相关字段含义 $bytes_sent #发送给客户端的字节数 $connection #连接序列号 $connection_requests #当前通过连接发出的请求数（1.1.18） $mse #日志写入时的时间（以毫秒为单位），以毫秒为单位 $pipe #“ p”（如果请求已传递），.否则为“ ” $request_length #请求长度（包括请求行，标头和请求正文） $request_time #以毫秒为单位请求处理时间，以毫秒为单位；从客户端读取第一个字节到将最后一个字节发送到客户端后的日志写入之间经过的时间 $status #反应状态 $time_iso8601 #ISO 8601标准格式的当地时间 $time_local #通用日志格式的本地时间 发送给客户端的头行具有sent_http_前缀，例如$sent_http_content_range。 配置总是包括预定义的“组合”格式: log_format combined '$remote_addr - $remote_user [$time_local] ' '\"$request\" $status $body_bytes_sent ' '\"$http_referer\" \"$http_user_agent\"'; open_log_file_cache指令 Syntax: open_log_file_cache max=N [inactive=time] [min_uses=N] [valid=time];open_log_file_cache off; Default: open_log_file_cache off; Context: http, server, location 定义一个缓存，用于存储名称中包含变量的常用日志的文件描述符。指令有以下参数: max #设置缓存中描述符的最大数量；如果缓存已满，则关闭最近最少使用（LRU）描述符 inactive #设置在此期间如果没有访问权限则关闭缓存的描述符的时间；默认情况下为10秒 min_uses #设置在inactive参数定义的时间内文件的最小使用量，以使描述符在缓存中保持打开状态；默认情况下，1 valid #设置时间，在该时间后应检查文件是否仍然具有相同名称；默认情况下为60秒 off #禁用缓存 用法示例 open_log_file_cache max=1000 inactive=20s valid=1m min_uses=2; nginx默认日志格式 log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log /var/log/nginx/access.log main; 各字段含义 字段 说明 对应真实日志内容 $remote_addr 记录客户端IP地址 10.0.0.51 $remote_user 记录客户端用户名称 - - $time_local 通用日志格式下的本地时间 [16/Jun/2020:10:35:12 +0800] $request 记录请求的URL和HTTP协议 \"GET / HTTP/1.0\" $status 记录请求状态码 200 $body_bytes_sent 发送给客户端的字节数，不包括响应头的大小 10 $http_referer 记录从哪个页面链接访问过来的 \"-\" $http_user_agent 记录客户端浏览器相关信息 \"curl/7.29.0\" $http_x_forwarded_for 记录客户端真实IP地址 \"10.0.0.10\" 真实日志内容 10.0.0.51 - - [16/Jun/2020:10:35:12 +0800] \"GET / HTTP/1.0\" 200 10 \"-\" \"curl/7.29.0\" \"10.0.0.10\" 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/nginx/6.nginx代理.html":{"url":"linux/linux服务/nginx/6.nginx代理.html","title":"nginx代理","keywords":"","body":"[toc] nginx代理 nginx反向代理官方文档 一、代理分类 nginx代理分为正向代理和反向代理 正向代理代理的对象是客户端 反向代理代理的对象是服务端 正向代理 概念 在如今的网络环境下，我们如果由于技术需要要去访问国外的某些网站，此时你会发现位于国外的某网站我们通过浏览器是没有办法访问的，此时大家可能都会用一个操作FQ进行访问，FQ的方式主要是找到一个可以访问国外网站的代理服务器，我们将请求发送给代理服务器，代理服务器去访问国外的网站，然后将访问到的数据传递给我们！ 上述这样的代理模式称为正向代理，正向代理最大的特点是客户端非常明确要访问的服务器地址；服务器只清楚请求来自哪个代理服务器，而不清楚来自哪个具体的客户端；正向代理模式屏蔽或者隐藏了真实客户端信息。 示意图 反向代理 概念 多个客户端给nginx服务器发送请求，nginx服务器接收到请求之后，按照一定的规则分发给了后端的业务处理服务器进行处理。此时请求的来源也就是客户端是明确的，但是请求具体由哪台服务器处理的就不明确了，nginx扮演的就是一个反向代理角色 反向代理，主要用于服务器集群分布式部署的情况下，反向代理隐藏了服务器的信息！ 示意图 项目场景 通常情况下，我们在实际项目操作时，正向代理和反向代理很有可能会存在在一个应用场景中，正向代理代理客户端的请求去访问目标服务器，目标服务器是一个反向代理服务器，反向代理了多台真实的业务处理服务器。公司的生产环境就是如下图所示，公司的核心业务SASS平台是这样的流程 域名解析到了阿里云的SLB，通过SLB把请求分发到公司公共nginx(两台ECS)，然后又在公共nginx上配置了具体的转发规则(location的匹配，upstream等)，最后把请求的流量转发到ECS中，下图中的正向代理就相当于我们的SLB，反向代理就相当于我们的公共nginx，业务服务器就相当于后端的真实web服务ECS 具体的拓扑图如下 二、nginx代理配置相关 语法 字段 说明 Syntax: proxy_pass URL; Default: — Context: location, if in location, limit_except 2.1 将请求传递到代理服务器 当NGINX代理请求时，它将请求发送到指定的代理服务器，获取响应，然后将其发送回客户端。可以使用指定协议将请求代理到HTTP服务器（另一个NGINX服务器或任何其他服务器）或非HTTP服务器（可以运行使用特定框架开发的应用程序，例如PHP或Python）。支持的协议包括FastCGI，uwsgi，SCGI和memcached。 要将请求传递到HTTP代理服务器，请在proxy_pass中指定指令location。例如： location /some/path/ { proxy_pass http://www.example.com/link/; } 此示例配置结果将在此位置处理的所有请求传递到指定地址的代理服务器。此地址可以指定为域名或IP地址。地址还可以包括一个端口: location ~ \\.php { proxy_pass http://127.0.0.1:8000; } 注意，在上面的第一个示例中，代理服务器的地址后跟URI /link/。如果URI与地址一起指定，它将替换与位置参数匹配的请求URI部分。例如，这里带有/some/path/page.html URI的请求将被代理到http://www.example.com/link/page.html。如果指定的地址没有URI，或者无法确定要替换的URI部分，则传递(可能是修改)完整的请求URI。 要将请求传递到非HTTP代理服务器，**_pass应使用适当的指令 fastcgi_pass 将请求传递给FastCGI服务器 uwsgi_pass 将请求传递给uwsgi服务器 scgi_pass 将请求传递给SCGI服务器 memcached_pass 将请求传递到内存缓存服务器 请注意，在这些情况下，用于指定地址的规则可能会有所不同。您可能还需要将其他参数传递给服务器（有关更多详细信息，请参见参考文档）。 该proxy_pass指令还可以指向服务器的命名组。在这种情况下，将根据指定的方法在组中的服务器之间分配请求。 代理到后端的TCP连接、响应、返回等超时时间 # nginx代理与后端服务器连接超时时间(代理连接超时) Syntax: proxy_connect_timeout time; Default: proxy_connect_timeout 60s; Context: http, server, location # nginx代理等待后端服务器的响应时间 Syntax: proxy_read_timeout time; Default: proxy_read_timeout 60s; Context: http, server, location # 后端服务器数据回传给nginx代理超时时间 Syntax: proxy_send_timeout time; Default: proxy_send_timeout 60s; Context: http, server, location 2.2 传递请求头 默认情况下，NGINX在代理请求中重新定义两个头字段“ Host”和“ Connection”，并消除其值为空字符串的标头字段。“主机”设置为$proxy_host变量，“连接”设置为close。 要更改这些设置以及修改其他标题字段，请使用proxy_set_header伪指令。可以在location或更高版本中指定此指令。也可以在特定server上下文或http块中指定。例如： location /some/path/ { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_pass http://localhost:8000; } 在此配置中，“主机”字段设置为$host变量。 为了防止将头字段传递给代理服务器，请按以下步骤将其设置为空字符串： location /some/path/ { proxy_set_header Accept-Encoding \"\"; proxy_pass http://localhost:8000; } 添加发往后端服务器的请求头信息 # 用户请求的时候HOST的值是www.abc.com, 那么代理服务会像后端传递请求的还是www.abc.com proxy_set_header Host $http_host; # 将$remote_addr的值放进变量X-Real-IP中，$remote_addr的值为客户端的ip proxy_set_header X-Real-IP $remote_addr; # 客户端通过代理服务访问后端服务, 后端服务通过该变量会记录真实客户端地址 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 2.3 配置缓冲区 默认情况下，NGINX缓冲来自代理服务器的响应。响应存储在内部缓冲区中，直到接收到整个响应后才发送给客户端。缓冲有助于优化慢速客户端的性能，如果响应从NGINX同步传递到客户端，则这可能会浪费代理服务器的时间。但是，启用缓冲后，NGINX允许代理服务器快速处理响应，而NGINX将响应存储的时间与客户端下载响应所需的时间一样长。 负责启用和禁用缓冲的指令是proxy_buffering。默认情况下将其设置为on并启用缓冲。 该proxy_buffers指令控制规模和分配的请求缓冲区的数目。来自代理服务器的响应的第一部分存储在单独的缓冲区中，缓冲区的大小由proxy_buffer_size伪指令设置。这部分通常包含一个相对较小的响应头，并且可以使其小于其余响应的缓冲区。 在以下示例中，增加了默认缓冲区数，并使响应第一部分的缓冲区大小小于默认值。 location /some/path/ { proxy_buffers 16 4k; proxy_buffer_size 2k; proxy_pass http://localhost:8000; } 如果禁用了缓冲，则响应将从客户端服务器接收到的响应同步发送到客户端。对于需要尽快开始接收响应的快速交互客户端，此行为可能是理想的。 要在特定位置禁用缓冲，请将proxy_buffering伪指令location与off参数一起放置在中，如下所示： location /some/path/ { proxy_buffering off; proxy_pass http://localhost:8000; } 在这种情况下，NGINX仅使用配置的缓冲区proxy_buffer_size来存储响应的当前部分。 反向代理的常见用法是提供负载平衡。在免费的《选择软件负载平衡器的五个理由》电子书中，了解如何提高性能，性能并通过快速部署专注于您的应用程序。 proxy_buffer代理缓冲区 # nignx会把后端返回的内容先放到缓冲区当中，然后再返回给客户端,边收边传, 不是全部接收完再传给客户端 Syntax: proxy_buffering on | off; Default: proxy_buffering on; Context: http, server, location # 设置nginx代理保存用户头信息的缓冲区大小 Syntax: proxy_buffer_size size; Default: proxy_buffer_size 4k|8k; Context: http, server, location # proxy_buffers 缓冲区 Syntax: proxy_buffers number size; Default: proxy_buffers 8 4k|8k; Context: http, server, location 2.4 选择转发IP地址 如果代理服务器具有多个网络接口，有时您可能需要选择特定的源IP地址以连接到代理服务器或上游服务器。如果将NGINX之后的代理服务器配置为接受来自特定IP网络或IP地址范围的连接，这可能很有用。 指定proxy_bind指令和必要的网络接口的IP地址： location /app1/ { proxy_bind 127.0.0.1; proxy_pass http://example.com/app1/; } location /app2/ { proxy_bind 127.0.0.2; proxy_pass http://example.com/app2/; } IP地址也可以用变量指定。例如，$server_addr变量传递接受请求的网络接口的IP地址： location /app3/ { proxy_bind $server_addr; proxy_pass http://example.com/app3/; } 三、配置nginx反向代理 在代理服务器和后端真实web中的nginx头文件proxy_params中加上参数X-Forwarded-For以获取客户端真实IP，然后模拟访问过程如下 客户端10.0.0.10-->通过代理服务器10.0.0.51-->访问后端真实web 10.0.0.52 代理服务器10.0.0.51 nginx配置 # 编辑nginx反向代理配置文件 cat > /etc/nginx/conf.d/www.abc.com.conf /etc/nginx/proxy_params 后端真实web 10.0.0.52操作 # 编辑nginx配置文件 cat > /etc/nginx/conf.d/www.abc.com.conf /etc/nginx/proxy_params /website/index.html 客户端10.0.0.10做本地hosts解析 10.0.0.51 www.abc.com 客户端10.0.0.10访问www.abc.com，经过nginx代理服务器10.0.0.51将请求转发至后端真实web 10.0.0.52，最终访问到的内容如下 $ curl www.abc.com 10.0.0.52 加上X-Forwarded-For参数后，nginx的访问日志(真实web服务器10.0.0.52查看)中就可以获取到客户端的真实IP地址了 10.0.0.51 - - [16/Jun/2020:10:35:12 +0800] \"GET / HTTP/1.0\" 200 10 \"-\" \"curl/7.29.0\" \"10.0.0.10\" 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/nginx/7.nginx location.html":{"url":"linux/linux服务/nginx/7.nginx location.html","title":"nginx location","keywords":"","body":"[toc] nginx location nginx location官方文档 匹配语法 location [ = | ~ | ~* | ^~ ] uri { ... } location @name { ... } location [匹配模式] uri { ... } location匹配规则 匹配规则总览 匹配模式 匹配符 优先级 精确匹配 = 1 前缀匹配 ^~ 2 正则匹配 ~ 3 正常匹配 uri 4 全匹配(通用匹配) / 5 匹配规则细分 匹配符 匹配规则 优先级 = 精确匹配 1 ^~ 以某个字符串开头 2 ~ 区分大小写的正则匹配 3 ~* 不区分大小写的正则匹配 4 !~ 区分大小写不匹配的正则 5 !~* 不区分大小写不匹配的正则 6 / 通用匹配，任何请求都会匹配到 7 location匹配优先级 路径匹配优先级 精确匹配 > 前缀匹配 > 正则匹配 > 正常匹配 > 全匹配 匹配示例 精确匹配 ⚠️server_name _ 中的_只是一个无效域名的表示方法 编辑nginx配置文件 server { listen 80; server_name _; # 第1段 location /nginx { #return 200 \"aaa\"; root /data/nginx/html4/; index index.html; } # 第2段 location = /nginx { #return 200 \"bbb\"; root /data/nginx/html3/; index index.html; } } 创建网站根目录 mkdir -p /data/nginx/html{3,4}/nginx echo 'html3' >/data/nginx/html3/nginx/index.html echo 'html4' >/data/nginx/html4/nginx/index.html 访问测试 $ curl 127.0.0.1/nginx 301 Moved Permanently 301 Moved Permanently nginx/1.18.0 $ curl 127.0.0.1/nginx/ html4 精确匹配中 '/nginx/'中优先匹配到第2段，再访问'/nginx/index.html'，此次内部跳转uri已经是/nginx/index.html，而非=的；最终访问结果是第1段中的index.html。 结论：精确匹配区分大小写，不能使用正则，访问的URI必须完全与=后面的一致，多一个\"/\"或者少一个\"/\"，都是不可以的。 前缀匹配 编辑nginx配置文件 server { listen 80; server_name _; location ^~ /nginx/ { rewrite ^ https://www.163.com break; } location ^~ /nginx/bcd/ { rewrite ^ https://www.qq.com break; } location ^~ /Abc/ { rewrite ^ https://www.sina.com.cn break; } } 访问测试 访问127.0.0.1/nginx/，在终端中会返回302，在浏览器中会跳转到www.163.com $ curl 127.0.0.1/nginx/ 302 Found 302 Found nginx/1.18.0 访问127.0.0.1/nginx/bcd/，在终端中会返回302，在浏览器中会跳转到www.qq.com ⚠️如果bcd后边不加/，跳转页面是www.163.com，即只匹配前缀/nginx $ curl 127.0.0.1/nginx/bcd/ 302 Found 302 Found nginx/1.18.0 访问一个不存在的页面127.0.0.1/nginx/abcd/，在终端中会返回302，在浏览器中会跳转到www.163.com $ curl 127.0.0.1/nginx/abcd/ 302 Found 302 Found nginx/1.18.0 访问127.0.0.1/Abc/，在终端中会返回302，在浏览器中会跳转到www.sina.com.cn $ curl 127.0.0.1/nginx/abc/ 302 Found 302 Found nginx/1.18.0 结论：前缀匹配不能使用正则，区分大小写，只要前缀相同，都可以匹配成功，不管后面有没有字符，保证前缀相同即可。 正则匹配 编辑nginx配置文件 server { listen 80; server_name _; location ~ /[a-z]nginx/ { rewrite ^ https://www.baidu.com break; } location ~* /[a-z]nginx/ { rewrite ^ https://www.163.com break; } } 访问测试 访问127.0.0.1/anginx/，在终端中会返回302，在浏览器中会跳转到www.baidu.com $ curl 127.0.0.1/anginx/ 302 Found 302 Found nginx/1.18.0 访问127.0.0.1/anginx/abc在终端中会返回302，在浏览器中会跳转到www.baidu.com $ curl 127.0.0.1/anginx/abc/ 302 Found 302 Found nginx/1.18.0 访问127.0.0.1/Anginx/，在终端中会返回302，在浏览器中会跳转到www.163.com $ curl 127.0.0.1/Anginx/ 302 Found 302 Found nginx/1.18.0 结论：正则匹配 ~ 区分大小写，~* 不区分大小写, 并且与前缀匹配比较类似，只需要匹配模式开头部分，这两种同时存在时，优先匹配区分大小写的。 正常匹配 正常匹配就是匹配模式为空的匹配规则 编辑 nginx配置文件 server { listen 80; server_name _; # 第1段 location /nginx/ { rewrite ^ https://www.baidu.com break; } # 第2段 location /[0-9]nginx/ { rewrite ^ https://www.qq.com break; } } 访问测试 访问127.0.0.1/nginx/，在终端中会返回302，在浏览器中会跳转到www.baidu.com $ curl 127.0.0.1/nginx/ 302 Found 302 Found nginx/1.18.0 上述配置文件中第2段访问不生效，404 ⚠️location ^~ /nginx/与location /nginx/不能同时出现 结论：正常匹配与前缀匹配的差别，只在于优先级。 全匹配(通用匹配) 编辑nginx配置文件 server { listen 80; server_name _; location / { root /data/nginx/html; index index.html index.htm; } } 全匹配没有匹配模式，并且匹配的uri仅是一个斜杠/，通常用在一个默认页面的地方 命名匹配 命名匹配一般用于静态页面或者错误页面(404，500等)，并且这个命名匹配中，不允许有alias。 error_page 404 = @notfound; location @notfound { rewrite ^ https://www.baidu.com break; } 优先级验证综合实验 编辑nginx配置文件 ⚠️前缀匹配和正常匹配不能同时存在 server { listen 80; server_name _; # 全匹配，这里/data/nginx/html/下面有一个nginx文件夹，里面有index.html，内容是nginx location test location / { root /data/nginx/html; index index.html; } # 正常匹配 location /nginx/ { rewrite ^/nginx/$ https://www.sina.com.cn/ break; } # 正则匹配 location ~ /[a-z]ginx/ { rewrite ^/nginx/$ https://www.163.com/ break; } # 前缀匹配 location ^~ /nginx/ { rewrite ^/nginx/$ https://www.baidu.com/ break; } # 精确匹配 location = /nginx/ { rewrite ^/nginx/$ https://www.qq.com/ break; } } 创建网站根目录 mkdir -p /data/nginx/html/nginx echo 'nginx location test' > /data/nginx/html/nginx/index.html 第一步、把正常匹配注释掉，留下前缀匹配，然后浏览器访问IP/nginx/，第一次返回的结果是跳转到了www.qq.com，证明了=精确匹配优先级是最大的 第二步、把精确匹配注释掉，然后浏览器访问IP/nginx/，结果是跳转到了www.baidu.com，证明了^~前缀匹配优先级是仅次于精确匹配 第三步、因为前缀匹配和正常匹配不能同时存在，所以这一步比较第一步和第二步，还是把精确匹配注释掉，这次把前缀匹配注释掉，把正常匹配注释打开，结果是跳转到了www.163.com，说明正则匹配的优先级是大于正常匹配的，同时也验证了前缀匹配优先级第2，正则匹配优先级第3，正常匹配优先级第4 第四步、现在只剩下全匹配了，所以全匹配的优先级最低 结论：各匹配优先级如下 精确匹配 > 前缀匹配 > 正则匹配 > 正常匹配 > 全匹配 匹配原则除了这个优先级外，还有一个就是在相同指令模式匹配中，匹配度最大的URI优先 root与alias nginx配置 location规则中的 uri 往往都是匹配一个目录。 root 编辑nginx配置文件 server { listen 80; server_name _; location /nginx/ { root /data/nginx/html/; index index.html index.htm; } } 使用root关键字指定网站根目录 当访问127.0.0.1/nginx/index.html时，如果/data/nginx/html/这个目录下没有nginx/index.html或者没有nginx目录，则会报错404 原因是使用root指定目录时，目录下边要包括location后面的uri，否则就会报错 使用root指定目录时，不会将location uri配置的路径去掉，即访问的路径是/data/nginx/html/nginx 访问测试 #不创建目录nginx mkdir -p /data/nginx/html/ echo 'abc' >/data/nginx.html/index.html #访问报错404 $ curl 127.0.0.1/nginx/ 404 Not Found 404 Not Found nginx/1.18.0 alias 编辑nginx配置文件 server { listen 80; server_name _; location /nginx/ { alias /data/nginx/html/; index index.html index.htm; } } 使用alias关键字指定网站根目录 当访问127.0.0.1/nginx/index.html时，只要保证alias指定的目录中有index.html即可，即使没有目录nginx，它会将location uri配置的路径去掉，实际访问的就是/data/nginx/html/index.html 访问测试 #不创建目录nginx mkdir -p /data/nginx/html/ echo 'abc' >/data/nginx.html/index.html $ curl 127.0.0.1/nginx/ abc 结论：root 指定的目录中，需要location中的 uri 路径目录确实存在，alias 指定的目录中不需要 location中的uri 路径目录存在。 代理服务器 proxy_pass 中有无 / 实验环境 角色 主机名 IP 真实web web01 10.0.0.10 nginx代理 nginx-proxy 10.0.0.51 后端真实web服务配置 后端真实web服务nginx 10.0.0.10配置 server { listen 80; server_name _; location /nginx/ { root /data/nginx/html/; index index.html index.htm; } location /ng/ { root /data/nginx/html/; index index.html index.htm; } location /k8s/ { root /data/nginx/html/; index index.html index.htm; } } 创建网站根目录 mkdir -p /data/nginx/html/{nginx,ng,k8s} echo 'nginx' >/data/nginx/html/nginx/index.html echo 'ng' >/data/nginx/html/ng/index.html echo 'k8s' >/data/nginx/html/k8s/index.html 访问测试 $ curl 127.0.0.1/nginx/ nginx $ curl 127.0.0.1/ng/ ng $ curl 127.0.0.1/k8s/ k8s proxy_pass无目录无/ 代理服务器nginx 10.0.0.51配置中 proxy_pass 无'/'验证 server { listen 80; server_name _; location /nginx { #没有/指的是proxy_pass后边的url没有/后缀 proxy_pass http://10.0.0.10; } } 访问测试 $ curl 10.0.0.51/nginx/ nginx $ curl -L 10.0.0.51/nginx nginx 真实web服务10.0.0.10nginx访问日志如下 10.0.0.51 - - [18/Jun/2020:17:46:53 +0800] \"GET /nginx/ HTTP/1.0\" 200 6 \"-\" \"curl/7.29.0\" \"-\" 结论：如果proxy_pass 反向代理中，在server后面没有 \"/\" 反斜杠的话，访问的是 http://10.0.0.10/nginx/index.html，它会去上游真实服务器去匹配代理服务器上面location URI。 proxy_pass无目录有/ 代理服务器nginx 10.0.0.51配置中 proxy_pass '/'验证 server { listen 80; server_name _; location /nginx { #有/指的是proxy_pass后边的url有/后缀 proxy_pass http://10.0.0.10/; } } 访问测试 因为访问的是真实服务器的/，但是并没有在真实服务器根中写入访问页面 $ curl 10.0.0.51/nginx/ 404 Not Found 404 Not Found nginx/1.18.0 真实web服务10.0.0.10nginx访问日志如下 10.0.0.51 - - [18/Jun/2020:17:51:32 +0800] \"GET / HTTP/1.0\" 404 153 \"-\" \"curl/7.29.0\" \"-\" 结论：如果proxy_pass 反向代理中，在server后面有 \"/\" 反斜杠的话，访问的是 http://10.0.0.10/index.html，它会忽略掉代理服务器上面location中的URI，直接访问代理服务器上面的\"/\"。 proxy_pass有目录无/ 代理服务器10.0.0.51proxy_pass后面增加目录，但不加\"/\" server { listen 80; server_name _; location /nginx/ { proxy_pass http://10.0.0.10/nginx; } } 访问测试 因为访问的是真实服务器的/，但是并没有在真实服务器根中写入访问页面 $ curl 10.0.0.51/nginx/ 404 Not Found 404 Not Found nginx/1.18.0 真实web服务10.0.0.10nginx访问日志如下 10.0.0.51 - - [18/Jun/2020:18:04:40 +0800] \"GET /nginx HTTP/1.0\" 404 153 \"-\" \"curl/7.29.0\" \"-\" 结论：如果proxy_pass 反向代理中，在server后面有URI， 但没有 \"/\" 反斜杠的话，访问的是 http://10.0.0.10/nginx，它会去上游真实服务器去匹配代理服务器上面的URI。 proxy_pass有目录有/ 代理服务器10.0.0.51proxy_pass后面增加目录并且加\"/\" server { listen 80; server_name _; location /nginx/ { proxy_pass http://10.0.0.10/nginx/; } } 访问测试 $ curl 10.0.0.51/nginx/ nginx 真实web服务10.0.0.10nginx访问日志如下 10.0.0.51 - - [18/Jun/2020:17:58:47 +0800] \"GET /nginx/ HTTP/1.0\" 200 6 \"-\" \"curl/7.29.0\" \"-\" 结论：如果proxy_pass反向代理中有目录，并且有\"/\"反斜杠的话，访问的是http://10.0.0.10/nginx/index.html，它会忽略掉代理服务器上面location中的URI，直接访问代理服务器上面的目录+\"/\"的形式。 测试结果最终对比 情况 proxy_pass配置 访问结果 日志中的访问路径 proxy_pass中无目录无/ location /nginx { proxy_pass http://10.0.0.10; } 访问的是 http://10.0.0.10/nginx/index.html，它会去上游真实服务器去匹配代理服务器上面location URI。 /nginx/ proxy_pass中无目录有/ location /nginx { proxy_pass http://10.0.0.10/; } 访问的是 http://10.0.0.10/index.html，它会忽略掉代理服务器上面location中的URI，直接访问代理服务器上面的\"/\"。 / proxy_pass中有目录无/ location /nginx/ { proxy_pass http://10.0.0.10/nginx; } 访问的是 http://10.0.0.10/nginx，它会去上游真实服务器去匹配代理服务器上面的URI。 /nginx proxy_pass中有目录有/ location /nginx/ { proxy_pass http://10.0.0.10/nginx/; } 访问的是 http://10.0.0.10/nginx/index.html，它会忽略掉代理服务器上面location中的URI，直接访问代理服务器上面的目录+\"/\"的形式。 /nginx/ 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/nginx/8.nginx虚拟主机.html":{"url":"linux/linux服务/nginx/8.nginx虚拟主机.html","title":"nginx虚拟主机","keywords":"","body":"[toc] nginx虚拟主机 虚拟机主机概念 虚拟主机就是在一台服务器上配置多个网站 虚拟主机分类 基于IP的虚拟主机(浪费IP，没用) 基于端口的虚拟主机 基于域名的虚拟主机 配置nginx虚拟主机 基于域名的虚拟主机 编辑aaa.com.conf文件 cat > /etc/nginx/conf.d/aaa.com.conf 编辑bbb.com.conf文件 cat > /etc/nginx/conf.d/bbb.com.conf 检测nginx语法并重载nginx $ nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful $ nginx -s reload 创建网站根目录 mkdir /website/{aaa,bbb} echo \"aaa.com\" > /website/aaa/index.html echo \"bbb.com\" > /website/bbb/index.html 绑定hosts cat >> /etc/hosts 测试访问 $ curl aaa.com aaa.com $ curl bbb.com bbb.com 基于端口的虚拟主机 虚拟主机监听不同端口，不与系统端口冲突即可 编辑aaa.com.conf文件 cat > /etc/nginx/conf.d/aaa.com.conf 检测nginx语法并重载nginx $ nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful $ nginx -s reload 测试访问 $ curl aaa.com:8001 aaa.com $ curl aaa.com:8002 bbb.com 配置虚拟主机别名 虚拟主机别名，就是虚拟主机设置除了主域名以外的一个域名，实现用户访问的多个域名对应同一个虚拟主机网站的功能。 编辑aaa.com.conf文件 cat > /etc/nginx/conf.d/aaa.com.conf 测试访问，aaa.com、bbb.com、ccc.com访问到的是同一个网站的同一资源 $ curl aaa.com aaa.com $ curl bbb.com aaa.com $ curl ccc.com aaa.com 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/nginx/9.1nginx七层负载均衡.html":{"url":"linux/linux服务/nginx/9.1nginx七层负载均衡.html","title":"nginx七层负载均衡","keywords":"","body":"[toc] nginx七层负载均衡概述 nginx七层负载均衡官方文档简介版 nginx七层负载均衡官方文档详细版 一、nginx负载均衡概念 后端web服务往往要承载大量并发请求，单台服务器难以负荷，后端使用多台web服务器组成集群，前端使用nginx作为负载均衡，将请求分散的打到后端服务器集群中，实现负载的分发。这样会大大提升系统的吞吐率、请求性能、高容灾等等。 示意图 二、nginx负载均衡upstream 2.1 语法 Syntax: upstream name { ... } Default: - Context: http 2.2 负载均衡方法 总揽 开源nginx调度算法 调度算法 概述 Round Robin 轮询(rr)，按时间顺序逐一分配到不同的后端服务器(默认) Weight Round Robin 加权轮询(wrr)，weight值越大，分配到的访问几率越高 IP Hash 每个请求按访问IP的hash结果分配，这样来自同一IP的请求固定访问同一个后端服务器 Generic Hash(url_hash) 按照访问URL的hash结果来分配请求，每个URL定向到同一个后端 Least Connections 最少链接数，哪个机器链接数少就分发 2.2.1 Round Robin 轮询 请求在服务器之间平均分配，同时考虑了服务器权重。默认情况下使用此方法（没有启用它的指令） upstream backend { # 没有配置就是Round Robin 轮询 server backend1.example.com; server backend2.example.com; } 2.2.2 Weight Round Robin 加权轮询 在此示例中，backend1.example.com具有weight 5；其他两台服务器的默认权重（1），但是具有IP地址的192.0.0.1一台backup服务器被标记为服务器，除非其他两台服务器均不可用，否则不会接收请求。随着权重的这种配置，每的6请求，5发送到backend1.example.com和1对backend2.example.com upstream backend { server backend1.example.com weight=5; server backend2.example.com; server 192.0.0.1 backup; } 2.2.3 Least Connections 最少连接 将活动连接最少的请求发送到服务器，再次考虑服务器权重 upstream backend { least_conn; server backend1.example.com; server backend2.example.com; } 2.2.4 IP Hash 从客户端IP地址确定向其发送请求的服务器。在这种情况下，可以使用IPv4地址的前三个八位位组或整个IPv6地址来计算哈希值。该方法保证了来自同一地址的请求将到达同一服务器，除非该请求不可用 upstream backend { ip_hash; server backend1.example.com; server backend2.example.com; } 如果其中一台服务器需要暂时从负载平衡循环中删除，则可以使用down参数对其进行标记，以保留客户端IP地址的当前哈希值。该服务器要处理的请求将自动发送到组中的下一个服务器 upstream backend { server backend1.example.com; server backend2.example.com; server backend3.example.com down; } 2.2.5 Generic Hash (url_hash) 发送请求到的服务器由用户定义的键决定，该键可以是文本字符串、变量或组合。例如，键可以是成对的源IP地址和端口，或者是一个URI，如本例所示 upstream backend { hash $request_uri consistent; server backend1.example.com; server backend2.example.com; } 哈希指令的可选consistent参数支持ketama一致哈希负载平衡。请求根据用户定义的散列键值均匀地分布在所有上游服务器上。如果上游服务器被添加到或从上游组移除，只有几个键被重新映射，在负载平衡的缓存服务器或其他积累状态的应用程序的情况下最小化缓存丢失。 应用场景 有一个服务器集群A，需要对外提供文件下载，由于文件上传量巨大，没法存储到服务器磁盘中，所以用到了第三方云存储来做文件存储。服务器集群A收到客户端请求之后，需要从云存储中下载文件然后返回，为了省去不必要的网络带宽和下载耗时，在服务器集群A上做了一层临时缓存（缓存一个月）。由于是服务器集群，所以同一个资源多次请求，可能会到达不同的服务器上，导致不必要的多次下载，缓存命中率不高，以及一些资源时间的浪费。在此类场景下，为了使得缓存命中率提高，很适合使用url_hash策略，同一个url（也就是同一个资源请求）会到达同一台机器，一旦缓存住了资源，再此收到请求，就可以从缓存中读取，既减少了带宽，也减少的下载时间。 upstream somestream { hash $request_uri; server 192.168.1.1:8080; server 192.168.1.2:8080; server 192.168.1.3:8080; } server { listen 80 default; server_name test.cdn.com; charset utf-8; location /get { proxy_pass http://somestream; } } 2.2.6 random 每个请求将传递到随机选择的服务器，如果two指定了参数，首先，NGINX考虑服务器权重随机选择两个服务器，然后使用指定的方法选择这些服务器之一 least_conn –活动连接最少 least_time=header（NGINX Plus）–从服务器接收响应标头的最短平均时间（$upstream_header_time） least_time=last_byte（NGINX Plus）–从服务器接收完整响应的最短平均时间（$upstream_response_time） upstream backend { random two least_time=last_byte; server backend1.example.com; server backend2.example.com; server backend3.example.com; server backend4.example.com; } 随机负载平衡方法应该用于分布式环境，其中多个负载平衡器将请求传递给同一组后端。对于负载平衡器对所有请求都有完整视图的环境，可以使用其他负载平衡方法，比如轮询、最少连接和最少时间。 nginx plus增加的调度算法 调度算法 概述 Least Time 选择具有最低平均延迟和最低数量的活动连接 第三方调度算法 调度算法 概述 fair 按照服务器端的响应时间来分配请求，响应时间短的优先分配 2.2.7 fair 按照服务器端的响应时间来分配请求，响应时间短的优先分配。 upstream dynamic_zuoyu { fair; #实现响应时间短的优先分配 server localhost:8080; #tomcat 7.0 server localhost:8081; #tomcat 8.0 server localhost:8082; #tomcat 8.5 server localhost:8083; #tomcat 9.0 } 2.3 后端web服务器在前端nginx负载均衡调度中的状态 ⚠️ip_hash和bakcup不能一起写，否则语法检测会有报错 状态 概述 down 当前的server暂时不参与负载均衡 backup 预留的备份服务器 max_fails 允许请求失败的次数 fail_timeout 经过max_fails失败后, 服务暂停时间 max_conns 限制最大的接收连接数 配置示例 upstream www { server 10.0.0.50:80 down; server 10.0.0.51:80 backup; server 10.0.0.52:80 max_fails=1 fail_timeout=10s; } location / { proxy_pass http://www; include proxy_params; } 2.4 配置示例 #upstream例子 upstream backend { server backend1.example.com weight=5; server backend2.example.com:8080; server unix:/tmp/backend3; server backup1.example.com:8080 backup; } server { location / { proxy_pass http://backend; } } 三、配置健康检查 http健康检查官方文档 3.1 被动健康检查 对于被动运行状况检查，NGINX会监视发生的事务，并尝试恢复失败的连接。如果仍然无法恢复交易，则NGINX会将服务器标记为不可用，并暂时停止向服务器发送请求，直到再次将其标记为活动。 使用块中server指令的参数为每个上游服务器定义了将上游服务器标记为不可用的条件upstream： fail_timeout –设置必须多次尝试失败才能将服务器标记为不可用的时间，以及将服务器标记为不可用的时间（默认为10秒）。 max_fails –设置在fail_timeout服务器标记为不可用的时间段内必须发生的失败尝试次数（默认为1次尝试）。 在以下示例中，如果NGINX无法在30秒内向服务器发送请求或没有收到3次响应，则它将服务器标记为30秒钟不可用： upstream backend { server backend1.example.com; server backend2.example.com max_fails=3 fail_timeout=30s; } 四、nginx负载均衡配置示例 实验环境 服务器角色 ip 主机名 lb 10.0.0.10 lb01 web01 10.0.0.51 web01 web02 10.0.0.52 web02 4.1 负载均衡lb配置 编辑nginx配置文件 upstream www { server 10.0.0.51; server 10.0.0.52; } server { listen 80; server_name _; location / { proxy_pass http://www; } } 4.2 web01配置 编辑nginx配置文件 server { listen 80; root /code; index index.html; } 创建网站根目录 mkdir /code cat > /code/index.html Code1 Code1 EOF 4.3 web02配置 编辑nginx配置文件 server { listen 80; root /code; index index.html; } 创建网站根目录 mkdir /code cat > /code/index.html Code1 Code2 EOF 4.4 浏览器访问lb IP地址 第一次访问 第二次访问 因为是使用默认的rr轮询算法，因此请求会依次转发到web01和web02 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/nginx/9.2nginx四层负载均衡.html":{"url":"linux/linux服务/nginx/9.2nginx四层负载均衡.html","title":"nginx四层负载均衡","keywords":"","body":"[toc] nginx四层负载均衡 nginx四层负载均衡官方文档 一、nginx四层负载均衡简介 nginx官网对于--with-stream模块的简介 nginx从1.9.0版本开始，新增了ngx_stream_core_module 模块，使nginx支持四层负载均衡。默认编译的时候该模块并未编译进去，需要编译的时候添加 --with-stream，使其支持stream代理。 负载平衡是指在多个后端服务器之间有效地分配网络流量。 nginx可以代理和负载平衡传输控制协议（TCP）通信。TCP是许多流行的应用程序和服务的协议，例如LDAP，MySQL和RTMP。 nginx可以代理和负载均衡UDP流量。UDP（用户数据报协议）是许多流行的非事务性应用程序的协议，例如DNS，syslog和RADIUS。 二、先决条件 需要模块--with-stream的支持 通过TCP或UDP进行通信的应用程序，数据库或服务 上游服务器，每个服务器运行应用程序，数据库或服务的相同实例 三、官方配置示例 3.1 配置反向代理 首先，需要配置反向代理，以便NGINX可以将TCP连接或UDP数据报从客户端转发到上游组或代理服务器。 3.1.1 创建一个顶级stream{}块： stream { # ... } server {}在顶级stream {}上下文中为每个虚拟服务器定义一个或多个配置块。 3.1.2 在server {}每个服务器的配置块中，包括listen用于定义服务器侦听的IP地址或端口的指令。 对于UDP通信，还包括udp参数。由于TCP是stream上下文的默认协议，因此tcp该listen指令没有参数： stream { server { listen 12345; # ... } server { listen 53 udp; # ... } # ... } 3.1.3 配置proxy_pass指令以定义代理服务器或服务器将流量转发到的上游组 stream { server { listen 12345; #TCP流量将转发到\"stream_backend\"上游组 proxy_pass stream_backend; } server { listen 12346; #TCP流量将被转发到指定的服务器 proxy_pass backend.example.com:12346; } server { listen 53 udp; #UDP流量将转发到\"dns_servers\"上游组 proxy_pass dns_servers; } # ... } 3.1.4 配置代理绑定 如果代理服务器具有多个网络接口，则可以选择将NGINX配置为在连接到上游服务器时使用特定的源IP地址。如果将NGINX之后的代理服务器配置为接受来自特定IP网络或IP地址范围的连接，这可能很有用 包括proxy_bind指令和相应网络接口的IP地址 stream { # ... server { listen 127.0.0.1:12345; proxy_pass backend.example.com:12345; proxy_bind 127.0.0.1:12345; } } 3.1.5 配置缓冲 可以调整两个内存缓冲区的大小，NGINX可以在其中放置来自客户端和上游连接的数据。如果数据量很小，则可以减少缓冲区，这可以节省内存资源。如果有大量数据，则可以增加缓冲区大小以减少套接字读/写操作的数量。在一个连接上接收到数据后，NGINX将读取该数据并通过另一连接转发该数据。缓冲区由proxy_buffer_size伪指令控制： stream { # ... server { listen 127.0.0.1:12345; proxy_pass backend.example.com:12345; proxy_buffer_size 16k; } } 3.2 配置TCP或UDP负载平衡 3.2.1 创建一组服务器 upstream{}在顶级stream {}上下文中定义一个或多个配置块，并为上游组（例如，stream_backendTCP服务器和dns_serversUDP服务器）设置名称： 确保上游组的名称由proxy_pass指令引用，就像上面为反向代理配置的指令一样。 stream { upstream stream_backend { # ... } upstream dns_servers { # ... } # ... } 3.2.2 在服务器组中添加后端真实服务器(上游服务器) 在该upstream {}块内，server为每个上游服务器添加一个指令，指定其IP地址或主机名（可以解析为多个IP地址）和一个必需的端口号。请注意，并未为每个服务器定义协议，因为该协议是由在前面创建listen的server块中的指令中包含的参数为整个上游组定义的。 stream { upstream stream_backend { server backend1.example.com:12345; server backend2.example.com:12345; server backend3.example.com:12346; # ... } upstream dns_servers { server 192.168.136.130:53; server 192.168.136.131:53; # ... } # ... } 3.2.3 配置上游服务器组的负载均衡算法 3.2.3.1 轮询 Round Robin 默认情况下，NGINX使用Round Robin算法来负载均衡流量，将其顺序地定向到已配置的上游组中的服务器。因为它是默认方法，所以没有round‑robin指令。只需在顶级上下文中创建配置块并添加上一步中所述的指令。upstream {} stream {} server 3.2.3.2 最少连接 NGINX选择当前活动连接数量较少的服务器。 3.2.3.3 哈希 hash NGINX根据用户定义的密钥（例如，源IP地址（$remote_addr））选择服务器： upstream stream_backend { hash $remote_addr; server backend1.example.com:12345; server backend2.example.com:12345; server backend3.example.com:12346; } 该Hash负载平衡方法也可以用来配置会话持久性。由于哈希函数基于客户端IP地址，因此除非服务器关闭或不可用，否则来自给定客户端的连接将始终传递到同一服务器。指定一个可选consistent参数以应用ketama一致性哈希方法： hash $remote_addr consistent; 3.2.4 为每个上游服务器指定服务器特定的参数，包括最大连接数，服务器权重等(可选) upstream stream_backend { hash $remote_addr consistent; server backend1.example.com:12345 weight=5; server backend2.example.com:12345; server backend3.example.com:12346 max_conns=3; } upstream dns_servers { least_conn; server 192.168.136.130:53; server 192.168.136.131:53; # ... } 另一种方法是将流量代理到单个服务器而不是上游组。如果您通过主机名标识服务器，并配置主机名以解析为多个IP地址，则NGINX使用该Round Robin算法对IP地址之间的流量进行负载平衡。在这种情况下，您必须在proxy_pass指令中指定服务器的端口号，并且不得在IP地址或主机名之前指定协议 stream { # ... server { listen 12345; proxy_pass backend.example.com:12345; } } 3.3 配置TCP健康检查 nginx TCP健康检查官方文档 3.3.1 简介 nginx TCP健康检查可以持续测试TCP上游服务器，避免出现故障的服务器，并可以将恢复的服务器正常地添加到负载平衡组中 3.3.2 前提条件 已在stream上下文中配置了TCP服务器的上游组** stream { #... upstream stream_backend { server backend1.example.com:12345; server backend2.example.com:12345; server backend3.example.com:12345; } #... } 已经配置了将TCP连接传递到服务器组的服务器 stream { #... server { listen 12345; proxy_pass stream_backend; } #... } 3.3.3 被动TCP运行状况检查 如果连接上游服务器的尝试超时或导致错误，NGINX可以将服务器标记为不可用，并在指定的时间内停止向其发送请求。要定义NGINX认为上游服务器不可用的条件，请在server指令中包含以下参数 fail_timeout –在指定的连接尝试次数内必须失败的时间，才能将服务器视为不可用。同样，NGINX将服务器标记为不可用后的时间。 max_fails –在指定时间内NGINX认为服务器不可用的失败尝试次数。 默认值为10秒数和1尝试次数。因此，如果连接尝试超时或在10秒钟内至少失败一次，NGINX会将服务器标记为10秒钟不可用。该示例显示了如何在30秒内将这些参数设置为2个失败 upstream stream_backend { server backend1.example.com:12345 weight=5; server backend2.example.com:12345 max_fails=2 fail_timeout=30s; server backend3.example.com:12346 max_conns=3; } 3.3.3.1 服务器缓慢启动(只有nginx plus可以使用) 最近恢复的上游服务器很容易被连接淹没，这可能导致服务器再次标记为不可用。慢速启动允许上游服务器在恢复或可用后将其权重从零逐渐恢复到其标称值。这可以通过slow_start上游server指令的参数来完成 upstream backend { server backend1.example.com:12345 slow_start=30s; server backend2.example.com; server 192.0.0.1 backup; } ⚠️请注意，如果组中只有一台服务器，则将slow_start忽略该参数，并且永远不会将服务器标记为不可用。慢速启动是NGINX Plus独有的 3.4 配置UDP健康检查 nginx UDP健康检查官方文档 3.4.1 前提条件 已配置上下文中的上游服务器组来处理UDP网络流量（DNS，RADIUS，系统日志），例如：stream {} stream { #... upstream dns_upstream { server 192.168.136.130:53; server 192.168.136.131:53; server 192.168.136.132:53; } #... } 已经配置了将UDP数据报传递到上游服务器组的服务器 stream { #... server { listen 53 udp; proxy_pass dns_upstream; proxy_timeout 1s; proxy_responses 1; error_log logs/dns.log; } #... } 3.4.2 被动UDP健康检查 如果服务器回复错误或超时，则NGINX可以将服务器标记为不可用，并在一段时间内停止向其发送UDP数据报。 max_fails使用上游服务器的参数设置在特定时间段内连续失败的连接尝试次数（默认值为1）。 时间段由fail_timeout参数设置（默认值为10秒）。该参数还设置了NGINX标记服务器后认为服务器不可用的时间。 因此，如果连接尝试超时或在10秒内至少失败一次，NGINX会将服务器标记为10秒钟不可用。该示例显示了如何在60秒内将这些参数设置为2个失败： upstream dns_upstream { server 192.168.136.130:53 fail_timeout=60s; server 192.168.136.131:53 fail_timeout=60s; } 四、TCP和UDP负载平衡配置官方总示例 在此示例中，所有与TCP和UDP代理相关的功能都在stream块内进行配置，就像在http块中配置了HTTP请求的设置一样。 有两个命名的upstream块，每个块包含三个托管彼此相同内容的服务器。在serverfor eadch服务器中，服务器名称后跟必需的端口号。根据“ 最少连接”负载平衡方法，连接在服务器之间分配：连接到活动连接最少的服务器。 这三个server块定义了三个虚拟服务器： 第一台服务器侦听端口12345，并将所有TCP连接代理到上游服务器的stream_backend组。请注意，在模块proxy_pass上下文中定义的指令stream不得包含协议。 指定了两个可选的超时参数：proxy_connect_timeout伪指令设置与stream_backend组中的服务器建立连接所需的超时。该proxy_timeout指令设置在代理到stream_backend组中的一台服务器已启动之后使用的超时。 第二台服务器侦听端口53，并将所有UDP数据报（指令的udp参数listen）代理到称为dns_servers的上游组。如果udp未指定该参数，则套接字监听TCP连接。 第三台虚拟服务器侦听端口12346，并代理到backend4.example.com的 TCP连接，后者可以解析为使用Round Robin方法实现负载平衡的多个IP地址。 stream { upstream stream_backend { least_conn; server backend1.example.com:12345 weight=5; server backend2.example.com:12345 max_fails=2 fail_timeout=30s; server backend3.example.com:12345 max_conns=3; } upstream dns_servers { least_conn; server 192.168.136.130:53; server 192.168.136.131:53; server 192.168.136.132:53; } server { listen 12345; proxy_pass stream_backend; proxy_timeout 3s; proxy_connect_timeout 1s; } server { listen 53 udp; proxy_pass dns_servers; } server { listen 12346; proxy_pass backend4.example.com:12346; } } 五、实际配置示例 5.1 实验说明 官方对于TCP负载均衡的说明，应该选择使用TCP协议的服务去验证 nginx可以代理和负载平衡传输控制协议（TCP）通信。TCP是许多流行的应用程序和服务的协议，例如LDAP，MySQL和RTMP。 这里选择使用TCP协议的mysql和ssh服务作为实验对象，便于验证 1.访问负载均衡的12345端口，连接至后端web的22端口 2.访问负载均衡的12346端口，连接至后端mysql的3306端口 实验环境 服务器角色 外网ip** 主机名 lb 10.0.0.10 lb web01 10.0.0.51 web mysql 10.0.0.52 mysql 5.2 负载均衡lb操作 ⚠️stream {} 必须与 http {}在同一级，而nginx主配置文件/etc/nginx/nginx.conf中include /etc/nginx/conf.d/*.conf;是在http中的，因此在/etc/nginx/conf.d/下编辑含有stream {}的配置文件是会报错的 编辑nginx主配置文件/etc/nginx/nginx.conf #在events下方 http上方加入以下内容，以解决stream必须与http同级的问题 include /etc/nginx/conf.e/*.conf; 创建目录 mkdir /etc/nginx/conf.e 编辑nginx配置文件 cat > /etc/nginx/conf.e/tcp.conf 检测nginx语法并重载nginx $ nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful $ nginx -s reload 5.3 测试 5.3.1 ssh连接测试 连接lb的12345端口，因为默认算法是rr，因此会轮流连接到web和mysql机器 5.3.2 mysql连接测试 测试前在mysql机器上进行一个授权 mysql> grant all on *.* to test@'%' identified by 'test'; Query OK, 0 rows affected, 1 warning (0.00 sec) 创建一个数据库，便于确认 mysql> create database tcp_test; Query OK, 1 row affected (0.00 sec) 连接成功，通过lb的四层负载均衡进行的端口转发连接到了mysql机器上的3306端口 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/nginx/9.3nginx负载均衡之基于不同客户端.html":{"url":"linux/linux服务/nginx/9.3nginx负载均衡之基于不同客户端.html","title":"nginx负载均衡之基于不同客户端","keywords":"","body":"[toc] nginx负载均衡之基于不同客户端 负载均衡还可以根据不同客户端进行相应的转发 以下示例为基于不同手机端及浏览器进行转发 实验环境 角色 IP地址 主机名 负载均衡 lb 10.0.0.10 lb 真实后端服务 web01 10.0.0.51 web01 真实后端服务 web02 10.0.0.52 web02 让手机端的访问转发到web01，让浏览器的访问和默认访问转发到web02 负载均衡操作 upstream iphone { server 10.0.0.51; } upstream android { server 10.0.0.51:8080; } upstream chrom { server 10.0.0.52; } upstream firefox { server 10.0.0.52:8080; } upstream defaults { server 10.0.0.52; } server { listen 80; server_name _; location / { #匹配iPhone手机访问 if ($http_user_agent ~* \"iphone\") { proxy_pass http://iphone; } #匹配Android手机访问 if ($http_user_agent ~* \"android\") { proxy_pass http://android; } #匹配谷歌浏览器访问 if ($http_user_agent ~* \"chrom\") { proxy_pass http://chrom; } #匹配firefox浏览器访问 if ($http_user_agent ~* \"firefox\") { proxy_pass http://Firefox; } #其他浏览器访问默认规则 proxy_pass http://defaults; } } web01操作 编辑nginx配置文件 server { listen 80; server_name _; root /data/iphone; index index.html; } server { listen 8080; server_name _; root /data/android; index index.html; } 创建网站根目录 mkdir -p /data/{iphone,android} echo 'iphone' >/data/iphone/index.html echo 'android' >/data/android/index.html web02操作 编辑nginx配置文件 server { listen 80; server_name _; root /data/chrom; index index.html; } server { listen 8080; server_name _; root /data/firefox; index index.html; } 创建网站根目录 mkdir -p /data/{chrom,firefox} echo 'chrom' >/data/chrom/index.html echo 'firefox' >/data/firefox/index.html 浏览器访问验证 模拟iphone nginx访问日志 10.0.0.2 - - [20/Jun/2020:19:22:50 +0800] \"GET / HTTP/1.1\" 200 7 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1\" \"-\" 模拟安卓手机 nginx访问日志 10.0.0.2 - - [20/Jun/2020:19:21:29 +0800] \"GET / HTTP/1.1\" 304 0 \"-\" \"Mozilla/5.0 (Linux; Android 5.0; SM-G900P Build/LRX21T) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Mobile Safari/537.36\" \"-\" 谷歌浏览器访问 nginx访问日志 10.0.0.2 - - [20/Jun/2020:19:26:17 +0800] \"GET / HTTP/1.1\" 304 0 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\" \"-\" 火狐浏览器访问 nginx访问日志 10.0.0.2 - - [20/Jun/2020:19:22:05 +0800] \"GET / HTTP/1.1\" 304 0 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:77.0) Gecko/20100101 Firefox/77.0\" \"-\" 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/nginx/10.lnmp平台.html":{"url":"linux/linux服务/nginx/10.lnmp平台.html","title":"lnmp平台","keywords":"","body":"[toc] lnmp平台 一、lnmp简介 LNMP就是Linux+Nginx+MySQL+PHP，Linux作为服务器的操作系统，Nginx作为Web服务器、PHP作为解析动态脚本语言、MySQL即为数据库。 Linux作为服务器的操作系统。 Nginx作为WebServer服务器。 PHP 作为动态解析服务(php)。 MySQL作为后端存储数据库服务。 二、fastcgi协议 Nginx服务本身不能处理PHP的请求，用户发起PHP动态请求, Nginx处理过程如下 用户-->http协议-->Nginx-->fastcgi协议-->php-fpm fastcgi是nginx连接php-fpm之间的协议 nginx结合PHP FastCGI运行原理图 1.用户发起的所有请求会先抵达LNMP架构中的Nginx 2.如果用户请求的是静态内容，则Nginx直接响应并处理 3.如果用户请求的是动态内容，则通过fastcgi协议发送至php-fpm管理进程 4.php-fpm接收到请求后，会派生对应的wrapper线程，来解析用户请求的动态内容 5.如果涉及到查询数据库操作，则需要php先连接数据库，然后进行查询操作(php-mysql) 6.最终由mysql-->php-fpm->fastcgi->nginx->client 三、搭建lnmp平台 3.1 安装nginx nginx centos rpm包官方下载地址 3.1.1 下载安装包并安装 wget https://nginx.org/packages/rhel/7/x86_64/RPMS/nginx-1.18.0-1.el7.ngx.x86_64.rpm yum -y localinstall nginx-1.18.0-1.el7.ngx.x86_64.rpm 3.1.2 创建www用户并把nginx运行用户修改为www useradd -u www -M -s /sbin/nologin sed -i.bak '/user nginx;/cuser www;' /etc/nginx/nginx.conf 3.1.3 启动nginx并设置开机自启 systemctl enable nginx && systemctl start nginx 3.2 安装mysql mysql官方下载地址 3.2.1 下载安装包并安装 wget https://cdn.mysql.com//Downloads/MySQL-8.0/mysql-8.0.20-1.el7.x86_64.rpm-bundle.tar yum -y localinstall *.rpm 3.2.2 启动mysql并设置开机自启 systemctl enable mysqld && systemctl start mysqld 3.2.3 从/var/log/mysqld.log中找到mysql8的默认roo密码 $ grep 'root@localhost' /var/log/mysqld.log 2020-06-21T00:46:49.942011Z 6 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: jB6wzV 3.2.4 修改root密码 mysql8中密码要求为要包含大写字母，小写字母，数字，特殊符号 在MySQL 8.04前，执行：SET PASSWORD=PASSWORD('[新密码]');修改密码 但是MySQL8.0.4开始，这样默认是不行的。因为之前，MySQL的密码认证插件是mysql_native_password，而现在使用的是caching_sha2_password。 ⚠️mysql8.0.20初次修改root密码需要用如下命令 mysql> alter user user() identified by 'Bxb123.com'; Query OK, 0 rows affected (0.00 sec) 查看mysql8.0.20密码策略 mysql> show variables like 'validate_password%'; +--------------------------------------+--------+ | Variable_name | Value | +--------------------------------------+--------+ | validate_password.check_user_name | ON | | validate_password.dictionary_file | | | validate_password.length | 8 | | validate_password.mixed_case_count | 1 | | validate_password.number_count | 1 | | validate_password.policy | MEDIUM | | validate_password.special_char_count | 1 | +--------------------------------------+--------+ validate_password.length 固定密码的总长度； validate_password.dictionary_file 指定密码验证的文件路径； validate_password.mixed_case_count 整个密码中至少要包含大/小写字母的总个数； validate_password.number_count 整个密码中至少要包含阿拉伯数字的个数； validate_password.policy 指定密码的强度验证等级，默认为 MEDIUM； 关于 validate_password.policy 的取值： 0/LOW：只验证长度； 1/MEDIUM：验证长度、数字、大小写、特殊字符； 2/STRONG：验证长度、数字、大小写、特殊字符、字典文件； 如果需要设置空密码或者简单密码 设置空密码 mysql> UNINSTALL COMPONENT \"file://component_validate_password\"; Query OK, 0 rows affected (0.00 sec) mysql> set password=''; Query OK, 0 rows affected (0.00 sec) 设置简单密码 #设置密码的验证强度等级，LOW表示只验证密码长 set global validate_password.policy=LOW; #设置密码固定密码总长度 set global validate_password.length=4; 3.3 安装php 3.3.1 添加第三方yum源 安装epel源并添加第三方yum源 yum -y install epel-release && \\ yum -y install https://rpms.remirepo.net/enterprise/remi-release-7.rpm 选择要安装的php版本 export phpversion=php73 yum -y install $phpversion-php-fpm $phpversion-php-cli $phpversion-php-bcmath $phpversion-php-gd $phpversion-php-json $phpversion-php-mbstring $phpversion-php-mcrypt $phpversion-php-mysqlnd $phpversion-php-opcache $phpversion-php-pdo $phpversion-php-pecl-crypto $phpversion-php-pecl-mcrypt $phpversion-php-pecl-geoip $phpversion-php-recode $phpversion-php-snmp $phpversion-php-soap $phpversion-php-xml 通过以下命令来获取更多安装信息 yum search php73 安装后的php配置文件路径 /etc/opt/remi/php73 3.3.2 修改php配置文件 编辑文件/etc/opt/remi/php73/php-fpm.d/www.conf修改php运行用户和组为www sed -i.bak '/^user/c user = www' /etc/opt/remi/php73/php-fpm.d/www.conf && \\ sed -i '/^group/c group = www' /etc/opt/remi/php73/php-fpm.d/www.conf 3.3.3 启动php并设置开机自启 systemctl enable php73-php-fpm && systemctl start php73-php-fpm 四、基于lnmp平台搭建wordpress wordpress中文官网 wordpress下载地址 4.1 nginx配置 编辑wordpress的nginx配置文件/etc/nginx.conf.d/blog.conf ⚠️server_name下的index后边必须是index.php，否则会报错403 cat > /etc/nginx/conf.d/blog.conf wordpress https配置文件 server { listen 80; server_name blog.nginx.com; rewrite (.*) https://$server_name$request_uri redirect; } server { server_name _; listen 443; client_max_body_size 20m; ssl on; ssl_certificate ssl_key/server.crt; ssl_certificate_key ssl_key/server.key; location / { root /website/wordpress; index index.php index.html; } location ~ \\.php$ { root /website/wordpress; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } } 文件fastcgi_params fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;#脚本文件请求的路径 #以下为默认内容 fastcgi_param QUERY_STRING $query_string; #请求的参数;如?app=123 fastcgi_param REQUEST_METHOD $request_method; #请求的动作(GET,POST) fastcgi_param CONTENT_TYPE $content_type; #请求头中的Content-Type字段 fastcgi_param CONTENT_LENGTH $content_length; #请求头中的Content-length字段。 fastcgi_param SCRIPT_NAME $fastcgi_script_name; #脚本名称 fastcgi_param REQUEST_URI $request_uri; #请求的地址不带参数 fastcgi_param DOCUMENT_URI document_uri; #与document_uri; #与uri相同。 fastcgi_param DOCUMENT_ROOT $document_root; #网站的根目录。在server配置中root指令中指定的值 fastcgi_param SERVER_PROTOCOL $server_protocol; #请求使用的协议，通常是HTTP/1.0或HTTP/1.1。 fastcgi_param GATEWAY_INTERFACE CGI/1.1;#cgi 版本 fastcgi_param SERVER_SOFTWARE nginx/$nginx_version;#nginx 版本号，可修改、隐藏 fastcgi_param REMOTE_ADDR $remote_addr; #客户端IP fastcgi_param REMOTE_PORT $remote_port; #客户端端口 fastcgi_param SERVER_ADDR $server_addr; #服务器IP地址 fastcgi_param SERVER_PORT $server_port; #服务器端口 fastcgi_param SERVER_NAME $server_name; #服务器名，域名在server配置中指定的server_name #fastcgi_param PATH_INFO $path_info;#可自定义变量 # PHP only, required if PHP was built with --enable-force-cgi-redirect #fastcgi_param REDIRECT_STATUS 200; 在php可打印出上面的服务环境变量 如：echo $_SERVER['REMOTE_ADDR'] 创建网站根目录 mkdir /website 编辑php测试配置文件 cat > /etc/nginx/conf.d/test.conf 创建info.php测试php是否正常解析 cat > /tmp/info.php EOF 注意端口是88 创建mysql.php测试php是否能连接mysql数据库服务 cat > /tmp/mysql.php EOF 本机测试即可 $ curl 127.0.0.1:88/mysql.php Connection successful 4.2 下载wordpress安装包并解压至nginx网站根目录 wget https://cn.wordpress.org/latest-zh_CN.tar.gz tar xf latest-zh_CN.tar.gz -C /website cd /website && chown -R www.www wordpress/ 4.3 创建数据库并授权 mysql> create database wordpress; Query OK, 1 row affected (0.00 sec) #mysql8不能直接创建用户授权，需要先创建用户，然后授权 mysql> create user wordpress@'localhost' identified by 'wordpress'; Query OK, 0 rows affected (0.01 sec) mysql> grant all on wordpress.* to wordpress@'localhost'; Query OK, 0 rows affected (0.00 sec) 4.4 浏览器访问IP开始安装 第一步、浏览器访问IP开始安装 点击现在就开始 第二步、填写数据库信息 第三步、开始安装 第四步、配置站点标题、用户名密码、邮箱 第五步、完成安装 第六步、登陆wordpress 登陆后首界面 五、文件上传限制问题 修改php配置文件php.ini upload_max_filesize = 20M post_max_size = 20M 修改nginx配置文件 client_max_body_size 300M; php.ini 配置对php上传文件大小的影响参数有： 配置项 值 功能 file_uploads ON 确定服务器上的PHP脚本是否可以接受HTTP文件上传 memory_limit 8M 设置脚本可以分配的最大内存量，防止失控的脚本独占服务器内存 post_max_size 8M 限制通过POST方法可以接受的信息最大量 upload_max_filesize 2M 限制PHP处理上传文件最大值，此值必须小于post_max_size值 ⚠️当post_max_size值小于upload_max_filesize的值，以post_max_size的值为准 而对应的$_FILES 中error对应的错误提示有： 文件上传时产生的错误 0：表示没有发生任何错误，文件上传成功 1：表示上传文件的大小超出了再PHP配置文件中upload_max_filesize选项限制的值 2：表示上传文件大小超出了HTML表单中MAX_FILE_SIZE选项所指定的值 3：表示文件只被部分上传 4：表示没有上传任何文件 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/nginx/11.nginx https.html":{"url":"linux/linux服务/nginx/11.nginx https.html","title":"nginx https","keywords":"","body":"[toc] nginx https openssl官网 一、检查环境并创建存放证书目录 1.1 openssl版本1.0.2以上 $ openssl version OpenSSL 1.0.2k-fips 26 Jan 2017 1.2 nginx必须支持--with-http_ssl_module模块 #先把输出放到一个文件中，然后从文件中过滤 nginx -V &> nginx.txt 1.3 创建存放nginx证书的目录 mkdir /etc/nginx/ssl_key && cd /etc/nginx/ssl_key 二、生成证书 2.1 生成私钥 使用openssl充当CA权威机构创建私钥(生产不可能使用此方式生成证书，不被互联网CA权威承认的黑户证书) 加-idea参数就会提示输入密码，最少4位 openssl genrsa -out ca.key 2048 2.2 生成自签证书 交互式 openssl req -x509 -new -nodes -sha256 -days 36500 -key ca.key -out ca.crt 参数说明 参数 说明 req 请求子命令 -x509 证书格式 -new 生成证书请求 -nodes 私钥不加密 -days 证书有效期 -key 指定私钥文件 -out 输入证书文件 免交互式 openssl req -x509 -new -nodes -sha256 -days 36500 \\ -subj \"/C=CN/ST=Beijing/L=Beijing/O=dev/OU=devops/CN=www.yzbpdcnm.com\" \\ -key ca.key \\ -out ca.crt 免交互式-subj参数 简写 完整单词 含义 C Country Name 国家 ST State or Province Name 省 L Locality Name 城市 O Organization Name 组织名称 OU Organization Unit Name 组织单位名称 CN Common Name 域名 三、配置nginx以https方式访问 3.1 编辑nginx配置文件 cat > /etc/nginx/conf.d/https-test.conf 3.2 创建网站根目录 mkdir /code && echo 'https test' >/code/index.html 3.3 本地绑定hosts浏览器访问 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/nginx/12.nginx rewrite.html":{"url":"linux/linux服务/nginx/12.nginx rewrite.html","title":"nginx rewrite","keywords":"","body":"[toc] nginx rewrite 本文部分抄袭于此 nginx rewrite官方文档 一、rewrite基本概述 1.1 什么是rewrite rewrite即URL重写， 主要实现url地址重写以及重定向, 就是把传入Web的请求重定向到其他URL的过程。 1.2 rewrite使用场景 1.URL地址跳转，例如用户访问aaa.com将其跳转到bbb.com , 或者当用户通过http的方式访问aaa.com时，将其跳转至https的方式访问bbb.com 2.URL伪静态, 将动态页面显示为静态页面方式的一种技术, 便于搜索引擎的录入, 同时减少动态URL地址对外暴露过多的参数, 提升更高的安全性。 3.搜索引擎SEO优化依赖于url路径, 以便支持搜索引擎录入 二、rewrite配置语法 Syntax: rewrite regex replacement [flag]; Default: — Context: server, location, if 在匹配过程中可以引用一些Nginx的全局变量 变量 说明 $document_root 针对当前请求的根路径设置值 $host 请求信息中的\"Host\"，如果请求中没有Host行，则等于设置的服务器名 $request_filename 当前请求的文件路径名（带网站的主目录/code/images/test.jpg） $request_uri 当前请求的文件路径名（不带网站的主目录/images/test.jpg） $scheme 请求用的协议，比如http或者https rewrite匹配优先级 1.执行server块的rewrite指令 2.执行location匹配 3.执行选定的location中的rewrite 三、flag rewrite指令根据表达式来重定向URI, 或者修改字符串。 可以应用于server、location、if环境下，每行rewrite指令最后跟一个flag标记，支持的flag标记有如下表格所示 flag 说明 last 本条规则匹配完成后，停止匹配，不在匹配后面的规则 break 本条规则匹配完成后，停止匹配，不在匹配后面的规则 redirect 返回302临时重定向， 地址栏会显示跳转后的地址 permanent 返回301永久重定向，地址栏会显示跳转后的地址 last 官方解释 停止处理当前ngx_http_rewrite_module指令集， 并开始搜索与更改后的URI相匹配的新位置 如果匹配的URI，rewrite在server块中，并且last做为flag，匹配到此rewrite URI时，不再向下匹配server块中的rewrite，进而继续下面location URI的查找匹配； 如果匹配的URI，rewrite在location块中，last做为flag，匹配到此rewrite URI时，会跳出此location块，继续从上到下查找其它的location块URI，但不会再匹配server块中的rewrite中的URI break 官方解释 ngx_http_rewrite_module与break指令一样， 停止处理当前的指令集 如果匹配的URI，rewrite在server块中，并且break做为flag，匹配到此rewrite URI时，不再向下匹配server块中的rewrite，进而继续下面location URI的查找匹配； 如果匹配的URI，rewrite在location块中，break做为flag，匹配到此rewrite时，不会跳出此location块，而是继续对location块下面的语句继续运行，不会跳出此location块，并且也不会匹配location 块下面的其它rewrite规则； 完成该rewrite规则的执行后，停止处理后续rewrite指令集，并不再重新查找；但是当前location内剩余非rewrite语句和location外的的非rewrite语句可以执行 redirect 官方解释 返回带有302代码的临时重定向；如果替换字符串不是以 \"http://\"，\" https://\"或\" $scheme\" 开头，则使用该字符串 permanent 官方解释 返回301永久重定向，地址栏会显示跳转后的地址，即表示如果客户端不清理浏览器缓存，那么返回的结果将永久保存在客户端浏览器中了 四、nginx处理http请求的11个阶段 ngx_http_post_read_phase 读取请求头，接收到http头部后处理阶段 ngx_http_server_rewrite_phase 执行server块中rewrite，独立http阶段 ngx_http_find_config_phase 根据uri查找替换location，uri寻找匹配的location阶段 ngx_http_rewrite_phase 根据替换结果，继续执行rewrite，寻找到匹配的location之后再修改请求的uri ngx_http_post_rewrite_phase 执行rewrite后处理，在rewrite重写url后，防止错误的nginx.conf配置导致死循环(递归的修改uri) ngx_http_preaccess_phase 认证预处理，请求限制，连接限制，表示在处理ngx_http_access_phase阶段请求访问限制前，http模块可以介入处理的阶段 ngx_http_access_phase 认证处理，用于让http模块判断是否允许这个请求访问nginx服务 ngx_http_post_access_phase 认证后处理，认证不通过，丢包，在ngx_http_access_phase阶段中，当http模块的handler处理函数返回不允许访问的错误码时(ngx_htp_forbidden或者ngx_http_unauhorized)，这里将负责向用户发送拒绝服务的错误相应 ngx_http_try_files_phase 尝试try标签，此阶段专门为try_files配置预设立，当http请求访问静态资源时，try_files配置项可以使这个配置顺序的访问多个静态资源 ngx_http_content_phase 内容处理，用于处理http请求内容的阶段，这是大部分http模块最愿意介入的阶段 ngx_http_log_phase 日志处理，处理完请求后记录日志的阶段 五、rewrite配置实例 5.1 无flag测试 在nginx中引入echo、sleep、time等功能 github地址 无flag测试用例1 server { listen 80; rewrite ^/(.*)$ /nginx_one/$1; rewrite ^/nginx_one/(.*)$ /nginx_two/$1; location / { echo \"This is default location\"; echo \"uri: ${uri}\"; } location /nginx_one/ { echo \"This is nginx_one location\"; echo \"uri: ${uri}\"; } location /nginx_two/ { echo \"This is nginx_two location\"; echo \"uri: ${uri}\"; } } 测试结果 $ curl 127.0.0.1/abc This is nginx_two location uri: /nginx_two/abc 无 flag 时，rewrite 会依次向下匹配，根据nginx在http请求处理的阶段中，我们 会先匹配server块中的rewrite规则； 第一次匹配rewrite ^/(.*)$ /nginx_one/$1; URI变成:/nginx_one/abc，无flag继续向下匹配； 第二次匹配rewrite ^/nginx_one/(.*)$ /nginx_two/$1; URI变成/nginx_two/abc； 再向下FIND_CONFIG阶段，查找location进行匹配，正好找到location /nginx_two/ 所以 response 如上； 无flag测试用例2 server { listen 80; rewrite ^/(.*)$ /nginx_one/$1; location / { echo \"This is default location\"; echo \"uri: ${uri}\"; } location /nginx_one/ { rewrite ^/nginx_one/(.*)$ /nginx_two/$1; echo \"This is nginx_one location\"; echo \"uri: ${uri}\"; } location /nginx_two/ { echo \"This is nginx_two location\"; echo \"uri: ${uri}\"; } } 测试结果 $ curl 127.0.0.1/abc This is nginx_two location uri: /nginx_two/abc 无flag，处理server块阶段，匹配rewrite ^/(.*)$ /nginx_one/$1 ，URI为：/nginx_one/abc; 到FIND_CONFIG阶段，匹配location, location /nginx_one/ ,这个location块中有rewrite再次匹配^/nginx_one/(.*)$ /nginx_two/$1; URI变为：/nginx_two/abc，这里没有flag，会跳出继续FIND_CONFIG阶段，而不会到 SERVER_REWRITE 阶段；而是匹配location /nginx_two/ ，所以response如上。 5.2 有flag测试 redirect和permanent的区别就是redirect是临时重定向302，而permanent是永久重定向301 5.2.1 flag redirect测试用例1 server { listen 80; #rewrite ^/(.*)$ /nginx_one/$1 redirect; rewrite ^/(.*)$ https://www.baidu.com/ redirect; rewrite ^/nginx_one/(.*)$ /nginx_two/$1; rewrite ^/nginx_two/(.*)$ /nginx_three/$1; location / { echo \"This is default location\"; echo \"uri: ${uri}\"; } location /nginx_one/ { echo \"This is nginx_one location\"; echo \"uri: ${uri}\"; } location /nginx_two/ { echo \"This is nginx_two location\"; echo \"uri: ${uri}\"; } location /nginx_three/ { echo \"This is nginx_three location\"; echo \"uri: ${uri}\"; } } 测试结果 浏览器访问 虚拟机IP地址/abc 会跳转到百度首页 状态码为临时重定向302 这里的flag是redirect，说明需要重定向到replacement，正好这里的replacement有\"https://\"，此时会直接跳转并返回给客户端； 如果打开#rewrite ^/(.*)$ /nginx_one/$1 redirect;的注释，浏览器中访问会提示重定向次数过多 5.2.2 flag redirect测试用例2 server { listen 80; rewrite_log on; rewrite ^/nginx_one/(.*)$ /nginx_two/$1; rewrite ^/nginx_two/(.*)$ /nginx_three/$1 redirect; rewrite ^/(.*)$ /nginx_one/$1; location / { echo \"This is default location\"; echo \"uri: ${uri}\"; } location /nginx_one/ { echo \"This is nginx_one location\"; echo \"uri: ${uri}\"; } location /nginx_two/ { echo \"This is nginx_two location\"; echo \"uri: ${uri}\"; } location /nginx_three/ { echo \"This is nginx_three location\"; echo \"uri: ${uri}\"; } } 测试结果 $ curl -I 127.0.0.1/nginx_one/abc HTTP/1.1 302 Moved Temporarily Server: nginx/1.16.1 Date: Mon, 22 Jun 2020 12:05:16 GMT Content-Type: text/html Content-Length: 145 Location: http://127.0.0.1/nginx_three/abc Connection: keep-alive 1.匹配rewrite ^/nginx_one/(.*)$ /nginx_two/$1; 后，URI变为：/nginx_two/abc ; 无flag，继续向下； 2.继续匹配 rewrite ^/nginx_two/(.*)$ /nginx_three/$1 redirect; 302临时 重定向，URI: http://127.0.0.1/nginx_three/abc, 3.再次访问，此时会从SERVER_REWRITE这个阶段开始，此时匹配的是 rewrite ^/(.*)$ /nginx_one/$1; URI变为：/nginx_one/nginx_three/abc ，无flag，向 > 下FIND_CONFIG阶段； 4.最后查看location 匹配location /nginx_one/ ,所以回显如上； 5.2.3 flag last测试用例1 server { listen 80; rewrite_log on; rewrite ^/nginx_one/(.*)$ /nginx_two/$1 last; rewrite ^/nginx_two/(.*)$ /nginx_three/$1; location / { echo \"This is default location\"; echo \"uri: ${uri}\"; } location /nginx_one/ { echo \"This is nginx_one location\"; echo \"uri: ${uri}\"; } location /nginx_two/ { echo \"This is nginx_two location\"; echo \"uri: ${uri}\"; } location /nginx_three/ { echo \"This is nginx_three location\"; echo \"uri: ${uri}\"; } } 测试结果 $ curl 127.0.0.1/nginx_one/abc This is nginx_two location uri: /nginx_two/abc 1.匹配 rewrite ^/k8svip_one/(.*)$ /k8svip_two/$1 last;遇到last，停止同级段的匹配，这里的意思是，中止server段向下的匹配，进行FIND_CONFIG阶段，URI变为：/k8svip_two/abc; 这里可以看出，如果last没有中止server段向下的匹配，会匹配rewrite ^/k8svip_two/(.*)$ /k8svip_three/$1，实际结果是没有匹配的； 2.由上面步骤之后，匹配location /k8svip_two/， 所以会出现上面的response结果; 5.2.4 flag last测试用例2 server { listen 80; rewrite_log on; rewrite ^/(.*)$ /nginx_one/$1; rewrite ^/nginx_three/(.*)$ /nginx_four/$1; location / { echo \"This is default location\"; echo \"uri: ${uri}\"; } location /nginx_one/ { rewrite ^/(.*)$ /nginx_three/$1 last; rewrite ^/nginx_three/(.*)$ /; echo \"This is nginx_one location\"; echo \"uri: ${uri}\"; } location /nginx_two/ { echo \"This is nginx_two location\"; echo \"uri: ${uri}\"; } location /nginx_three/ { echo \"This is nginx_three location\"; echo \"uri: ${uri}\"; } location /nginx_four/ { echo \"This is nginx_four location\"; echo \"uri: ${uri}\"; } } 测试结果 $ curl 127.0.0.1/abc This is nginx_three location uri: /nginx_three/nginx_one/abc 1.匹配rewrite ^/(.*)$ /k8svip_one/$1; URI变为：/nginx_one/abc; 2.匹配location /nginx_one/ 进而匹配location块中的rewrite ^/(.*)$ /nginx_three/$1 last; 因为是last，会跳出location继续FIND_CONFIG阶段 3.URI为：/nginx_three/nginx_one/abc；而不会到SERVER_WRITE阶段； 匹配 location /nginx_three/ ,所以看到上面的response； 5.2.5 flag break测试用例1 server { listen 80; rewrite_log on; rewrite ^/nginx_one/(.*)$ /nginx_two/$1 break; rewrite ^/nginx_two/(.*)$ /nginx_three/$1; location / { echo \"This is default location\"; echo \"uri: ${uri}\"; } location /nginx_one/ { echo \"This is nginx_one location\"; echo \"uri: ${uri}\"; } location /nginx_two/ { echo \"This is nginx_two location\"; echo \"uri: ${uri}\"; } location /nginx_three/ { echo \"This is nginx_three location\"; echo \"uri: ${uri}\"; } } 测试结果 $ curl 127.0.0.1/nginx_one/abc This is nginx_two location uri: /nginx_two/abc 1.匹配rewrite ^/nginx_one/(.*)$ /nginx_two/$1 break; flag为break，结束本层级的rewirte匹配，URI变为：/nginx_two/abc 2.继续FIND_CONFIG阶段，匹配location /nginx_two/; 所以response 如上； 5.2.6 flag break测试用例2 server { listen 80; rewrite_log on; rewrite ^/(.*)$ /nginx_one/$1; location / { echo \"This is default location\"; echo \"uri: ${uri}\"; } location /nginx_one/ { rewrite ^/(.*)$ /nginx_three/$1 break; rewrite ^/nginx_two/(.*)$ /; echo \"This is nginx_one location\"; echo \"uri: ${uri}\"; } location /nginx_two/ { echo \"This is nginx_two location\"; echo \"uri: ${uri}\"; } location /nginx_three/ { echo \"This is nginx_three location\"; echo \"uri: ${uri}\"; } } 测试结果 $ curl 127.0.0.1/abc This is nginx_one location uri: /nginx_three/nginx_one/abc 1.匹配rewrite ^/(.*)$ /nginx_one/$1; URI变为：/nginx_one/abc 2.匹配location /nginx_one/，然后继续 rewrite ^/(.)$ /nginx_three/$1 break; flag为break，结束本层级的rewrite ^/(.)$ /nginx_three/$1 break;，并且继续进行本层级的其它操作； 3.此时的URI：/nginx_three/nginx_one/abc,所以response如上； 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/nginx/13.nginx动静分离.html":{"url":"linux/linux服务/nginx/13.nginx动静分离.html","title":"nginx动静分离","keywords":"","body":"[toc] nginx动静分离 一、动静分离简介 动静分离，通过中间件将动态请求和静态请求进行分离, 分离资源, 减少不必要的请求消耗, 减少请求延时。 好处 动静分离后, 即使动态服务不可用, 但静态资源不会受到影响 实验环境 角色 服务 IP地址 主机名 负载均衡 Nginx Proxy 10.0.0.10 lb01 静态资源 Nginx Static 10.0.0.51 web01 动态资源 Tomcat Server 10.0.0.52 tomcat 二、配置过程 2.1 负载均衡配置 编辑nginx配置文件 upstream static { server 10.0.0.51:80; } upstream tomcat { server 10.0.0.52:8080; } server { listen 80; server_name _; location / { root /website; index index.html; } location ~* .*\\.(png|jpg|gif)$ { proxy_pass http://static; } location ~* .*\\.jsp$ { proxy_pass http://tomcat; } } 2.2 web01配置静态资源 编辑nginx配置文件 server { listen 80; server_name _; location ~* .*\\.(png|jpg|gif)$ { root /website/images; } } 创建网站根目录 #创建网站根目录 mkdir -p /website/images && cd /website/images #下载一个图片 wget -O /website/images/nginx.png https://nginx.org/nginx.png 2.3 tomcat配置动态资源 编辑jsp测试文件 ⚠️这里已经二进制安装tomcat8.5 cat > /usr/local/tomcat-8.5.56/webapps/ROOT/tomcat_test.jsp JSP Test Page Random number:\"); out.println(rand.nextInt(99)+100); %> EOF 三、访问测试 访问静态资源 访问动态资源 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/nginx/nginx增加新模块.html":{"url":"linux/linux服务/nginx/nginx增加新模块.html","title":"nginx增加新模块","keywords":"","body":"[toc] nginx增加新模块 场景：编译安装的nginx后续可能会增加一些第三方模块或者未编译的nginx模块 一、编译安装的nginx增加新模块 1.1 查看nginx编译安装参数 $ nginx -V nginx version: nginx/1.16.1 built by gcc 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) built with OpenSSL 1.0.2k-fips 26 Jan 2017 TLS SNI support enabled configure arguments: --prefix=/etc/nginx --user=nginx --group=nginx --sbin-path=/usr/sbin/nginx --conf-path=/etc/nginx/nginx.conf --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --with-http_gzip_static_module --with-http_stub_status_module --with-http_ssl_module --with-pcre --with-file-aio --with-http_realip_module --without-http_scgi_module --without-http_uwsgi_module --without-http_fastcgi_module 1.2 下载第三方模块 echo-nginx-module github地址 ngx_echo 为Nginx配置文件带来\"echo\"，\"sleep\"，\"time\"，\"exec\"和更多shell样式的东西 wget https://github.com/openresty/echo-nginx-module/archive/v0.61.tar.gz 解压缩至/usr/local tar xf v0.61.tar.gz -C /usr/local 使用选项--add-module=/path/to/echo-nginx-module添加模块 1.3 重新编译nginx ⚠️make之后不要执行make install ！！！ #进入nginx源码目录 cd nginx-1.16.1 #重新./configure ./configure --prefix=/etc/nginx --user=nginx --group=nginx --sbin-path=/usr/sbin/nginx --conf-path=/etc/nginx/nginx.conf --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --with-http_gzip_static_module --with-http_stub_status_module --with-http_ssl_module --with-pcre --with-file-aio --with-http_realip_module --without-http_scgi_module --without-http_uwsgi_module --without-http_fastcgi_module --add-module=/usr/local/echo-nginx-module-0.61 #编译，不能执行make install make 安装完成后查看，最后就是添加的第三方模块 $ nginx -V nginx version: nginx/1.16.1 built by gcc 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) built with OpenSSL 1.0.2k-fips 26 Jan 2017 TLS SNI support enabled configure arguments: --prefix=/etc/nginx --user=nginx --group=nginx --sbin-path=/usr/sbin/nginx --conf-path=/etc/nginx/nginx.conf --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --with-http_gzip_static_module --with-http_stub_status_module --with-http_ssl_module --with-pcre --with-file-aio --with-http_realip_module --without-http_scgi_module --without-http_uwsgi_module --without-http_fastcgi_module --add-module=/usr/local/echo-nginx-module-0.61 1.4 nginx二进制文件操作 备份原有文件 mv /usr/sbin/nginx{,.bak} 拷贝新文件 cp nginx-1.16.1/objs/nginx /usr/sbin 1.5 编译nginx配置文件以测试模块安装是否成功 server { listen 88; server_name _; location /test { echo \"This is default location\"; echo \"uri: ${uri}\"; } } 检测nginx语法并重载nginx 这里语法不报错就证明第三方模块echo安装成功了 $ nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful $ nginx -s reload 本机访问测试 $ curl 127.0.0.1:88/test This is default location uri: /test 二、rpm包安装的nginx增加新模块 nginx是使用rpm包安装的，如果想要安装第三方模块的解决方法 2.1 查看nginx安装的模块 $ nginx -V nginx version: nginx/1.16.1 built by gcc 4.8.5 20150623 (Red Hat 4.8.5-36) (GCC) built with OpenSSL 1.0.2k-fips 26 Jan 2017 TLS SNI support enabled configure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -fPIC' --with-ld-opt='-Wl,-z,relro -Wl,-z,now -pie' 安装完成后查看，最后就是添加的第三方模块 $ nginx -V nginx version: nginx/1.16.1 built by gcc 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) built with OpenSSL 1.0.2k-fips 26 Jan 2017 TLS SNI support enabled configure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -fPIC' --with-ld-opt='-Wl,-z,relro -Wl,-z,now -pie' --add-module=/usr/local/echo-nginx-module-0.61 2.2 下载一个相同版本的nginx源码包 nginx官方源码下载地址 wget https://nginx.org/download/nginx-1.16.1.tar.gz 2.3 重新编译nginx 解压缩源码包并进入目录 tar xf nginx-1.16.1.tar.gz cd cd nginx-1.16.1/ ⚠️make之后不要执行make install ！！！ #重新./configure ./configure --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -fPIC' --with-ld-opt='-Wl,-z,relro -Wl,-z,now -pie' --add-module=/usr/local/echo-nginx-module-0.61 #编译 make 2.4 nginx二进制文件操作 备份原有文件 mv /usr/sbin/nginx{,.bak} 拷贝新文件 cp nginx-1.16.1/objs/nginx /usr/sbin 2.5 编译nginx配置文件以测试模块安装是否成功 server { listen 88; server_name _; location /test { echo \"This is default location\"; echo \"uri: ${uri}\"; } } ⚠️因为是rpm包安装的nginx，因此需要使用systemctl命名重启一下nginx systemctl restart nginx 检测nginx语法并重载nginx 这里语法不报错就证明第三方模块echo安装成功了 $ nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful $ nginx -s reload 本机访问测试 $ curl 127.0.0.1:88/test This is default location uri: /test 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/nginx/nginx解决非80端口https问题.html":{"url":"linux/linux服务/nginx/nginx解决非80端口https问题.html","title":"nginx解决非80端口https问题","keywords":"","body":"nginx解决非80端口https问题 解决思路： 监听80端口，80重定向到https443端口，443端口代理别的端口 server { listen 80; server_name pan.pptfz.top; return 301 https://$server_name$request_uri; } server { listen 443 ssl; server_name pan.pptfz.top; if ($host != 'pan.pptfz.top') { rewrite ^(.*)$ https://pan.pptfz.top/$1 permanent; } if ($request_uri ~ \"^[^?]*//\") { rewrite \"(.*)\" $scheme://$host$1 permanent; } location / { try_files /_not_exists_ @backend; } location @backend { proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header Host $http_host; proxy_set_header X-Forwarded-Proto $scheme; proxy_pass http://127.0.0.1:5212; } access_log /var/log/nginx/pan.pptfz.top.access.log main; error_log /var/log/nginx/pan.pptfz.top.error.log; ssl_certificate /etc/nginx/ssl_key/pan/3356127_pan.pptfz.top.pem; ssl_certificate_key /etc/nginx/ssl_key/pan/3356127_pan.pptfz.top.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; } 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/nginx/14.nginx+keepalived实现负载均衡高可用.html":{"url":"linux/linux服务/nginx/14.nginx+keepalived实现负载均衡高可用.html","title":"nginx+keepalived实现负载均衡高可用","keywords":"","body":"[toc] nginx+keepalived实现负载均衡高可用 keepalived官网 keepalived github地址 keepalived官方文档 keepalived源码包官方下载地址 一、keepalived简介 Keepalived是用C语言编写的路由软件。该项目的主要目标是为Linux系统和基于Linux的基础架构提供负载均衡和高可用性的简单而强大的功能。 负载平衡框架依赖于提供第4层负载平衡的著名且广泛使用的Linux虚拟服务器（IPVS）内核模块。Keepalived实现了一组检查器，以根据其运行状况动态，自适应地维护和管理负载平衡的服务器池。另一方面，VRRP实现了高可用性协议。VRRP是路由器故障转移的基础。此外，Keepalived还实现了一组VRRP有限状态机的钩，从而提供了低级和高速协议交互。为了提供最快的网络故障检测，Keepalived实施BFD协议。VRRP状态转换可以考虑BFD提示来驱动快速状态转换。Keepalived框架可以独立使用，也可以一起使用以提供弹性基础架构。 二、keepalived负载均衡高可用配置过程 实验环境 角色 IP 主机名 keepalived-master 10.0.0.10 keepalived01 keepalived-slave 10.0.0.11 keepalived02 web01 10.0.0.51 nginx01 web02 10.0.0.52 nginx02 2.1 负载均衡端配置 2.1.1 安装keepalived yum -y install keepalived centos7.7中默认的keepalive版本是1.3.5，如果需要高版本请到keepalived github地址或者keepalived官网下载，这里仅做实验直接yum安装 $ keepalived -version Keepalived v1.3.5 (03/19,2017), git commit v1.3.5-6-g6fa32f2 2.1.2 lb01(keepalived master)编辑配置文件 lb01编辑keepalived配置文件/etc/keepalived/keepalived.conf #备份原有文件 mv /etc/keepalived/keepalived.conf{,.bak} #重新编辑文件 cat > /etc/keepalived/keepalived.conf 配置文件参数含义 #配置含义 global_defs { router_id lb01 #表示id身份，名称随意 } vrrp_instance VI_1 { state MASTER #状态，角色为master interface eth0 #VIP绑定的网卡 virtual_router_id 50 #主和备的虚拟id号要相同，表明在一个组当中，名称随意，不要超过255 priority 150 #优先级，越大优先级越高 advert_int 1 #心跳检测，单位秒，备机每隔1秒检测一次主的存活状态 authentication { #认证信息 auth_type PASS auth_pass 123456 } virtual_ipaddress { 10.0.0.100 #虚拟IP } } 2.1.3 lb02(keepalived backup)编辑配置文件 #备份原有文件 mv /etc/keepalived/keepalived.conf{,.bak} #重新编辑文件 cat > /etc/keepalived/keepalived.conf 配置文件参数含义 global_defs { router_id lb02 #表示id身份，名称随意 } vrrp_instance VI_1 { state BACKUP #状态，因为是slave，所以是BACKUP interface eth0 #VIP绑定的网卡 virtual_router_id 50 #主和备的虚拟id号要相同，表明在一个组当中，名称随意，不要 超过255 priority 100 #优先级，越大优先级越高 advert_int 1 #心跳检测，单位秒，备机每隔1秒检测一次主的存活状态 authentication { #认证信息 auth_type PASS auth_pass 123456 } virtual_ipaddress { 10.0.0.100 #虚拟IP } } 对比keepalived的master与backup配置的区别 Keepalived配置区别 Master配置 Backup节配置 route_id(唯一标识) route_id lb01 route_id lb02 state(角色状态) state Master state Backup priority(竞选优先级) priority 150 priority 100 2.1.4 启动keepalived lb01和lb02相同操作 systemctl start keepalived && systemctl enable keepalived 2.1.5 检测lb01是否有vip 可以看到，启动keepalived后，lb01 eth0网卡就有了VIP 10.0.0.100 $ ip a s eth0 2: eth0: mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:1c:42:f3:3c:78 brd ff:ff:ff:ff:ff:ff inet 10.0.0.10/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever inet 10.0.0.100/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::21c:42ff:fef3:3c78/64 scope link valid_lft forever preferred_lft forever 2.1.6 验证VIP是否能够正常漂移 VIP首先在master上，当master宕机后，VIP会漂移至backup，当master恢复时VIP会自动漂移回来！！！ 先停止lb01上的keepalived systemctl stop keepalived 查看lb01上是否有VIP 可以看到，VIP此时已经没有了 $ ip a s eth0 2: eth0: mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:1c:42:f3:3c:78 brd ff:ff:ff:ff:ff:ff inet 10.0.0.10/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::21c:42ff:fef3:3c78/64 scope link valid_lft forever preferred_lft forever 查看lb02上是否有VIP 当keepalived master宕机后，VIP就会漂移到lb02 $ ip a s eth0 2: eth0: mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:1c:42:c1:0b:16 brd ff:ff:ff:ff:ff:ff inet 10.0.0.11/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever inet 10.0.0.100/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::21c:42ff:fec1:b16/64 scope link valid_lft forever preferred_lft forever 重新启动lb01上的keepalived systemctl start keepalived #查看VIP $ ip a s eth0 2: eth0: mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:1c:42:f3:3c:78 brd ff:ff:ff:ff:ff:ff inet 10.0.0.10/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever inet 10.0.0.100/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::21c:42ff:fef3:3c78/64 scope link valid_lft forever preferred_lft forever 2.1.7 编辑nginx配置文件 lb01和lb02编辑nginx配置文件 cat > /etc/nginx/conf.d/kd.test.conf 检测nginx语法并重载nginx $ nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful $ nginx -s reload 2.2 后端真实服务web配置 web01操作 2.2.1 编辑nginx配置文件 cat >> /etc/nginx/conf.d/kd.test.conf 2.2.2 创建网站根目录 mkdir /code && echo \"web01 web01 web01\" > /code/index.html 2.2.3 检测nginx语法并重载nginx $ nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful $ nginx -s reload web02操作 2.2.4 编辑nginx配置文件 cat >> /etc/nginx/conf.d/kd.test.conf 2.2.5 创建网站根目录 mkdir /code && echo \"web02 web02 web02\" > /code/index.html 2.2.6 检测nginx语法并重载nginx $ nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful $ nginx -s reload 2.3 浏览器验证负载均衡是否正常工作 浏览器访问VIP 10.0.0.100 三、nginx宕机问题 如果Nginx宕机，会导致用户请求失败， 但Keepalived并不会进行切换，所以编写nginx检测脚本，检测nginx存活状态，不存活则kill掉nginx和keepalived，然后VIP进行漂移 3.1 编辑脚本 lb01操作 #创建存放脚本的目录 mkdir -p /server/scripts #编辑脚本内容 cat >> /server/scripts/check_web.sh 3.2 验证脚本是否生效 故意把nginx配置文件修改错误(比如删除一个括号或者分号)，然后停止lb01上的nginx，因为上边的脚本中会判断nginx如果不存活就启动nginx，把nginx配置文件改错了nginx就无法正常启动过了 systemctl stop nginx 后台脚本检测到nginx不存活会kill掉keepalived，此时浏览器访问会报错，隔几秒就恢复了，因为VIP已经漂移至lb02上 3.3 在keepalived中调用检测脚本 ⚠️nohup sh /server/scripts/check_web.sh &执行脚本属于手动执行，还可以在keepalived配置文件中配置调用监控脚本 lb01、lb02都需要操作 编辑keepalived配置文件/etc/keepalived/keepalived.conf #在global_defs标签下增加如下内容 vrrp_script check_web { script \"/server/scripts/check_web.sh\" interval 10 weight 50 } #track_script标签要写在vrrp_instance VI_1{}中 track_script { check_web } ⚠️⚠️⚠️vrrp_script{}中的interval时间需大于脚本中的sleep时间 否则会报错Keepalived_vrrp[2612]: /server/scripts/check_web.sh exited due to signal 15 这样就能够使keepalived自动调用监控脚本了，keepalived会根据配置文件中vrrp_script {}中的interval参数来决定每隔几秒执行脚本，interval后面的数字就表明执行脚本的间隔时间 在脚本中我们定义了，keepalived master上检测nginx是否存活，不存活尝试启动nginx，当无法成功启动nginx的时候停止master上的keepalived，然后让VIP漂移，这样的话就实现了当nginx服务挂掉时keepalived VIP能够漂移从而不影响客户端访问 四、keepalived高可用脑裂 概念 脑裂就是由于某些原因，导致两台keepalived高可用服务器在指定时间内，无法检测到对方的心跳消息，各自取得资源及服务的所有权，而此时的两台高可用服务器又都还活着，产生裂脑的情况下，master和backup上都会有VIP 产生脑裂的原因 1.服务器网络故障 2.服务器硬件故障发生损坏现象而崩溃 3.主备都开启firewalld防火墙 4.1 模拟脑裂 lb01和lb02开启firewalld防火墙 systemctl start firewalld 开启防火墙后产生裂脑原因 lb01和lb02都开启防火墙后，因为没有允许vrrp，当backup去检测master的时候无法检测到，此时backup认为master已经宕机，所以将VIP抢占；而master实际上并没有宕机，所以VIP不会漂移，这样就造成了master和backup都在抢占VIP。此时就出现了裂脑的情况 4.2 测试脑裂问题 在keepalived备上编写检测脚本, 测试如果能ping通主keepalived并且备节点还有VIP的话则认为产生了脑裂 lb02操作 #创建存放脚本的目录 mkdir -p /server/scripts #编辑脚本内容 cat >/server/scripts/check_split_brain.sh /dev/null if [ $? -eq 0 -a `ip add|grep \"$lb01_vip\"|wc -l` -eq 1 ];then echo \"keepalived出现了脑裂！！！\" else echo \"keepalived完全objk\" fi sleep 5 done EOF 运行脚本，因为此时是脑裂的，keepalived主备上都有VIP，因为前期会报出现脑裂，当把主备上的firewalld关掉时就没有问题了 #脚本会一直运行，ctrl+c停止 $ sh /server/scripts/check_split_brain.sh keepalived出现了脑裂！！！ keepalived出现了脑裂！！！ keepalived出现了脑裂！！！ keepalived出现了脑裂！！！ keepalived出现了脑裂！！！ keepalived出现了脑裂！！！ keepalived出现了脑裂！！！ keepalived完全objk keepalived完全objk 五、源码安装keepalived keepalived源码包官方下载地址 5.1 下载包 wget https://www.keepalived.org/software/keepalived-2.1.2.tar.gz 5.2 编译安装 安装依赖包 yum -y install openssl openssl-devel libnl3-devel pcre-devel 解压缩包 tar xf keepalived-2.1.2.tar.gz && cd keepalived-2.1.2 编译安装 ./configure --prefix=/usr/local/keepalived make && make install 5.3 配置keepalived 5.3.1 软连接keepalived命令 ln -s /usr/local/keepalived/sbin/keepalived /usr/sbin 验证 $ keepalived -v Keepalived v2.1.2 (06/14,2020) Copyright(C) 2001-2020 Alexandre Cassen, Built with kernel headers for Linux 3.10.0 Running on Linux 3.10.0-1127.el7.x86_64 #1 SMP Tue Mar 31 23:36:51 UTC 2020 configure options: --prefix=/etc/keepalived --sysconfdir=/etc/keepalived Config options: LVS VRRP VRRP_AUTH OLD_CHKSUM_COMPAT FIB_ROUTING System options: PIPE2 SIGNALFD INOTIFY_INIT1 VSYSLOG EPOLL_CREATE1 IPV6_ADVANCED_API RTA_ENCAP RTA_EXPIRES RTA_PREF FRA_SUPPRESS_PREFIXLEN FRA_TUN_ID RTAX_CC_ALGO RTAX_QUICKACK RTA_VIA FRA_OIFNAME IFA_FLAGS IP_MULTICAST_ALL NET_LINUX_IF_H_COLLISION LIBIPTC_LINUX_NET_IF_H_COLLISION VRRP_VMAC IFLA_LINK_NETNSID CN_PROC SOCK_NONBLOCK SOCK_CLOEXEC O_PATH GLOB_BRACE INET6_ADDR_GEN_MODE SO_MARK SCHED_RESET_ON_FORK 5.3.2 keepalived.service文件 ⚠️keepalived编译安装完成后会自动生成文件/usr/lib/systemd/system/keepalived.service 5.3.3 拷贝安装目录下keepalived配置文件 否则会报错Unable to find configuration file /etc/keepalived/keepalived.conf (glob returned 3) mkdir /etc/keepalived cp /usr/local/keepalived/etc/keepalived/keepalived.conf /etc/keepalived/ 5.3.4 拷贝启动文件(可选) cp /root/keepalived-2.1.2/keepalived/etc/init.d/keepalived /etc/init.d 5.3.5 启动keepalived并设置开机自启 systemctl daemon-reload && systemctl enable keepalived && systemctl start keepalived 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/nginx/nginx实际应用场景/nginx locaiton匹配实际应用场景.html":{"url":"linux/linux服务/nginx/nginx实际应用场景/nginx locaiton匹配实际应用场景.html","title":"nginx locaiton 匹配实际应用场景","keywords":"","body":"[toc] nginx locaiton匹配实际应用场景 1.location 自动添加后缀 有这么一种场景，一些服务安装启动后，访问的的url是这样的 IP:端口/xxx，例如 zabbix，访问的url是 IP:端口/zabbix，还有 dolphinscheduler，访问的url是 IP:12345/dolphinscheduler，这个是服务源码路由写死的，不可以更改，zabbix还好，单词可以记住，但是 dolphinscheduler 实在是记不住，那现在就想配置直接以域名的形式访问，然后自动匹配后边的uir，例如访问zabbix，直接输入设置的域名 zabbix.ac.com ，然后跳转到 zabbix.abc.com/zabbix ，在nginx中需要做如下配置 zabbix配置示例 server { listen 80; server_name zabbix.abc.com; return 301 https://$server_name$request_uri; } server { listen 443 ssl; server_name zabbix.abc.com; client_max_body_size 10240m; location / { proxy_pass http://127.0.0.1:81/zabbix; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-NginX-Proxy true; proxy_hide_header Server; proxy_redirect off; location /zabbix { proxy_pass http://127.0.0.1:81; } } access_log /var/log/zabbix/zabbix.abc.com.access.log; error_log /var/log/zabbix/zabbix.abc.com.error.log; ssl_certificate ssl_key/zabbix/1_zabbix.abc.com.pem; ssl_certificate_key ssl_key/zabbix/2_zabbix.abc.com.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; } dolphinscheduler配置示例 server { listen 80; server_name ds.abc.com; return 301 https://$server_name$request_uri; } server { listen 443 ssl; server_name ds.abc.com; client_max_body_size 10240m; location / { proxy_pass http://127.0.0.1:12345/dolphinscheduler/; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-NginX-Proxy true; proxy_hide_header Server; proxy_redirect off; location /dolphinscheduler { proxy_pass http://127.0.0.1:12345; } } access_log /var/log/ds/ds.abc.com.access.log; error_log /var/log/ds/ds.abc.com.error.log; ssl_certificate ssl_key/1_ds.abc.com_bundle.crt; ssl_certificate_key ssl_key/2_ds.abc.com.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; } 配置模版，配置为location中套location，注意两个location中的 proxy_pass 后的IP和端口要一致 location / { proxy_pass http://127.0.0.1:12345/xxx/; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-NginX-Proxy true; proxy_hide_header Server; proxy_redirect off; location /xxx { proxy_pass http://127.0.0.1:12345; } } 在ingress中配置为如下，以nacos为例 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/configuration-snippet: | proxy_set_header Upgrade \"websocket\"; proxy_set_header Connection \"Upgrade\"; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; nginx.ingress.kubernetes.io/proxy-body-size: 4096m kubernetes.io/ingress.class: \"nginx\" nginx.ingress.kubernetes.io/app-root: /nacos/ nginx.ingress.kubernetes.io/rewrite-target: /nacos/$2 nginx.ingress.kubernetes.io/configuration-snippet: | rewrite ^(/nacos)$ $1/ redirect; generation: 1 name: nacos-uat namespace: ops spec: ingressClassName: nginx rules: - host: nacos-uat.rd.com http: paths: - path: /nacos(/|$)(.*) pathType: Prefix backend: service: name: nacos-uat port: number: 8848 status: loadBalancer: ingress: - ip: 172.18.4.112 2.location 配置静态资源 location ~ .*\\.(gif|jpg|jpeg|bmp|png|ico|txt|js|css|ttf)$ { root /var/www; } 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/ssh/ssh服务.html":{"url":"linux/linux服务/ssh/ssh服务.html","title":"ssh服务","keywords":"","body":"[toc] 一、ssh禁止root远程登陆 1.编辑文件/etc/ssh/sshd_config # 禁止root远程登陆 PermitRootLogin no # 禁用密码验证 PasswordAuthentication no # 启用密钥验证 RSAAuthentication yes //centos7没有这一项 PubkeyAuthentication yes 2.sudo免密配置等root权限用户 visudo或者编辑文件/etc/sudoers # 创建一个用户 useradd lcc # visudo编辑,101行写入以下内容 lcc ALL=NOPASSWD :ALL 3.配置ssh密钥 # 切换到lcc用户 su - lcc # 生成密钥 $ ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/home/lcc/.ssh/id_rsa): Created directory '/home/lcc/.ssh'. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/lcc/.ssh/id_rsa. Your public key has been saved in /home/lcc/.ssh/id_rsa.pub. The key fingerprint is: SHA256:nSW5jEvQwr8zPZok5CfK+fnrhPJfk2motWOgx1/eNZ4 lcc@experiment The key's randomart image is: +---[RSA 2048]----+ | | | . . . | | + . o . | | + + = | | . S = | | o.o = o | | .o=.@ X o | | ..=oXo@ + o o | | +o=*Xo. . E | +----[SHA256]-----+ # 向authorized_keys文件写入公钥 cd .ssh && cat id_rsa.pub >authorized_keys # 修改authorized_keys文件权限至少为644，默认为664，无法使用密钥登陆 chmod 644 authorized_keys ⚠️ssh服务配置文件/etc/ssh/sshd_config中有一项配置是AuthorizedKeysFile .ssh/authorized_keys，如果想要使用私钥免密登陆，则公钥必须写入到文件.ssh/authorized_keys中，即注册私钥，否则免密会失败！！！ 4.配置完后验证 # root无法远程登陆 baixuebingdeMacBook-Pro:~ baixuebing$ ssh root@10.0.0.13 root@10.0.0.13: Permission denied (publickey,gssapi-keyex,gssapi-with-mic). # 无法使用密码登陆，只能使用密钥登陆 baixuebingdeMacBook-Pro:~ baixuebing$ ssh lcc@10.0.0.13 lcc@10.0.0.13: Permission denied (publickey,gssapi-keyex,gssapi-with-mic). 二、ssh免交互配置 ssh-keygen免交互生成密钥 # 免交互生成密钥 ssh-keygen -t rsa -f /root/.ssh/id_dsa -P \"\" -q -f filename #指定密钥文件的文件名 -P passphrase #提供旧密钥口令 -q Silence ssh-keygen #静默输出 -t key type #密钥类型 dsa ecdsa ed25519 rsa(默认) rsa1 ⚠️ssh服务配置文件/etc/ssh/sshd_config中有一项配置是AuthorizedKeysFile .ssh/authorized_keys，如果想要使用私钥免密登陆，则公钥必须写入到文件.ssh/authorized_keys中，即注册私钥，否则免密会失败！！！ ssh-copy免交互推送密钥 sshpass -p1 ssh-copy-id -i ~/.ssh/id_rsa.pub -o StrictHostKeyChecking=no root@IP 批量分发密钥脚本 #!/bin/bash # 生成密钥 \\rm -f /root/.ssh/id_* ssh-keygen -t rsa -f /root/.ssh/id_rsa -P \"\" -q # 分发密钥 for ip in IP do echo \"=== 分发主机 10.0.0.$ip ===\" sshpass -p1 ssh-copy-id -i ~/.ssh/id_rsa.pub -o StrictHostKeyChecking=no root@10.0.0.$ip echo \"=== 分发ojbk ===\" echo \"\" done 三、ssh自动断开远程服务器问题 编辑ssh服务配置文件/etc/ssh/sshd_config修改以下两项 # 向客户端每30秒发一次保持连接的信号 ClientAliveInterval 30 # 如果客户端30次未响应就断开连接 ClientAliveCountMax 30 重启服务 systemctl restart sshd 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/ssh/ssh密钥.html":{"url":"linux/linux服务/ssh/ssh密钥.html","title":"ssh密钥","keywords":"","body":"[toc] 生成密钥 使用 ssh-keygen 命令生成密钥 ssh-keygen -t rsa -P '' -q -f ~/.ssh/id_rsa 参数说明 -t 密钥类型，可以选择 dsa | ecdsa | ed25519 | rsa; 省略的情况下默认是rsa -f 密钥目录位置，指定生成密钥的保存路径和文件名。省略的情况下， 默认为当前用户 home 路径下的 .ssh 隐藏目录, 也就是 ~/.ssh/ ， 同时默认密钥文件名以 id_rsa 开头. 　　 -f省略的情况下，默认目录就是 ~/.ssh/，不会再次提醒输入。但是会再次提是你输入文件名，如果不输入直接回车，则默认的密钥文件名就是 id_rsa -N -N: 指定此密钥对的密码，如果指定此参数，则命令执行过程中就不会出现交互确认密码的信息了，如果省略此参数，会提示你输入密码和密码确认。一般情况下不用输入，直接回车就行。 　　 如果输入密码之后，以后每次都要输入密码。这里请根据你的安全需要决定是否需要密码，如果不需要，直接回车。 -P -P(大写)： 提供旧密码 -p -p(小写)：要求改变某私钥文件的密码而不重建私钥。程序将提示输入私钥文件名、原来的密码、以及两次输入新密码 -C 指定此密钥的备注信息，需要配置多个免密登录时，建议携带；生成的公钥会在最后面显示此备注信息 -q 静默模式 密钥格式转换 一些工具，例如 ZenTermLite(mac ssh工具) ，Another Redis Desktop Manager(redis远程连接工具)不支持openssh格式的私钥，这个时候就需要将openssh格式的私钥转换为rsa格式 openssh格式开头如下 -----BEGIN OPENSSH PRIVATE KEY----- rsa格式开头如下 -----BEGIN RSA PRIVATE KEY----- 转换命令 ssh-keygen -p -N\"\" -m pem -f 旧私钥 key格式开头如下 -----BEGIN PRIVATE KEY----- 转换为pem命令 openssl rsa -in xxx.key -out xxx.pem pem格式开头如下 -----BEGIN RSA PRIVATE KEY----- 关于自定义秘钥名称的问题 在 /etc/ssh/ssh_config 中定义了默认的私钥文件名 IdentityFile ~/.ssh/id_rsa ，也就是说，当我们去使用密钥认证登陆的时候，会使用 id_rsa 这个私钥去进行认证，如果我们自定义了密钥文件名，在不指定私钥名称的情况下想进行认证，有2种方法解决 1.修改 /etc/ssh/ssh_config 文件中 IdentityFile ~/.ssh/id_rsa 一项，指定私钥文件名称 2.在 .ssh 目录下做私钥文件软连接，如私钥文件名称自定义为 id_rsa_abc ，执行命令 ln -s id_rsa_abc id_rsa 即可 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/ssh/ssh config.html":{"url":"linux/linux服务/ssh/ssh config.html","title":"ssh config","keywords":"","body":"ssh config 配置 .ssh 下的 config 后，就可以通过 ssh 别名 的方式连接机器了 Host 别名 Hostname 主机名 Port 端口 User 用户名 IdentityFile 密钥 配置示例如下 Host test01 HostName 10.0.0.10 User test Port 222 IdentityFile /home/test/.ssh/id_rsa IdentitiesOnly yes Host test02 HostName 10.0.0.11 User test Port 222 IdentityFile /home/test/.ssh/id_rsa IdentitiesOnly yes 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/nfs/centos7搭建NFS.html":{"url":"linux/linux服务/nfs/centos7搭建NFS.html","title":"nfs","keywords":"","body":"[toc] centos7搭建NFS 1.NFS基本概述 基本概念 NFS是 Network File System 的缩写，即网络文件系统 主要功能 通过局域网络让不同的主机系统之间可以共享文件或目录 用处 NFS系统和Windows网络共享、网络驱动器类似, 只不过windows用于局域网, NFS用于企业集群架构中, 如果是大型网站, 会用到更复杂的分布式文件系统 小文件存储系统：(Moosefs,FastDFS) 大文件存储系统：(glusterfs,HDFS) 为什么要用NFS服务进行数据存储 1.实现数据信息共享 2.实现数据信息一致 2.NFS实现原理 相关进程 | 进程名称 | 说明 | | --------- | ------------------------------------------------------------ | | rpc.nfsd | 基本的nfs守护进程，主要功能是管理客户端是否能够登陆服务器 | | rpc.mount | 管理nfs的文件系统，当客户端顺利通过nfsd登陆nfs服务器后，在使用nfs服务所提供的文件前，还必须通过文件使用权限的验证，它会读取nfs的配置文件 /etc/exports 来对比客户端权限 | | portmap | 进行端口映射 | 本地文件操作方式 当用户进程发起本地文件访问或修改，该用户请求传递至内核，由内核驱动硬件完成操作 NFS访问方式 1.用户进程访问NFS客户端，使用不同的函数对数据进行处理 2.请求会通过TCP/IP的方式传递给NFS服务端 3.NFS服务端接收到请求后，会调用portmap进程进行端口映射 4.nfsd进程用于判断NFS客户端是否拥有权限连接NFS服务端 5.Rpc.mount用于判断客户端是否有对应的权限进行验证 6.idmap进程实现用户映射和压缩 7.最后NFS服务端会将对应请求的函数转换为本地能识别的命令，传递至内核，由内核驱动硬件 3.NFS服务搭建过程 实验环境 角色 IP 主机名 nfs服务端 10.0.0.10 nfs-server nfs客户端 10.0.0.11 nfs-client 3.1 nfs服务端操作 3.1.1 安装nfs-utils yum -y install nfs-utils 3.1.2 编辑nfs配置文件 /etc/exports，文件默认没有内容 nfs配置文件格式 NFS共享目录 NFS客户端地址1(参数1,参数2,...) 客户端地址2(参数1,参数2,...) NFS共享目录 NFS客户端地址(参数1,参数2,...) 执行 man exports 命令可以查看帮助，如下为nfs参数 nfs共享参数 参数作用 rw 读写权限 ro 只读权限 root_squash 当NFS客户端以root管理员访问时，映射为NFS服务器的匿名用户 no_root_squash 当NFS客户端以root管理员访问时，映射为NFS服务器的root管理员 all_squash 无论NFS客户端使用什么账户访问，均映射为NFS服务器的匿名用户（默认选项） no_all_squash 无论NFS客户端使用什么账户访问，不映射为NFS服务器的匿名用户 sync 同时将数据写入到内存与硬盘中，保证不丢失数据 async 优先将数据保存到内存，然后再写入硬盘；这样效率更高，但可能会丢失数据 anonuid 配置all_squash使用,指定NFS的用户UID,必须存在于系统中 anongid 配置all_squash使用,指定NFS的用户UID,必须存在于系统中 编辑nfs配置文件 cat > /etc/exports 参数说明 参数 说明 /date nfs服务端共享目录 10.0.0.0/24 允许挂载的客户端网段 rw 挂载权限 sync 同时将数据写入到内存与硬盘中，保证不丢失数据 all_squash 无论NFS客户端使用什么账户访问，均映射为NFS服务器的匿名用户 3.1.3 创建共享目录并修改目录所有者为 nfsnobody [ -d /data ] || mkdir /data chown -R nfsnobody.nfsnobody /data 3.1.4 启动nfs并设置开机自启 systemctl start rpcbind nfs-server && systemctl enable rpcbind nfs-server 3.1.5 验证配置是否生效 $ exportfs /data 10.0.0.0/24 3.1.6 检查nfs共享记录 nfs启动后会在 /var/lib/nfs/etab 文件中记录共享内容 $ cat /var/lib/nfs/etab /data 10.0.0.0/24(rw,sync,wdelay,hide,nocrossmnt,secure,root_squash,all_squash,no_subtree_check,secure_locks,acl,no_pnfs,anonuid=65534,anongid=65534,sec=sys,rw,secure,root_squash,all_squash) 3.2 nfs客户端操作 3.2.1 安装nfs-utils yum -y install nfs-utils 3.2.2 启动rpcbind并设置开机自启 systemctl start rpcbind && systemctl enable rpcbind 3.2.3 创建挂载点并修改挂载点所有者为 nfsnobody [ -d /data ] || mkdir /data chown -R nfsnobody.nfsnobody /data 3.2.4 查询nfs服务端共享信息 $ showmount -e 10.0.0.10 Export list for 10.0.0.10: /data 10.0.0.0/24 showmount命令 参数 作用 -e 显示NFS服务器的共享列表 -a 显示本机挂载的文件资源的情况NFS资源的情况 -v 显示版本号 3.2.5 客户端挂载nfs mount -t 文件系统 服务器IP:共享目录 客户端本机挂载点 mount -t nfs 10.0.0.10:/data /data ⚠️在企业工作场景，通常情况NFS服务器共享的只是普通静态数据（图片、附件、视频），不需要执行suid、exec等权限，挂载的这个文件系统只能作为数据存取之用，无法执行程序，对于客户端来讲增加了安全性。例如: 很多木马篡改站点文件都是由上传入口上传的程序到存储目录然后执行的 通过 mount -o 指定挂载参数，禁止使用 suid 、 exec ，增加安全性能 mount -t nfs -o nosuid,noexec,nodev 10.0.0.10:/data /mnt 3.2.6 查看挂载信息 $ df -h 10.0.0.10:/data 17G 4.4G 13G 26% /data 3.2.7 设置开机自动挂载 cat >> /etc/fstab 3.2.8 如果不需要使用nfs共享，可以卸载 umount /data 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/同步服务/centos7.7搭建rsync.html":{"url":"linux/linux服务/同步服务/centos7.7搭建rsync.html","title":"rsync","keywords":"","body":"[toc] centos7.7搭建rsync rsync基本概述 rsync是一款开源的备份工具，可以在不同主机之间进行同步，可实现全量备份与增量备份，保持链接和权限，且采用优化的同步算法，传输前执行压缩，因此非常适合用于架构集中式备份或异地备份等应用。 rsync官网 rsync监听端口：tcp/873 rsync运行模式：C/S rsync备份方式 1.完全备份(效率低、占用空间) 2.增量备份(提高备份效率,节省空间, 适合异地备份) 3.差异备份 rsync关于数据同步的两种方式 推：一台主机负责把数据推送至其他主机，服务器开销大(适合推送少量主机) 拉：所有主机定时去找一主机拉数据。可能会导致数据同步缓慢 rsync传输模式 本地方式 远程方式 守护进程方式 rsync命令选项 -a 归档模式传输, 等于-tropgDl -v 详细模式输出, 打印速率, 文件数量等 -z 传输时进行压缩以提高效率 -r 递归传输目录及子目录，即目录下的所有目录都同样传输 -t 保持文件时间信息 -o 保持文件属主信息 -p 保持文件权限 -g 保持文件属组信息 -l 保留软连接 -P 显示同步的过程及传输时的进度等信息 -D 保持设备文件信息 -L 保留软连接指向的目标文件 -e 使用的信道协议,指定替代rsh的shell程序 --exclude=PATTERN 指定排除不需要传输的文件模式 --exclude-from=file 文件名所在的目录文件 --bwlimit=100 限速传输 --partial 断点续传 --delete 让目标目录和源目录数据保持一致 rsync传输示例 本地传输 rsync 选项 源文件或目录 目标路径 例：将/etc/passwd文件同步到/opt rsync -avz /etc/passwd /opt 远程传输 ```python 推传输 rsync 选项 源文件或目录 远程主机目标路径 例：将本机/etc/passwd同步到另一台主机的/opt rsync -zav /etc/passwd root@10.0.0.10:/opt 拉传输 rsync 选项 远程主机源文件或目录 本地路径 例：将远程主机的/etc/passwd同步到本地/opt rsync -avz root@10.0.0.10:/etc/passwd /opt --- **试验环境** | 角色 | IP | 主机名 | | ------------ | --------- | ------------ | | rsync server | 10.0.0.10 | rsync-server | | rsync client | 10.0.0.11 | rsync-client | **实验过程** # rsync服务端操作 ## 1.安装rsync ```python yum -y install rsync 2.编辑rsync配置文件 #备份原有文件 cp /etc/rsyncd.conf{,.bak} #编辑rsync配置文件 cat >/etc/rsyncd.conf 3.建立rsync用户及相关目录 #创建rsync用户 useradd -M -s /sbin/nologin rsync #创建真正的共享目录并修改目录所有者为rsync mkdir /backup && chown rsync.rsync /backup 4.创建用户密码文件 ⚠️用户密码文件权限必须为600！！！ #创建密码文件，密码文件要与/etc/rsyncd.conf中\"secrets file = /etc/rsync.passwor\"相同 echo \"rsync_backup:1\" > /etc/rsync.password #修改密码文件权限，必须为600！！！ chmod 600 /etc/rsync.password 5.启动rsync并加入开机自启 systemctl start rsyncd && systemctl enable rsyncd rsync客户端操作 安装rsync、启动 yum -y install rsync systemctl start rsyncd && systemctl enable rsyncd 测试一、客户端推送及拉取不需要输入密码 ⚠️客户端使用--password-file=选项时，密码文件的权限必须为600！！！ #拉取语法 rsync -avz 用户@IP::共享名称 本机文件|目录 #推送语法 rsync -avz 本机文件|目录 用户@IP::共享名称 方法一： 在客户端/etc/rsync.password(文件随意)中写入服务端密码文件/etc/rsync.password中的密码 rsync拉取或推送时加选项--password-file= ⚠️这里的密码文件权限必须为600！！！ $ echo 1 > /etc/rsync.password && chmod 600 /etc/rsync.password $ rsync -avz rsync_backup@10.0.0.10::backup . --password-file=/etc/rsync.password receiving incremental file list ./ 1.txt 2.txt 3.txt 4.txt 5.txt sent 126 bytes received 317 bytes 295.33 bytes/sec total size is 0 speedup is 0.00 ############################################################### 方法二： 在客户端中导出变量，变量名必须为RSYNC_PASSWORD $ export RSYNC_PASSWORD=1 $ rsync -avz rsync_backup@10.0.0.10::backup . receiving incremental file list ./ 1.txt 2.txt 3.txt 4.txt 5.txt sent 126 bytes received 317 bytes 80.55 bytes/sec total size is 0 speedup is 0.00 测试二、实现数据无差异同步 --delete选项 此选项非常危险，生产环境不要使用！！！ ⚠️生产环境千万不要使用--delete选项 服务端 10.0.0.10 客户端 10.0.0.11 1.查看文件 服务端文件 [root@rsync-server backup]# ls 1.txt 2.txt 3.txt 4.txt 5.txt 客户端文件 [root@rsync-client backup]# ls 1.txt 2.txt 3.txt 4.txt 5.txt 2.删除服务端1.txt [root@rsync-server backup]# rm -rf 1.txt [root@rsync-server backup]# ls 2.txt 3.txt 4.txt 5.txt 3.不加--delete参数再次同步，可以看到客户端文件没有变化，但是服务端没有1.txt [root@rsync-client backup]# rsync -avz rsync_backup@10.0.0.10::backup . receiving incremental file list ./ sent 27 bytes received 127 bytes 308.00 bytes/sec total size is 0 speedup is 0.00 [root@rsync-client backup]# ls 1.txt 2.txt 3.txt 4.txt 5.txt 4.加--delete参数再次同步，可以看到，客户端同步服务端全部文件，删除1.txt [root@rsync-client backup]# rsync -avz --delete rsync_backup@10.0.0.10::backup . receiving incremental file list deleting 1.txt sent 24 bytes received 120 bytes 96.00 bytes/sec total size is 0 speedup is 0.00 [root@test1 backup]# ls 2.txt 3.txt 4.txt 5.txt 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/同步服务/centos7.7搭建sersync.html":{"url":"linux/linux服务/同步服务/centos7.7搭建sersync.html","title":"sersync","keywords":"","body":"[toc] centos7.7搭建sersync sersync已于2015年8月停止更新，作者推荐使用lsyncd lsyncd github地址 sersync官网 sersync github地址 一、sersync简介 sersync主要用于服务器同步，web镜像等功能。基于boost1.41.0,inotify api,rsync command.开发。目前使用的比较多的同步解决方案是inotify-tools+rsync ，另外一个是google开源项目Openduckbill（依赖于inotify- tools），这两个都是基于脚本语言编写的。相比较上面两个项目，本项目优点是： 1.sersync是使用c++编写，而且对linux系统文件系统产生的临时文件和重复的文件操作进行过滤（详细见附录，这个过滤脚本程序没有实现），所以在结合rsync同步的时候，节省了运行时耗和网络资源。 因此更快。 2.相比较上面两个项目，sersync配置起来很简单，其中bin目录下 已经有基本上静态编译的2进制文件，配合bin目录下的xml配置文件直接使用即可。 3.另外本项目相比较其他脚本开源项目，使用多线程进行同步，尤其在同步较大文件时，能够保证多个服务器实时保持同步状态。 4.本项目有出错处理机制，通过失败队列对出错的文件重新同步，如果仍旧失败，则 每10个小时对同步失败的文件重新同步。 5.本项目自带crontab功能，只需在xml配置文件中开启，即可按您的要求，隔一段时间整体同步一次。无需再额外配置crontab功能。 6.本项目socket与http插件扩展，满足您二次开发的需要。 同步原理图 二、sersync搭建过程 sersync流程 安装sersync的服务器角色为客户端，实时检测在sersync中配置的共享目录文件变化，采用客户端主动推送的方式将发生变化的文件传输到服务端 sersync负责检测文件变化，真正同步文件还是rsync 试验环境 角色 IP 主机名 安装服务 rsync server 10.0.0.10 rsync-server rsync rsync client 10.0.0.11 rsync-client sersync、rsync、inotify-tools 服务端操作 1.安装rsync yum -y install rsync 2.编辑rsync配置文件 #备份原有文件 cp /etc/rsyncd.conf{,.bak} #编辑rsync配置文件 cat >/etc/rsyncd.conf 3.建立rsync用户及共享目录 #创建rsync用户 useradd -M -s /sbin/nologin rsync #创建真正的共享目录并修改目录所有者为rsync mkdir /backup && chown rsync.rsync /backup 4.创建用户密码文件 ⚠️用户密码文件权限必须为600！！！ #创建密码文件，密码文件要与/etc/rsyncd.conf中\"secrets file = /etc/rsync.password\"相同 echo \"rsync_backup:1\" > /etc/rsync.password #修改密码文件权限，必须为600！！！ chmod 600 /etc/rsync.password 5.启动rsync并加入开机自启 systemctl start rsyncd && systemctl enable rsyncd 客户端操作 1.安装inotify-tools yum -y install inotify-tools 2.下载sersync #下载sersync git clone https://github.com.cnpmjs.org/wsgzao/sersync.git #解压缩包并重命名 tar xf sersync/sersync2.5.4_64bit_binary_stable_final.tar.gz mv GNU-Linux-x86/ /usr/local/sersync 3.配置sersync ⚠️13行这个选项是否开启完全同步，比较危险，一般设置为false #查看sersync目录内容，包含配置文件confxml.xml和启动文件sersync2 $ cd /usr/local/sersync $ pwd /usr/local/sersync $ ls confxml.xml sersync2 #配置confxml，精简版修改 13行，是否开启完全同步，比较危险，一般为false 24行，配置本机共享目录 25行，远程主机IP及rsync共享名称 30行，rsync同步时命令的选项，一般默认即可 31行，指定服务端rsync配置中的认证用户及密码文件 #参数说明 -a //归档模式传输, 等于-tropgDl -v //详细模式输出, 打印速率, 文件数量等 -z //传输时进行压缩以提高效率 -r //递归传输目录及子目录，即目录下的所有目录都同样传输 -t //保持文件时间信息 -o //保持文件属主信息 -p //保持文件权限 -g //保持文件属组信息 -l //保留软连接 -P //显示同步的过程及传输时的进度等信息 -D //保持设备文件信息 -L //保留软连接指向的目标文件 -e //使用的信道协议,指定替代rsh的shell程序 --exclude=PATTERN //指定排除不需要传输的文件模式 --exclude-from=file //文件名所在的目录文件 --bwlimit=100 //限速传输 --partial //断点续传 --delete //让目标目录和源目录数据保持一致 31行，开启用户认证，用户名，密码文件 4.创建用户认证密码文件及共享目录 ⚠️密码文件权限必须为600！！！ #创建用户认证密码文件 echo 1 > /etc/rsync.password #修改文件权限，必须为600！！！ chmod 600 /etc/rsync.password #创建rsync用户 useradd -M -s /sbin/nologin rsync #创建共享目录 mkdir /backup && chown rsync.rsync /backup 5.启动sersync #启动sersync /usr/local/sersync/sersync2 -dro /usr/local/sersync/confxml.xml set the system param execute：echo 50000000 > /proc/sys/fs/inotify/max_user_watches execute：echo 327679 > /proc/sys/fs/inotify/max_queued_events parse the command param option: -d run as a daemon option: -r rsync all the local files to the remote servers before the sersync work option: -o config xml name： /usr/local/sersync/confxml.xml daemon thread num: 10 parse xml config file host ip : localhost host port: 8008 daemon start，sersync run behind the console config xml parse success please set /etc/rsyncd.conf max connections=0 Manually sersync working thread 12 = 1(primary thread) + 1(fail retry thread) + 10(daemon sub threads) Max threads numbers is: 22 = 12(Thread pool nums) + 10(Sub threads) please according your cpu ，use -n param to adjust the cpu rate ------------------------------------------ rsync the directory recursivly to the remote servers once working please wait... execute command: cd /backup && rsync -artuz -R --delete ./ 10.0.0.12::backup >/dev/null 2>&1 #查看sersync进程 $ ps aux|grep sersyn[c] root 2213 0.0 0.1 92324 704 ? Ssl 15:05 0:00 /usr/local/sersync/sersync2 -dro /usr/local/sersync/confxml.xml #启动参数说明 -d 启用守护进程模式 -r 在监控前，将监控目录与远程主机用rsync命令推送一遍 -n 指定开启守护线程的数量，默认为10个 -o 指定配置文件，默认使用confxml.xml文件 -m 单独启用其他模块，使用 -m refreshCDN 开启刷新CDN模块 -m 单独启用其他模块，使用 -m socket 开启socket模块 -m 单独启用其他模块，使用 -m http 开启http模块 不加-m参数，则默认执行同步程序 6.验证同步 #文件、目录同步验证 1.进入客户端10.0.0.10 /backup目录创建文件、目录 $ touch {1..5}.txt && mkdir dir{1..3} $ ls 1.txt 2.txt 3.txt 4.txt 5.txt dir1 dir2 dir3 2.服务端10.0.0.11验证，可以看到文件已经同步 $ cd /backup $ ls 1.txt 2.txt 3.txt 4.txt 5.txt dir1 dir2 dir3 #文件内容同步验证 1.客户端10.0.0.10 /backup中向1.txt写入内容 $ cd /backup && echo 'test sersync' >1.txt 2.服务端10.0.0.11 /backup中查看1.txt文件内容 $ cd /backup && cat 1.txt test sersync 7.同步过程总结 sersync同步过程中需要注意的点 sersync端需要用到rsyncd服务端配置文件中指定的认证用户的密码，并且密码文件权限必须为600 sersync的作用其实就是实时检测本地文件，一旦有变化就把文件推送到配置文件中指定的主机 sersync端推送文件实际上还是利用了rsync 有关密码文件的说明 rsync认证用户:密码 一定是写在rsyncd服务端的 只有单纯的密码 一定是写在推送端的 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/同步服务/centos7.7搭建lsyncd.html":{"url":"linux/linux服务/同步服务/centos7.7搭建lsyncd.html","title":"lsyncd","keywords":"","body":"[toc] centos7.7搭建lsyncd lsyncd官网 lsyncd github地址 lsyncd简介 Lsyncd使用文件系统事件接口（inotify或fsevents）来监视本地文件和目录的更改。Lsyncd整理这些事件几秒钟，然后生成一个或多个进程以将更改同步到远程文件系统。默认的同步方法是rsync。因此，Lsyncd是一种轻型的实时镜像解决方案。Lsyncd相对易于安装，并且不需要新的文件系统或块设备。Lysncd不会妨碍本地文件系统的性能。 作为rsync的替代方法，Lsyncd还可以通过rsync + ssh推送更改。当文件或目录被重命名或移动到本地树中的新位置时，Rsync + ssh允许更高效的同步。（相反，普通rsync通过删除旧文件然后重新传输整个文件来执行移动。） 细粒度的定制可以通过配置文件来实现。自定义操作配置甚至可以从头开始写在层叠的层次上，从外壳脚本到用Lua语言编写的代码。因此，简单，强大和灵活的配置是可能的。 Lsyncd 2.2.1在所有源计算机和目标计算机上都要求rsync> = 3.1。 lsyncd同步流程 服务器A中部署rsync客户端+lsyncd，lsyncd通过内核的inotify触发机制监控文件的动向，并将改动发送给rsync，由rsync同步到服务器B；服务器B以守护进程的方式部署rsync服务端，接收A发来的文件同步请求，并将文件同步！ 部署lsyncd端的服务器的角色为客户端 试验环境 角色 IP 主机名 安装服务 lsyncd server 10.0.0.10 lsyncd-server rsync lsyncd client 10.0.0.11 lsyncd-client rsync、lsyncd 服务端操作 1.安装rsync yum -y install rsync 2.编辑rsync配置文件 # 备份原有文件 cp /etc/rsyncd.conf{,.bak} # 编辑rsync配置文件 cat >/etc/rsyncd.conf 3.建立rsync用户及共享目录 # 创建rsync用户 useradd -M -s /sbin/nologin rsync # 创建真正的共享目录并修改目录所有者为rsync mkdir /backup && chown rsync.rsync /backup 4.创建用户密码文件 ⚠️用户密码文件权限必须为600！！！ # 创建密码文件，密码文件要与/etc/rsyncd.conf中\"secrets file = /etc/rsync.password\"相同 echo \"rsync_backup:1\" > /etc/rsync.password # 修改密码文件权限，必须为600！！！ chmod 600 /etc/rsync.password 5.启动rsync并加入开机自启 systemctl start rsyncd && systemctl enable rsyncd 客户端操作 1.安装lsyncd、rsync # 安装lsyncd需要epel仓库 yum install -y epel-release # 安装lsyncd、rsync yum -y install lsyncd rsync # 查看版本 $ rpm -qa lsyncd lsyncd-2.2.2-1.el7.x86_64 $ rpm -qa rsync rsync-3.1.2-6.el7_6.1.x86_64 2.配置lsyncd lsyncd配置文件/etc/lsyncd.conf原先内容如下，--标示注释 ---- -- User configuration file for lsyncd. -- -- Simple example for default rsync, but executing moves through on the target. -- -- For more examples, see /usr/share/doc/lsyncd*/examples/ -- sync{default.rsyncssh, source=\"/var/www/html\", host=\"localhost\", targetdir=\"/tmp/htmlcopy/\"} 同步一台机器(密码文件方式) 编辑/etc/lsyncd.conf ⚠️--delete = true这个选项千万不要在生产环境中使用！！！ # 备份文件 cp /etc/lsyncd.conf{,.bak} # 编辑文件 cat >/etc/lsyncd.conf 同步多台机器(密码文件方式) 以上为仅同步一台机器，如果需要同步到多台机器，只需要在加几个sync{xxx}配置即可，同时需要注意的是如果采用密码文件的方式，则每一个sync标签中都必须包含rsync标签，用来指定密码文件 官方说明 # 在/etc/lsyncd.conf配置文件中多加几个sync标签即为同步到多台机器，需要注意的是，如果采用密码文件的方式，则每一个sync标签中都必须包含一个rsync标签，用来指定密码文件 sync { default.rsync, source = \"/backup\", --监控目录 target = \"rsync_backup@10.0.0.134::backup\", rsync = { binary = \"/usr/bin/rsync\", --rsync可执行文件路径， 必须为绝对路径 password_file = \"/etc/rsync.password\",--密码认证文件 archive = true, compress = false, verbose = false, _extra = { \"--bwlimit=200\", \"--omit-link-times\" } } } sync { default.rsync, source = \"/backup\", --监控目录 target = \"rsync_backup@10.0.0.130::backup\", --rsync的认证用户名、IP、模块 --delete = true, exclude = { '.**', '.git/**', '*.bak', '*.tmp', 'runtime/**', 'cache/**' }, rsync = { binary = \"/usr/bin/rsync\", --rsync可执行文件路径， 必须为绝对路径 password_file = \"/etc/rsync.password\",--密码认证文件 archive = true, compress = false, verbose = false, _extra = { \"--bwlimit=200\", \"--omit-link-times\" } } } 官方说明文档(同步到多台机器) 使用ssh免密方式同步(非密码文件方式) 官方配置文件示意图 主机之间先做ssh免密登陆 # 生成密钥，默认rsa加密类型，长度2048 -b指定长度 -t指定加密类型 $ ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is: SHA256:hV+uhg4hRTGHy3U3891ANGojAeAYmu2rA9E316XaBkc root@k8s-node1 The key's randomart image is: +---[RSA 2048]----+ | . =+o.. o+ | | + =.oE.o.+... | | .o o.o=.+o.=+ o.| |. ..o.= +o = .. o| | . .oo.=S . . | |. o..o. . | | . . ... o | | .. o . | | .. . | +----[SHA256]-----+ # 拷贝公钥到其他需要同步到主机 ssh-copy-id -i ~/.ssh/id_rsa.pub root@10.0.0.11 使用ssh免密方式的lsyncd配置文件如下，如需同步到多台主机，则和之前使用密码文件同步方式都相同，都是增加多个sync标签 settings { logfile = \"/var/log/lsyncd/lsyncd.log\", statusFile = \"/var/log/lsyncd/lsyncd.status\", inotifyMode = \"CloseWrite\", maxProcesses = 1, maxDelays = 1, nodaemon = false, } sync { default.rsyncssh, source = \"/backup\", --监控目录 host = \"10.0.0.130\", --同步远程主机IP targetdir = \"/backup\", --同步远程主机目录 rsync = { archive = true, compress = false, verbose = false, _extra = { \"--bwlimit=200\", \"--omit-link-times\" } }, ssh = { port = 22 } } sync { default.rsyncssh, source = \"/backup\", --监控目录 host = \"10.0.0.134\", --同步远程主机IP targetdir = \"/backup\", --同步远程主机目录 --delete = true, exclude = { '.**', '.git/**', '*.bak', '*.tmp', 'runtime/**', 'cache/**' }, rsync = { archive = true, compress = false, verbose = false, _extra = { \"--bwlimit=200\", \"--omit-link-times\" } }, ssh = { port = 22 } } 还是使用lsyncd --nodaemon /etc/lsyncd.conf启动查看是否报错，如果不报错则使用systemctl启动lsyncd即可 3.创建用户认证密码文件及共享目录 ⚠️密码文件权限必须为600！！！ # 创建用户认证密码文件 echo 1 > /etc/rsync.password # 修改文件权限，必须为600！！！ chmod 600 /etc/rsync.password # 创建rsync用户，与rsync服务端共享目录权限相同 useradd -M -s /sbin/nologin rsync # 创建共享目录 mkdir /backup && chown rsync.rsync /backup 4.启动lsyncd # 先使用如下命令启动看是否报错，如果不报错则ctrl+c停止然后用systemctl启动lsyncd $ lsyncd -nodaemon /etc/lsyncd.conf 16:31:18 Normal: --- Startup --- 16:31:18 Normal: recursive startup rsync: /backup/ -> rsync_backup@10.0.0.10::backup/ excluding .git/** runtime/** .** *.tmp cache/** *.bak 16:31:18 Normal: Startup of /backup/ -> rsync_backup@10.0.0.10::backup/ finished. # systemctl启动 systemcl start lsyncd && systemctl enable lsyncd 5.验证 启动lsyncd服务后，在lsyncd本地创建文件，然后到另外同步的机器目录查看是否同步文件即可 lsyncd密码文件方式与ssh免密方式配置文件需要注意的地方 密码文件方式 sync { default.rsync, source = \"/backup\", --监控目录 target = \"rsync_backup@10.0.0.10::backup\", --rsync的认证用户名、IP、模块 rsync = { binary = \"/usr/bin/rsync\", --rsync可执行文件路径， 必须为绝对路径 password_file = \"/etc/rsync.password\", --密码认证文件 } } ssh免密方式 sync { default.rsyncssh, source = \"/backup\", --监控目录 host = \"10.0.0.134\", --同步远程主机IP targetdir = \"/backup\", --同步远程主机目录 rsync = { archive = true, compress = false, verbose = false }, ssh = { port = 22 } } 不同点一 密码文件方式中sync标签中是default.rsync ssh免密方式中sync标签中是default.rsyncssh 不同点二 密码文件方式中sync标签中的target用于指定rsync的认证用户名、IP、模块 ssh免密方式中sync标签中则用到了host(指定同步远程主机IP)和targetdir(指定同步远程主机目录) 不同点三 密码文件方式中sync标签中的rsync标签下用到了binary(指定rsync命令绝对路径)和password_file(密码认证文件) ssh免密方式中sync标签中的rsync标签下不需要指定其他标注信息 详细配置看官网吧，写的非常清楚 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/php/安装/源码安装.html":{"url":"linux/linux服务/php/安装/源码安装.html","title":"源码安装","keywords":"","body":"[toc] php源码安装 php官网 php官方下载地址 php历史版本官方下载地址 php官方中文文档 php github地址 php国内下载地址1 php国内下载地址2 lnmp一健安装 1.下载源码包 php国内下载地址 export PHP_VERSION=7.3.19 wget https://mirrors.sohu.com/php/php-${PHP_VERSION}.tar.xz php官网下载地址 wget https://www.php.net/distributions/php-${PHP_VERSION}.tar.xz 2.安装依赖包 # 安装开发者工具包 yum -y group install \"Development Tools\" # 安装依赖包 yum -y install systemd-devel libacl libacl-devel libxml2 libxml2-devel curl curl-devel libjpeg libjpeg-devel libpng libpng-devel gmp-devel libxslt libxslt-devel openssl openssl-devel zlib zlib-devel pcre pcre-devel glib2 glib2-devel bzip2 bzip2-devel glibc glibc-devel liblzf liblzf-devel libzstd libzstd-devel freetype-devel readline-devel 3.解压缩包并编译安装 3.1 编译安装libzip 解决报错 configure: error: Please reinstall the libzip distribution 3.1.1 编译安装libzip需要安装高版本的cmake cmake github下载地址 # 下载源码包 wget https://github.com/Kitware/CMake/releases/download/v3.16.8/cmake-3.16.8.tar.gz # 解压缩源码包 tar xf cmake-3.16.8.tar.gz # 进入解压缩目录并编译安装 cd cmake-3.16.8/ ./configure --prefix=/usr/local/cmake gmake -j`nproc` && make install # 导出PATH环境变量并使配置生效 echo 'export PATH=/usr/local/cmake/bin:$PATH' >/etc/profile.d/cmake.sh source /etc/profile # 验证 $ cmake --version cmake version 3.16.8 3.1.2 编译安装libzip libzip官网 # 下载源码包 wget https://libzip.org/download/libzip-1.7.1.tar.xz # 解压缩源码包 tar xf libzip-1.7.1.tar.xz # 进入解压缩目录并编译安装 cd libzip-1.7.1 mkdir build && cd build && cmake .. && make -j`nproc` && make install 3.2 编辑配置文件 /etc/ld.so.conf内容 解决报错configure: error: off_t undefined; check your library configuration # 备份配置文件 cp /etc/ld.so.conf{,.bak} # 向配置文件写入以下内容 cat >/etc/ld.so.conf 3.3 解压缩包编译安装 可以使用 php -i | grep config 查看php编译参数 # 创建www用户 useradd -M www -s /sbin/nologin # 解压缩包 tar xf php-${PHP_VERSION}.tar.xz # 进入解压后的目录 cd php-${PHP_VERSION} # 编译安装 ./configure --prefix=/usr/local/php${PHP_VERSION} \\ --with-config-file-path=/usr/local/php${PHP_VERSION}/etc \\ --with-fpm-user=www \\ --with-fpm-group=www \\ --with-fpm-systemd \\ --with-fpm-acl \\ --with-mysql-sock \\ --with-mysqli \\ --with-libxml-dir \\ --with-openssl \\ --with-mhash \\ --with-pcre-regex \\ --with-zlib \\ --with-iconv \\ --with-bz2 \\ --with-curl \\ --with-cdb \\ --with-pcre-dir \\ --with-gd \\ --with-openssl-dir \\ --with-jpeg-dir \\ --with-png-dir \\ --with-zlib-dir \\ --with-freetype-dir \\ --with-gettext \\ --with-gmp \\ --with-mhash \\ --with-onig \\ --with-pdo-mysql \\ --with-zlib-dir \\ --with-readline \\ --with-libxml-dir \\ --with-xsl \\ --with-pear \\ --enable-fpm \\ --enable-soap \\ --enable-bcmath \\ --enable-calendar \\ --enable-dom \\ --enable-exif \\ --enable-fileinfo \\ --enable-filter \\ --enable-ftp \\ --enable-json \\ --enable-mbstring \\ --enable-mbregex \\ --enable-mbregex-backtrack \\ --enable-pdo \\ --enable-session \\ --enable-shmop \\ --enable-simplexml \\ --enable-sockets \\ --enable-sysvmsg \\ --enable-sysvsem \\ --enable-sysvshm \\ --enable-zip \\ --enable-mysqlnd-compression-support && make -j`nproc` && make install ubuntu16 ./configure 可能遇到的报错 configure: error: Cannot find OpenSSL's libraries，先使用命令 find / -name libssl.so 找到文件 libssl.so，然后做一下软连接即可 ln -s /usr/lib/x86_64-linux-gnu/libssl.so /usr/lib 3.4 拷贝相关文件 3.4.1 创建php安装目录软连接 ln -s /usr/local/php${PHP_VERSION}/ /usr/local/php 3.4.2 导出PATH环境变量 echo 'export PATH=/usr/local/php/bin:$PATH' >/etc/profile.d/php.sh source /etc/profile 3.4.3 拷贝php.ini文件 cp php-${PHP_VERSION}/php.ini-development /usr/local/php/etc/php.ini 3.4.4 配置php-fpm 方式一 拷贝php-fpm文件 cp /usr/local/php/etc/php-fpm.d/www.conf.default /usr/local/php/etc/php-fpm.conf 方式二 编辑php-fpm文件 这里贴一下生产中php的配置文件 php-fpm.conf ⚠️这里php启动是监听的sock文件，放在了 /tmp 下，如果指定其他目录，则这个目录权限需要设置为php运行用户所有 [global] pid = /usr/local/php72/var/run/php-fpm.pid error_log = /usr/local/php72/var/log/php-fpm.error.log log_level = notice [www] listen = /tmp/php73-cgi.sock listen.backlog = -1 listen.allowed_clients = 127.0.0.1 listen.owner = www listen.group = www listen.mode = 0666 user = www group = www pm = dynamic pm.max_children = 60 pm.start_servers = 30 pm.min_spare_servers = 30 pm.max_spare_servers = 60 pm.max_requests = 1024 pm.process_idle_timeout = 10s request_terminate_timeout = 100 request_slowlog_timeout = 0 slowlog = var/log/slow.log 3.4.5 拷贝php-fpm服务文件 cp php-${PHP_VERSION}/sapi/fpm/php-fpm.service /usr/lib/systemd/system/php-fpm.service 3.4.6 启动php-fpm 方式一 使用systemd管理 systemctl daemon-reload && systemctl enable php-fpm && systemctl start php-fpm 方式二 指定配置文件直接启动 /usr/local/php/sbin/php-fpm -c /usr/local/php/etc/php.ini -y /usr/local/php/etc/php-fpm.conf -D 3.4.7 检查php-fpm启动 # php默认监听tcp/9000端口 $ netstat -ntpl|grep 9000 tcp 0 0 127.0.0.1:9000 0.0.0.0:* LISTEN 12392/php-fpm: mast 4.结合nginx测试php-fpm功能是否正常 4.1 安装nginx并启动 # 安装nginx yum -y install nginx # 配置nginx以www用户运行 sed -i.bak '/^user/c user www;' /etc/nginx/nginx.conf # 启动nginx systemctl start nginx 4.2 编辑nginx配置文件 cat >/etc/nginx/conf.d/php-test.conf 4.3 重载nginx # 检测nginx语法 $ nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful # 重载nginx nginx -s reload 4.4 编辑php测试页面 cat >/opt/index.php EOF 本机做hosts解析，然后浏览器直接访问域名或IP即可，因为这里配置的nginx根目录下只有一个index.php文件，返回结果如下即为成功 5.安装扩展(可选) 5.1 安装redis扩展 5.1.1 安装方法一 pecl安装 pecl install redis echo \"extension=redis.so\" >> /usr/local/php/etc/php.ini 5.1.2 安装方法二 源码安装 从官网中获取最新稳定发行版 github phpredis地址 5.1.2.1 下载包 cd /opt && wget https://github.com/phpredis/phpredis/archive/5.2.2.tar.gz github phpredis安装说明 5.1.2.2 解压缩包并编译安装 # 解压缩包 tar xf 5.2.2.tar.gz cd phpredis-5.2.2/ # 生成configure文件 phpize # 编译安装 ./configure --with-php-config=/usr/local/php/bin/php-config make -j`nproc` && make install # 配置文件中启用redis模块 echo \"extension=redis.so\" >> /usr/local/php/etc/php.ini 报错1 checking for igbinary includes... configure: error: Cannot find igbinary.h 解决方法 pecl install igbinary echo \"extension=igbinary.so\" >> /usr/local/php/etc/php.ini 报错2 checking for msgpack includes... configure: error: Cannot find php_msgpack.h 解决方法 pecl install msgpack echo \"extension=msgpack.so\" >> /usr/local/php/etc/php.ini 5.1.2.3 在php配置文件中配置redis扩展 修改 /usr/local/php/etc/php.ini ，新增 extension=redis.so extension=redis.so 5.1.2.4 验证redis扩展是否安装成功 方式一 可以使用 /usr/local/php/bin/php -m |grep redis 查看redis扩展是否安装 /usr/local/php/bin/php -m |grep redis 方式二 往index.php 中写入以下内容 cat > index.php EOF 启动php web # -t后边必须是一个目录，并且index.php就在这个目录下 /usr/local/php/bin/php -S 0.0.0.0:8080 -t ./ 浏览器访问 IP:8080 5.2 安装mongo扩展 5.2.1 安装方法一 pecl安装 pecl install mongodb echo \"extension=mongodb.so\" >> /usr/local/php/etc/php.ini 5.2.2 安装方法二 源码安装 从官网中获取最新稳定发行版 php mongodb驱动程序官方说明 5.2.2.1 下载包 wget https://pecl.php.net/get/mongodb-1.7.3.tgz 5.2.2.2 解压缩包并编译安装 # 解压缩包 tar xf mongodb-1.7.3.tgz cd mongodb-1.7.3 # 生成configure文件 phpize ./configure --with-php-config=/usr/local/php/bin/php-config # 编译安装 make -j`nproc` && make install # 配置文件中启用mongo模块 echo \"extension=mongodb.so\" >> /usr/local/php/etc/php.ini php核心配置参数列表 php官方核心配置选项列表 杂项选项 参数 说明 --enable-debug 编译时加入调试符号 --with-layout=TYPE 设置被安装文件的布局。TYPE 是 PHP（默认）或 GNU --with-pear=DIR 在 DIR（默认为 PREFIX/lib/php）中安装 PEAR --without-pear 不安装 PEAR --enable-sigchild 使用 PHP 自带的 SIGCHLD 处理器 --disable-rpath 禁用在搜索路径中传递其他运行库 --enable-libgcc 启用 libgcc 的精确链接 --enable-php-streams 包含试验性的 PHP 流。不要使用此选项，除非是要测试代码 --with-zlib-dir[=DIR] 定义 zlib 的安装目录 --with-tsrm-pthreads 使用 POSIX 线程（默认） --enable-shared[=PKGS] 编译共享库 [default=yes] --enable-static[=PKGS] 编译静态库 [default=yes] --enable-fast-install[=PKGS] 为快速安装优化 [default=yes] --with-gnu-ld 假设 C 编译器使用 GNU ld [default=no] --disable-libtool-lock 避免锁死（可能破坏并联的编译） --with-pic 尝试仅使用 PIC/非 PIC 对象 [default=use both] --enable-memory-limit 编译内存限制支持功能。(自PHP 5.2.1开始不可用，默认enable) --disable-url-fopen-wrapper 禁用 URL 形式的 fopen 封装协议。该协议允许通过 HTTP 或者 FTP 访问文件。 （自PHP5.2.5开始不可用） --enable-versioning 仅导出必须的符号。查看 INSTALL 文件以获得更多信息 php选项 参数 说明 --enable-maintainer-mode 对偶然安装一下的情形启用此选项，使得不检查编译规则和依赖关系 --with-config-file-path=PATH 设置 php.ini 的搜索路径。默认为 PREFIX/lib --disable-short-tags 默认禁用短形式的开始标签 --with-libdir 指定Uxin系统库文件目录用于构建 PHP。 对于64位系统, 需要指定 lib64 目录,比如--with-libdir=lib64 SAPI选项 下面的列表包含 PHP 可用的SAPI（服务器应用编程接口） 参数 说明 --with-aolserver=DIR 指定 AOLserver 的安装路径 --with-apxs[=FILE] 编译共享的 Apache 模块。FILE 是可选的 Apache apxs 工具的路径，默认指向 apxs。请确认指定的 apxs 已经安装在服务器中，并且它不是 Apache 源码包中的那个 apxs --with-apache[=DIR] 编译静态 Apache 模块。DIR 是 Apache 编译目录的顶层，默认为 /usr/local/apache --with-mod_charset 启用 mod_charset 的转换表（俄文的 Apache 使用） --with-apxs2[=FILE] 编译共享的 Apache 2.0 模块。FILE 是可选的 Apache apxs 工具的路径，默认指向 apxs --with-caudium=DIR 为使用 Caudium 编译 PHP 为一个 Pike 模块。DIR 是 Caudium 服务器目录，默认为 /usr/local/caudium/server --disable-cli PHP 4.3.0 之后的版本有效。禁止编译 PHP 的 CLI 版本（使用它将同时强制使用 --without-pear 选项）。更多信息请查考 PHP 的命令行模式 --enable-embed[=TYPE] 启用编译嵌入的 SAPI 库。TYPE 或者为 shared 或者为 static，默认为 shared。PHP 4.3.0 之后的版本有效 --with-fhttpd[=DIR] 编译 fhttpd 模块。DIR 是 fhttpd 源代码目录，默认为 /usr/local/src/fhttpd。PHP 4.3.0 及以后的版本此选项不再有效 --with-isapi=DIR 为 Zeus 服务器以 ISAPI 模块方式编译 PHP --with-nsapi=DIR 指定 Netscape/iPlanet/SunONE 的安装目录 --with-phttpd=DIR 编译PHP为phttpd模块 --with-pi3web=DIR 为 Pi3Web 服务器编译 PHP 模块 --with-roxen=DIR 以 Pike 模块方式编译 PHP。DIR 是 Roxen 的根目录，默认为 /usr/local/roxen/server --enable-roxen-zts 使用 Zend 线程安全（ZTS）编译 Roxen 模块 --with-servlet[=DIR] 包含 servlet 支持。DIR 是 JSDK 的安装目录。此 SAPI 要求 java 扩展必须作为共享模块编译到 PHP 中 --with-thttpd=SRCDIR 编译 PHP 为 thttpd 模块 --with-tux=MODULEDIR 编译 PHP 为 TUX 模块（仅在 Linux 下有效） --with-webjames=SRCDIR 编译 PHP 为 WebJames 模块（仅在 RISC 操作系统中有效） --disable-cgi 禁止编译 CGI 版本的 PHP。PHP 4.3.0 之后的版本有效，PHP5.3.0起，这个选项会启用FastCGI，而在以前，必须使用--enable-fastcgi启用FastCGI --enable-force-cgi-redirect 启用内部服务器重定向的安全检测。如果在 Apache 下使用 CGI 版本的 PHP，请启用该选项，PHP 5.3.0起，默认有效并不再存在。要禁用此功能,设置cgi.force_redirect ini指令为 0 --enable-discard-path 如果启用该选项，PHP CGI 目录可以安全的放在 web 目录树的外面，人们无法避开 .htaccess 的安全限制，PHP 5.3.0起，默认禁用并不在存在。要启用此功能，设置 cgi-redirect ini指令为1 --enable-fastcgi 如果启用，CGI 模块将被编译为支持 FastCGI。PHP 4.3.0 之后的版本有效，PHP 5.3.0起，此参数不再存在，并使用 --enable-cgi替代 --disable-path-info-check 如果该选项被禁用，例如 /info.php/test?a=b 形式的路径将不能工作。PHP 4.3.0 之后的版本有效。更多信息请参考 » Apache 手册 fpm选项 参数 说明 --with-fpm-user 设置 FPM 运行的用户身份（默认 - nobody） --with-fpm-group 设置 FPM 运行时的用户组（默认 - nobody） --with-fpm-systemd 启用 systemd 集成 (默认 - no) --with-fpm-acl 使用POSIX 访问控制列表 (默认 - no) 5.6.5版本起有效 php部分编译参数说明，更多参数使用命令./configure --help查看 php官方函数参考说明地址 参数 说明 --prefix php安装的路径 --with-config-file-path php配置文件路径 --with-fpm-user 设置 FPM 运行的用户身份（默认 - nobody） --with-fpm-group 设置 FPM 运行时的用户组（默认 - nobody） --with-fpm-systemd 启用systemd集成，默认为no --with-fpm-acl 使用POSIX 访问控制列表 (默认 - no) 5.6.5版本起有效 --with-mysql-sock 设置为所有MySQL扩展（包括PDO_MYSQL）的MySQL unix套接字指针的位置 --with-mysqli 支持mysql扩展 --with-libxml-dir 打开libxml2库的支持 --with-openssl openssl的支持，加密传输时用到的 --with-mhash mhash算法的扩展 --with-pcre-regex 支持perl正则表达式 --with-zlib 支持zlib库 --with-iconv 支持iconv函数，这个函数的作用就是字符编码强制转换 --with-bz2 支持bz2文件 --with-curl 支持curl浏览工具 --with-cdb cdb库添加了cdb_make处理程序，该处理程序允许创建cdb文件并允许使用PHP的流访问网络上的cdb文件。 --with-pcre-dir perl的正则库案安装位置 --with-gd 支持gd库 --with-openssl-dir openssl安装位置 --with-jpeg-dir 支持jpeg图片 --with-png-dir 支持png图片 --with-zlib-dir 支持zlib库 --with-freetype-dir 支持freetype字体库 --with-gettext 支持gnu的gettext，编码库用到 --with-gmp 支持gnu的mp --with-mhash 支持mhash算法 --with-onig 使用外部oniguruma --with-pdo-mysql PDO_MYSQL是一个驱动程序，该驱动程序实现PHP数据对象（PDO）接口 以允许从PHP访问MySQL数据库 --with-zlib-dir zlib安装位置 --with-readline 支持readline库 --with-libxml-dir libxml2安装位置 --with-xsl 支持xsl --with-pear 支持pear --enable-fpm 允许fpm --enable-soap 支持soap，SOAP扩展可用于编写SOAP服务器和客户端 --enable-bcmath 支持用字符串表示的任意大小和精度的数字的二进制计算 --enable-calendar 支持日历转换 --enable-dom 支持dom扩展，DOM扩展使您可以通过带有PHP的DOM API对XML文档进行操作 --enable-exif 支持exif扩展，操作图像元数据 --enable-fileinfo 支持fileinfo，文件给定位置查找特定魔术字节猜测文件的内容类型以及编码 --enable-filter 支持filter，通过验证或清除数据来过滤数据 --enable-ftp 支持ftp --enable-json 支持json --enable-mbstring 支持多字节字符串 --enable-mbregex 支持多字节正则表达式 --enable-pdo 支持php数据对象 --enable-session 支持 session --enable-shmop 支持shmop，允许PHP读取、写入、创建和删除Unix共享内存段的函数集 --enable-simplexml 支持simplexml，将XML转换成一个带有一般属性选择器和数组迭代器的对象 --enable-sockets 支持socket --enable-sysvmsg 支持sysvmsg消息队列 --enable-sysvsem 支持系统V信号量 --enable-sysvshm 支持sysvshm，实现进程间通信共享内存的操作 --enable-zip 支持zip压缩 --enable-mysqlnd-compression-support 支持mysql压缩 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/php/安装/yum安装.html":{"url":"linux/linux服务/php/安装/yum安装.html","title":"yum安装","keywords":"","body":"[toc] yum安装 方式一 1.安装epel-release yum -y install epel-release 2.安装第三方yum源 这个yum只有php7.2 rpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpm 3.安装php7.2 yum -y install php72w php72w-cli php72w-common php72w-curl php72w-gd \\ php72w-mbstring php72w-mysqlnd php72w-process php72w-xml php72w-zip \\ php72w-opcache php72w-pecl-apcu php72w-intl php72w-pecl-redis php72w-fpm 4.启动php并设置开机自启 systemctl start php-fpm && systemctl enable php-fpm 方式二 1.添加第三方yum源 安装epel源并添加第三方yum源 yum -y install epel-release && \\ yum -y install https://rpms.remirepo.net/enterprise/remi-release-7.rpm 2.选择要安装的php版本 export phpversion=php73 yum -y install $phpversion-php-fpm $phpversion-php-cli $phpversion-php-bcmath $phpversion-php-gd $phpversion-php-json $phpversion-php-mbstring $phpversion-php-mcrypt $phpversion-php-mysqlnd $phpversion-php-opcache $phpversion-php-pdo $phpversion-php-pecl-crypto $phpversion-php-pecl-mcrypt $phpversion-php-pecl-geoip $phpversion-php-recode $phpversion-php-snmp $phpversion-php-soap $phpversion-php-xml 通过以下命令来获取更多安装信息 yum search php73 安装后的php配置文件路径 /etc/opt/remi/php73 3.启动php并设置开机自启 systemctl enable php73-php-fpm && systemctl start php73-php-fpm 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/php/知识点/nginx与php通信方式.html":{"url":"linux/linux服务/php/知识点/nginx与php通信方式.html","title":"nginx与php通信方式","keywords":"","body":"[toc] nginx与php通信方式 unix socket 配置示例 location ~ \\.php$ { include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_pass unix:/var/run/php7-fpm.sock; fastcgi_index index.php; } 生产配置示例 php-fpm.conf [global] pid = /usr/local/php/var/run/php-fpm.pid error_log = /usr/local/php/var/log/php-fpm.log log_level = notice [www] listen = /tmp/php-cgi.sock listen.backlog = -1 listen.allowed_clients = 127.0.0.1 listen.owner = www listen.group = www listen.mode = 0666 user = www group = www pm = dynamic pm.max_children = 100 pm.start_servers = 20 pm.min_spare_servers = 20 pm.max_spare_servers = 40 request_terminate_timeout = 100 request_slowlog_timeout = 0 slowlog = var/log/slow.log 在nginx后端api接口配置文件中有如下配置，把nginx与php通信的配置单独写到了一个文件中 include enable-php.conf; enable-php.conf 文件内容如下 location ~ [^/]\\.php(/|$) { try_files $uri =404; fastcgi_pass unix:/tmp/php-cgi.sock; fastcgi_index index.php; include fastcgi.conf; } 而 enable-php.conf 又有 include fastcgi.conf 配置，fastcgi.conf内容如下 fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param QUERY_STRING $query_string; fastcgi_param REQUEST_METHOD $request_method; fastcgi_param CONTENT_TYPE $content_type; fastcgi_param CONTENT_LENGTH $content_length; fastcgi_param SCRIPT_NAME $fastcgi_script_name; fastcgi_param REQUEST_URI $request_uri; fastcgi_param DOCUMENT_URI $document_uri; fastcgi_param DOCUMENT_ROOT $document_root; fastcgi_param SERVER_PROTOCOL $server_protocol; fastcgi_param REQUEST_SCHEME $scheme; fastcgi_param HTTPS $https if_not_empty; fastcgi_param GATEWAY_INTERFACE CGI/1.1; fastcgi_param SERVER_SOFTWARE nginx/$nginx_version; fastcgi_param REMOTE_ADDR $remote_addr; fastcgi_param REMOTE_PORT $remote_port; fastcgi_param SERVER_ADDR $server_addr; fastcgi_param SERVER_PORT $server_port; fastcgi_param SERVER_NAME $server_name; # PHP only, required if PHP was built with --enable-force-cgi-redirect fastcgi_param REDIRECT_STATUS 200; fastcgi_param PHP_ADMIN_VALUE \"open_basedir=$document_root/:/tmp/:/proc/:/data/ald_api\"; tcp socket 配置示例 location ~ \\.php$ { include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; } 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/tomcat/jdk安装.html":{"url":"linux/linux服务/tomcat/jdk安装.html","title":"jdk安装","keywords":"","body":"jdk安装 jdk官网下载地址 jdk历史版本下载地址 1.解压缩包 tar xf jdk-8u211-linux-x64.tar.gz -C /usr/local 2.导出环境变量 cat >>/etc/profil 3.使配置生效 source /etc/profile 4.查看版本 $ java -version java version \"1.8.0_211\" Java(TM) SE Runtime Environment (build 1.8.0_211-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode) 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/tomcat/centos7二进制安装tomcat8.html":{"url":"linux/linux服务/tomcat/centos7二进制安装tomcat8.html","title":"centos7二进制安装tomcat","keywords":"","body":"[toc] centos7二进制安装tomcat8 tomcat官网 tomcat官方安装文档 1.下载安装包 wget https://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-8/v8.5.56/bin/apache-tomcat-8.5.56.tar.gz 2.解压缩包 tar xf apache-tomcat-8.5.56.tar.gz -C /usr/local/ 3.做软连接 ln -s /usr/local/apache-tomcat-8.5.56/ /usr/local/tomcat-8.5.56 4.安装jdk oracle jdk官方下载地址 这里选择下载tar包，因为下载jdk需要登陆oracle账号，因此不能使用wget $ ls jdk-8u251-linux-x64.tar.gz 解压缩包 tar xf jdk-8u251-linux-x64.tar.gz -C /usr/local 导出环境变量 #导出jdk环境变量 cat >/etc/profile.d/jdk8.sh 5.使用systemd管理tomcat 5.1 为Tomcat添加启动参数 catalina.sh在执行的时候会调用同级路径下的setenv.sh来设置额外的环境变量，因此在/usr/local/tomcat-8.5.56/bin路径下创建setenv.sh文件，内容如下 cat >/usr/local/tomcat-8.5.56/bin/setenv.sh JVM选项说明 JAVA_OPTS='-Xms【初始化内存大小】 -Xmx【可以使用的最大内存】' 5.2 编写tomcat.service文件 在/usr/lib/systemd/system路径下添加tomcat.service文件，内容如下： cat >/usr/lib/systemd/system/tomcat.service 参数说明 [unit] 配置了服务的描述，规定了在network启动之后执行 [service] 配置服务的pid，服务的启动，停止，重启 [install] 配置了使用用户 5.3 修改tomcat bin目录下的catalina.sh 在catalina.sh文件中第2行开始添加如下内容，导出JAVA_HOME和JRE_HOME环境变量 sed -i.bak '2cexport JAVA_HOME=/usr/local/jdk1.8.0_251\\nexport JRE_HOME=/usr/local/jdk1.8.0_251/jre' /usr/local/tomcat-8.5.56/bin/catalina.sh 6.启动tomcat systemctl daemon-reload systemctl start tomcat && systemctl enable tomcat 浏览器访问8080端口 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/supervisor/1.supervisor安装.html":{"url":"linux/linux服务/supervisor/1.supervisor安装.html","title":"supervisor安装","keywords":"","body":"[toc] supervisor安装 一、Supervisor简介 Supervisor github地址 Supervisor官网 1.1 官网对于Supervisor的介绍 1.1.1 总览 Supervisor是一个客户端/服务器系统，允许其用户控制类似UNIX的操作系统上的许多进程。它受到以下方面的启发 方便 需要为每个单个流程实例编写rc.d脚本通常很不方便。 rc.d脚本是进程初始化/自动启动/管理的一个很好的最低公分母形式，但是编写和维护它们可能很麻烦。此外，rc.d脚本无法自动重新启动崩溃的进程，并且许多程序在崩溃时无法正确地自行重启。Supervisord将进程作为其子进程启动，并且可以配置为在崩溃时自动重新启动它们。也可以将其自动配置为自行调用启动进程。 准确性 在UNIX上，通常很难获得准确的启动/关闭状态信息。Pidfile经常说谎。Supervisord将流程作为子流程启动，因此它始终知道其子级的真实上/下状态，并且可以方便地查询该数据。 授权 需要控制流程状态的用户通常只需要这样做。他们不希望或不需要完全的Shell访问运行这些进程的计算机。侦听\"低\" TCP端口的进程通常需要以root用户身份启动和重新启动（UNIX功能不全）。通常情况下，允许\"正常\"人员停止或重新启动这样的过程是完全可以的，但是为他们提供shell访问通常是不切实际的，并且为他们提供root访问或sudo访问通常是不可能的。（正确）也很难向他们解释为什么存在此问题。如果超级用户以root用户身份启动，则可以允许\"普通\"用户控制此类过程，而无需向他们解释问题的复杂性。Supervisorctl允许以非常有限的形式访问计算机， 进程组 流程通常需要成组地启动和停止，有时甚至需要按照\"优先级顺序\"进行。通常很难向人们解释如何执行此操作。Supervisor允许您为进程分配优先级，并允许用户通过Supervisorctl客户端发出命令，如\"全部启动\"和\"全部重新启动\"，这将按预先分配的优先级顺序启动它们。此外，可以将进程分为\"进程组\"，并且可以将一组逻辑相关的进程作为一个单元停止和启动。 1.1.2 特征 简单 通过简单易懂的INI样式配置文件配置Supervisor。它提供了许多按进程选择的选项，使您的生活更加轻松，例如重新启动失败的进程和自动日志轮换。 集中 Supervisor为您提供了一个开始，停止和监视过程的地方。可以单独或成组控制过程。您可以配置Supervisor以提供本地或远程命令行和Web界面。 高效的 Supervisor通过fork / exec启动其子进程，并且子进程不守护。进程终止时，操作系统会立即向Supervisor发送信号，这与某些依赖麻烦的PID文件和定期轮询以重新启动失败的进程的解决方案不同。 可扩展的 Supervisor具有一个简单的事件通知协议，该协议可以使用任何语言编写的程序对其进行监视，并且具有用于控制的XML-RPC接口。它还使用扩展点构建，Python开发人员可以利用这些扩展点。 兼容 除Windows外，Supervisor几乎适用于所有其他方面。它已在Linux，Mac OS X，Solaris和FreeBSD上经过测试和支持。它完全用Python编写，因此安装不需要C编译器。 久经考验 尽管Supervisor如今非常活跃，但它不是新软件。Supervisor已经存在了很多年，并且已经在许多服务器上使用。 1.1.3 Supervisor组件 supervisord 服务器Supervisor的名称为supervisor。它负责自行调用启动子程序，响应来自客户端的命令，重新启动崩溃或退出的子进程，记录其子进程stdout和stderr 输出以及生成和处理与子进程生存期中的点相对应的\"事件\"。 服务器进程使用配置文件。它通常位于/etc/supervisord.conf中。此配置文件是\" Windows-INI\"样式的配置文件。通过适当的文件系统权限来确保此文件的安全很重要，因为它可能包含未加密的用户名和密码。 supervisorctl Supervisor的命令行客户端名为 supervisorctl。它提供了类似于shell的界面，可与supervisor提供的功能结合使用。从 supervisorctl，用户可以连接到不同的 supervisord进程（一次一个），对，停止控制的子流程获得地位和启动的子进程，并获得运行的进程的列表supervisord。 命令行客户端通过UNIX域套接字或Internet（TCP）套接字与服务器对话。服务器可以断言客户端的用户应该在允许客户端执行命令之前出示身份验证凭据。客户端进程通常使用与服务器相同的配置文件，但是任何带有[supervisorctl]节的配置文件都可以使用。 Web Server 与功能媲美A（稀疏）的Web用户界面 supervisorctl可以通过浏览器，如果你开始访问 supervisord对互联网插座。激活配置文件的[inet_http_server]部分后，请访问服务器URL（例如http://localhost:9001/）以通过Web界面查看和控制进程状态。 XML-RPC Interface 服务于Web UI的同一HTTP服务器提供XML-RPC接口，该接口可用于询问和控制管理程序及其运行的程序。请参阅XML-RPC API文档。 二、安装supervisor 2.1 系统python环境 $ python -V Python 2.7.5 2.2 安装suprvisor 配置国内pip源 mkdir ~/.pip cat > .pip/pip.conf 安装最新版 #方法一 pip install supervisor #方法二 yum -y install python-setuptools easy_install supervisor #查看版本 $ supervisord -v 4.2.0 安装指定版本 pip install supervisor==3.3.5 三、配置supervisor 3.1 运行echo_supervisord_conf命令生成默认配置文件 运行echo_supervisord_conf命令，会在当前终端的标准输出中打印一个样本Supervisor配置文件 $ echo_supervisord_conf ; Sample supervisor config file. ; ; For more information on the config file, please see: ; http://supervisord.org/configuration.html ; ; Notes: ; - Shell expansion (\"~\" or \"$HOME\") is not supported. Environment ; variables can be expanded using this syntax: \"%(ENV_HOME)s\". ; - Quotes around values are not supported, except in the case of ; the environment= options as shown below. ; - Comments must have a leading space: \"a=b ;comment\" not \"a=b;comment\". ; - Command will be truncated if it looks like a config file comment, e.g. ; \"command=bash -c 'foo ; bar'\" will truncate to \"command=bash -c 'foo \". ; ; Warning: ; Paths throughout this example file use /tmp because it is available on most ; systems. You will likely need to change these to locations more appropriate ; for your system. Some systems periodically delete older files in /tmp. ; Notably, if the socket file defined in the [unix_http_server] section below ; is deleted, supervisorctl will be unable to connect to supervisord. [unix_http_server] file=/tmp/supervisor.sock ; the path to the socket file ;chmod=0700 ; socket file mode (default 0700) ;chown=nobody:nogroup ; socket file uid:gid owner ;username=user ; default is no username (open server) ;password=123 ; default is no password (open server) ; Security Warning: ; The inet HTTP server is not enabled by default. The inet HTTP server is ; enabled by uncommenting the [inet_http_server] section below. The inet ; HTTP server is intended for use within a trusted environment only. It ; should only be bound to localhost or only accessible from within an ; isolated, trusted network. The inet HTTP server does not support any ; form of encryption. The inet HTTP server does not use authentication ; by default (see the username= and password= options to add authentication). ; Never expose the inet HTTP server to the public internet. ;[inet_http_server] ; inet (TCP) server disabled by default ;port=127.0.0.1:9001 ; ip_address:port specifier, *:port for all iface ;username=user ; default is no username (open server) ;password=123 ; default is no password (open server) [supervisord] logfile=/tmp/supervisord.log ; main log file; default $CWD/supervisord.log logfile_maxbytes=50MB ; max main logfile bytes b4 rotation; default 50MB logfile_backups=10 ; # of main logfile backups; 0 means none, default 10 loglevel=info ; log level; default info; others: debug,warn,trace pidfile=/tmp/supervisord.pid ; supervisord pidfile; default supervisord.pid nodaemon=false ; start in foreground if true; default false silent=false ; no logs to stdout if true; default false minfds=1024 ; min. avail startup file descriptors; default 1024 minprocs=200 ; min. avail process descriptors;default 200 ;umask=022 ; process file creation umask; default 022 ;user=supervisord ; setuid to this UNIX account at startup; recommended if root ;identifier=supervisor ; supervisord identifier, default is 'supervisor' ;directory=/tmp ; default is not to cd during start ;nocleanup=true ; don't clean up tempfiles at start; default false ;childlogdir=/tmp ; 'AUTO' child log dir, default $TEMP ;environment=KEY=\"value\" ; key value pairs to add to environment ;strip_ansi=false ; strip ansi escape codes in logs; def. false ; The rpcinterface:supervisor section must remain in the config file for ; RPC (supervisorctl/web interface) to work. Additional interfaces may be ; added by defining them in separate [rpcinterface:x] sections. [rpcinterface:supervisor] supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface ; The supervisorctl section configures how supervisorctl will connect to ; supervisord. configure it match the settings in either the unix_http_server ; or inet_http_server section. [supervisorctl] serverurl=unix:///tmp/supervisor.sock ; use a unix:// URL for a unix socket ;serverurl=http://127.0.0.1:9001 ; use an http:// url to specify an inet socket ;username=chris ; should be same as in [*_http_server] if set ;password=123 ; should be same as in [*_http_server] if set ;prompt=mysupervisor ; cmd line prompt (default \"supervisor\") ;history_file=~/.sc_history ; use readline history if available ; The sample program section below shows all possible program subsection values. ; Create one or more 'real' program: sections to be able to control them under ; supervisor. ;[program:theprogramname] ;command=/bin/cat ; the program (relative uses PATH, can take args) ;process_name=%(program_name)s ; process_name expr (default %(program_name)s) ;numprocs=1 ; number of processes copies to start (def 1) ;directory=/tmp ; directory to cwd to before exec (def no cwd) ;umask=022 ; umask for process (default None) ;priority=999 ; the relative start priority (default 999) ;autostart=true ; start at supervisord start (default: true) ;startsecs=1 ; # of secs prog must stay up to be running (def. 1) ;startretries=3 ; max # of serial start failures when starting (default 3) ;autorestart=unexpected ; when to restart if exited after running (def: unexpected) ;exitcodes=0 ; 'expected' exit codes used with autorestart (default 0) ;stopsignal=QUIT ; signal used to kill process (default TERM) ;stopwaitsecs=10 ; max num secs to wait b4 SIGKILL (default 10) ;stopasgroup=false ; send stop signal to the UNIX process group (default false) ;killasgroup=false ; SIGKILL the UNIX process group (def false) ;user=chrism ; setuid to this UNIX account to run the program ;redirect_stderr=true ; redirect proc stderr to stdout (default false) ;stdout_logfile=/a/path ; stdout log path, NONE for none; default AUTO ;stdout_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stdout_logfile_backups=10 ; # of stdout logfile backups (0 means none, default 10) ;stdout_capture_maxbytes=1MB ; number of bytes in 'capturemode' (default 0) ;stdout_events_enabled=false ; emit events on stdout writes (default false) ;stdout_syslog=false ; send stdout to syslog with process name (default false) ;stderr_logfile=/a/path ; stderr log path, NONE for none; default AUTO ;stderr_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stderr_logfile_backups=10 ; # of stderr logfile backups (0 means none, default 10) ;stderr_capture_maxbytes=1MB ; number of bytes in 'capturemode' (default 0) ;stderr_events_enabled=false ; emit events on stderr writes (default false) ;stderr_syslog=false ; send stderr to syslog with process name (default false) ;environment=A=\"1\",B=\"2\" ; process environment additions (def no adds) ;serverurl=AUTO ; override serverurl computation (childutils) ; The sample eventlistener section below shows all possible eventlistener ; subsection values. Create one or more 'real' eventlistener: sections to be ; able to handle event notifications sent by supervisord. ;[eventlistener:theeventlistenername] ;command=/bin/eventlistener ; the program (relative uses PATH, can take args) ;process_name=%(program_name)s ; process_name expr (default %(program_name)s) ;numprocs=1 ; number of processes copies to start (def 1) ;events=EVENT ; event notif. types to subscribe to (req'd) ;buffer_size=10 ; event buffer queue size (default 10) ;directory=/tmp ; directory to cwd to before exec (def no cwd) ;umask=022 ; umask for process (default None) ;priority=-1 ; the relative start priority (default -1) ;autostart=true ; start at supervisord start (default: true) ;startsecs=1 ; # of secs prog must stay up to be running (def. 1) ;startretries=3 ; max # of serial start failures when starting (default 3) ;autorestart=unexpected ; autorestart if exited after running (def: unexpected) ;exitcodes=0 ; 'expected' exit codes used with autorestart (default 0) ;stopsignal=QUIT ; signal used to kill process (default TERM) ;stopwaitsecs=10 ; max num secs to wait b4 SIGKILL (default 10) ;stopasgroup=false ; send stop signal to the UNIX process group (default false) ;killasgroup=false ; SIGKILL the UNIX process group (def false) ;user=chrism ; setuid to this UNIX account to run the program ;redirect_stderr=false ; redirect_stderr=true is not allowed for eventlisteners ;stdout_logfile=/a/path ; stdout log path, NONE for none; default AUTO ;stdout_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stdout_logfile_backups=10 ; # of stdout logfile backups (0 means none, default 10) ;stdout_events_enabled=false ; emit events on stdout writes (default false) ;stdout_syslog=false ; send stdout to syslog with process name (default false) ;stderr_logfile=/a/path ; stderr log path, NONE for none; default AUTO ;stderr_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stderr_logfile_backups=10 ; # of stderr logfile backups (0 means none, default 10) ;stderr_events_enabled=false ; emit events on stderr writes (default false) ;stderr_syslog=false ; send stderr to syslog with process name (default false) ;environment=A=\"1\",B=\"2\" ; process environment additions ;serverurl=AUTO ; override serverurl computation (childutils) ; The sample group section below shows all possible group values. Create one ; or more 'real' group: sections to create \"heterogeneous\" process groups. ;[group:thegroupname] ;programs=progname1,progname2 ; each refers to 'x' in [program:x] definitions ;priority=999 ; the relative start priority (default 999) ; The [include] section can just contain the \"files\" setting. This ; setting can list multiple files (separated by whitespace or ; newlines). It can also contain wildcards. The filenames are ; interpreted as relative to this file. Included files *cannot* ; include files themselves. ;[include] ;files = relative/directory/*.ini 去掉注释后的内容如下 $ echo_supervisord_conf > hehe $ egrep -v '^$|^;' hehe [unix_http_server] file=/tmp/supervisor.sock ; the path to the socket file [supervisord] logfile=/tmp/supervisord.log ; main log file; default $CWD/supervisord.log logfile_maxbytes=50MB ; max main logfile bytes b4 rotation; default 50MB logfile_backups=10 ; # of main logfile backups; 0 means none, default 10 loglevel=info ; log level; default info; others: debug,warn,trace pidfile=/tmp/supervisord.pid ; supervisord pidfile; default supervisord.pid nodaemon=false ; start in foreground if true; default false silent=false ; no logs to stdout if true; default false minfds=1024 ; min. avail startup file descriptors; default 1024 minprocs=200 ; min. avail process descriptors;default 200 [rpcinterface:supervisor] supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface [supervisorctl] serverurl=unix:///tmp/supervisor.sock ; use a unix:// URL for a unix socket 以上就是supervisor的默认配置文件 3.2 supervisor查找配置文件的顺序 supervisor配置文件通常被命名为supervisor.conf，supervisor和supervisorctl都使用这个配置文件，如果在没有-c选项的情况下启动了任一应用程序（该选项用于显式告知应用程序配置文件名），则该应用程序将在以下位置按指定顺序查找名为supervisord.conf的文件。它将使用找到的第一个文件。 ../etc/supervisord.conf (相对于可执行文件) ../supervisord.conf (相对于可执行文件) $CWD/supervisord.conf $CWD/etc/supervisord.conf /etc/supervisord.conf /etc/supervisor/supervisord.conf (自Supervisor 3.3.0版本开始) 3.3 手动编辑supervisor配置文件 创建配置文件和日志文件目录 mkdir -p /etc/supervisor/config.d mkdir /var/log/supervisor 创建supervisor运行用户supervisor useradd supervisor -s /sbin/nologin -M 手动编辑supervisor配置文件 ⚠️需要注意的点 supervisor默认以root身份运行，如果想要指定用户，需要在标签 [supervisord]下添加 user=xxx 如果没有以-c选项指定配置文件，则supervisor会按照以下顺序查找配置文件 ，否则会报错 Error: No config file found at default paths (/usr/etc/supervisord.conf, /usr/supervisord.conf, supervisord.conf, etc/supervisord.conf, /etc/supervisord.conf, /etc/supervisor/supervisord.conf); use the -c option to specify a config file at a different path 1 ../etc/supervisord.conf（相对于可执行文件） 2 ../supervisord.conf（相对于可执行文件） 3 $CWD/supervisord.conf 4 $CWD/etc/supervisord.conf 5 /etc/supervisord.conf 6 /etc/supervisor/supervisord.conf (since Supervisor 3.3.0) 官方对于配置文件查找顺序的说明 cat > /etc/supervisor/supervisord.conf 当有如下配置时，supervisor还提供了一个web界面 [inet_http_server] port=10.0.0.10:9001 username=test password=test 浏览器访问IP:9001，用户名和密码都是test 3.4 设置supervisor日志滚动 cat >/etc/logrotate.d/supervisor 3.5 设置Tmpfiles防止sock文件被清理 cat >> /usr/lib/tmpfiles.d/tmp.conf 3.6 使用systemd管理supervisor 对各操作系统提供supervisor脚本的github地址 克隆项目 $ git clone https://github.com/Supervisor/initscripts.git $ cd initscripts $ ls centos-systemd-etcs gentoo-matagus redhat-init-equeffelec redhat-sysconfig-equeffelec ubuntu debian-norrgard opensuse-garymonson redhat-init-jkoppe redhat-sysconfig-jkoppe fedora-bmbouter README.md redhat-init-mingalevme slackware #将centos-systemd-etcs中的内容拷贝到/usr/lib/systemd/system/supervisord.service cat centos-systemd-etcs >/usr/lib/systemd/system/supervisord.service centos-systemd-etcs文件内容 ⚠️要注意 supervisord 和 supervisorctl 这两个命令的绝对路径 #centos-systemd-etcs文件内容 cat > /usr/lib/systemd/system/supervisord.service 启动supervisord并加入开机自启 systemctl daemon-reload systemctl start supervisord && systemctl enable supervisord 四、supervisor相关命令 4.1 supervisord命令选项 supervisord命令 选项 说明 -c FILE, --configuration=FILE 指定supervisor配置文件 -n, --nodaemon 在前台运行supervisord -h, --help 查看帮助 -u USER, --user=USER UNIX用户名或数字用户标识。如果以root用户身份启动supervisord，请在启动期间尽快setuid给该用户 -m OCTAL, --umask=OCTAL 表示supervisord启动后应该使用的 umask 八进制数（例如022） -d PATH, --directory=PATH 当supervisord作为守护进程运行时，在守护进程之前cd到此目录 -l FILE, --logfile=FILE 用作supervisord活动日志的文件名路径 -y BYTES, --logfile_maxbytes=BYTES 旋转发生前，supervisord活动日志文件的最大大小。该值是后缀倍增的，例如“1”是一个字节，“1MB”是1兆字节，“1GB”是1千兆字节 -z NUM, --logfile_backups=NUM 要保持的supervisord活动日志的备份副本数。每个日志文件的大小为logfile_maxbytes -e LEVEL, --loglevel=LEVEL supervisor应该写入活动日志的日志记录级别。有效级别包括trace(追踪), debug(调试), info(信息), warn(警告), error(错误),critical(严重) -j FILE, --pidfile=FILE supervisord应该将其pid文件写入的文件名 -i STRING, --identifier=STRING 由supervisor实例的各种客户端UI公开的任意字符串标识符 -q PATH, --childlogdir=PATH 目录的路径（它必须已经存在），其中supervisor将写入其AUTO -mode子进程日志 -k, --nocleanup 防止supervisord在启动时执行清理（删除旧的AUTO进程日志文件） -a NUM, --minfds=NUM 在成功启动之前，supervisord进程必须可用的最小文件描述符数 -t, --strip_ansi 从所有子日志进程中剥离ANSI转义序列 -v, --version 查看版本 --profile_options=LIST 用于分析的逗号分隔选项列表。导致 supervisord在探查器下运行，并根据选项输出结果，这些选项是以逗号分隔的以下列表：累积，呼叫，呼叫者。例如累计，来电者 --minprocs=NUM 在成功启动之前，supervisord进程必须可用的最小OS进程槽数 4.2 supervisorctl命令选项及动作 supervisorctl命令选项 选项 说明 -c, --configuration 配置文件路径（默认为/etc/supervisord.conf） -h, --help 查看帮助 -i, --interactive 执行命令后启动交互式shell -s, --serverurl URL supervisord服务器正在侦听的URL(default “http://localhost:9001”) -u, --username 用于与服务器进行身份验证的用户名 -p, --password 用于与服务器进行身份验证的密码 -r, --history-file 保留readline历史记录（如果readline可用） supervisorctl命令动作 五、supervisor信号 supervisor程序可能会被发送信号，使其在运行时执行某些操作。 你可以将这些信号中的任何一个发送到单个supervisord进程id，可以在配置文件的[supervisord]部分的pidfile参数表示的文件中找到此进程ID （默认情况下为$CWD/supervisord.pid） SIGTERM supervisord的所有子进程都将关闭。这可能需要几秒钟。 SIGINT supervisord的所有子进程都将关闭。这可能需要几秒钟。 SIGQUIT supervisord的所有子进程都将关闭。这可能需要几秒钟。 SIGHUP supervisord将停止所有进程，从它找到的第一个配置文件重新加载配置，然后启动所有进程。 SIGUSR2 supervisord将关闭并重新打开主活动日志和所有子日志文件。 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/supervisor/2.supervisor管理mysql.html":{"url":"linux/linux服务/supervisor/2.supervisor管理mysql.html","title":"supervisor管理mysql","keywords":"","body":"supervisor管理mysql supervisor配置文件 /etc/supervisor/supervisord.conf 中定义了include，因此如果想要管理服务，就需要编辑 /etc/supervisor/config.d/*.ini 文件 [include] files = /etc/supervisor/config.d/*.ini 编辑mysql服务配置文件 /etc/supervisor/config.d/mysql.ini ⚠️示例中的mysql是二进制安装在了 /usr/local/mysql ，例如数据目录是 /usr/local/mysql/data ，其余插件目录、错误日志目录、pid目录等还需要根据实际情况修改 cat > /etc/supervisor/config.d/mysql.ini 创建存放日志目录 mkdir -p /var/log/supervisor 将mysql加入supervisor 方法一 使用 supervisorctl update 程序名 把相应服务加入supervisor $ supervisorctl update mysql mysql: added process group 方法二 使用 supervisorctl 命令进入交互操作界面，然后再添加服务 $ supervisorctl $ update mysql 查看服务运行 $ supervisorctl status mysql mysql RUNNING pid 4415, uptime 0:00:36 详细配置 [program:mysql] command=/usr/local/mysql/bin/mysqld --basedir=/usr/local/mysql --datadir=/data/db/mysql/data --plugin-dir=/usr/local/mysql/lib/plugin --user=mysql --log-error=/data/db/mysql/logs/mysqld.log --pid-file=/data/db/mysql/mysqld/mysqld.pid --socket=/data/db/mysql/sock/mysqld.sock priority=1 numprocs=1 autostart=true autorestart=true startretries=10 exitcodes=0 stopsignal=KILL stopwaitsecs=10 redirect_stderr=true stdout_logfile_maxbytes = 1024MB stdout_logfile_backups = 10 stdout_logfile = /var/log/supervisord/mysql.log mysql配置文件 [client] port = 3306 socket = /data/db/mysql/sock/mysql.sock [mysqld] port = 3306 socket = /data/db/mysql/sock/mysql.sock pid-file = /data/db/mysql/sock/mysqld.pid log-error = /data/db/mysql/logs/mysql-err.log log = /data/db/mysql/logs/mysqld.log tmpdir = /data/db/mysql/tmp/ slow_query_log = ON long_query_time = 5 slow_query_log_file = /data/db/mysql/logs/slow_query.log log_queries_not_using_indexes = ON skip-external-locking key_buffer_size = 256M max_allowed_packet = 64M table_open_cache = 256 sort_buffer_size = 1M read_buffer_size = 1M read_rnd_buffer_size = 4M myisam_sort_buffer_size = 64M thread_cache_size = 8 query_cache_size= 16M thread_concurrency = 8 max_connections = 20000 max_connect_errors = 20000 log-bin=/data/db/mysql/log-bin/mysql-bin expire_logs_days=4 relay-log =/data/db/mysql/relay-log log_slave_updates =1 relay_log_purge =1 relay_log_space_limit = 10G binlog_format=mixed server-id = 21 innodb_data_home_dir = /data/db/mysql/data innodb_data_file_path = ibdata1:20G;ibdata2:20G;ibdata3:20G;ibdataext:10M:autoextend innodb_log_group_home_dir = /data/db/mysql/data innodb_buffer_pool_size = 4800M innodb_additional_mem_pool_size = 20M innodb_log_file_size = 64M innodb_log_buffer_size = 8M innodb_lock_wait_timeout = 50 [mysqldump] quick max_allowed_packet = 16M [mysql] no-auto-rehash [myisamchk] key_buffer_size = 128M sort_buffer_size = 128M read_buffer = 2M write_buffer = 2M [mysqlhotcopy] interactive-timeout 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/supervisor/3.supervisor管理nginx.html":{"url":"linux/linux服务/supervisor/3.supervisor管理nginx.html","title":"supervisor管理nginx","keywords":"","body":"supervisor管理nginx supervisor配置文件 /etc/supervisor/supervisord.conf 中定义了include，因此如果想要管理服务，就需要编辑 /etc/supervisor/config.d/*.ini 文件 [include] files = /etc/supervisor/config.d/*.ini 编辑nginx服务配置文件 /etc/supervisor/config.d/nginx.ini ⚠️需要注意的是 /usr/sbin/nginx 表示在后台运行，但是supervisor不能监控后台程序， 所以supervisor就一直执行这个命令 ，因此会报错 /usr/sbin/nginx后必须加参数 -g 'daemon off;' 表示在前台运行 cat > /etc/supervisor/config.d/nginx.ini 将nginx加入supervisor $ supervisorctl update nginx nginx: added process group 查看状态 $ supervisorctl status nginx nginx RUNNING pid 9464, uptime 0:00:03 详细配置 [program:nginx] # use default nginx config file and dir command = /usr/local/nginx/sbin/nginx -g 'daemon off;' stdout_logfile = /var/log/supervisord/nginx.log redirect_stderr = true autorestart = true nginx配置文件 #user www www; worker_processes auto; # worker_cpu_affinity auto; # openresty-1.9.15 worker_rlimit_nofile 65535; error_log logs/error.log; pid /var/run/nginx.pid; events { use epoll; worker_connections 65565; } http { server_tokens off; sendfile on; tcp_nodelay on; tcp_nopush on; keepalive_timeout 0; charset utf-8; include mime.types; default_type application/json; log_format main ' $http_X_Forwarded_Proto - $http_SLB_IP - $upstream_addr - $http_X_Forwarded_For - ' '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status [$upstream_response_time] ' '[$request_time] $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\"'; client_header_buffer_size 16k; large_client_header_buffers 8 16k; server_names_hash_bucket_size 128; client_max_body_size 4096m; client_header_timeout 30s; client_body_timeout 30s; send_timeout 30s; lingering_close off; gzip on; gzip_vary on; gzip_min_length 1000; gzip_comp_level 6; gzip_types text/plain text/xml text/css application/javascript application/json; gzip_http_version 1.0; #index index.html index.shtml index.php; include upstream.conf; include default.conf; include vhosts/*.conf; include vhosts/jr/*.conf; include vhosts/yg/*.conf; include vhosts/yjk/*.conf; include vhosts/ys/*.conf; include vhosts/sec/*.conf; include vhosts/saas/*.conf; include vhosts/bd/*.conf; include vhosts/autom/*.conf; lua_code_cache on; lua_package_path \"../?.lua;../lib/?.lua;../lib/lua-resty-core/lib/?.lua;;\"; lua_need_request_body on; } 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/supervisor/4.supervisor管理tomcat.html":{"url":"linux/linux服务/supervisor/4.supervisor管理tomcat.html","title":"supervisor管理tomcat","keywords":"","body":"supervisor管理tomcat ⚠️在生产环境中是不允许以root用户运行tomcat的，我们是以一个有root权限的运维专用用户来运行supervisor和tomcat的 supervisor配置文件 /etc/supervisor/supervisord.conf 中定义了include，因此如果想要管理服务，就需要编辑 /etc/supervisor/config.d/*.ini 文件 [include] files = /etc/supervisor/config.d/*.ini 编辑tomcat服务配置文件 /etc/supervisor/config.d/tomcat.ini tomcat安装目录 /usr/local/tomcat-8.5.56/ jdk1.8安装目录 /usr/local/jdk1.8.0_251/ cat > /etc/supervisor/config.d/tomcat.ini 修改tomcat启动脚本文件 startup.sh 本文示例是 /usr/local/tomcat-8.5.56/bin/startup.sh 最后一行，将start修改为run #exec \"$PRGDIR\"/\"$EXECUTABLE\" start \"$@\" exec \"$PRGDIR\"/\"$EXECUTABLE\" run \"$@\" # 用以下命令修改 sed -i.bak '$s/start/run/' startup.sh 将tomcat加入supervisor $ supervisorctl update tomcat tomcat: added process group 查看状态 $ supervisorctl status tomcat nginx RUNNING pid 9464, uptime 0:00:03 详细配置 [program:webServer] command=/data/webapp/webServer/bin/startup.sh environment=JAVA_HOME=\"/usr/local/jdk1.8.0/\",JAVA_BIN=\"/usr/local/jdk1.8.0/bin\" autostart = true autorestart=true redirect_stderr=true stdout_logfile=/data/logs/webServer/catalina.out 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/supervisor/5.supervisor管理redis.html":{"url":"linux/linux服务/supervisor/5.supervisor管理redis.html","title":"supervisor管理redis","keywords":"","body":"supervisor管理redis supervisor配置文件 /etc/supervisor/supervisord.conf 中定义了include，因此如果想要管理服务，就需要编辑 /etc/supervisor/config.d/*.ini 文件 [include] files = /etc/supervisor/config.d/*.ini supervisor管理redis有一个很奇怪的问题 redis配置文件中如果配置了 daemonize yes 用supervisor管理的时候是会报错如下 redis FATAL Exited too quickly (process log may have details) ，虽然有报错但是redis进程却是正常运行的，进程在端口也能启动！！！ 如果把daemonize改成no则没有问题，但是这样就不能把redis以守护进程的方式运行了 daemonize 设置 yes 或者 no 区别 yes redis采用的是单进程多线程的模式。当 redis.conf 中选项 daemonize 设置成yes时，代表开启守护进程模式。在该模式下，redis会在后台运行，并将进程pid号写入至redis.conf 选项 pidfile 设置的文件中，此时redis将一直运行，除非手动kill该进程。 no 当 daemonize 选项设置成no时，当前界面将进入redis的命令行界面，exit强制退出或者关闭连接工具(putty,xshell等)都会导致redis进程退出。 编辑redis服务配置文件 /etc/supervisor/config.d/redis.ini cat > /etc/supervisor/config.d/redis.ini 将redis加入supervisor $ supervisorctl update redis redis: added process group 查看状态 $ supervisorctl status redis redis RUNNING pid 15430, uptime 0:00:06 详细配置 [program:redis] command=/usr/local/bin/redis-server /data/db/redis/cfg/redis.conf directory=/usr/local/redis/src/ autostart=true stdout_logfile=/var/log/supervisord/redis.log redis配置文件 daemonize yes pidfile /data/db/redis/pid/redis_6379.pid port 6379 tcp-backlog 511 timeout 300 tcp-keepalive 0 loglevel notice logfile \"/data/db/redis/logs/redis_6379.log\" databases 16 save 7200 1 stop-writes-on-bgsave-error yes rdbcompression yes rdbchecksum yes dbfilename dump_6379.rdb dir /data/db/redis/data slave-serve-stale-data yes slave-read-only yes repl-diskless-sync yes repl-diskless-sync-delay 5 repl-disable-tcp-nodelay no slave-priority 100 protected-mode no maxmemory 4gb appendonly no appendfilename \"appendonly_6379.aof\" no-appendfsync-on-rewrite no auto-aof-rewrite-percentage 50 auto-aof-rewrite-min-size 64mb aof-load-truncated yes lua-time-limit 5000 slowlog-log-slower-than 50000 slowlog-max-len 1024 latency-monitor-threshold 0 notify-keyspace-events \"\" hash-max-ziplist-entries 512 hash-max-ziplist-value 64 list-max-ziplist-entries 512 list-max-ziplist-value 64 set-max-intset-entries 512 zset-max-ziplist-entries 128 zset-max-ziplist-value 64 hll-sparse-max-bytes 3000 activerehashing yes client-output-buffer-limit normal 0 0 0 client-output-buffer-limit slave 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 hz 10 aof-rewrite-incremental-fsync yes 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/supervisor/6.supervisor管理mongodb.html":{"url":"linux/linux服务/supervisor/6.supervisor管理mongodb.html","title":"supervisor管理mongodb","keywords":"","body":"supervisor管理mongodb supervisor配置文件 /etc/supervisor/supervisord.conf 中定义了include，因此如果想要管理服务，就需要编辑 /etc/supervisor/config.d/*.ini 文件 [include] files = /etc/supervisor/config.d/*.ini 编辑mongodb服务配置文件 /etc/supervisor/config.d/mongodb.ini mongodb配置文件中要设置这一项 fork = false ，即不以守护进程方式运行mongodb cat >/etc/supervisor/config.d/mongodb.ini 将mongodb加入supervisor $ supervisorctl update mongodb mongodb: added process group 查看状态 $ supervisorctl status mongodb mongodb RUNNING pid 30352, uptime 0:10:26 详细配置 [program:mongodb] command=/usr/local/mongodb/bin/mongod -f /data/db/mongodb/cfg/mongod.conf directory=/usr/local/mongodb autostart=true user=testinadmin mongodb配置文件 bind_ip = 172.20.1.40 logpath = /data/db/mongodb/logs/mongod.log logappend = true pidfilepath = /data/db/mongodb/pid/mongod.pid dbpath = /data/db/mongodb/data storageEngine = wiredTiger directoryperdb = true #replSet = replset #rest = true oplogSize = 61440 #fork = true auth = false shardsvr = true port = 27010 journal = true maxConns = 30000 master = true #slave = true #source = 10.31.133.145:27010 #source = 10.47.125.99:27010 autoresync=true 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/supervisor/7.supervisor管理gitbook.html":{"url":"linux/linux服务/supervisor/7.supervisor管理gitbook.html","title":"supervisor管理gitbook","keywords":"","body":"supervisor管理gitbook supervisor配置文件 /etc/supervisor/supervisord.conf 中定义了include，因此如果想要管理服务，就需要编辑 /etc/supervisor/config.d/*.ini 文件 [include] files = /etc/supervisor/config.d/*.ini 编辑gitbook服务配置文件 /etc/supervisor/config.d/gitbook.ini ⚠️gitbook不能单独启动，必须指定gitbook工作目录，添加参数 directory=/path 指定 [program:gitbook] command=/usr/local/node-v13.8.0-linux-x64/bin/gitbook serve directory=/gitbook ; 指定gitbook文件目录(必须！！！) priority=1 ; 数字越高，优先级越高 numprocs=1 ; 启动几个进程 autostart=true ; 随着supervisord的启动而启动 autorestart=true ; 自动重启 startretries=10 ; 启动失败时的最多重试次数 exitcodes=0 ; 正常退出代码 stopsignal=KILL ; 用来杀死进程的信号 stopwaitsecs=10 ; 发送SIGKILL前的等待时间 redirect_stderr=true ; 重定向stderr到stdout stdout_logfile_maxbytes = 1024MB stdout_logfile_backups = 10 stdout_logfile = /var/log/supervisord/gitbook.log 将gitbook加入supervisor $ supervisorctl update gitbook gitbook: added process group 查看状态 $ supervisorctl status gitbook gitbook RUNNING pid 27311, uptime 0:07:53 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/supervisor/8.supervisor管理php.html":{"url":"linux/linux服务/supervisor/8.supervisor管理php.html","title":"supervisor管理php","keywords":"","body":"supervisor管理php supervisor配置文件 /etc/supervisor/supervisord.conf 中定义了include，因此如果想要管理服务，就需要编辑 /etc/supervisor/config.d/*.ini 文件 [include] files = /etc/supervisor/config.d/*.ini 编辑php服务配置文件/etc/supervisor/config.d/php72.ini cat >/etc/supervisor/config.d/php72.ini 将php加入supervisor $ supervisorctl update php72 php72: added process group 查看状态 $ supervisorctl status php72 php72 RUNNING pid 30352, uptime 0:10:26 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/supervisor/9.supervisor管理大数据任务.html":{"url":"linux/linux服务/supervisor/9.supervisor管理大数据任务.html","title":"supervisor管理大数据任务","keywords":"","body":"supervisor管理大数据任务 supervisor配置文件 /etc/supervisor/supervisord.conf 中定义了include，因此如果想要管理服务，就需要编辑 /etc/supervisor/config.d/*.ini 文件 [include] files = /etc/supervisor/config.d/*.ini 编辑服务配置文件 /etc/supervisor/config.d/RealtimeReadKafkaDemo3.ini 由于机器上有多个大数据任务，命名的时候一定要规范，最好有唯一标识符，比如这里我们以类名命名 大数据任务分为实时和离线，这个一般以类名或者包名区分 [program:RealtimeReadKafkaDemo3] command=/usr/local/service/spark/bin/spark-submit --class com.xmadx.game.realTime.RealtimeReadKafkaDemo3 --master local[*] --driver-memory 2g --executor-memory 6g --num-executors 5 --executor-cores 3 --conf \"spark.streaming.backpressure.enabled=true\" --conf spark.default.parallelism=75 --conf \"spark.streaming.kafka.maxRatePerPartition=5000\" --driver-class-path /usr/local/service/hbase/phoenix-client/phoenix-4.11.0-HBase-1.3-client.jar /home/hadoop/mrpora/xmadx-game-0.0.1-SNAPSHOT.jar autostart=true autorestart=true environment=JAVA_HOME=\"/usr/local/jdk\" stdout_logfile=/home/hadoop/mrpora/ss-xmadx-game.log redirect_stderr=true user=hadoop 将服务加入supervisor $ supervisorctl update RealtimeReadKafkaDemo3 RealtimeReadKafkaDemo3: added process group 查看状态 $ supervisorctl status RealtimeReadKafkaDemo3 RUNNING pid 91417, uptime 0:00:02 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/supervisor/supervisor自定义服务文件参数.html":{"url":"linux/linux服务/supervisor/supervisor自定义服务文件参数.html","title":"supervisor自定义服务文件参数","keywords":"","body":"supervisor自定义服务文件参数 supervisor配置文件 /etc/supervisor/supervisord.conf 中定义了include，因此如果想要管理服务，就需要编辑 /etc/supervisor/config.d/*.ini 文件，以下为 /etc/supervisor/config.d/*.ini 文件中用到的参数 参数 说明 command 启动程序使用的命令，可以是绝对路径或者相对路径 process_name 一个python字符串表达式，用来表示supervisor进程启动的这个的名称，默认值是%(program_name)s numprocs Supervisor启动这个程序的多个实例，如果numprocs>1，则process_name的表达式必须包含%(process_num)s，默认是1 numprocs_start 一个int偏移值，当启动实例的时候用来计算numprocs的值 priority 权重，可以控制程序启动和关闭时的顺序，权重越低：越早启动，越晚关闭。默认值是999 autostart 如果设置为true，当supervisord启动的时候，进程会自动重启 autorestart 值可以是false、true、unexpected。false：进程不会自动重启，unexpected：当程序退出时的退出码不是exitcodes中定义的时，进程会重启，true：进程会无条件重启当退出的时候 startsecs 程序启动后等待多长时间后才认为程序启动成功 startretries supervisord尝试启动一个程序时尝试的次数。默认是3 exitcodes 一个预期的退出返回码，默认是0,2 stopsignal 当收到stop请求的时候，发送信号给程序，默认是TERM信号，也可以是 HUP, INT, QUIT, KILL, USR1, or USR2 stopwaitsecs 在操作系统给supervisord发送SIGCHILD信号时等待的时间 stopasgroup 如果设置为true，则会使supervisor发送停止信号到整个进程组 killasgroup 如果设置为true，则在给程序发送SIGKILL信号的时候，会发送到整个进程组，它的子进程也会受到影响 user 如果supervisord以root运行，则会使用这个设置用户启动子程序 redirect_stderr 如果设置为true，进程则会把标准错误输出到supervisord后台的标准输出文件描述符 stdout_logfile 把进程的标准输出写入文件中，如果stdout_logfile没有设置或者设置为AUTO，则supervisor会自动选择一个文件位置 stdout_logfile_maxbytes 标准输出log文件达到多少后自动进行轮转，单位是KB、MB、GB。如果设置为0则表示不限制日志文件大小 stdout_logfile_backups 标准输出日志轮转备份的数量，默认是10，如果设置为0，则不备份 stdout_capture_maxbytes 当进程处于stderr capture mode模式的时候，写入FIFO队列的最大bytes值，单位可以是KB、MB、GB stdout_events_enabled 如果设置为true，当进程在写它的stderr到文件描述符的时候，PROCESS_LOG_STDERR事件会被触发 stderr_logfile 把进程的错误日志输出一个文件中，除非redirect_stderr参数被设置为true stderr_logfile_maxbytes 错误log文件达到多少后自动进行轮转，单位是KB、MB、GB。如果设置为0则表示不限制日志文件大小 stderr_logfile_backups 错误日志轮转备份的数量，默认是10，如果设置为0，则不备份 stderr_capture_maxbytes 当进程处于stderr capture mode模式的时候，写入FIFO队列的最大bytes值，单位可以是KB、MB、GB stderr_events_enabled 如果设置为true，当进程在写它的stderr到文件描述符的时候，PROCESS_LOG_STDERR事件会被触发 environment 一个k/v对的list列表 directory supervisord在生成子进程的时候会切换到该目录 umask 设置进程的umask serverurl 是否允许子进程和内部的HTTP服务通讯，如果设置为AUTO，supervisor会自动的构造一个url 重要参数 服务自动重启设置，supervisor提供了当服务挂掉时自动重启服务的功能autorestart autorestart可以写在supervisor配置文件中，也可以写在服务自定义文件中（supervisor配置文件中include指定的目录下的文件/etc/supervisor/config.d/*.ini） autorestart值可以是false、true、unexpected 值 说明 false 进程不会自动重启 unexpected 当程序退出时的退出码不是exitcodes中定义的时侯，进程会重启 true 当服务退出的时候进程会无条件重启 示例： [program:nginx] command = /usr/sbin/nginx -g 'daemon off;' stdout_logfile = /var/log/nginx/nginx.log redirect_stderr = true autorestart = unexpected 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/VPN/CentOS7一键安装OpenVPN.html":{"url":"linux/linux服务/VPN/CentOS7一键安装OpenVPN.html","title":"OpenVPN一键安装","keywords":"","body":"[toc] CentOS7一键安装OpenVPN openvpn github地址 openvpn官网 需求及使用场景 公司的一些资源不想对外开放访问，例如gitlab、jenkins等等，现在想要的效果是部分资源只允许公司公网IP以及特定IP访问，这个时候就需要用到VPN了，但是公司花钱买VPN是不可能的，那么就需要一款免费好用的VPN，OpenVPN免费开源又好用，配置完OpenVPN后再加上云主机的安全组就完美解决问题了。 说明 系统 openvpn版本 内网IP openvpn分配客户端网段 CentOS7.9 2.4.11 10.206.0.9 10.8.0.0 1.安装OpenVPN 1.1 克隆项目 git clone https://github.com.cnpmjs.org/Nyr/openvpn-install.git 1.2 执行安装脚本 cd openvpn-install && sh openvpn-install.sh ⚠️安装完成后再次执行脚本会提示如下 OpenVPN is already installed. Select an option: # 添加新的客户端 1) Add a new client # 移除已存在的客户端 2) Revoke an existing client # 移除OpenVPN 3) Remove OpenVPN # 退出 4) Exit Option: 第一步、输入本机私网IP地址 Welcome to this OpenVPN road warrior installer! Which IPv4 address should be used? 1) 10.9.95.147 2) 172.17.0.1 3) 172.20.0.1 IPv4 address [1]: 1 第二步、输入本机公网IP This server is behind NAT. What is the public IPv4 address or hostname? Public IPv4 address / hostname [8.8.8.8]: 8.8.8.8 第三步、选择OpenVPN协议，推荐使用UDP Which protocol should OpenVPN use? 1) UDP (recommended) 2) TCP Protocol [1]: 1 第四步、输入OpenVPN监听的端口 What port should OpenVPN listen to? Port [1194]: 第五步、为客户端选择一个DNS服务器 Select a DNS server for the clients: 1) Current system resolvers 2) Google 3) 1.1.1.1 4) OpenDNS 5) Quad9 6) AdGuard DNS server [1]: 1 第六步、为第一个客户端输入一个名称 Enter a name for the first client: Name [client]: 第七步、按任意键开始安装 OpenVPN installation is ready to begin. Press any key to continue... 完整输出 OpenVPN installation is ready to begin. Press any key to continue... Loaded plugins: fastestmirror Determining fastest mirrors 10gen | 1.2 kB 00:00:00 base | 3.6 kB 00:00:00 centosplus | 2.9 kB 00:00:00 docker-ce-stable | 3.5 kB 00:00:00 epel | 4.7 kB 00:00:00 extras | 2.9 kB 00:00:00 nginx-stable | 2.9 kB 00:00:00 updates | 2.9 kB 00:00:00 zabbix | 2.9 kB 00:00:00 zabbix-non-supported | 951 B 00:00:00 (1/7): 10gen/primary | 32 kB 00:00:00 (2/7): extras/7/x86_64/primary_db | 222 kB 00:00:00 (3/7): epel/x86_64/updateinfo | 1.0 MB 00:00:00 (4/7): centosplus/7/x86_64/primary_db | 1.6 MB 00:00:01 (5/7): base/7/x86_64/primary_db | 6.1 MB 00:00:01 (6/7): updates/7/x86_64/primary_db | 3.7 MB 00:00:01 (7/7): epel/x86_64/primary_db | 6.9 MB 00:00:01 10gen 279/279 Resolving Dependencies --> Running transaction check ---> Package epel-release.noarch 0:7-13 will be installed --> Finished Dependency Resolution Dependencies Resolved ======================================================================================================================================================== Package Arch Version Repository Size ======================================================================================================================================================== Installing: epel-release noarch 7-13 epel 15 k Transaction Summary ======================================================================================================================================================== Install 1 Package Total download size: 15 k Installed size: 25 k Downloading packages: epel-release-7-13.noarch.rpm | 15 kB 00:00:00 Running transaction check Running transaction test Transaction test succeeded Running transaction Installing : epel-release-7-13.noarch 1/1 warning: /etc/yum.repos.d/epel.repo created as /etc/yum.repos.d/epel.repo.rpmnew Verifying : epel-release-7-13.noarch 1/1 Installed: epel-release.noarch 0:7-13 Complete! Loaded plugins: fastestmirror Loading mirror speeds from cached hostfile Package 1:openssl-1.0.2k-19.el7.x86_64 already installed and latest version Package 2:tar-1.26-35.el7.x86_64 already installed and latest version Resolving Dependencies --> Running transaction check ---> Package ca-certificates.noarch 0:2019.2.32-76.el7_7 will be updated ---> Package ca-certificates.noarch 0:2020.2.41-70.0.el7_8 will be an update ---> Package openvpn.x86_64 0:2.4.9-1.el7 will be installed --> Processing Dependency: libpkcs11-helper.so.1()(64bit) for package: openvpn-2.4.9-1.el7.x86_64 --> Running transaction check ---> Package pkcs11-helper.x86_64 0:1.11-3.el7 will be installed --> Finished Dependency Resolution Dependencies Resolved ======================================================================================================================================================== Package Arch Version Repository Size ======================================================================================================================================================== Installing: openvpn x86_64 2.4.9-1.el7 epel 524 k Updating: ca-certificates noarch 2020.2.41-70.0.el7_8 base 382 k Installing for dependencies: pkcs11-helper x86_64 1.11-3.el7 epel 56 k Transaction Summary ======================================================================================================================================================== Install 1 Package (+1 Dependent package) Upgrade 1 Package Total download size: 962 k Downloading packages: Delta RPMs disabled because /usr/bin/applydeltarpm not installed. (1/3): ca-certificates-2020.2.41-70.0.el7_8.noarch.rpm | 382 kB 00:00:00 (2/3): pkcs11-helper-1.11-3.el7.x86_64.rpm | 56 kB 00:00:00 (3/3): openvpn-2.4.9-1.el7.x86_64.rpm | 524 kB 00:00:00 -------------------------------------------------------------------------------------------------------------------------------------------------------- Total 3.4 MB/s | 962 kB 00:00:00 Running transaction check Running transaction test Transaction test succeeded Running transaction Installing : pkcs11-helper-1.11-3.el7.x86_64 1/4 Installing : openvpn-2.4.9-1.el7.x86_64 2/4 Updating : ca-certificates-2020.2.41-70.0.el7_8.noarch 3/4 Cleanup : ca-certificates-2019.2.32-76.el7_7.noarch 4/4 Verifying : ca-certificates-2020.2.41-70.0.el7_8.noarch 1/4 Verifying : openvpn-2.4.9-1.el7.x86_64 2/4 Verifying : pkcs11-helper-1.11-3.el7.x86_64 3/4 Verifying : ca-certificates-2019.2.32-76.el7_7.noarch 4/4 Installed: openvpn.x86_64 0:2.4.9-1.el7 Dependency Installed: pkcs11-helper.x86_64 0:1.11-3.el7 Updated: ca-certificates.noarch 0:2020.2.41-70.0.el7_8 Complete! init-pki complete; you may now create a CA or requests. Your newly created PKI dir is: /etc/openvpn/server/easy-rsa/pki Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Generating RSA private key, 2048 bit long modulus ....+++ ...................................+++ e is 65537 (0x10001) Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Generating a 2048 bit RSA private key .................................................................+++ ....+++ writing new private key to '/etc/openvpn/server/easy-rsa/pki/easy-rsa-2726385.U7ScUb/tmp.FTK8rI' ----- Using configuration from /etc/openvpn/server/easy-rsa/pki/easy-rsa-2726385.U7ScUb/tmp.9FN60w Check that the request matches the signature Signature ok The Subject's Distinguished Name is as follows commonName :ASN.1 12:'server' Certificate is to be certified until Nov 28 02:24:33 2030 GMT (3650 days) Write out database with 1 new entries Data Base Updated Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Generating a 2048 bit RSA private key .....+++ ........................................................................................+++ writing new private key to '/etc/openvpn/server/easy-rsa/pki/easy-rsa-2726473.aJtBJi/tmp.bmyQVL' ----- Using configuration from /etc/openvpn/server/easy-rsa/pki/easy-rsa-2726473.aJtBJi/tmp.zwz1tQ Check that the request matches the signature Signature ok The Subject's Distinguished Name is as follows commonName :ASN.1 12:'pptfz' Certificate is to be certified until Nov 28 02:24:34 2030 GMT (3650 days) Write out database with 1 new entries Data Base Updated Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Using configuration from /etc/openvpn/server/easy-rsa/pki/easy-rsa-2726540.Fvyapy/tmp.eJmfVQ An updated CRL has been created. CRL file: /etc/openvpn/server/easy-rsa/pki/crl.pem Created symlink from /etc/systemd/system/multi-user.target.wants/openvpn-iptables.service to /etc/systemd/system/openvpn-iptables.service. Created symlink from /etc/systemd/system/multi-user.target.wants/openvpn-server@server.service to /usr/lib/systemd/system/openvpn-server@.service. Finished! The client configuration is available in: /root/pptfz.ovpn New clients can be added by running this script again. 客户端文件是 /root/pptfz.ovpn ，在最后的输出中有提示，这里的客户端文件名称是自定义的，然后把这个文件下载到本地，后续配置VPN认证的时候需要用到这个客户端文件 1.3 查看OpenVPN相关信息 1.3.1 查看OpenVPN进程 默认是以 nobody 用户运行，在 /etc/openvpn/server/server.conf 中可以自定义 $ ps aux|grep [o]penvpn nobody 2726623 0.0 0.2 77168 4040 ? Ss 10:24 0:00 /usr/sbin/openvpn --status /run/openvpn-server/status-server.log --status-version 2 --suppress-timestamps --config server.conf 1.3.2 OpenVPN默认监听 udp 1194 端口 $ netstat -nupl|grep 1194 udp 0 0 10.9.95.147:1194 0.0.0.0:* 2726623/openvpn 1.3.3 查看版本 $ openvpn --version OpenVPN 2.4.11 x86_64-redhat-linux-gnu [Fedora EPEL patched] [SSL (OpenSSL)] [LZO] [LZ4] [EPOLL] [PKCS11] [MH/PKTINFO] [AEAD] built on Apr 21 2021 library versions: OpenSSL 1.0.2k-fips 26 Jan 2017, LZO 2.06 Originally developed by James Yonan Copyright (C) 2002-2018 OpenVPN Inc Compile time defines: enable_async_push=no enable_comp_stub=no enable_crypto=yes enable_crypto_ofb_cfb=yes enable_debug=yes enable_def_auth=yes enable_dependency_tracking=no enable_dlopen=unknown enable_dlopen_self=unknown enable_dlopen_self_static=unknown enable_fast_install=yes enable_fragment=yes enable_iproute2=yes enable_libtool_lock=yes enable_lz4=yes enable_lzo=yes enable_management=yes enable_multihome=yes enable_pam_dlopen=no enable_pedantic=no enable_pf=yes enable_pkcs11=yes enable_plugin_auth_pam=yes enable_plugin_down_root=yes enable_plugins=yes enable_port_share=yes enable_selinux=yes enable_server=yes enable_shared=yes enable_shared_with_static_runtimes=no enable_small=no enable_static=yes enable_strict=no enable_strict_options=no enable_systemd=yes enable_werror=no enable_win32_dll=yes enable_x509_alt_username=yes with_aix_soname=aix with_crypto_library=openssl with_gnu_ld=yes with_mem_check=no with_sysroot=no 2.配置OnenVPN使用账号密码认证 2.1 编辑脚本 这个是现在公司线上用到的文件，目前没有找到出处，不知道为什么，总之就是利用一个存放用户名密码的自定义文件 /etc/openvpn/psw-file 来作为认证文件 编辑如下脚本，后续openvpn的配置文件 /etc/openvpn/server/server.conf 中会引用这个脚本 ⚠️openvpn运行用户对于这个脚本至少有rx权限，否则认证会失败 cat >> /etc/openvpn/checkpsw.sh # # This script will authenticate OpenVPN users against # a plain text file. The passfile should simply contain # one row per user with the username first followed by # one or more space(s) or tab(s) and then the password. PASSFILE=\"/etc/openvpn/psw-file\" LOG_FILE=\"/etc/openvpn/openvpn-password.log\" TIME_STAMP=`date \"+%Y-%m-%d %T\"` ########################################################### if [ ! -r \"${PASSFILE}\" ]; then echo \"${TIME_STAMP}: Could not open password file \\\"${PASSFILE}\\\" for reading.\" >> ${LOG_FILE} exit 1 fi CORRECT_PASSWORD=`awk '!/^;/&&!/^#/&&$1==\"'${username}'\"{print $2;exit}' ${PASSFILE}` if [ \"${CORRECT_PASSWORD}\" = \"\" ]; then echo \"${TIME_STAMP}: User does not exist: username=\\\"${username}\\\", password=\\\"${password}\\\".\" >> ${LOG_FILE} exit 1 fi if [ \"${password}\" = \"${CORRECT_PASSWORD}\" ]; then echo \"${TIME_STAMP}: Successful authentication: username=\\\"${username}\\\".\" >> ${LOG_FILE} exit 0 fi echo \"${TIME_STAMP}: Incorrect password: username=\\\"${username}\\\", password=\\\"${password}\\\".\" >> ${LOG_FILE} exit 1 EOF 赋予脚本执行权限 chmod +x /etc/openvpn/checkpsw.sh 2.2 修改openvpn配置文件 /etc/openvpn/server/server.conf 2.2.1 追加以下内容 其中 auth-user-pass-verify 对应的文件一定要与上一步创建的脚本名相同 cat >> /etc/openvpn/server/server.conf /etc/openvpn/server/server.conf 文件默认内容 local 10.206.0.9 port 1194 proto udp dev tun ca ca.crt cert server.crt key server.key dh dh.pem auth SHA512 tls-crypt tc.key topology subnet server 10.8.0.0 255.255.255.0 push \"redirect-gateway def1 bypass-dhcp\" ifconfig-pool-persist ipp.txt push \"dhcp-option DNS 183.60.83.19\" push \"dhcp-option DNS 183.60.82.98\" keepalive 10 120 cipher AES-256-CBC user nobody group nobody persist-key persist-tun verb 3 crl-verify crl.pem explicit-exit-notify 2.2.2 修改 server.conf 删除开头的 local 一行 sed -i.bak '1d' /etc/openvpn/server/server.conf 2.3 重启服务 systemctl enable openvpn@server.service systemctl restart openvpn-server@server 2.4 添加账号 后续的账号都在这个文件 /etc/openvpn/psw-file 中添加，用户名和密码以空格隔开，每行一个 # 用户名密码文件 cat > /etc/openvpn/psw-file 2.5 修改客户端文件 执行完一键安装脚本后会提示客户端文件位置 The client configuration is available in: /root/pptfz.ovpn New clients can be added by running this script again. 客户端文件需要追加一行 auth-user-pass 内容，开启用户名密码认证，然后下载到本地 sed -i '14aauth-user-pass' /root/pptfz.ovpn 3.客户端安装配置 3.1 mac连接示例 这里以mac为例，我下载的是 Tunnelblick Tunnelblick github地址 安装完成后把客户端文件 pptfz.ovpn(文件名是自定义的) 拖入到 Tunnelblick 选择安装的用户 输入本机密码 输入完成后会提示如下 找到相应图标，点击要连接的VPN 输入用户名和密码 连接成功 连接成功后，就会在本机生成一个 utun1 的虚拟网卡，并获取openvpn server.conf 中设置的 server 10.8.0.0 255.255.255.0 给客户端分配的网段，IP地址从 10.8.0.2 开始分配 utun1: flags=8051 mtu 1500 inet 10.8.0.2 --> 10.8.0.2 netmask 0xffffff00 此时mac本机是能与服务器内网相互ping通的 mac本机ping服务器内网 $ ping -c2 10.206.0.9 PING 10.206.0.9 (10.206.0.9): 56 data bytes 64 bytes from 10.206.0.9: icmp_seq=0 ttl=64 time=90.547 ms 64 bytes from 10.206.0.9: icmp_seq=1 ttl=64 time=63.691 ms --- 10.206.0.9 ping statistics --- 2 packets transmitted, 2 packets received, 0.0% packet loss round-trip min/avg/max/stddev = 63.691/77.119/90.547/13.428 ms 服务器ping mac本机 $ ping -c2 10.8.0.2 PING 10.8.0.2 (10.8.0.2) 56(84) bytes of data. 64 bytes from 10.8.0.2: icmp_seq=1 ttl=64 time=85.3 ms 64 bytes from 10.8.0.2: icmp_seq=2 ttl=64 time=90.3 ms --- 10.8.0.2 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1000ms rtt min/avg/max/mdev = 85.362/87.876/90.390/2.514 ms 3.2 windows连接示例 下载windows版安装包 windows安装就是一路下一步 导入客户端文件 输入用户名和密码 连接成功 4.云主机安全组配置说明 使用示例 假设公司代码库gitlab、上线工具jenkins等等部署在云主机中，现在只允许公司IP和openvpn IP访问 则只需要在安全组中加入以下两条规则即可 1.允许公司IP访问 2.允许OpenVPN所在机器公网IP访问 在云主机中还可以根据具体情况设置相关安全策略，包括网络ACL访问控制，安全组等等 openvpn+云主机安全组的应用场景 如果服务所在机器是经典网络(有公网IP)，则内网和公网都可以访问 如果服务所在机器是VPC网络(无公网IP)，则只允许内网访问 5.脚本内容 #!/bin/bash # # https://github.com/Nyr/openvpn-install # # Copyright (c) 2013 Nyr. Released under the MIT License. # Detect Debian users running the script with \"sh\" instead of bash if readlink /proc/$$/exe | grep -q \"dash\"; then echo 'This installer needs to be run with \"bash\", not \"sh\".' exit fi # Discard stdin. Needed when running from an one-liner which includes a newline read -N 999999 -t 0.001 # Detect OpenVZ 6 if [[ $(uname -r | cut -d \".\" -f 1) -eq 2 ]]; then echo \"The system is running an old kernel, which is incompatible with this installer.\" exit fi # Detect OS # $os_version variables aren't always in use, but are kept here for convenience if grep -qs \"ubuntu\" /etc/os-release; then os=\"ubuntu\" os_version=$(grep 'VERSION_ID' /etc/os-release | cut -d '\"' -f 2 | tr -d '.') group_name=\"nogroup\" elif [[ -e /etc/debian_version ]]; then os=\"debian\" os_version=$(grep -oE '[0-9]+' /etc/debian_version | head -1) group_name=\"nogroup\" elif [[ -e /etc/almalinux-release || -e /etc/rocky-release || -e /etc/centos-release ]]; then os=\"centos\" os_version=$(grep -shoE '[0-9]+' /etc/almalinux-release /etc/rocky-release /etc/centos-release | head -1) group_name=\"nobody\" elif [[ -e /etc/fedora-release ]]; then os=\"fedora\" os_version=$(grep -oE '[0-9]+' /etc/fedora-release | head -1) group_name=\"nobody\" else echo \"This installer seems to be running on an unsupported distribution. Supported distros are Ubuntu, Debian, AlmaLinux, Rocky Linux, CentOS and Fedora.\" exit fi if [[ \"$os\" == \"ubuntu\" && \"$os_version\" -lt 1804 ]]; then echo \"Ubuntu 18.04 or higher is required to use this installer. This version of Ubuntu is too old and unsupported.\" exit fi if [[ \"$os\" == \"debian\" && \"$os_version\" -lt 9 ]]; then echo \"Debian 9 or higher is required to use this installer. This version of Debian is too old and unsupported.\" exit fi if [[ \"$os\" == \"centos\" && \"$os_version\" -lt 7 ]]; then echo \"CentOS 7 or higher is required to use this installer. This version of CentOS is too old and unsupported.\" exit fi # Detect environments where $PATH does not include the sbin directories if ! grep -q sbin /dev/net/tun ) 2>/dev/null; then echo \"The system does not have the TUN device available. TUN needs to be enabled before running this installer.\" exit fi new_client () { # Generates the custom client.ovpn { cat /etc/openvpn/server/client-common.txt echo \"\" cat /etc/openvpn/server/easy-rsa/pki/ca.crt echo \"\" echo \"\" sed -ne '/BEGIN CERTIFICATE/,$ p' /etc/openvpn/server/easy-rsa/pki/issued/\"$client\".crt echo \"\" echo \"\" cat /etc/openvpn/server/easy-rsa/pki/private/\"$client\".key echo \"\" echo \"\" sed -ne '/BEGIN OpenVPN Static key/,$ p' /etc/openvpn/server/tc.key echo \"\" } > ~/\"$client\".ovpn } if [[ ! -e /etc/openvpn/server/server.conf ]]; then # Detect some Debian minimal setups where neither wget nor curl are installed if ! hash wget 2>/dev/null && ! hash curl 2>/dev/null; then echo \"Wget is required to use this installer.\" read -n1 -r -p \"Press any key to install Wget and continue...\" apt-get update apt-get install -y wget fi clear echo 'Welcome to this OpenVPN road warrior installer!' # If system has a single IPv4, it is selected automatically. Else, ask the user if [[ $(ip -4 addr | grep inet | grep -vEc '127(\\.[0-9]{1,3}){3}') -eq 1 ]]; then ip=$(ip -4 addr | grep inet | grep -vE '127(\\.[0-9]{1,3}){3}' | cut -d '/' -f 1 | grep -oE '[0-9]{1,3}(\\.[0-9]{1,3}){3}') else number_of_ip=$(ip -4 addr | grep inet | grep -vEc '127(\\.[0-9]{1,3}){3}') echo echo \"Which IPv4 address should be used?\" ip -4 addr | grep inet | grep -vE '127(\\.[0-9]{1,3}){3}' | cut -d '/' -f 1 | grep -oE '[0-9]{1,3}(\\.[0-9]{1,3}){3}' | nl -s ') ' read -p \"IPv4 address [1]: \" ip_number until [[ -z \"$ip_number\" || \"$ip_number\" =~ ^[0-9]+$ && \"$ip_number\" -le \"$number_of_ip\" ]]; do echo \"$ip_number: invalid selection.\" read -p \"IPv4 address [1]: \" ip_number done [[ -z \"$ip_number\" ]] && ip_number=\"1\" ip=$(ip -4 addr | grep inet | grep -vE '127(\\.[0-9]{1,3}){3}' | cut -d '/' -f 1 | grep -oE '[0-9]{1,3}(\\.[0-9]{1,3}){3}' | sed -n \"$ip_number\"p) fi # If $ip is a private IP address, the server must be behind NAT if echo \"$ip\" | grep -qE '^(10\\.|172\\.1[6789]\\.|172\\.2[0-9]\\.|172\\.3[01]\\.|192\\.168)'; then echo echo \"This server is behind NAT. What is the public IPv4 address or hostname?\" # Get public IP and sanitize with grep get_public_ip=$(grep -m 1 -oE '^[0-9]{1,3}(\\.[0-9]{1,3}){3}$' /dev/null; then if [[ \"$os\" == \"centos\" || \"$os\" == \"fedora\" ]]; then firewall=\"firewalld\" # We don't want to silently enable firewalld, so we give a subtle warning # If the user continues, firewalld will be installed and enabled during setup echo \"firewalld, which is required to manage routing tables, will also be installed.\" elif [[ \"$os\" == \"debian\" || \"$os\" == \"ubuntu\" ]]; then # iptables is way less invasive than firewalld so no warning is given firewall=\"iptables\" fi fi read -n1 -r -p \"Press any key to continue...\" # If running inside a container, disable LimitNPROC to prevent conflicts if systemd-detect-virt -cq; then mkdir /etc/systemd/system/openvpn-server@server.service.d/ 2>/dev/null echo \"[Service] LimitNPROC=infinity\" > /etc/systemd/system/openvpn-server@server.service.d/disable-limitnproc.conf fi if [[ \"$os\" = \"debian\" || \"$os\" = \"ubuntu\" ]]; then apt-get update apt-get install -y openvpn openssl ca-certificates $firewall elif [[ \"$os\" = \"centos\" ]]; then yum install -y epel-release yum install -y openvpn openssl ca-certificates tar $firewall else # Else, OS must be Fedora dnf install -y openvpn openssl ca-certificates tar $firewall fi # If firewalld was just installed, enable it if [[ \"$firewall\" == \"firewalld\" ]]; then systemctl enable --now firewalld.service fi # Get easy-rsa easy_rsa_url='https://github.com/OpenVPN/easy-rsa/releases/download/v3.0.8/EasyRSA-3.0.8.tgz' mkdir -p /etc/openvpn/server/easy-rsa/ { wget -qO- \"$easy_rsa_url\" 2>/dev/null || curl -sL \"$easy_rsa_url\" ; } | tar xz -C /etc/openvpn/server/easy-rsa/ --strip-components 1 chown -R root:root /etc/openvpn/server/easy-rsa/ cd /etc/openvpn/server/easy-rsa/ # Create the PKI, set up the CA and the server and client certificates ./easyrsa init-pki ./easyrsa --batch build-ca nopass EASYRSA_CERT_EXPIRE=3650 ./easyrsa build-server-full server nopass EASYRSA_CERT_EXPIRE=3650 ./easyrsa build-client-full \"$client\" nopass EASYRSA_CRL_DAYS=3650 ./easyrsa gen-crl # Move the stuff we need cp pki/ca.crt pki/private/ca.key pki/issued/server.crt pki/private/server.key pki/crl.pem /etc/openvpn/server # CRL is read with each client connection, while OpenVPN is dropped to nobody chown nobody:\"$group_name\" /etc/openvpn/server/crl.pem # Without +x in the directory, OpenVPN can't run a stat() on the CRL file chmod o+x /etc/openvpn/server/ # Generate key for tls-crypt openvpn --genkey --secret /etc/openvpn/server/tc.key # Create the DH parameters file using the predefined ffdhe2048 group echo '-----BEGIN DH PARAMETERS----- MIIBCAKCAQEA//////////+t+FRYortKmq/cViAnPTzx2LnFg84tNpWp4TZBFGQz +8yTnc4kmz75fS/jY2MMddj2gbICrsRhetPfHtXV/WVhJDP1H18GbtCFY2VVPe0a 87VXE15/V8k1mE8McODmi3fipona8+/och3xWKE2rec1MKzKT0g6eXq8CrGCsyT7 YdEIqUuyyOP7uWrat2DX9GgdT0Kj3jlN9K5W7edjcrsZCwenyO4KbXCeAvzhzffi 7MA0BM0oNC9hkXL+nOmFg/+OTxIy7vKBg8P+OxtMb61zO7X8vC7CIAXFjvGDfRaD ssbzSibBsu/6iGtCOGEoXJf//////////wIBAg== -----END DH PARAMETERS-----' > /etc/openvpn/server/dh.pem # Generate server.conf echo \"local $ip port $port proto $protocol dev tun ca ca.crt cert server.crt key server.key dh dh.pem auth SHA512 tls-crypt tc.key topology subnet server 10.8.0.0 255.255.255.0\" > /etc/openvpn/server/server.conf # IPv6 if [[ -z \"$ip6\" ]]; then echo 'push \"redirect-gateway def1 bypass-dhcp\"' >> /etc/openvpn/server/server.conf else echo 'server-ipv6 fddd:1194:1194:1194::/64' >> /etc/openvpn/server/server.conf echo 'push \"redirect-gateway def1 ipv6 bypass-dhcp\"' >> /etc/openvpn/server/server.conf fi echo 'ifconfig-pool-persist ipp.txt' >> /etc/openvpn/server/server.conf # DNS case \"$dns\" in 1|\"\") # Locate the proper resolv.conf # Needed for systems running systemd-resolved if grep -q '^nameserver 127.0.0.53' \"/etc/resolv.conf\"; then resolv_conf=\"/run/systemd/resolve/resolv.conf\" else resolv_conf=\"/etc/resolv.conf\" fi # Obtain the resolvers from resolv.conf and use them for OpenVPN grep -v '^#\\|^;' \"$resolv_conf\" | grep '^nameserver' | grep -oE '[0-9]{1,3}(\\.[0-9]{1,3}){3}' | while read line; do echo \"push \\\"dhcp-option DNS $line\\\"\" >> /etc/openvpn/server/server.conf done ;; 2) echo 'push \"dhcp-option DNS 8.8.8.8\"' >> /etc/openvpn/server/server.conf echo 'push \"dhcp-option DNS 8.8.4.4\"' >> /etc/openvpn/server/server.conf ;; 3) echo 'push \"dhcp-option DNS 1.1.1.1\"' >> /etc/openvpn/server/server.conf echo 'push \"dhcp-option DNS 1.0.0.1\"' >> /etc/openvpn/server/server.conf ;; 4) echo 'push \"dhcp-option DNS 208.67.222.222\"' >> /etc/openvpn/server/server.conf echo 'push \"dhcp-option DNS 208.67.220.220\"' >> /etc/openvpn/server/server.conf ;; 5) echo 'push \"dhcp-option DNS 9.9.9.9\"' >> /etc/openvpn/server/server.conf echo 'push \"dhcp-option DNS 149.112.112.112\"' >> /etc/openvpn/server/server.conf ;; 6) echo 'push \"dhcp-option DNS 94.140.14.14\"' >> /etc/openvpn/server/server.conf echo 'push \"dhcp-option DNS 94.140.15.15\"' >> /etc/openvpn/server/server.conf ;; esac echo \"keepalive 10 120 cipher AES-256-CBC user nobody group $group_name persist-key persist-tun verb 3 crl-verify crl.pem\" >> /etc/openvpn/server/server.conf if [[ \"$protocol\" = \"udp\" ]]; then echo \"explicit-exit-notify\" >> /etc/openvpn/server/server.conf fi # Enable net.ipv4.ip_forward for the system echo 'net.ipv4.ip_forward=1' > /etc/sysctl.d/99-openvpn-forward.conf # Enable without waiting for a reboot or service restart echo 1 > /proc/sys/net/ipv4/ip_forward if [[ -n \"$ip6\" ]]; then # Enable net.ipv6.conf.all.forwarding for the system echo \"net.ipv6.conf.all.forwarding=1\" >> /etc/sysctl.d/99-openvpn-forward.conf # Enable without waiting for a reboot or service restart echo 1 > /proc/sys/net/ipv6/conf/all/forwarding fi if systemctl is-active --quiet firewalld.service; then # Using both permanent and not permanent rules to avoid a firewalld # reload. # We don't use --add-service=openvpn because that would only work with # the default port and protocol. firewall-cmd --add-port=\"$port\"/\"$protocol\" firewall-cmd --zone=trusted --add-source=10.8.0.0/24 firewall-cmd --permanent --add-port=\"$port\"/\"$protocol\" firewall-cmd --permanent --zone=trusted --add-source=10.8.0.0/24 # Set NAT for the VPN subnet firewall-cmd --direct --add-rule ipv4 nat POSTROUTING 0 -s 10.8.0.0/24 ! -d 10.8.0.0/24 -j SNAT --to \"$ip\" firewall-cmd --permanent --direct --add-rule ipv4 nat POSTROUTING 0 -s 10.8.0.0/24 ! -d 10.8.0.0/24 -j SNAT --to \"$ip\" if [[ -n \"$ip6\" ]]; then firewall-cmd --zone=trusted --add-source=fddd:1194:1194:1194::/64 firewall-cmd --permanent --zone=trusted --add-source=fddd:1194:1194:1194::/64 firewall-cmd --direct --add-rule ipv6 nat POSTROUTING 0 -s fddd:1194:1194:1194::/64 ! -d fddd:1194:1194:1194::/64 -j SNAT --to \"$ip6\" firewall-cmd --permanent --direct --add-rule ipv6 nat POSTROUTING 0 -s fddd:1194:1194:1194::/64 ! -d fddd:1194:1194:1194::/64 -j SNAT --to \"$ip6\" fi else # Create a service to set up persistent iptables rules iptables_path=$(command -v iptables) ip6tables_path=$(command -v ip6tables) # nf_tables is not available as standard in OVZ kernels. So use iptables-legacy # if we are in OVZ, with a nf_tables backend and iptables-legacy is available. if [[ $(systemd-detect-virt) == \"openvz\" ]] && readlink -f \"$(command -v iptables)\" | grep -q \"nft\" && hash iptables-legacy 2>/dev/null; then iptables_path=$(command -v iptables-legacy) ip6tables_path=$(command -v ip6tables-legacy) fi echo \"[Unit] Before=network.target [Service] Type=oneshot ExecStart=$iptables_path -t nat -A POSTROUTING -s 10.8.0.0/24 ! -d 10.8.0.0/24 -j SNAT --to $ip ExecStart=$iptables_path -I INPUT -p $protocol --dport $port -j ACCEPT ExecStart=$iptables_path -I FORWARD -s 10.8.0.0/24 -j ACCEPT ExecStart=$iptables_path -I FORWARD -m state --state RELATED,ESTABLISHED -j ACCEPT ExecStop=$iptables_path -t nat -D POSTROUTING -s 10.8.0.0/24 ! -d 10.8.0.0/24 -j SNAT --to $ip ExecStop=$iptables_path -D INPUT -p $protocol --dport $port -j ACCEPT ExecStop=$iptables_path -D FORWARD -s 10.8.0.0/24 -j ACCEPT ExecStop=$iptables_path -D FORWARD -m state --state RELATED,ESTABLISHED -j ACCEPT\" > /etc/systemd/system/openvpn-iptables.service if [[ -n \"$ip6\" ]]; then echo \"ExecStart=$ip6tables_path -t nat -A POSTROUTING -s fddd:1194:1194:1194::/64 ! -d fddd:1194:1194:1194::/64 -j SNAT --to $ip6 ExecStart=$ip6tables_path -I FORWARD -s fddd:1194:1194:1194::/64 -j ACCEPT ExecStart=$ip6tables_path -I FORWARD -m state --state RELATED,ESTABLISHED -j ACCEPT ExecStop=$ip6tables_path -t nat -D POSTROUTING -s fddd:1194:1194:1194::/64 ! -d fddd:1194:1194:1194::/64 -j SNAT --to $ip6 ExecStop=$ip6tables_path -D FORWARD -s fddd:1194:1194:1194::/64 -j ACCEPT ExecStop=$ip6tables_path -D FORWARD -m state --state RELATED,ESTABLISHED -j ACCEPT\" >> /etc/systemd/system/openvpn-iptables.service fi echo \"RemainAfterExit=yes [Install] WantedBy=multi-user.target\" >> /etc/systemd/system/openvpn-iptables.service systemctl enable --now openvpn-iptables.service fi # If SELinux is enabled and a custom port was selected, we need this if sestatus 2>/dev/null | grep \"Current mode\" | grep -q \"enforcing\" && [[ \"$port\" != 1194 ]]; then # Install semanage if not already present if ! hash semanage 2>/dev/null; then if [[ \"$os_version\" -eq 7 ]]; then # Centos 7 yum install -y policycoreutils-python else # CentOS 8 or Fedora dnf install -y policycoreutils-python-utils fi fi semanage port -a -t openvpn_port_t -p \"$protocol\" \"$port\" fi # If the server is behind NAT, use the correct IP address [[ -n \"$public_ip\" ]] && ip=\"$public_ip\" # client-common.txt is created so we have a template to add further users later echo \"client dev tun proto $protocol remote $ip $port resolv-retry infinite nobind persist-key persist-tun remote-cert-tls server auth SHA512 cipher AES-256-CBC ignore-unknown-option block-outside-dns block-outside-dns verb 3\" > /etc/openvpn/server/client-common.txt # Enable and start the OpenVPN service systemctl enable --now openvpn-server@server.service # Generates the custom client.ovpn new_client echo echo \"Finished!\" echo echo \"The client configuration is available in:\" ~/\"$client.ovpn\" echo \"New clients can be added by running this script again.\" else clear echo \"OpenVPN is already installed.\" echo echo \"Select an option:\" echo \" 1) Add a new client\" echo \" 2) Revoke an existing client\" echo \" 3) Remove OpenVPN\" echo \" 4) Exit\" read -p \"Option: \" option until [[ \"$option\" =~ ^[1-4]$ ]]; do echo \"$option: invalid selection.\" read -p \"Option: \" option done case \"$option\" in 1) echo echo \"Provide a name for the client:\" read -p \"Name: \" unsanitized_client client=$(sed 's/[^0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_-]/_/g' /dev/null | grep \"Current mode\" | grep -q \"enforcing\" && [[ \"$port\" != 1194 ]]; then semanage port -d -t openvpn_port_t -p \"$protocol\" \"$port\" fi systemctl disable --now openvpn-server@server.service rm -f /etc/systemd/system/openvpn-server@server.service.d/disable-limitnproc.conf rm -f /etc/sysctl.d/99-openvpn-forward.conf if [[ \"$os\" = \"debian\" || \"$os\" = \"ubuntu\" ]]; then rm -rf /etc/openvpn/server apt-get remove --purge -y openvpn else # Else, OS must be CentOS or Fedora yum remove -y openvpn rm -rf /etc/openvpn/server fi echo echo \"OpenVPN removed!\" else echo echo \"OpenVPN removal aborted!\" fi exit ;; 4) exit ;; esac fi 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/VPN/CentOS7手动安装OpenVPN.html":{"url":"linux/linux服务/VPN/CentOS7手动安装OpenVPN.html","title":"OpenVPN手动安装","keywords":"","body":"[toc] CentOS7手动安装OpenVPN 参考链接 1.前期环境准备 1.1 实验环境 OpenVPN软件版本 2.4.9 系统 IP 公网IP 配置 内核 CentOS7.9 172.16.0.71 8.8.8.8 1c1g 3.10.0-1160.11.1.el7.x86_64 mac本机 10.0.18.249 9.9.9.9 - - 1.2 OpenVPN机器配置必要修改 1.2.1 开启路由转发 # 不存在则配置路由转发 grep 'net.ipv4.ip_forward = 1' /etc/sysctl.conf || echo 'net.ipv4.ip_forward = 1' >> /etc/sysctl.conf # 使配置生效 sysctl -p 1.2.2 iptables配置 设置iptables规则 # 客户端连接vpn后，默认分配 10.8.0.0/24网段，需要进行nat设置 iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -o eth0 -j MASQUERADE # 配置开机启动 echo 'iptables -t nat -A POSTROUTING -s 10.7.0.0/24 -o eth0 -j MASQUERADE' >> /etc/rc.d/rc.local # 给rc.local文件增加可执行权限，否则开机不会执行 chmod u+x /etc/rc.d/rc.local 验证 # 验证 $ iptables -L -n -t nat Chain PREROUTING (policy ACCEPT) target prot opt source destination Chain INPUT (policy ACCEPT) target prot opt source destination Chain OUTPUT (policy ACCEPT) target prot opt source destination Chain POSTROUTING (policy ACCEPT) target prot opt source destination MASQUERADE all -- 10.8.0.0/24 0.0.0.0/0 删除上述iptables配置信息使用如下命令，可以对比正常和异常访问 iptables -t nat -D POSTROUTING 1 1.2.3 系统时间与硬件时间同步 # 配置时间同步 $ crontab -l */10 * * * * /usr/sbin/ntpdate ntp1.aliyun.com >/dev/null 2>&1 # 系统使用上海时间 $ ll /etc/localtime lrwxrwxrwx. 1 root root 35 Mar 7 2019 /etc/localtime -> ../usr/share/zoneinfo/Asia/Shanghai # 查看硬件时间 $ hwclock --show Thu 27 May 2021 02:49:25 PM CST -0.109431 seconds # 系统时间同步到硬件时间 hwclock --systohc 2.安装过程 2.1 安装依赖包 yum -y install lz4-devel lzo-devel pam-devel openssl-devel systemd-devel sqlite-devel autoconf automake libtool libtool-ltdl 2.2 编译安装openvpn openvpn github地址 openvpn官网 openvpn官网源码包下载地址 2.2.1 下载源码包 wget https://github.com/OpenVPN/openvpn/archive/v2.4.9.tar.gz 2.2.2 解压缩并进入源码目录 tar xf openvpn-2.4.9.tar.gz && cd openvpn-2.4.9/ 2.2.3 开始编译安装 nproc 命令可以直接获取系统核心数 # 生成configure文件 autoreconf -i -v -f # 编译安装 ./configure --prefix=/usr/local/openvpn --enable-lzo --enable-lz4 --enable-crypto --enable-server --enable-plugins --enable-port-share --enable-iproute2 --enable-pf --enable-plugin-auth-pam --enable-pam-dlopen --enable-systemd make -j${nproc} && make install 2.2.4 做一下openvpn命令的软连接 ln -s /usr/local/openvpn/sbin/openvpn /usr/local/sbin/openvpn 2.2.5 查看openvpn版本 $ openvpn --version OpenVPN 2.4.9 x86_64-unknown-linux-gnu [SSL (OpenSSL)] [LZO] [LZ4] [EPOLL] [MH/PKTINFO] [AEAD] built on Jul 20 2021 library versions: OpenSSL 1.0.2k-fips 26 Jan 2017, LZO 2.06 Originally developed by James Yonan Copyright (C) 2002-2018 OpenVPN Inc Compile time defines: enable_async_push=no enable_comp_stub=no enable_crypto=yes enable_crypto_ofb_cfb=yes enable_debug=yes enable_def_auth=yes enable_dlopen=unknown enable_dlopen_self=unknown enable_dlopen_self_static=unknown enable_fast_install=yes enable_fragment=yes enable_iproute2=yes enable_libtool_lock=yes enable_lz4=yes enable_lzo=yes enable_management=yes enable_multihome=yes enable_pam_dlopen=yes enable_pedantic=no enable_pf=yes enable_pkcs11=no enable_plugin_auth_pam=yes enable_plugin_down_root=yes enable_plugins=yes enable_port_share=yes enable_selinux=no enable_server=yes enable_shared=yes enable_shared_with_static_runtimes=no enable_small=no enable_static=yes enable_strict=no enable_strict_options=no enable_systemd=yes enable_werror=no enable_win32_dll=yes enable_x509_alt_username=no with_crypto_library=openssl with_gnu_ld=yes with_mem_check=no with_sysroot=no 2.2.6 修改启动配置文件 `/usr/local/openvpn/lib/systemd/system/` 在此目录下存放着openvpn的客户服务启动文件和openvpn的启动服务文件 编辑文件 /usr/local/openvpn/lib/systemd/system/openvpn-server@.service 修改 ExecStart=/usr/local/openvpn/sbin/openvpn --status %t/openvpn-server/status-%i.log --status-version 2 --suppress-timestamps --config %i.conf 修改为 ExecStart=/usr/local/openvpn/sbin/openvpn --config server.conf 使用如下命令修改 sed -i.bak '/^ExecStart/cExecStart=\\/usr\\/local\\/openvpn\\/sbin\\/openvpn --config server.conf' /usr/local/openvpn/lib/systemd/system/openvpn-server@.service 2.2.7 拷贝openvpn systemd文件 cp -a /usr/local/openvpn/lib/systemd/system/openvpn-server@.service /usr/lib/systemd/system/openvpn.service 2.3 生成证书 easy-rsa github地址 2.3.1 下载 easy-rsa 工具 下载包 wget https://github.com/OpenVPN/easy-rsa/releases/download/v3.0.8/EasyRSA-3.0.8.tgz 解压缩 tar xf EasyRSA-3.0.8.tgz && cd EasyRSA-3.0.8/ 2.3.2 生成全局配置文件 vars 根据 EasyRSA-3.0.8/vars.example 文件生成全局配置文件 vars # 新建一个专门存放easy-rsa相关文件的目录，以后所有生成证书等操作在这个目录下执行 mkdir -p /etc/openvpn/easy-rsa # 拷贝EasyRSA-3.0.8下的部分文件到/etc/openvpn/easy-rsa/ cp -r easyrsa openssl-easyrsa.cnf x509-types /etc/openvpn/easy-rsa/ # 拷贝全局配置文件 vars cp vars.example /etc/openvpn/easy-rsa/vars 向 vars 文件追加以下内容 cat >> /etc/openvpn/easy-rsa/vars 2.3.3 生成服务端证书和CA证书 ./easyrsa 命令生成各类证书的时候，如果不想设置密码，只需要在最后添加参数 nopass 即可 2.3.3.1 初始化 执行命令 ./easyrsa init-pki 会生成 pki 目录，用于存储一些中间变量及最终生成的证书 # 切换目录 cd /etc/openvpn/easy-rsa # 生成pki目录 $ ./easyrsa init-pki Note: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/vars init-pki complete; you may now create a CA or requests. Your newly created PKI dir is: /etc/openvpn/easy-rsa/pki pki 目录内容如下 -rw------- 1 root root 4616 May 27 17:03 openssl-easyrsa.cnf drwx------ 2 root root 4096 May 27 17:03 private drwx------ 2 root root 4096 May 27 17:03 reqs -rw------- 1 root root 4648 May 27 17:03 safessl-easyrsa.cnf 2.3.3.2 创建CA根证书 如果不想给CA根证书设置密码，可以执行命令 ./easyrsa build-ca nopass $ ./easyrsa build-ca Note: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/vars Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Enter New CA Key Passphrase: # 这里输入ca根证书的密码 Re-Enter New CA Key Passphrase: # 确认密码 Generating RSA private key, 2048 bit long modulus .............+++ ......................................................................................................+++ e is 65537 (0x10001) You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- # 这里输入common name通用名，我这里就直接输入openvpn Common Name (eg: your user, host, or server name) [Easy-RSA CA]:openvpn CA creation complete and you may now import and sign cert requests. Your new CA certificate file for publishing is at: /etc/openvpn/easy-rsa/pki/ca.crt 2.3.3.3 生成服务端证书 为服务端生成证书对并在本地签名。nopass参数生成一个无密码的证书；在此过程中会让你确认ca密码 $ ./easyrsa build-server-full server nopass Note: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/vars Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Generating a 2048 bit RSA private key ..+++ .......+++ writing new private key to '/etc/openvpn/easy-rsa/pki/easy-rsa-10492.BY7Hgo/tmp.W1d5Ju' ----- Using configuration from /etc/openvpn/easy-rsa/pki/easy-rsa-10492.BY7Hgo/tmp.wO6HgM Enter pass phrase for /etc/openvpn/easy-rsa/pki/private/ca.key: # 这里输入上一步创建ca根证书时设置的密码 Check that the request matches the signature Signature ok The Subject's Distinguished Name is as follows commonName :ASN.1 12:'server' Certificate is to be certified until Jun 28 10:00:33 2121 GMT (36500 days) Write out database with 1 new entries Data Base Updated 2.3.3.4 创建 Diffie-Hellman 确保key穿越不安全网络的操作，需要执行 ./easyrsa gen-dh $ ./easyrsa gen-dh Note: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/vars Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Generating DH parameters, 2048 bit long safe prime, generator 2 This is going to take a long time ......................................................................................................+...................................................................+....................................++*++* DH parameters of size 2048 created at /etc/openvpn/easy-rsa/pki/dh.pem 2.3.4 生成客户端证书 2.3.4.1 生成客户端证书 这里客户端证书名称为test 为客户端生成证书对并在本地签名。nopass参数生成一个无密码的证书；在此过程中都会让你确认ca密码 $ ./easyrsa build-client-full test Note: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/vars Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Generating a 2048 bit RSA private key ...............................................................................+++ ...........+++ writing new private key to '/etc/openvpn/easy-rsa/pki/easy-rsa-10608.RMC1Ry/tmp.0C7VzI' Enter PEM pass phrase: # 输入客户端证书密码 Verifying - Enter PEM pass phrase: # 确认密码 ----- Using configuration from /etc/openvpn/easy-rsa/pki/easy-rsa-10608.RMC1Ry/tmp.vzknm6 Enter pass phrase for /etc/openvpn/easy-rsa/pki/private/ca.key: # 这里输入ca根证书的密码 Check that the request matches the signature Signature ok The Subject's Distinguished Name is as follows commonName :ASN.1 12:'test' Certificate is to be certified until Jun 28 10:04:48 2121 GMT (36500 days) Write out database with 1 new entries Data Base Updated 2.3.4.2 为了提高安全性，生成ta.key 加强认证方式，防攻击。如果配置文件中启用此项(默认是启用的)，就需要执行以下命令，并把ta.key放到 /etc/openvpn/server目录。配置文件中服务端第二个参数为0，同时客户端也要有此文件，且client.conf中此指令的第二个参数需要为1。【服务端有该配置，那么客户端也必须要有】 openvpn --genkey --secret ta.key 2.3.5 整理服务端证书 mkdir -p /etc/openvpn/server cp pki/ca.crt /etc/openvpn/server cp pki/private/server.key /etc/openvpn/server cp pki/issued/server.crt /etc/openvpn/server cp pki/dh.pem /etc/openvpn/server cp ta.key /etc/openvpn/server 2.4 创建服务端配置文件 server.conf 示例文件在 openvon源码目录下的 openvpn-2.4.9/sample/sample-config-files/server.conf 直接创建 server.conf 文件 ⚠️ 一定要添加openvpn服务器私有地址，注意掩码 push \"route 10.0.10.0 255.255.255.0\" ⚠️要在配置文件中添加 crl-verify /etc/openvpn/easy-rsa/pki/crl.pem 参数，/etc/openvpn/easy-rsa/pki/crl.pem 是在进行删除vpn用户时执行命令 ./easyrsa gen-crl 所产生的文件，这个 crl.pem 文件是用于管控被删除用户无法连接vpn cat > /etc/openvpn/server/server.conf 执行删除vpn账户的步骤记录，在执行 ./easyrsa revoke 用户名 删除用户后会提示 Revocation was successful. You must run gen-crl and upload a CRL to your infrastructure in order to prevent the revoked cert from being accepted. 这样一段话，按照提示执行 ./easyrsa gen-crl 后会提示 crl.pem 文件所在位置 CRL file: /etc/openvpn/easy-rsa/pki/crl.pem ，因此需要在 server.conf 中添加 crl-verify /etc/openvpn/easy-rsa/pki/crl.pem 一行参数，避免在服务端删除用户后用户还能登陆的情况 ./easyrsa revoke nima Note: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/vars Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Please confirm you wish to revoke the certificate with the following subject: subject= commonName = nima Type the word 'yes' to continue, or any other input to abort. Continue with revocation: yes Using configuration from /etc/openvpn/easy-rsa/pki/easy-rsa-28838.yiflJs/tmp.lFsJfm Enter pass phrase for /etc/openvpn/easy-rsa/pki/private/ca.key: Revoking Certificate 8AB483C2DC76BC3D0E016323D4BF86A9. Data Base Updated IMPORTANT!!! Revocation was successful. You must run gen-crl and upload a CRL to your infrastructure in order to prevent the revoked cert from being accepted. $ ./easyrsa gen-crl Note: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/vars Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Using configuration from /etc/openvpn/easy-rsa/pki/easy-rsa-28926.C3C2l3/tmp.A4N3ul Enter pass phrase for /etc/openvpn/easy-rsa/pki/private/ca.key: An updated CRL has been created. CRL file: /etc/openvpn/easy-rsa/pki/crl.pem server.conf 详细配置说明 # 表示openvpn服务端的监听地址 local 0.0.0.0 # 监听的端口，默认是1194 port 1194 # 使用的协议，有udp和tcp。建议选择tcp proto tcp # 使用三层路由IP隧道(tun)还是二层以太网隧道(tap)。一般都使用tun dev tun # ca证书、服务端证书、服务端密钥和密钥交换文件。如果它们和server.conf在同一个目录下则可以不写绝对路径，否则需要写绝对路径调用 ca ca.crt cert server.crt key server.key dh dh2048.pem # vpn服务端为自己和客户端分配IP的地址池。 # 服务端自己获取网段的第一个地址(此处为10.8.0.1)，后为客户端分配其他的可用地址。以后客户端就可以和10.8.0.1进行通信。 # 注意：该网段地址池不要和已有网段冲突或重复。其实一般来说是不用改的。除非当前内网使用了10.8.0.0/24的网段。 server 10.8.0.0 255.255.255.0 # 使用一个文件记录已分配虚拟IP的客户端和虚拟IP的对应关系，以后openvpn重启时，将可以按照此文件继续为对应的客户端分配此前相同的IP。也就是自动续借IP的意思。 ifconfig-pool-persist ipp.txt # 使用tap模式的时候考虑此选项。 server-bridge XXXXXX # vpn服务端向客户端推送vpn服务端内网网段的路由配置，以便让客户端能够找到服务端内网。多条路由就写多个push指令 push \"route 10.0.10.0 255.255.255.0\" push \"route 192.168.10.0 255.255.255.0\" push \"route 10.206.0.0 255.255.240.0\" # 配置客户端获取的dns push \"dhcp-option DNS 223.5.5.5\" # 让vpn客户端之间可以互相看见对方，即能互相通信。默认情况客户端只能看到服务端一个人；默认是注释的，不能客户端之间相互看见 client-to-client # 允许多个客户端使用同一个VPN帐号连接服务端，默认是注释的，不支持多个客户登录一个账号 duplicate-cn # 每10秒ping一次，120秒后没收到ping就说明对方挂了 keepalive 10 120 # 加强认证方式，防攻击。如果配置文件中启用此项(默认是启用的)需要执行openvpn --genkey --secret ta.key，并把ta.key放到/etc/openvpn/server录，服务端第二个参数为0；同时客户端也要有此文件，且client.conf中此指令的第二个参数需要为1 tls-auth ta.key 0 # 选择一个密码。如果在服务器上使用了cipher选项，那么您也必须在这里指定它。注意，v2.4客户端/服务器将在TLS模式下自动协商AES-256-GCM。 cipher AES-256-CBC # openvpn 2.4版本的vpn才能设置此选项。表示服务端启用lz4的压缩功能，传输数据给客户端时会压缩数据包。Push后在客户端也配置启用lz4的压缩功能，向服务端发数据时也会压缩。如果是2.4版本以下的老版本，则使用用comp-lzo指令 compress lz4-v2 push \"compress lz4-v2\" # 启用lzo数据压缩格式。此指令用于低于2.4版本的老版本。且如果服务端配置了该指令，客户端也必须要配置 comp-lzo # 并发客户端的连接数 max-clients 100 # 通过ping得知超时时，当重启vpn后将使用同一个密钥文件以及保持tun连接状态 persist-key persist-tun # 在文件中输出当前的连接信息，每分钟截断并重写一次该文件 status openvpn-status.log # 默认vpn的日志会记录到rsyslog中，使用这两个选项可以改变。log指令表示每次启动vpn时覆盖式记录到指定日志文件中，log-append则表示每次启动vpn时追加式的记录到指定日志中。但两者只能选其一，或者不选时记录到rsyslog中 ;log openvpn.log ;log-append openvpn.log # 日志记录的详细级别。 verb 3 # 沉默的重复信息。最多20条相同消息类别的连续消息将输出到日志。 ;mute 20 # 当服务器重新启动时，通知客户端，以便它可以自动重新连接。仅在UDP协议是可用 explicit-exit-notify 1 EOF 2.5 创建客户端配置文件 编辑 /etc/openvpn/client/client.ovpn mkdir /etc/openvpn/client cat > /etc/openvpn/client/client.ovpn client.ovpn 配置文件详细说明 # 标识这是个客户端 client # 使用三层路由IP隧道(tun)还是二层以太网隧道(tap)。服务端是什么客户端就是什么 dev tun # 使用的协议，有udp和tcp。服务端是什么客户端就是什么 proto tcp # 服务端的地址和端口 remote 8.8.8.8 1194 # 一直尝试解析OpenVPN服务器的主机名。在机器上非常有用，不是永久连接到互联网，如笔记本电脑。 resolv-retry infinite # 大多数客户机不需要绑定到特定的本地端口号。 nobind # 初始化后的降级特权(仅非windows) ;user nobody ;group nobody # 尝试在重新启动时保留某些状态。 persist-key persist-tun # ca证书、客户端证书、客户端密钥，如果它们和client.conf或client.ovpn在同一个目录下则可以不写绝对路径，否则需要写绝对路径调用 ca ca.crt cert client.crt key client.key # 通过检查certicate是否具有正确的密钥使用设置来验证服务器证书。 remote-cert-tls server # 加强认证方式，防攻击。服务端有配置，则客户端必须有 tls-auth ta.key 1 # 选择一个密码。如果在服务器上使用了cipher选项，那么您也必须在这里指定它。注意，v2.4客户端/服务器将在TLS模式下自动协商AES-256-GCM。 cipher AES-256-CBC # 服务端用的什么，客户端就用的什么，表示客户端启用lz4的压缩功能，传输数据给客户端时会压缩数据包。 compress lz4-v2 # 日志级别 verb 3 # 沉默的重复信息。最多20条相同消息类别的连续消息将输出到日志。 ;mute 20 2.6 启动openvpn systemctl enable openvpn && systemctl start openvpn 2.7 下载配置文件 有3个文件是固定的，其中 ca.crt、ta.key 是服务端根证书，client.ovpn(文件名可任意修改) 是客户端文件，这3个文件是固定的 再加上执行命令 ./easyrsa build-client-full 用户名 生成的 用户名.crt 和 用户名.key 一共5个文件 ，把这5个文件+密码给到用户即可 ca.crt、ta.key、client.ovpn # 服务端根证书 /etc/openvpn/server/ca.crt /etc/openvpn/server/ta.key # 客户端文件 /etc/openvpn/client/client.ovpn 在 client.ovpn 文件中，注意修改客户端证书和密钥的名称 cert xxx.crt # 客户端证书 key xxx.key # 客户端密钥 2.8 连接测试 连接成功后，就会在本机生成一个 utun1 的虚拟网卡，并获取openvpn server.conf 中设置的 server 10.8.0.0 255.255.255.0 给客户端分配的网段，IP地址从 10.8.0.2 开始分配 utun1: flags=8051 mtu 1500 inet 10.8.0.2 --> 10.8.0.2 netmask 0xffffff00 此时mac本机是能与服务器内网相互ping通的 mac本机ping服务器内网 $ ping -c2 172.16.0.71 PING 172.16.0.71 (172.16.0.71): 56 data bytes 64 bytes from 172.16.0.71: icmp_seq=0 ttl=64 time=10.523 ms 64 bytes from 172.16.0.71: icmp_seq=1 ttl=64 time=6.925 ms --- 172.16.0.71 ping statistics --- 2 packets transmitted, 2 packets received, 0.0% packet loss round-trip min/avg/max/stddev = 6.925/8.724/10.523/1.799 ms 服务器ping mac本机 $ ping -c2 10.8.0.6 PING 10.8.0.6 (10.8.0.6) 56(84) bytes of data. 64 bytes from 10.8.0.6: icmp_seq=1 ttl=64 time=59.1 ms 64 bytes from 10.8.0.6: icmp_seq=2 ttl=64 time=47.0 ms --- 10.8.0.6 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1001ms rtt min/avg/max/mdev = 47.014/53.081/59.148/6.067 ms 2.9 开通、删除vpn步骤 2.9.1 开通vpn账户 这里以给小明开通vpn为例 执行这个步骤会提示输入3次密码，前2次是用户个人密码，最后一次是ca根证书密码 ./easyrsa build-client-full xiaoming 下载用户个人证书文件 # 只需要下载 crt 和 key 文件即可 find / -name \"xiaoming*\" 2.9.2 删除vpn账户 执行这个步骤需要输入ca根证书密码 ./easyrsa revoke xiaoming 执行 gen-crl 命令 ./easyrsa gen-crl 重启openvpn systemctl restart openvpn 2.10 扩展：openvpn配置用户获取固定IP地址 编辑openvpn配置文件 server.conf ，增加 ifconfig-pool-persist 参数 ifconfig-pool-persist /etc/openvpn/server/fixed_ip 编辑 /etc/openvpn/server/fixed_ip 文件，用户名和IP以逗号分隔 test,10.8.0.250 编辑完成后重启openvpn systemctl restart openvpn 验证 $ ifconfig utun2: flags=8051 mtu 1500 inet 10.8.0.250 --> 10.8.0.249 netmask 0xffffffff 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/VPN/openvpn对接ldap.html":{"url":"linux/linux服务/VPN/openvpn对接ldap.html","title":"OpenVPN对接ldap","keywords":"","body":"openvpn对接ldap 1.安装 openvpn ldap认证插件 yum -y install openvpn-auth-ldap 2.修改 openvpn ldap认证文件 修改 /etc/openvpn/auth/ldap.conf URL ldap://xxx:389　　# 指定ldap server地址以及端口 BindDN \"cn=xxx,dc=xxx,dc=com\"　　 # 指定binddn信息即管理员信息 Password \"xxx\"　　　　　　　　　　　 # 指定管理员密码 Timeout 15　　 # 设置网络超时时间 TLSEnable no # 是否使用TLS FollowReferrals yes BaseDN \"dc=xxx,dc=com\"　　 # 指定base dn 域 SearchFilter \"uid=%u\"　　　 # 指定搜索条件，此处若使用cn作为用户名，则不用修改，默认为 (&(uid=%u)(accountStatus=active)) ，但是不好使，原因未知 RequireGroup false /etc/openvpn/auth/ldap.conf 默认内容如下 # LDAP server URL URL ldap://ldap1.example.org # Bind DN (If your LDAP server doesn't support anonymous binds) # BindDN uid=Manager,ou=People,dc=example,dc=com # Bind Password # Password SecretPassword # Network timeout (in seconds) Timeout 15 # Enable Start TLS TLSEnable yes # Follow LDAP Referrals (anonymously) FollowReferrals yes # TLS CA Certificate File TLSCACertFile /usr/local/etc/ssl/ca.pem # TLS CA Certificate Directory TLSCACertDir /etc/ssl/certs # Client Certificate and key # If TLS client authentication is required TLSCertFile /usr/local/etc/ssl/client-cert.pem TLSKeyFile /usr/local/etc/ssl/client-key.pem # Cipher Suite # The defaults are usually fine here # TLSCipherSuite ALL:!ADH:@STRENGTH # Base DN BaseDN \"ou=People,dc=example,dc=com\" # User Search Filter SearchFilter \"(&(uid=%u)(accountStatus=active))\" # Require Group Membership RequireGroup false # Add non-group members to a PF table (disabled) #PFTable ips_vpn_users BaseDN \"ou=Groups,dc=example,dc=com\" SearchFilter \"(|(cn=developers)(cn=artists))\" MemberAttribute uniqueMember # Add group members to a PF table (disabled) #PFTable ips_vpn_eng 3.修改 openvpn 配置文件 修改 /etc/openvpn/server/server.conf新增如下两行 # 指定ldap认证插件地址，此处操作系统为64位。并指定auth ldap认证配置文件位置。 plugin /usr/lib64/openvpn/plugin/lib/openvpn-auth-ldap.so \"/etc/openvpn/auth/ldap.conf %u\" # 设置客户端可以不用通过证书认证，输入ldap中用户名和密码即可实现认证。 client-cert-not-required 4.修改openvpn客户端文件 修改 client.ovpn （此文件为安装openvpn时指定的以 ovpn 结尾的文件） ;cert xxx.crt # 客户端证书，因使用ldap认证，所以注释 ;key xxx.key # 客户端密钥，因使用ldap认证，所以注释 auth-user-pass # 开启用户名密码认证 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/ldap/CentOS7搭建OpenLDAP服务端.html":{"url":"linux/linux服务/ldap/CentOS7搭建OpenLDAP服务端.html","title":"安装","keywords":"","body":"[toc] CentOS7搭建OpenLDAP服务端 openldap官网 openldap官方下载地址 ldap相关术语 Entry (or object) 条目(或对象)：LDAP中的每个单元都认为是条目 dn：条目名称 ou：组织名称 dc：域组件，例如，baidu.com是这样写的 dc=baidu,dc=com cn：通用名称，如原文链接：https://www.baidu.com 名或某个对象的名字 一、安装openldap 1.1 安装包 yum -y install openldap compat-openldap openldap-clients openldap-servers openldap-servers-sql openldap-devel migrationtools 1.2 查看版本 $ slapd -VV @(#) $OpenLDAP: slapd 2.4.44 (Sep 30 2020 17:16:39) $ mockbuild@x86-02.bsys.centos.org:/builddir/build/BUILD/openldap-2.4.44/openldap-2.4.44/servers/slapd 二、配置openldap 2.1 设置管理员密码 会生成一堆加密后的字符，记录好，之后配置文件里会需要的 $ slappasswd -s 123456 {SSHA}KDATg8AaahEG0R3SIWz52JQQOviDsTLP 2.2 修改相关配置文件 2.2.1 修改 /etc/openldap/slapd.d/cn=config/olcDatabase={2}hdb.ldif vim /etc/openldap/slapd.d/cn=config/olcDatabase={2}hdb.ldif 修改如下两行 olcSuffix: dc=my-domain,dc=com olcRootDN: cn=Manager,dc=my-domain,dc=com 修改为 olcSuffix: dc=pptfz,dc=com olcRootDN: cn=admin,dc=pptfz,dc=com 添加如下一行，冒号后边是2.1中生成的管理员密码随机字符串 olcRootPW: {SSHA}KDATg8AaahEG0R3SIWz52JQQOviDsTLP 2.2.2 修改 /etc/openldap/slapd.d/cn=config/olcDatabase={1}monitor.ldif vim /etc/openldap/slapd.d/cn=config/olcDatabase={1}monitor.ldif 修改如下两行 olcAccess: {0}to * by dn.base=\"gidNumber=0+uidNumber=0,cn=peercred,cn=extern al,cn=auth\" read by dn.base=\"cn=Manager,dc=my-domain,dc=com\" read by * none 修改为 olcAccess: {0}to * by dn.base=\"gidNumber=0+uidNumber=0,cn=peercred,cn=extern al,cn=auth\" read by dn.base=\"cn=admin,dc=pptfz,dc=com\" read by * none 2.2.3 验证配置文件 忽略报错 $ slaptest -u 6087d0e7 ldif_read_file: checksum error on \"/etc/openldap/slapd.d/cn=config/olcDatabase={1}monitor.ldif\" 6087d0e7 ldif_read_file: checksum error on \"/etc/openldap/slapd.d/cn=config/olcDatabase={2}hdb.ldif\" config file testing succeeded 三、启动openldap 3.1 启动openldap并设置开机自启 systemctl enable slapd && systemctl start slapd 3.2 查看运行状态 $ systemctl status slapd ● slapd.service - OpenLDAP Server Daemon Loaded: loaded (/usr/lib/systemd/system/slapd.service; enabled; vendor preset: disabled) Active: active (running) since Wed 2021-06-30 17:35:52 CST; 7s ago Docs: man:slapd man:slapd-config man:slapd-hdb man:slapd-mdb file:///usr/share/doc/openldap-servers/guide.html Process: 21432 ExecStart=/usr/sbin/slapd -u ldap -h ${SLAPD_URLS} $SLAPD_OPTIONS (code=exited, status=0/SUCCESS) Process: 21417 ExecStartPre=/usr/libexec/openldap/check-config.sh (code=exited, status=0/SUCCESS) Main PID: 21434 (slapd) CGroup: /system.slice/slapd.service └─21434 /usr/sbin/slapd -u ldap -h ldapi:/// ldap:/// Jun 30 17:35:52 VM-0-29-centos systemd[1]: Starting OpenLDAP Server Daemon... Jun 30 17:35:52 VM-0-29-centos runuser[21420]: pam_unix(runuser:session): session opened for user ldap by (uid=0) Jun 30 17:35:52 VM-0-29-centos slapd[21432]: @(#) $OpenLDAP: slapd 2.4.44 (Apr 28 2021 13:32:00) $ mockbuild@x86-02.bsys.centos.org:/builddir/build/BUILD/openldap-2.4.44/openldap-2.4.44/servers/slapd Jun 30 17:35:52 VM-0-29-centos slapd[21432]: ldif_read_file: checksum error on \"/etc/openldap/slapd.d/cn=config/olcDatabase={1}monitor.ldif\" Jun 30 17:35:52 VM-0-29-centos slapd[21432]: ldif_read_file: checksum error on \"/etc/openldap/slapd.d/cn=config/olcDatabase={2}hdb.ldif\" Jun 30 17:35:52 VM-0-29-centos slapd[21432]: tlsmc_get_pin: INFO: Please note the extracted key file will not be protected with a PIN any more, however it will b...rmissions. Jun 30 17:35:52 VM-0-29-centos slapd[21434]: hdb_db_open: warning - no DB_CONFIG file found in directory /var/lib/ldap: (2). Expect poor performance for suffix \"dc=qike366,dc=com\". Jun 30 17:35:52 VM-0-29-centos slapd[21434]: slapd starting Jun 30 17:35:52 VM-0-29-centos systemd[1]: Started OpenLDAP Server Daemon. Hint: Some lines were ellipsized, use -l to show in full. 3.3 查看端口 openldap默认监听tcp/389端口 $ netstat -antup | grep 389 tcp 0 0 0.0.0.0:389 0.0.0.0:* LISTEN 21434/slapd tcp6 0 0 :::389 :::* LISTEN 21434/slapd 四、配置openldap数据库 拷贝文件，修改权限 cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIG chown ldap:ldap -R /var/lib/ldap && chmod 700 -R /var/lib/ldap 查看 $ ll /var/lib/ldap/ total 348 -rwx------ 1 ldap ldap 2048 Jun 30 17:35 alock -rwx------ 1 ldap ldap 286720 Jun 30 17:35 __db.001 -rwx------ 1 ldap ldap 32768 Jun 30 17:35 __db.002 -rwx------ 1 ldap ldap 49152 Jun 30 17:35 __db.003 -rwx------ 1 ldap ldap 845 Jun 30 17:42 DB_CONFIG -rwx------ 1 ldap ldap 8192 Jun 30 17:35 dn2id.bdb -rwx------ 1 ldap ldap 32768 Jun 30 17:35 id2entry.bdb -rwx------ 1 ldap ldap 10485760 Jun 30 17:35 log.0000000001 五、导入基本Schema 导入 cosine.ldif $ ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/cosine.ldif SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 adding new entry \"cn=cosine,cn=schema,cn=config\" 导入 nis.ldif $ ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/nis.ldif SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 adding new entry \"cn=nis,cn=schema,cn=config\" 导入 inetorgperson.ldif $ ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/inetorgperson.ldif SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 adding new entry \"cn=inetorgperson,cn=schema,cn=config\" 六、修改 migrate_common.ph 文件 /usr/share/migrationtools/migrate_common.ph 文件主要是用于生成ldif文件使用 vim /usr/share/migrationtools/migrate_common.ph +71 修改如下三行 $DEFAULT_MAIL_DOMAIN = \"padl.com\"; $DEFAULT_BASE = \"dc=padl,dc=com\"; $EXTENDED_SCHEMA = 0; 修改为 $DEFAULT_MAIL_DOMAIN = \"pptfz.com\"; $DEFAULT_BASE = \"dc=pptfz,dc=com\"; $EXTENDED_SCHEMA = 1; 修改完成后重启服务 systemctl restart slapd 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/ldap/安装phpLDAPadmin管理LDAP.html":{"url":"linux/linux服务/ldap/安装phpLDAPadmin管理LDAP.html","title":"安装","keywords":"","body":"安装phpLDAPadmin管理LDAP phpLDAPadmin官网 1.安装phpldapadmin 安装phpldapadmin会同时安装php5.4以及httpd2.4 yum -y install phpldapadmin 查看安装 $ rpm -qa | grep phpldapadmin phpldapadmin-1.2.5-1.el7.noarch 2.配置phpldapadmin 2.1 编辑 /etc/httpd/conf.d/phpldapadmin.conf 允许从远程访问 # 备份文件 cp /etc/httpd/conf.d/phpldapadmin.conf{,.old} # 重新编辑文件 cat > /etc/httpd/conf.d/phpldapadmin.conf Order Deny,Allow Allow from all EOF 2.2 编辑 /etc/httpd/conf/httpd.conf # 注释102行到105行 AllowOverride none Require all denied # 106行新增如下 Options Indexes FollowSymLinks AllowOverride None 2.3 编辑 /etc/phpldapadmin/config.php 配置使用dn登陆 修改 $servers->setValue('login','attr','uid'); 修改为 $servers->setValue('login','attr','dn'); 2.4 重启httpd systemctl restart httpd 3.访问phpldapadmin 浏览器访问 IP/phpldapadmin 访问首页面 之前搭建openldap服务端中设置的用户名为 cn=ldap,dc=pptfz,dc=com，密码为 123456 用户名为 cn=ldap,dc=pptfz,dc=com 密码为 123456 4.解决 phpldapadmin管理页面提示 This base cannot be created with PLA 问题 phpldapadmin登陆成功后会报错 This base cannot be created with PLA 解决方法 在 /etc/openldap 目录下新建一个 base.ldif 文件，为初始化根节点做准备工作，这里需要修改dn一行为自己的信息 cat > /etc/openldap/base.ldif 执行命令，会提示输入密码，这里输入搭建openldap时设置的管理员密码 $ ldapadd -f /etc/openldap/base.ldif -x -D cn=admin,dc=pptfz,dc=com -W Enter LDAP Password: adding new entry \"dc=pptfz,dc=com\" 执行成功后重新登陆 phpldapadmin，可以看到之前的报错已经没有了 5.关闭匿名访问 ldap默认是允许匿名访问的 编辑 /etc/phpldapadmin/config.php # 关闭匿名访问 修改 // $servers->setValue('login','anon_bind',true); 修改为 $servers->setValue('login','anon_bind',false); # 设置只有管理员能登陆 在上边那行下新增一行 $servers->setValue('login','allowed_dns',array('cn=admin,dc=pptfz,dc=com')); 重启服务 systemctl restart slapd 配置完成后匿名登陆按钮就取消了 5.解决模版不能使用问题 有些模版提示不能使用 5.1 导入基本schema 导入 cosine.ldif $ ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/cosine.ldif SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 adding new entry \"cn=cosine,cn=schema,cn=config\" 导入 nis.ldif $ ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/nis.ldif SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 adding new entry \"cn=nis,cn=schema,cn=config\" 导入 inetorgperson.ldif $ ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/inetorgperson.ldif SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 adding new entry \"cn=inetorgperson,cn=schema,cn=config\" 5.2 编辑 /etc/phpldapadmin/config.php # 备份文件 cp /etc/phpldapadmin/config.php{,.old} # 530行左右插入以下内容 $servers->newServer('ldap_pla'); $servers->setValue('server','name','LDAP Server'); $servers->setValue('server','host','127.0.0.1'); $servers->setValue('server','port',389); $servers->setValue('server','base',array('dc=pptfz,dc=com')); // 需要修改 $servers->setValue('login','auth_type','cookie'); $servers->setValue('login','bind_id','cn=admin,dc=pptfz,dc=com'); // 需要修改 $servers->setValue('login','bind_pass','123456'); // 需要修改管理员密码 $servers->setValue('server','tls',false); 重启服务 systemctl restart slapd httpd 模板不能使用提示没了 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/ldap/phpLDAPadmin创建用户.html":{"url":"linux/linux服务/ldap/phpLDAPadmin创建用户.html","title":"phpLDAPadmin创建用户","keywords":"","body":"phpLDAPadmin创建用户 1.点击 创建新条目 这里会有各种模板，根据自己的需求来创建不同的对象 2.创建组和用户 创建一个组 Generic: Posix Group，下方的用户都是之前已经创建的用户，可以选择相应的用户 点击 提交 即可创建一个组 创建后的test组 选中test组，然后选择右边的 创建一个子条目 右边可以根据不同的模板创建相应的对象 例如这里选择 Generic: User Account 创建一个用户 创建的用户 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/ldap/Self Service Password.html":{"url":"linux/linux服务/ldap/Self Service Password.html","title":"Self Service Pasword","keywords":"","body":"[toc] Self Service Password Self Service Password github地址 Self Service Password 官网 Self Service Password官方安装文档 1.Self Service Password简介 简介 Self Service Password是一个PHP应用程序，允许用户在LDAP目录中更改密码。 该应用程序可以用于标准的LDAPv3目录(OpenLDAP、OpenDS、ApacheDS、Sun Oracle DSEE、Novell等)，也可以用于Active Directory。 特点: Samba模式，修改Samba密码 活动目录模式 本地密码策略: 最小/最大长度 禁止字符 上、下、数字或特殊字符计数器 重用旧密码检查 密码与登录名相同 复杂性(不同类型的字符) 帮助信息 重置的问题 通过邮件挑战重置(通过邮件发送的令牌) 通过短信重置(通过外部Email 2短信服务或短信API) 修改LDAP目录下的SSH Key 验证码(内置) 更改密码后的邮件通知 修改密码之前和之后的挂钩脚本 2.安装 2.1 标准安装 2.2 dokcer安装 self service password dockerhub地址 ⚠️官方文档docker启动命令有坑，这里稍做了一下修改 # /var/www/conf/config.inc.local.php在ssp容器中是一个空目录！！！ -v /home/test/ssp.conf.php:/var/www/conf/config.inc.local.php 创建volume docker volume create ssp 直接启动容器 docker run \\ -d \\ --restart=always \\ --name self-service-password \\ -h self-service-password \\ -p 8000:80 \\ -v ssp:/var/www/conf/ \\ -it docker.io/ltbproject/self-service-password:latest 3.配置ldap连接 修改配置文件 config.inc.php # LDAP $ldap_url = \"ldap://172.30.100.17:389\"; // 修改为ldap服务器地址 $ldap_starttls = false; $ldap_binddn = \"cn=admin,dc=qike,dc=com\"; // ldap管理员账号 $ldap_bindpw = '123456'; // ldap管理员密码 // for GSSAPI authentication, comment out ldap_bind* and uncomment ldap_krb5ccname lines //$ldap_krb5ccname = \"/path/to/krb5cc\"; $ldap_base = \"dc=qike,dc=com\"; // ldap搜索参数 $ldap_login_attribute = \"uid\"; $ldap_fullname_attribute = \"cn\"; $ldap_filter = \"(&(objectClass=*)($ldap_login_attribute={login}))\"; // 用户搜索规则 $ldap_use_exop_passwd = false; $ldap_use_ppolicy_control = false; 修改完成后重启docker容器 docker restart self-service-password 4.访问 浏览器访问 IP:8000 5.配置修改密码策略 修改配置文件 config.inc.php # 设置密码长度 $pwd_min_length = 4; # 最少位数 $pwd_max_length = 8; # 最多位数 # 设置大小写、数字、特殊字符最小数量 $pwd_min_lower = 3; # 小写字母 $pwd_min_upper = 1; # 大写字母 $pwd_min_digit = 1; # 数字 $pwd_min_special = 1; # 特殊字符 # 禁止符号出现 $pwd_forbidden_chars = \"@%\"; # @和%不能出现在密码中 # 不同类别字符 $pwd_complexity = 2; # 大小写、数字、特殊字符最少2种 # 禁止使用旧密码作为新密码 $pwd_no_reuse = true; # 新密码最少包含3种不同的字符 $pwd_diff_last_min_chars = 3; # 密码中不能出现的单词 $pwd_forbidden_words = array(\"azerty\", \"qwerty\", \"password\"); # 总是向用户显示密码策略 $pwd_show_policy = always # 配置策略是显示在表单上方还是下方 $pwd_show_policy_pos = \"above\"; # 当密码被拒绝时，您可以显示目录返回的错误信息。消息内容取决于您的 LDAP 服务器软件。 $show_extended_error = true; $pwd_show_policy 选项用于向用户显示密码策略，有3个值 always: 策略总是显示 never: 策略从不显示 onerror: 仅当密码因此被拒绝时才显示策略，并且用户正确提供了他的旧密码。 配置完成后重启容器，刷新页面，刚才配置的密码策略就显示出来了 6.配置邮件重置密码 ⚠️在self service password中使用邮箱重置密码功能的前提是邮箱必须是ldap中用户绑定的邮箱 修改配置文件 config.inc.php 配置邮件 有关PHPMailer更多信息，请参阅 https://github.com/PHPMailer/PHPMailer # 配置发件人名称 $mail_from = \"admin@example.com\"; $mail_from_name = \"Self Service Password administrator\"; $mail_signature = \"\"; # 更改密码通知，使用此选项在邮件更改成功后立即向用户发送确认邮件 $notify_on_change = true; # PHPMailer 配置 $mail_sendmailpath = '/usr/sbin/sendmail'; $mail_protocol = 'smtp'; $mail_smtp_debug = 0; $mail_debug_format = 'html'; $mail_smtp_host = 'localhost'; # smtp地址 $mail_smtp_auth = true; # 这里设置为true $mail_smtp_user = ''; # 发件人邮箱用户名 $mail_smtp_pass = ''; # 发件人邮箱密码 $mail_smtp_port = 25; # 端口 $mail_smtp_timeout = 30; $mail_smtp_keepalive = false; $mail_smtp_secure = 'tls'; $mail_smtp_autotls = true; $mail_smtp_options = array(); $mail_contenttype = 'text/plain'; $mail_wordwrap = 0; $mail_charset = 'utf-8'; $mail_priority = 3; 配置邮件重置密码 # 开启邮件重置密码功能 $use_tokens = true; # 配置加密，保护会话标识 $crypt_tokens = true; # 令牌ttl，以便在未使用时将其删除，单位秒 $token_lifetime = \"3600\"; # 配置日志路径，默认情况下，生成的 URL 会记录在默认的 Apache 错误日志中。可以更改此行为以登录特定文件。apache用户对此目录必须有写入权限 $reset_request_log = \"/var/log/self-service-password\"; # 配置重定向url，在ssp前有代理的情况下使用，例如使用了nginx $reset_url = $_SERVER['HTTP_X_FORWARDED_PROTO'] . \"://\" . $_SERVER['HTTP_X_FORWARDED_HOST'] . $_SERVER['SCRIPT_NAME']; 在邮件选项下输入ldap中绑定的邮箱就可以发出重置邮件了 收到的密码重置邮件 使用电脑客户端链接是纯文本的，但是在浏览器中打开是超链接 在密码重置邮件中点击链接访问就可以修改密码了 重置密码的邮箱必须是ldap中用户绑定的邮箱，输入其他邮箱会报错邮箱与用户不一致 遇到的报错 报错1 访问报错 Token encryption requires a random string in keyphrase setting github issue中有提到这个问题 问题说明链接 解决方法 修改 $keyphrase = \"secret\"; 修改为任意字符的随机字符串 $keyphrase = \"yaldnfaopewnrganadnfa\"; 报错2 无法修改密码，日志报错用户未发现 [Sat Sep 18 08:06:20.175684 2021] [php7:notice] [pid 18] [client 10.0.17.251:56444] LDAP - User xiaoming not found, referer: http://172.30.100.4:8000/index.php 10.0.17.251 - - [18/Sep/2021:08:06:20 +0000] \"POST /index.php HTTP/1.1\" 200 1841 \"http://172.30.100.4:8000/index.php\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36\" 问题所在，需要修改以下配置，objectClass=person 是官方示例的写法，需要把person修改为具体的过滤内容，例如修改为 * $ldap_filter = \"(&(objectClass=person)($ldap_login_attribute={login}))\"; 修改为如下 $ldap_filter = \"(&(objectClass=*)($ldap_login_attribute={login}))\"; 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/邮件服务/linux使用命令发送邮件.html":{"url":"linux/linux服务/邮件服务/linux使用命令发送邮件.html","title":"linux使用命令发送邮件","keywords":"","body":"[toc] linux使用命令发送邮件 1.编辑 /etc/mail.rc ，加入以下内容 set from=123456@qq.com set smtp=smtp.qq.com set smtp-auth-user=xiaoming set smtp-auth-password=123456 set smtp-auth=login 常用smtp smtp地址 说明 smtp.exmail.qq.com 腾讯企业邮箱smtp smtp.qq.com qq邮箱smtp smtp.163.com 163邮箱smtp 2.发送邮件 2.1 普通发送 echo hello word | mail -s \"title\" 123456@qq.com 或者 mail -s \"title\" 123456@qq.com 2.2 发送html格式的邮件 mail -s \"$(echo -e \"标题 \\nContent-Type: text/html; charset=utf-8\")\" 123456@qq.com 2.3 发送附件 mail命令 -a 选项为发送附件，只能发送一个附件 echo test | mail -s test -a pkg.zip 1234562qq.com 使用命令 postconf message_size_limit 查看当前信息大小限制 $ postconf message_size_limit message_size_limit = 10240000 使用命令 postconf -e message_size_limit= 设置大小限制值 postconf -e message_size_limit=20480000 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/邮件服务/linux配置企业邮箱smtps 465端口发送邮件.html":{"url":"linux/linux服务/邮件服务/linux配置企业邮箱smtps 465端口发送邮件.html","title":"linux配置企业邮箱smtps 465端口发送邮件","keywords":"","body":"linux配置企业邮箱smtps 465端口发送邮件 参考链接 1.生成证书 mkdir -p /etc/mail/.certs echo -n | openssl s_client -connect smtp.exmail.qq.com:465 | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' > /etc/mail/.certs/qq.crt certutil -A -n \"GeoTrust SSL CA\" -t \"C,,\" -d /etc/mail/.certs -i /etc/mail/.certs/qq.crt certutil -A -n \"GeoTrust Global CA\" -t \"C,,\" -d /etc/mail/.certs -i /etc/mail/.certs/qq.crt certutil -L -d /etc/mail/.certs/ cd /etc/mail/.certs/ && certutil -A -n \"GeoTrust SSL CA – G3\" -t \"Pu,Pu,Pu\" -d ./ -i qq.crt 最后会提示如下 Notice: Trust flag u is set automatically if the private key is present. 2.编辑 /etc/mail.rc cp /etc/mail.rc{,.bak} cat >> /etc/mail.rc 3.验证 echo message | mail -s \" title\" xxx@163.com 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/elk/elasticsearch/elasticsearch安装.html":{"url":"linux/linux服务/elk/elasticsearch/elasticsearch安装.html","title":"elasticsearch安装","keywords":"","body":"elasticsearch安装 elasticsearch官网 elasticsearch官方下载地址 elasticsearch各版本官方下载地址 elasticsearch依赖java环境，安装前需要配置好jdk环境 下载安装包 wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.12.1-linux-x86_64.tar.gz 解压缩包 tar xf elasticsearch-7.12.1-linux-x86_64.tar.gz -C /usr/local/ 新建es用户 useradd es 修改elasticsearch目录所有者 chown -R es.es /usr/local/elasticsearch-7.12.1/ 启动elasticsearch cd /usr/local/elasticsearch-7.12.1 su es -c './bin/elasticsearch' ⚠️使用root用户启动会报错 java.lang.RuntimeException: can not run elasticsearch as root Future versions of Elasticsearch will require Java 11; your Java version from [/usr/local/jdk1.8.0_251/jre] does not meet this requirement. Consider switching to a distribution of Elasticsearch with a bundled JDK. If you are already using a distribution with a bundled JDK, ensure the JAVA_HOME environment variable is not set. [2021-05-23T23:25:34,933][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [test1] uncaught exception in thread [main] org.elasticsearch.bootstrap.StartupException: java.lang.RuntimeException: can not run elasticsearch as root at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:163) ~[elasticsearch-7.12.1.jar:7.12.1] at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:150) ~[elasticsearch-7.12.1.jar:7.12.1] at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:75) ~[elasticsearch-7.12.1.jar:7.12.1] at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:116) ~[elasticsearch-cli-7.12.1.jar:7.12.1] at org.elasticsearch.cli.Command.main(Command.java:79) ~[elasticsearch-cli-7.12.1.jar:7.12.1] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:115) ~[elasticsearch-7.12.1.jar:7.12.1] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:81) ~[elasticsearch-7.12.1.jar:7.12.1] Caused by: java.lang.RuntimeException: can not run elasticsearch as root at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:101) ~[elasticsearch-7.12.1.jar:7.12.1] at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:168) ~[elasticsearch-7.12.1.jar:7.12.1] at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:397) ~[elasticsearch-7.12.1.jar:7.12.1] at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:159) ~[elasticsearch-7.12.1.jar:7.12.1] ... 6 more uncaught exception in thread [main] java.lang.RuntimeException: can not run elasticsearch as root at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:101) at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:168) at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:397) at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:159) at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:150) at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:75) at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:116) at org.elasticsearch.cli.Command.main(Command.java:79) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:115) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:81) For complete error details, refer to the log at /usr/local/elasticsearch-7.12.1/logs/elasticsearch.log 查看启动 elasticsearch监听tcp 9200和9300，其中9300为集群内节点通信接口，9200为elasticsearch开放的REST接口 jps查看 $ jps|grep Elasticsearch 2407 Elasticsearch $ ps aux|grep 2407 es 2407 31.3 32.6 3691292 1320184 ? Ssl 23:37 0:25 /usr/local/jdk1.8.0_251/bin/java -Xshare:auto -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dio.netty.allocator.numDirectArenas=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Djava.locale.providers=SPI,JRE -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -Djava.io.tmpdir=/tmp/elasticsearch-4825559070521700706 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=data -XX:ErrorFile=logs/hs_err_pid%p.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -Xloggc:logs/gc.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=32 -XX:GCLogFileSize=64m -Xms1024m -Xmx1024m -XX:MaxDirectMemorySize=536870912 -Des.path.home=/usr/local/elasticsearch-7.12.1 -Des.path.conf=/usr/local/elasticsearch-7.12.1/config -Des.distribution.flavor=default -Des.distribution.type=tar -Des.bundled_jdk=true -cp /usr/local/elasticsearch-7.12.1/lib/* org.elasticsearch.bootstrap.Elasticsearch 开启elasticsearch远程访问 elasticsearch默认监听127.0.0.1，如果想要开启远程访问，需要修改 config/elasticsearch.yml，将 network.host 一行修改为如下 network.host: 0.0.0.0 验证elsticsearch启动 浏览器访问 IP:9200，返回如下结果即为成功 { \"name\" : \"test1\", \"cluster_name\" : \"elasticsearch\", \"cluster_uuid\" : \"bnOFYR5fRFmF7pPaXxLuDA\", \"version\" : { \"number\" : \"7.12.1\", \"build_flavor\" : \"default\", \"build_type\" : \"tar\", \"build_hash\" : \"3186837139b9c6b6d23c3200870651f10d3343b7\", \"build_date\" : \"2021-04-20T20:56:39.040728659Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.8.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" } 或者使用curl命令 curl 127.0.0.1:9200 { \"name\" : \"test1\", \"cluster_name\" : \"elasticsearch\", \"cluster_uuid\" : \"bnOFYR5fRFmF7pPaXxLuDA\", \"version\" : { \"number\" : \"7.12.1\", \"build_flavor\" : \"default\", \"build_type\" : \"tar\", \"build_hash\" : \"3186837139b9c6b6d23c3200870651f10d3343b7\", \"build_date\" : \"2021-04-20T20:56:39.040728659Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.8.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" } 检查elasticsearch健康状态 $ curl 127.0.0.1:9200/_cat/health 1621785784 16:03:04 elasticsearch green 1 1 0 0 0 0 0 0 - 100.0% elasticsearch启动报错 ERROR: [2] bootstrap checks failed. You must address the points described in the following [2] lines before starting Elasticsearch. bootstrap check failure [1] of [2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] bootstrap check failure [2] of [2]: the default discovery settings are unsuitable for production use; at least one of [discovery.seed_hosts, discovery.seed_providers, cluster.initial_master_nodes] must be configured ERROR: Elasticsearch did not exit normally - check the logs at /usr/local/elasticsearch-7.12.1/logs/elasticsearch.log elasticsearch启动时错误解决方案 错误1 **ERROR: bootstrap checks failed[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]** 解决方法 # 切换到root用户修改 /etc/security/limits.conf 文件，在最后面追加下面内容 * soft nofile 65536 * hard nofile 65536 * soft nproc 4096 * hard nproc 4096 # 退出重新登录检测配置是否生效: ulimit -Hn ulimit -Sn ulimit -Hu ulimit -Su 错误2 **ERROR: max number of threads [3802] for user [chenyn] is too low,increase to at least [4096]** 解决方法 # 进入limits.d目录下修改 /etc/security/limits.d/20-nproc.conf 配置文件 启动ES用户名 soft nproc 4096 错误3 **ERROR: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]** 解决方法 # 编辑文件 /etc/sysctl.conf 增加下一行 vm.max_map_count=655360 # 执行以下命令生效 sysctl -p 错误4 ERROR: [1] bootstrap checks failed. You must address the points described in the following [1] lines before starting Elasticsearch. bootstrap check failure [1] of [1]: the default discovery settings are unsuitable for production use; at least one of [discovery.seed_hosts, discovery.seed_providers, cluster.initial_master_nodes] must be configured 解决方法 # 编辑elasticsearch配置文件 config/elasticsearch.yml 编辑如下内容，单节点就协议主机名，多节点写多个主机名 cluster.initial_master_nodes: [\"test1\"] 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/elk/kibana/kibana安装.html":{"url":"linux/linux服务/elk/kibana/kibana安装.html","title":"kibana安装","keywords":"","body":"kibana安装 kibana官方 kibana官方下载地址 kibana各版本官方下载地址 kibana是基于node.js开发的 kIbana是运行于elasticsearch基础之上的，可以将kibana视为elasticsearch的用户图形界面(Graphic User Interface GUI) 下载安装包 wget https://artifacts.elastic.co/downloads/kibana/kibana-7.12.1-linux-x86_64.tar.gz 解压缩安装包 tar xf kibana-7.12.1-linux-x86_64.tar.gz -C /usr/local/ 修改kibana目录所有者 这里elasticsearch和kibana安装在同一台主机，并且目录所有者为es chown -R es.es /usr/local/kibana-7.12.1-linux-x86_64/ 开启kibana远程访问 kiban默认监听127.0.0.1，如果想要开启远程访问，需要修改 conf/kibana.yml，将 server.host 一行修改为如下 server.host: \"0.0.0.0\" 启动kibana su es -c '/usr/local/kibana-7.12.1-linux-x86_64/bin/kibana' & 验证kibana启动 kibana默认监听tcp:5601端口 $ netstat -ntpl|grep 5601 tcp 0 0 0.0.0.0:5601 0.0.0.0:* LISTEN 3672/node $ ps aux|grep 3672 es 3672 6.2 5.2 1097364 213860 ? Ssl 00:28 0:14 /usr/local/kibana-7.12.1-linux-x86_64/bin/../node/bin/node /usr/local/kibana-7.12.1-linux-x86_64/bin/../src/cli/dist 访问kibana 浏览器输入 IP:5601 第一次访问的欢迎页面会有2个按钮，添加数据源 Add data 和 独自探索 Explore on my own 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux服务/elk/kibana/kibana报错.html":{"url":"linux/linux服务/elk/kibana/kibana报错.html","title":"kibana报错","keywords":"","body":"kibana报错 报错1：Unable to navigate to space \\\"default\\\", redirecting to Space Selector. Error: Saved object [space/default] not found kibana是通过从 artifacthub 中下载的chart安装的，运行一段时间后开发反馈说kibana挂了，于是登录机器执行命令 kubectl get pod 看到 READY 为 0/1 ，查看日志报错如下 {\"type\":\"log\",\"@timestamp\":\"2022-06-13T08:40:09Z\",\"tags\":[\"spaces\",\"error\"],\"pid\":1,\"message\":\"Unable to navigate to space \\\"default\\\", redirecting to Space Selector. Error: Saved object [space/default] not found\"} {\"type\":\"response\",\"@timestamp\":\"2022-06-13T08:40:09Z\",\"tags\":[],\"pid\":1,\"method\":\"get\",\"statusCode\":302,\"req\":{\"url\":\"/app/kibana\",\"method\":\"get\",\"headers\":{\"user-agent\":\"curl/7.29.0\",\"host\":\"localhost:5601\",\"accept\":\"*/*\"},\"remoteAddress\":\"127.0.0.1\",\"userAgent\":\"127.0.0.1\"},\"res\":{\"statusCode\":302,\"responseTime\":1,\"contentLength\":9},\"message\":\"GET /app/kibana 302 1ms - 9.0B\"} 暂时的结局方式是删除pod后重启，然后就可以了，但是运行一段时间后kibana又会自动挂掉 谷歌 Unable to navigate to space \\\"default\\\", redirecting to Space Selector. Error: Saved object [space/default] not found 找到一些链接有人也遇到过同样的问题 在github这个isseu中可以看到 https://github.com/elastic/kibana/issues/35213 在es的官方论坛中有人提到过解决方法 https://discuss.elastic.co/t/unable-to-navigate-to-space-default-redirecting-to-space-selector-error-saved-object-space-default-not-found/177393/3 官方给到的解释是当存储空间很低时，kibana会把 .kibana 索引设置为只读，从而导致日志中的只读报错，可以通过如下命令解决此问题 在这个链接中可以看到解决方法 PUT _settings { \"index\": { \"blocks\": { \"read_only_allow_delete\": \"false\" } } } PUT your_index_name/_settings { \"index\": { \"blocks\": { \"read_only_allow_delete\": \"false\" } } } 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux内核/Linux升级内核.html":{"url":"linux/linux内核/Linux升级内核.html","title":"Linux升级内核","keywords":"","body":"Linux升级内核 CentOS7.6默认内核版本是3.10 $ uname -r 3.10.0-1062.el7.x86_64 CentOS7内核下载地址 可根据自己实际需求下载对应版本 现在升级到最新版，编辑安装最新内核脚本 cat > /opt/kernel_update.sh 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/walle/CentOS7.5安装walle1.2.html":{"url":"linux/自动化运维平台/walle/CentOS7.5安装walle1.2.html","title":"walle1.2","keywords":"","body":"[toc] CentOS7.5安装walle1.2 walle1.0是基于php开发的，需要php5.4以上版本 瓦力官方文档 瓦力上线流程 一、依赖 Bash(git、ssh) 意味着不支持win、mac的zsh LNMP/LAMP(php5.4+) php需要开启pdo_mysql，exec函数执行 Composer 如果国内环境安装极慢，可以直接下载vendor解压到项目根目录 ansible 二、系统环境 2.1系统版本 $ cat /etc/redhat-release CentOS Linux release 7.5.1804 (Core) 2.2php版本 $ php -v PHP 7.2.16 (cli) (built: Mar 10 2019 21:22:49) ( NTS ) Copyright (c) 1997-2018 The PHP Group Zend Engine v3.2.0, Copyright (c) 1998-2018 Zend Technologies with Zend OPcache v7.2.16, Copyright (c) 1999-2018, by Zend Technologies 三、安装步骤 3.1更换系统yum源为阿里云yum源及添加epel源 //备份原有base源 mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup //下载阿里云yum源 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo //下载epel源 wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo //清空缓存、生成yum缓存 yum clean all yum makecache 3.2安装php-7.2 1.下载php7.2yum源 需要先安装epel-release yum -y install https://mirror.webtatic.com/yum/el7/webtatic-release.rpm 2.安装php7.2 yum -y install php72w php72w-cli php72w-common php72w-curl php72w-gd \\ php72w-mbstring php72w-mysqlnd php72w-process php72w-xml php72w-zip \\ php72w-opcache php72w-pecl-apcu php72w-intl php72w-pecl-redis php72w-fpm 3.启动php-fpm并设置为开机自启 systemctl start php-fpm && systemctl enable php-fpm 3.3安装mysql-5.7.22 gitbook链接-安装msql-5.7.22 有道云链接-安装mysql-5.7.22 3.4安装nginx-1.14并编辑配置文件 //添加nginx官方yum源 cat >/etc/yum.repos.d/nginx.repo /etc/nginx/conf.d/my.walle1.com.conf 3.5安装ansible yum -y install ansible 3.6代码检出 //创建目录 mkdir -p /data/www/walle-web && cd /data/www/walle-web //克隆代码 git clone https://github.com/meolu/walle-web-v1.x.git . 3.7设置mysql [root@walle walle-web]# pwd /data/www/walle-web [root@walle walle-web]# vim config/local.php 修改24行，25行，写入mysql用户名和密码 'username' => isset($_ENV['WALLE_DB_USER']) ? $_ENV['WALLE_DB_USER'] : 'root', 'password' => isset($_ENV['WALLE_DB_PASS']) ? $_ENV['WALLE_DB_PASS'] : '123456', //创建数据库walle mysql -uroot -p -e \"create database walle\" 3.8安装composer，Composer 是 PHP5.3以上 的一个依赖管理工具 //下载安装脚本composer-setup.php到当前目录 php -r \"copy('https://install.phpcomposer.com/installer', 'composer-setup.php');\" //执行安装过程 php composer-setup.php //删除安装脚本 php -r \"unlink('composer-setup.php');\" //将composer.phar移动至/usr/local/bin,以便能直接执行composer命令 mv composer.phar /usr/local/bin/composer 3.9安装vendor omposer install --prefer-dist --no-dev --optimize-autoloader -vvvv 3.10初始化项目 ./yii walle/setup 3.11绑定hosts文件 //windows C:\\Windows\\System32\\drivers\\etc 10.0.0.51 my.walle1.com //mac /etc/hosts 3.12浏览器访问my.walle1.com 初始化管理员账号密码为：admin/admin 初始化开发者账号密码为：demo/demo 登陆后首界面 到此，瓦力1.2安装完成！！！ 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/walle/CentOS7.5安装walle2.0.html":{"url":"linux/自动化运维平台/walle/CentOS7.5安装walle2.0.html","title":"walle2.0","keywords":"","body":"[toc] CentOS7.5安装walle2.0 标准安装 walle2.0是基于python开发的，需要python2.7+，mysql5.6.5以上版本 官方文档 瓦力上线流程 一、系统环境 1.1Linux系统版本 [root@walle ~]# cat /etc/redhat-release CentOS Linux release 7.5.1804 (Core) 1.2python版本 [root@walle ~]# python --version Python 2.7.5 二、安装步骤 2.1更换阿里云yum源及添加epel源 #备份原有base源 [root@walle ~]# mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup #下载阿里云yum源 [root@walle ~]# wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo #下载epel源 [root@walle ~]# wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo #清空缓存、生成yum缓存 [root@walle ~]# yum clean all && yum makecache 2.2安装依赖包 [root@walle ~]# yum -y install python2-pip python-virtualenv 2.3安装mysql-5.7.22 gitbook链接-安装msql-5.7.22 有道云链接-安装mysql-5.7.22 2.4安装nginx1.14并配置 #添加nginx官方yum源 [root@walle ~]# cat >/etc/yum.repos.d/nginx.repo /etc/nginx/conf.d/my.walle.com.conf 2.5克隆瓦力 #克隆瓦力项目 [root@walle ~]# git clone https://github.com/meolu/walle-web.git #将瓦利项目移至根下 [root@walle ~]# mv walle-web / 2.6编辑hosts文件 #域名要与nginx配置文件中一样 [root@walle ~]# echo \"127.0.0.1 my.walle2.com\" >>/etc/hosts 2.7安装python2.7+pip [root@walle ~]# cd /walle-web [root@walle walle-web]# sh admin.sh init 安装后提示如下即为成功 init walle success welcome to walle 2.0 # 注意：安装mysqlclient失败，需要先安装libmysqlclient-dev(ubuntu) # 注意：安装失败请指定python路径. mac 可能会有用anaconda的python，找到自己系统的python 2.7追加参数指定 -p /usr/bin/python2.7 即可 vim admin.sh +20 virtualenv --no-site-packages -p /usr/local/bin/python2.7 venv 2.8编辑python配置文件/walle-web/walle/config/settings_prod.py #编辑配置文件 [root@walle walle-web]# pwd /walle-web [root@walle walle-web]# vim walle/config/settings_prod.py 修改以下几项 25行，域名设置，要与nginx配置文件中的域名相同 HOST = 'my.walle2.com' 31行，数据库设置，修改user和password，这里为root用户，密码123456，端口为3306 SQLALCHEMY_DATABASE_URI = 'mysql://user:password@localhost:3306/walle?charset=utf8' 34行，指定本地代码检出路径（用户查询分支, 编译, 打包） CODE_BASE = '/data/walle/codebase/' 45行以下为邮箱设置 MAIL_SERVER = 'smtp.163.com' MAIL_PORT = 465 MAIL_USE_SSL = True MAIL_USE_TLS = False MAIL_DEFAULT_SENDER = 'xxx@163.com' MAIL_USERNAME = 'xxx' MAIL_PASSWORD = '123456' #创建本地代码检出路径 [root@walle walle-web]# mkdir -p /data/walle/codebase 2.9创建walle数据库并进行数据迁移 #创建walle数据库 [root@walle walle-web]# mysql -uroot -p -e \"create database walle\" #创建软连接，否则后续执行迁移脚本会出错 先找到libmysqlclient.so.20位置，这里为/usr/local/mysql-5.7.22/lib/libmysqlclient.so.20， 因为做了/usr/local/mysql软连接，所以可以直接使用/usr/local/mysql/lib/路径 [root@walle walle-web]# find / -name \"libmysqlclient.so.20\" /usr/local/mysql-5.7.22/lib/libmysqlclient.so.20 [root@walle walle-web]# ln -s /usr/local/mysql/lib/libmysqlclient.so.20 /usr/lib64/libmysqlclient.so.20 [root@walle walle-web]# sh admin.sh migration 最后提示OK即为成功 2.10启动瓦力并加入开机自启 [root@walle walle-web]# sh admin.sh start [root@walle walle-web]# echo \"cd /walle-web/ && sh admin.sh start\" >>/etc/rc.local [root@walle ~]# chmod +x /etc/rc.d/rc.local 2.11绑定hosts文件 //windows C:\\Windows\\System32\\drivers\\etc 10.0.0.11 my.walle2.com //mac /etc/hosts 2.12登陆瓦力 初始登陆账号 超管：super@walle-web.io \\ Walle123 所有者：owner@walle-web.io \\ Walle123 负责人：master@walle-web.io \\ Walle123 开发者：developer@walle-web.io \\ Walle123 访客：reporter@walle-web.io \\ Walle123 瓦力重启、升级、数据迁移 sh admin.sh restart # 重启 sh admin.sh upgrade # 升级walle，升级完需要重启walle服务。 升级前最好 git stash 暂存本地修改，升级后git stash pop弹出暂存， 然后重启服务。 sh admin.sh migration # Migration，数据迁移 登陆界面 登陆后首界面 到此，瓦力2.0标准安装完成！！！ docker安装 一、安装docker 1.1下载官方yum源 #添加yum源 [root@docker01 ~]# curl -o /etc/yum.repos.d/docker-ce.repo \\ https://mirrors.ustc.edu.cn/docker-ce/linux/centos/docker-ce.repo #添加官方yum源 [root@docker01 ~]# yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 1.2.修改docker官方yum源地址为清华源 [root@docker01 ~]# sed -i 's#download.docker.com#mirrors.tuna.tsinghua.edu.cn/docker-ce#g' \\ /etc/yum.repos.d/docker-ce.repo 1.3安装docker docker-ce 社区版 [root@docker01 ~]# yum -y install docker-ce 1.4.启动docker [root@docker01 ~]# systemctl start docker && systemctl enable docker 1.5查看docker版本 [root@docker01 ~]# docker version Client: Version: 18.09.1 API version: 1.39 Go version: go1.10.6 Git commit: 4c52b90 Built: Wed Jan 9 19:35:01 2019 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 18.09.1 API version: 1.39 (minimum version 1.12) Go version: go1.10.6 Git commit: 4c52b90 Built: Wed Jan 9 19:06:30 2019 OS/Arch: linux/amd64 Experimental: false 1.6配置docker镜像加速 #配置docker官方镜像加速地址 [root@docker01 ~]# cat > /etc/docker/daemon.json /etc/docker/daemon.json 二、安装docker-compose 2.1下载安装包 [root@docker01 ~]# curl -L \"https://github.com/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose 2.2给二进制文件添加可执行权限 [root@docker01 ~]# chmod +x /usr/local/bin/docker-compose 2.3完成安装，查看版本 [root@docker01 ~]# docker-compose -v docker-compose version 1.24.1, build 4667896b 三、编辑walle2.0 docker-compose文件并启动walle2.0容器 3.1创建存放文件目录 [root@docker01 ~]# mkdir /usr/local/walle2.0 && cd /usr/local/walle2.0 3.2新建walle.env，连接数据库MYSQL_USER默认使用root,如需使用其他用户，需自建用户更改walle.env文件 cat >walle.env 3.3创建docker-compose文件 cat >docker-compose.yml =1024) # 0.0.0.0:要绑定的宿主机端口:docker容器内端口80 - \"80:80\" depends_on: - python networks: - walle-net restart: always python: image: alenx/walle-python:2.1 container_name: walle-python hostname: walle-python env_file: # walle.env需和docker-compose在同级目录 - ./walle.env command: bash -c \"cd /opt/walle_home/ && /bin/bash admin.sh migration && python waller.py\" expose: - \"5000\" volumes: - /opt/walle_home/plugins/:/opt/walle_home/plugins/ - /opt/walle_home/codebase/:/opt/walle_home/codebase/ - /opt/walle_home/logs/:/opt/walle_home/logs/ - /root/.ssh:/root/.ssh/ depends_on: - db networks: - walle-net restart: always db: image: mysql container_name: walle-mysql hostname: walle-mysql env_file: - ./walle.env command: [ '--default-authentication-plugin=mysql_native_password', '--character-set-server=utf8mb4', '--collation-server=utf8mb4_unicode_ci'] ports: - \"3306:3306\" expose: - \"3306\" volumes: - /data/walle/mysql:/var/lib/mysql networks: - walle-net restart: always networks: walle-net: driver: bridge EOF 3.4启动容器 docker-compose up -d 启动完成后浏览器访问宿主机IP地址80端口完成登陆 3.5初始账号及常用操作 //初始账号 超管：super@walle-web.io \\ Walle123 所有者：owner@walle-web.io \\ Walle123 负责人：master@walle-web.io \\ Walle123 开发者：developer@walle-web.io \\ Walle123 访客：reporter@walle-web.io \\ Walle123 //常用操作 # 构建服务 docker-compose build # 启动服务,启动过程中可以直接查看终端日志，观察启动是否成功 docker-compose up # 启动服务在后台，如果确认部署成功，则可以使用此命令，将应用跑在后台，作用类似 nohup python waller.py & docker-compose up -d # 查看日志,效果类似 tail -f waller.log docker-compose logs -f # 停止服务,会停止服务的运行，但是不会删除服务所所依附的网络，以及存储等 docker-compose stop # 删除服务，并删除服务产生的网络，存储等，并且会关闭服务的守护 docker-compose down 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/spug/spug安装.html":{"url":"linux/自动化运维平台/spug/spug安装.html","title":"spug安装","keywords":"","body":"[toc] spug安装 spug官网 spug github地址 关于Spug Spug 面向中小型企业设计的轻量级无 Agent 的自动化运维平台，整合了主机管理、主机批量执行、主机在线终端、文件在线上传下载、应用发布部署、在线任务计划、配置中心、监控、报警等一系列功能。 特性 批量执行: 主机命令在线批量执行 在线终端: 主机支持浏览器在线终端登录 文件管理: 主机文件在线上传下载 任务计划: 灵活的在线任务计划 发布部署: 支持自定义发布部署流程 配置中心: 支持 KV、文本、json 等格式的配置 监控中心: 支持站点、端口、进程、自定义等监控 报警中心: 支持短信、邮件、钉钉、微信等报警方式 优雅美观: 基于 Ant Design 的 UI 界面 开源免费: 前后端代码完全开源 一、docker安装 1.1 安装docker 1.1.1 下载二进制包 wget https://download.docker.com/linux/static/stable/x86_64/docker-20.10.2.tgz 1.1.2 解压缩并拷贝文件 tar xf docker-20.10.2.tgz && cp docker/* /usr/bin 1.1.3 使用systemd管理docker Control Docker with systemd 官方文档关于使用systemd管理docker的说明 如果你是使用二进制方式安装的 docker，那么你也许需要整合 docker 到 systemd 中去。为了完成这个任务，你需要安装两个单元文件（service 和 socket）到 /etc/systemd/system 中去 Manually create the systemd unit files When installing the binary without a package, you may want to integrate Docker with systemd. For this, install the two unit files (service and socket) from the github repository to /etc/systemd/system. ⚠️需要下载的是docker.service.rpm和docker.socket这两个文件，需要把docker.service.rpm重命名为docker.service，然后再移动到/etc/systemd/system下 wget https://github.com/moby/moby/raw/branch/branch/master/contrib/init/systemd/docker.service.rpm wget https://github.com/moby/moby/raw/branch/branch/master/contrib/init/systemd/docker.socket 这里我们直接向文件写入内容 docker.service cat > /etc/systemd/system/docker.service docker.socket cat >/etc/systemd/system/docker.socket linux 中 /etc/systemd/system和/usr/lib/systemd/system 的区别 每一个 Unit（服务等） 都有一个配置文件，告诉 Systemd 怎么启动这个 Unit 。 Systemd 默认从目录/etc/systemd/system/读取配置文件。 但是，里面存放的大部分文件都是符号链接，指向目录/usr/lib/systemd/system/，真正的配置文件存放在那个目录。 systemctl enable命令用于在上面两个目录之间，建立符号链接关系。 sudo systemctl enable clamd@scan.service # 等同于 sudo ln -s '/usr/lib/systemd/system/clamd@scan.service' '/etc/systemd/system/multi-user.target.wants/clamd@scan.service' 1.1.4 重新加载服务并启动docker systemctl daemon-reload systemctl start docker && systemctl enable docker 1.1.5 配置阿里云加速 cat > /etc/docker/daemon.json 1.2 安装spug 1.2.1 启动容器 Docker镜像内部使用的 Mysql 数据库。 如果需要持久化存储代码和数据，可以添加：-v 映射容器内/data路径 docker run \\ -d \\ --restart=always \\ --name=spug \\ -p 8000:80 \\ -v /mydata/:/data \\ registry.aliyuncs.com/openspug/spug 1.2.2 初始化 以下操作会创建一个用户名为 admin 密码为 spug.dev 的管理员账户，可自行替换管理员账户。 # 初始化管理员账户 docker exec spug init_spug admin spug.dev # 执行完毕后需要重启容器 docker restart spug 1.3 使用nginx反向代理spug http配置 server { listen 80; server_name spug.test.com; location / { proxy_pass http://127.0.0.1:8000; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } location ^~ /api/ws/ { proxy_pass http://127.0.0.1:8000; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"Upgrade\"; proxy_set_header X-Real-IP $remote_addr; } error_page 404 /index.html; access_log /var/log/spug/spug.test.com.access.log; error_log /var/log/spug/spug.test.com.error.log; } https配置 server { listen 80; server_name spug.test.com; return 301 https://$server_name$request_uri; client_max_body_size 1000m; } server { listen 443 ssl; server_name spug.test.com; location / { proxy_pass http://127.0.0.1:8000; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } location ^~ /api/ws/ { proxy_pass http://127.0.0.1:8000; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"Upgrade\"; proxy_set_header X-Real-IP $remote_addr; } error_page 404 /index.html; access_log /var/log/spug/spug.test.com.access.log; error_log /var/log/spug/spug.test.com.error.log; ssl_certificate ssl_key/spug/1_spug.test.com_bundle.crt; ssl_certificate_key ssl_key/spug/2_spug.test.com.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; } 浏览器访问 用户名：admin 密码：spug.dev 1.4 关于nginx配置遇到的问题 官方文档关于console连接问题的说明 问题一：主机console连接空白 原因： 这种情况大部分都是 Websocket 连接建立失败了，一般出现在部署时自己加了一层 nginx 之类的代理工具，这些代理工具默认无法处理 Weboscket 请求， 这就需要你配置其支持转发 Websocket 请求 解决方法： 在nginx中配置如下内容 location ^~ /api/ws/ { proxy_pass http://127.0.0.1:8000; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"Upgrade\"; proxy_set_header X-Real-IP $remote_addr; } 问题二：无法获取客户端真实IP以及spug console无法连接 在宿主机nginx中如果不配置 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 在登陆spug的时候就会提示无法获取客户端真实IP，但是配置了 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 后可以获取客户端真实IP，然而确无法连接主机console 原因： spug容器中已经有一层nginx做代理了 解决方法： 注释spug容器中nginx配置文件 /etc/nginx/nginx.conf 中 location ^~ /api/ws/ 下的 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/spug/spug上线示例.html":{"url":"linux/自动化运维平台/spug/spug上线示例.html","title":"spug上线示例","keywords":"","body":"[toc] spug上线示例 一、spug上线说明 spug官方文档 上线配置说明 1.1 spug上线方式 spug上线方式分为 常规发布和自定义发布，其中常规发布方式是沿用了 瓦力 的上线流程，但是 瓦力 的大神们在19年的时候被字节给撸走了，可以看到 瓦力 github 中最新的版本是 v2.0.1 ，并且时间停留在了19年4月16日，瓦力2是python写的(瓦力1是php写的，之前我们的生产环境就是使用的瓦力1)，新增了一些实用的功能，但是 v2.0.1 版本有一个重大的BUG，那就是在拉取git代码的时候可能会无法获取分支，spug完美的修复了这个问题，目前生产环境中使用spug上线暂未发现重大BUG。 walle github 1.2 瓦力上线流程 spug上线方式，常规发布是沿用了瓦力的上线逻辑 瓦力上线流程示意图 二、spug上线实践示例 实验环境说明 spug部署主机 IP 10.0.0.10 docker部署 git地址 ssh://git@10.0.0.11:10022/web_test/web_test_one.git 目标主机 IP 10.0.0.12 nginx配置 server { listen 80; server_name webtest.pptfz.com; root /data/a01_web/webtest.pptfz.com; location / { index index.html; } } 2.1 上传密钥 spug中新建主机有 密码 和 密钥 两种方式，其中密钥有 全局密钥 和 独立密钥 两种，如果上传了独立密钥则以上传的为主，反之以全局密钥为主，生产中我们是选择的修改全局密钥，因为我们是有一台跳板机，可以免密登陆其他机器 2.1 导入机器 后续的上线中会涉及到项目部署到的机器，因此需要先导入机器 机器导入有 单个新建 和 批量导入 两种方式，批量导入按照官方提供的模版填写导入即可，这里仅作演示，选择新建即可 新建主机会让你选择主机类别，如果是第一次添加的话需要手动设置类别 自定义主机类别 填写其他信息，包括 主机名称，连接信息，备注信息 如果连接成功则会提示操作成功 这样我们就手动添加了一台主机，如果想要批量添加主机则下载官方提供的主机导入模版填写上传即可 2.1 新建项目 在 应用发布 中选择 应用管理，点击 新建 新建应用 新建后的应用 2.2 新建发布 在要发布的项目中点击 新建发布 选择 常规发布 需要选择发布环境，如果是第一次发布则需要新建环境 新建环境可以在新建发布中选择 新建环境，也可以在 配置中心中选择 环境管理然后再新建环境 这里我们新建一个测试环境 选择发布环境、git仓库地址、是否发布审核(需要管理员审核)、消息通知(可选钉钉、企业微信、Webhook) ⚠️如果远程仓库是非22端口，则需要修改git仓库地址 原远程仓库中地址  git@10.0.0.11:web_test/web_test_one.git 需要修改为  ssh://git@10.0.0.11:10022/web_test/web_test_one.git 填写发布主机的信息 目标主机部署路径 目标主机部署路径就是程序的根目录，比如在nginx中配置的 root /data/xxx 目标主机仓库路径 spug会将从远程仓库拉取的最新代码推送到目标主机，然后程序的根目录会软链接到此目录 保留历史版本数量 这里可以设置保留发布代码的个数，方便做回滚操作 发布目标主机 代码部署到机器，可以是单个，也可以是多个 这里可以做自定义配置 文件过滤 可以选择传输包含的文件，也可以选择排除的文件，这个就看实际需求了，生产中我们暂无文件过滤需求 自定义全局变量 spug官网内置了全局变量，当然也可以自定义全局变量 代码检出前执行 在部署spug的服务器上执行 代码检出后执行 在部署spug的服务器上执行，当前这个目录是spug从远程仓库拉取代码后存储的路径 /data/spug/spug_api/repos ，可以通过spug官方提供的全局变量 SPUG_REPOS_DIR 查看到 在这里一般进行项目的依赖包安装和编译工作 应用发布前执行 在发布的目标主机上运行，当前目录为目标主机上待发布的源代码目录，生产中我们设置为 /data/release/项目名(一般为域名) 应用发布后执行 在发布的目标主机上执行，当前目录为已发布的应用目录，生产中我们设置为 /data/公司简称_web|api/项目名(一般为域名) 生产中我们的php项目有用到这一块，php项目的 vendor 目录和 .env文件开发是不往远程仓库上传的，因为这2个目录很少做修改，但是spug每次上线都是生成最新的软链接，因此需要把vendor 目录和 .env文件拷贝到最新的软链接中 # 项目名称 PROJECT_NAME='callback.abc.com' # 获取瓦力最新链接目录 LAST_DIR=`cd /data/xmadx_api && ls -l | grep $PROJECT_NAME | awk '{print $NF}'` # 项目vendor .env路径 VENDOR_PATH=\"/data/vendor_xmadx/$PROJECT_NAME\" # 拷贝vendor目录和env文件到瓦力最新链接目录 \\cp -rp $VENDOR_PATH/{vendor,.env} $LAST_DIR # 修改目录所有者 chown -R www.www /data/release/$PROJECT_NAME /data/xmadx_api/$PROJECT_NAME 2.3 发布申请 新建发布完成自定义配置后就可以发布申请了 在 应用发布 中选择 发布申请 ，然后点击 新建发布申请 新建发布申请前先做一下环境准备 提交测试页面到远程仓库，这里选择 gogs，当然也可以选择他的孪生兄弟 gitea cat > index.html 第一次测试 第一次测试 EOF 目标主机配置nginx 需要说明以下几点 1、nginx中指定的root根目录 /data/a01_web/webtest.pptfz.com 中/data/a01_web是必须存在的目录，而 /data/a01_web/webtest.pptfz.com则不能存在，因为spug会自动生成这个目录 2、源代码目录存放在自定义的 /data/release/webtest.pptfz.com，程序的根目录会软链接到此目录下以时间命名的目录 cat > /usr/local/nginx/conf/vhost/webtest.pptfz.com.conf 开始发布，选择应用的环境(如测试、生产、灰度等)，点击要上线的项目 填写发布相关信息 开始发布，点击 发布 还需要再点击 发布 发布成功 2.4 查看上线 浏览器访问 这里可以看到nginx配置文件中的root根目录 /data/a01_web/webtest.pptfz.com实际上是软链接到了 /data/release/webtest.pptfz.com/时间目录 2.5 回滚操作 编辑测试文件 index.html 并提交至git cat > index.html 增加一个功能 增加一个功能 EOF 在gogs中查看提交的文件 spug上线发布，操作流程和之前一样，找到对应的项目，然后点击发布 浏览器查看，已经更新 web机器查看，可以看到 /data/release/webtest.pptfz.com下已经有2个时间命名的目录了，这就是每一次发布的源码，而nginx中配置的root根目录总是指向最新的软链接， 1_1_20210208125502 是第一次发布的内容，1_2_20210208143842是第二次发布的内容 现在做一下回滚操作，再要回滚的项目中点击 回滚 回滚确认 点击 回滚 后会重新生成上线单，点击 发布 即可，和正常上线流程一样 回滚成功 浏览器验证，可以看到，内容又回到了第一次提交的内容 第一次测试 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/spug/spug+gitlab实现webhook.html":{"url":"linux/自动化运维平台/spug/spug+gitlab实现webhook.html","title":"spug+gitlab实现webhook","keywords":"","body":"spug+gitlab实现webhook 1.spug获取 Webhook URL 和 Secret Token 在 应用管理 下找到相应项目，然后点击 Webhook 选择分支或者tag，然后复制 webhook URL 和 Secret Token 2.gitlab配置webhook 在gitlab项目下，点击 Settings -> Webhooks 分别填写spug项目中的 webhook url 和 sceret token，设置触发条件 点击 Add webhook 报错 Url is blocked: Requests to the local network are not allowed 解决方法 Settings -> network -> Outbound requests -> Expand -> 勾选 Allow requests to the local network from web hooks and services webhook配置完成后点击下方的 Test 按钮，然后点击 push events 提示如下即表示成功 3.验证 spug中项目配置如下，即本机提交代码至指定git仓库，则spug会自动触发构建，之后会在目标机器的 /opt/test/test 目录下生成一个当天日期txt文件 目标机器需要生成 /opt/test/release 目录，/opt/test/test 目录不需要生成，spug会自动生成，否则会报错 本机新建测试文件并提交至gitlab 在 构建仓库 下会看到相应的构建 在 发布申请 下会看到之前的构建已经成功发布 在服务器的 /opt/test/test 目录下也会生成相应文件 $ ls /opt/test/test/*.txt /opt/test/test/2021-09-09.txt 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/gitea/gitea安装.html":{"url":"linux/自动化运维平台/gitea/gitea安装.html","title":"gitea安装","keywords":"","body":"[toc] gitea安装 一、gitea简介 Gitea是用Go编写的由社区管理的轻量级代码托管解决方案，类似gitlab，但是比gitlab占用资源小太多了，gitlab起码2G+内存，而gitea挤需要90M就能跑起来！！！ 挤需体验三翻钟，里造会干我一样，爱上介款软件！！！ ​ --- 渣渣灰 gitea官网 gitea英文文档 gitea中文文档 gitea与Gogs Gitea其实是Gogs的孪生兄弟，因为这是从Gogs源码的基础上开发的，算是分叉?官方介绍是\"Gitea 是一个开源社区驱动的 Gogs 克隆\"，关于原因可以参考官网上的一篇介绍——>传送门 基本上就是有一部分开发者认为Gogs的开发者效率比较慢，而且不接受他人加入开发，所有修改和PR都需要经过他一个人的审核，这对Gogs的发展很不利。因而部分开发者决定基于Gogs重开一个项目，这就是Gitea。 Gogs docker安装官方文档 二、gitea安装 gitea安装方式有很多种，详情看官网，这里选择docker安装，docker安装中的数据库有3种，sqlite3、mysql、pg 2.1 下载gitea镜像 可以通过dockerhub下载对应的gitea镜像 docker pull gitea/gitea:1.11.1 2.2 下载dcoker-compose docker-compose 国内地址 docker-compose 官方地址 curl -L https://get.daocloud.io/docker/compose/releases/download/1.12.0/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose && chmod +x /usr/local/bin/docker-compose 2.3 编辑gitea docker-compose文件 2.3.1 创建目录 mkdir /usr/local/gitea && cd /usr/local/gitea 2.3.2 编辑gitea docker-compose文件 cat >docker-compose.yaml 2.3.3 启动 docker-compose up -d 2.3.4 查看启动的容器 $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1278b606ea46 gitea/gitea:1.11.1 \"/usr/bin/entrypoint…\" 26 seconds ago Up 25 seconds 0.0.0.0:3000->3000/tcp, 0.0.0.0:222->22/tcp gitea_server_1 b8f0be18fe78 postgres:9.6 \"docker-entrypoint.s…\" 27 seconds ago Up 26 seconds 5432/tcp gitea_db_1 2.4 gitea数据库设置 浏览器访问 IP:3000 初始界面如下，第一个注册的用户就是管理员，后续可以设置只有管理员能注册账号，可以修改配置文件，也可以在可选设置中设置 数据库设置 一般设置 可以自定义仓库根目录和日志目录 可选设置 ⚠️如果这里勾选了禁止用户自主注册就必须设置管理员信息，否则你不允许注册又没设置管理员信息那企不是🐔🐔斯密达了？ 登陆后首界面 剩下的操作就不用多说了，创建仓库、组织、用户，上传代码、拉取代码等等 2.5 配置文件修改项 关于服务的一些修改，配置文件是gitea/gitea/conf/app.ini 例如，手动关闭页面注册按钮，修改app.ini文件中的SHOW_REGISTRATION_BUTTON一项 其他的配置上官网看 我喜欢这个软件最重要的一点就是这个软件支持中文！！！ 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/gitlab/gitlab安装/CentOS7安装gitlab-ce.html":{"url":"linux/自动化运维平台/gitlab/gitlab安装/CentOS7安装gitlab-ce.html","title":"CentOS7安装gitlab-ce","keywords":"","body":"[toc] CentOS7安装gitlab-ce gitlab官网 gitlab github地址 gitlab官方安装文档 gitlab官方下载地址 一、rpm包安装 1.1 安装依赖包 不使用postfix使用其他方式发送邮件参考官方文档 # 安装依赖包 yum -y install curl openssh-server openssh-clients postfix cronie policycoreutils-python # 启动postfix systemctl start postfix && systemctl enable postfix 1.2 下载安装包 gitlab官方rpm包下载地址 也可以从 清华源 下载 wget --content-disposition https://packages.gitlab.com/gitlab/gitlab-ce/packages/el/7/gitlab-ce-13.12.3-ce.0.el7.x86_64.rpm/download.rpm 1.3 安装 yum -y localinstall gitlab-ce-13.12.3-ce.0.el7.x86_64.rpm 1.4 修改配置文件 修改 /etc/gitlab/gitlab.rb 中 xternal_url 一行，修改为自己的域名或IP export IP=10.0.0.100 sed -i.bak \"/^external_url/c external_url 'http://$IP'\" /etc/gitlab/gitlab.rb 1.5 启动gitlab # 启动gitlab gitlab-ctl start # 重载gitlab配置文件 gitlab-ctl reconfigure # 设置gitlab开机自启 systemctl enable gitlab-runsvdir.service 重载配置文件成功提示如下 gitlab启动的端口 $ netstat -ntpl Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 127.0.0.1:8082 0.0.0.0:* LISTEN 3276/sidekiq 5.2.9 tcp 0 0 127.0.0.1:9236 0.0.0.0:* LISTEN 3770/gitaly tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 969/sshd tcp 0 0 127.0.0.1:3000 0.0.0.0:* LISTEN 3926/grafana-server tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 1194/master tcp 0 0 0.0.0.0:8060 0.0.0.0:* LISTEN 3330/nginx: master tcp 0 0 127.0.0.1:9121 0.0.0.0:* LISTEN 3815/redis_exporter tcp 0 0 127.0.0.1:9090 0.0.0.0:* LISTEN 3897/prometheus tcp 0 0 127.0.0.1:9187 0.0.0.0:* LISTEN 3919/postgres_expor tcp 0 0 127.0.0.1:9093 0.0.0.0:* LISTEN 3911/alertmanager tcp 0 0 127.0.0.1:9100 0.0.0.0:* LISTEN 3791/node_exporter tcp 0 0 127.0.0.1:9229 0.0.0.0:* LISTEN 3778/gitlab-workhor tcp 0 0 0.0.0.0:111 0.0.0.0:* LISTEN 701/rpcbind tcp 0 0 127.0.0.1:9168 0.0.0.0:* LISTEN 3802/ruby tcp 0 0 127.0.0.1:8080 0.0.0.0:* LISTEN 3253/puma 5.1.1 (un tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 3330/nginx: master tcp6 0 0 :::22 :::* LISTEN 969/sshd tcp6 0 0 ::1:25 :::* LISTEN 1194/master tcp6 0 0 :::9094 :::* LISTEN 3911/alertmanager tcp6 0 0 :::111 :::* LISTEN 701/rpcbind tcp6 0 0 ::1:9168 :::* LISTEN 3802/ruby 二、yum安装 2.1 添加官方yum源 ⚠️这个源需要科学上网 curl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash 2.2 安装 需要修改为自己的url，安装完成后会自动启动gitlab-ce # 默认安装最新版 sudo EXTERNAL_URL=\"https://gitlab.example.com\" yum -y install gitlab-ce # 安装指定版本 sudo EXTERNAL_URL=\"https://gitlab.example.com\" yum -y install -y gitlab-ce-13.12.3 gitlab启动的端口 $ netstat -ntpl Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 127.0.0.1:9236 0.0.0.0:* LISTEN 3581/gitaly tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1254/sshd tcp 0 0 127.0.0.1:3000 0.0.0.0:* LISTEN 3671/grafana-server tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 1181/master tcp 0 0 0.0.0.0:5050 0.0.0.0:* LISTEN 3098/nginx: master tcp 0 0 0.0.0.0:443 0.0.0.0:* LISTEN 3098/nginx: master tcp 0 0 0.0.0.0:8060 0.0.0.0:* LISTEN 3098/nginx: master tcp 0 0 127.0.0.1:9121 0.0.0.0:* LISTEN 3636/redis_exporter tcp 0 0 127.0.0.1:9090 0.0.0.0:* LISTEN 3641/prometheus tcp 0 0 127.0.0.1:9187 0.0.0.0:* LISTEN 3665/postgres_expor tcp 0 0 127.0.0.1:9093 0.0.0.0:* LISTEN 3658/alertmanager tcp 0 0 127.0.0.1:5000 0.0.0.0:* LISTEN 3599/registry tcp 0 0 127.0.0.1:9100 0.0.0.0:* LISTEN 3629/node_exporter tcp 0 0 127.0.0.1:9229 0.0.0.0:* LISTEN 3587/gitlab-workhor tcp 0 0 0.0.0.0:111 0.0.0.0:* LISTEN 588/rpcbind tcp 0 0 127.0.0.1:9168 0.0.0.0:* LISTEN 3634/ruby tcp 0 0 127.0.0.1:8080 0.0.0.0:* LISTEN 2922/puma 5.1.1 (un tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 3098/nginx: master tcp 0 0 127.0.0.1:8082 0.0.0.0:* LISTEN 2943/sidekiq 5.2.9 tcp6 0 0 :::22 :::* LISTEN 1254/sshd tcp6 0 0 ::1:25 :::* LISTEN 1181/master tcp6 0 0 :::9094 :::* LISTEN 3658/alertmanager tcp6 0 0 :::111 :::* LISTEN 588/rpcbind tcp6 0 0 ::1:9168 :::* LISTEN 3634/ruby 2.3 关闭https自动重定向 使用gitlab-ce官方提供的脚本安装后，gitlab-ce会默认开启 http->https 重定向，如果使用nginx做代理则关闭https自动重定向 编辑 /etc/gitlab/gitlab.rb 文件，取消以下行的注释 nginx['redirect_http_to_https'] = false 重载配置文件 gitlab-ctl reconfigure 三、docker安装 gitlab docker安装官方文档 3.1 编辑docker-compose.yml文件 # 自行修改相对应的域名、映射的端口、挂载的卷 cat > docker-compose.yml 3.2 启动 docker-compose up -d 查看启动的容器 $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a9ed8420da5d gitlab/gitlab-ce:latest \"/assets/wrapper\" 3 minutes ago Up 3 minutes (healthy) 0.0.0.0:80->80/tcp, :::80->80/tcp, 0.0.0.0:443->443/tcp, :::443->443/tcp, 0.0.0.0:222->22/tcp, :::222->22/tcp gitlab 四、访问gitlab gitlab默认端口为80，第一次访问需要设置root密码，最少8位 初始页面 这里的警告为gitlab默认开启了开放注册，点击 learn more 查看关闭方法 点击 Admin Area -> Settings -> General -> Sign-up restrictions 取消勾选 Sign-up enabled ，保存即可 gitlab默认开启注册 关闭后 五、gitlab相关文件、命令、服务 5.1 gitlab相关文件 gitlab相关文件路径说明 路径 说明 /opt/gitlab gitlab的程序安装目录 /var/opt/gitlab gitlab目录数据目录 /var/opt/gitlab/git-data/repositories 存放仓库数据 /etc/gitlab/gitlab.rb 主配置文件 /var/opt/gitlab/nginx/conf/gitlab-http.conf nginx配置文件 /var/opt/gitlab/postgresql/data Postgresql数据目录 5.2 gitlab相关命令 运维管理命令 命令 说明 cat /opt/gitlab/embedded/service/gitlab-rails/VERSION 查看版本 服务控制命令 命令 说明 gitlab-ctl status 查看目前gitlab所有服务运维状态 gitlab-ctl start/stop/restart 启动/停止/重启所有 gitlab 组件 gitlab-ctl start/stop nginx 单独启动/停止某个服务 gitlab-ctl reconfigure 重载配置文件 日志相关命令 命令 说明 gitlab-ctl tail 实时查看所有日志 gitlab-ctl tail redis/postgresql/gitlab-workhorse/logrotate/nginx/sidekiq/unicorn 实时查看各服务日志 5.3 gitlab相关服务 运行命令 gitlab-ctl status 查看gitlab所有服务 $ gitlab-ctl status run: alertmanager: (pid 3911) 54193s; run: log: (pid 3614) 54248s run: gitaly: (pid 3804) 54195s; run: log: (pid 3031) 54349s run: gitlab-exporter: (pid 3802) 54195s; run: log: (pid 3393) 54268s run: gitlab-workhorse: (pid 3778) 54196s; run: log: (pid 3307) 54286s run: grafana: (pid 3926) 54192s; run: log: (pid 3735) 54210s run: logrotate: (pid 16507) 1784s; run: log: (pid 2874) 54363s run: nginx: (pid 3330) 54281s; run: log: (pid 3348) 54279s run: node-exporter: (pid 3791) 54196s; run: log: (pid 3377) 54274s run: postgres-exporter: (pid 3919) 54192s; run: log: (pid 3644) 54243s run: postgresql: (pid 3067) 54346s; run: log: (pid 3099) 54343s run: prometheus: (pid 3897) 54194s; run: log: (pid 3446) 54256s run: puma: (pid 3253) 54300s; run: log: (pid 3266) 54297s run: redis: (pid 2896) 54358s; run: log: (pid 2913) 54355s run: redis-exporter: (pid 3815) 54194s; run: log: (pid 3412) 54262s run: sidekiq: (pid 3271) 54294s; run: log: (pid 3286) 54291s 官方文档对于各服务指标的说明 服务名 默认监听端口 说明 alertmanager TCP:9093 告警工具 gitaly TCP:9236 提供集群功能 gitlab-exporter --- 监控gitlab指标 gitlab-workhorse TCP:9229 gitlab反向代理，处理文件上传、下载，git推拉等操作 grafana TCP:3000 出图工具 logrotate --- 日志切割 nginx TCP:80 静态web服务器 node-exporter TCP:9100 Prometheus用来监控服务器指标 postgres-exporter TCP:9187 导出PostgreSQL指标 postgresql UDP:60387 默认数据库 prometheus TCP:9090 监控 puma TCP:8080 默认的web服务器 redis --- 缓存服务 redis-exporter TCP:9121 监控redis sidekiq TCP:8082 依赖redis的消息队列 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/gitlab/gitlab问题/gitlab一些问题汇总.html":{"url":"linux/自动化运维平台/gitlab/gitlab问题/gitlab一些问题汇总.html","title":"gitlab一些问题汇总","keywords":"","body":"[toc] gitlab一些问题汇总 1.关于gitlab7.12.0初始密码的问题 背景：公司用的gitlab版本是7.12.0，自己在虚拟机中安装的时候发现找不到初始密码，各种百度总结出以下两点 1.必须执行以下授权命令，否则会报502，原因未知 chmod -R o+x /var/opt/gitlab/gitlab-rails 2.gitlab7.12.0初始密码 root 5iveL!fe 2.gitlab官网注册时遇到的问题 注册gitlab时提示如下 原因 上面的错误是因为注册时有一个google的验证码需要输入。但是中国无法访问google,因此无法访问并输入该验证码导致 解决方法 翻墙或者通过下方的Github登陆 3.yum安装gitlab最新版 curl -s https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash yum -y install gitlab-ce 4.修改gitlab默认80端口 ⚠️修改完gitlab默认的80端口后只需重启，不能重载配置文件，否则会还原 //修改gitlab默认80端口 vim /var/opt/gitlab/nginx/conf/gitlab-http.conf //重启即可，不能重载配置文件，否则会覆盖修改 gitlab-ctl restart 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/gitlab/gitlab问题/gitlab非22端口问题.html":{"url":"linux/自动化运维平台/gitlab/gitlab问题/gitlab非22端口问题.html","title":"gitlab非22端口问题","keywords":"","body":"gitlab非22端口问题 背景说明： gitlab是docker启动的，ssh端口22映射到了宿主机的222端口，在添加远程仓库地址时写成了这样 git remote add origin git@10.0.0.13:222/root/jenkins-test1.git，已经把机器的共钥添加到了gitlab代码仓库中，但是推送代码的时候还是提示需要输入密码 原因： 远程仓库地址不正确 错误地址：git remote add origin git@10.0.0.13:222/root/jenkins-test1.git 正确地址： git remote add origin ssh://git@10.0.0.13:222/root/jenkins-test.git 遇到的一个问题 只能先把仓库克隆下来然后再提交代码，直接提交代码会报错，原因未知 1.删除之前添加的远程地址并重新添加 git remote rm origin git remote add origin ssh://git@10.0.0.13:222/root/jenkins-test.git 2.提交代码，报错仓库不存在 #提交代码，报错仓库不存在 $ git push origin master remote: remote: ======================================================================== remote: remote: The project you were looking for could not be found. remote: remote: ======================================================================== remote: fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. 解决方法 只能先把远程仓库克隆下来，然后再提交就可以了 #会提示添加了一个空存储库 $ git clone ssh://git@10.0.0.13:222/root/jenkins-test1.git Cloning into 'jenkins-test1'... warning: You appear to have cloned an empty repository. 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/gitlab/gitlab问题/gitlab报错502.html":{"url":"linux/自动化运维平台/gitlab/gitlab问题/gitlab报错502.html","title":"gitlab报错502","keywords":"","body":"gitlab报错502 背景说明 公司腾讯云gitlab服务器要迁移到阿里云，然后利用阿里云提供的 腾讯云CVM迁移至阿里云ECS 文档做相关迁移操作，迁移完成后在阿里云使用迁移的镜像启动gitlab，结果gitlab报错502，注意此502并非真正意义上的502，因为gitlab的各个服务均运行正常 错误页面 排查 使用命令 gitlab-ctl status 查看gitlab所有服务运行状态均无问题 使用命令 gitlab-ctl tail 查看gitlab实时日志，发现有如下报错，日志文件位置是 /var/log/gitlab/gitlab-workhorse/current ==> /var/log/gitlab/gitlab-workhorse/current 在stackoverflow中有外国老哥回答了这个问题，文章中有提到 puma.id 和 puma.state 这2个文件权限问题，这2个文件位于 /opt/gitlab/var/puma， puma.id 和 puma.state 文件原先权限为 git 用户所有，但是gitlab从腾讯云迁移到阿里云后，这2个文件的权限发生了变化，变成了腾讯云机器上的sudo用户，原因未知！ 解决方法 修改 /opt/gitlab/var/puma 下 puma.pid 和 puma.state 文件权限为 git 用户所有，然后执行命令 gitlab-ctl restart 重启gitlab即可 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/gitlab/gitlab配置文件修改/gitlab配置文件修改记录.html":{"url":"linux/自动化运维平台/gitlab/gitlab配置文件修改/gitlab配置文件修改记录.html","title":"gitlab配置文件修改记录","keywords":"","body":"[toc] gitlab配置文件修改记录 gitlab修改时区 修改gitlab配置文件 gitlab.rb 修改 gitlab_rails['time_zone'] = 'UTC' 修改为 gitlab_rails['time_zone'] = 'Asia/Shanghai' gitlab修改默认80端口 修改 /var/opt/gitlab/nginx/conf/gitlab-http.conf 文件中 listen 处 ⚠️ 依次执行完命令 gitlab-ctl restart 和 gitlab-ctl reconfigure 才会有文件 /var/opt/gitlab/nginx/conf/gitlab-http.conf server { listen *:80; ... ⚠️ 修改完后执行 gitlab-ctl restart重启即可，不能执行 gitlab-ctl reconfigure 重载配置文件，否则会覆盖修改 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/gitlab/gitlab知识点/1.关闭gitlab自动注册功能.html":{"url":"linux/自动化运维平台/gitlab/gitlab知识点/1.关闭gitlab自动注册功能.html","title":"关闭gitlab注册功能","keywords":"","body":"[toc] 关闭gitlab自动注册功能 gitlab登陆界面默认有注册功能，可根据需求关闭 第一步、选择 Admin area 第二步、选择 Settings 第三步、找到 Sing-up Restrictions 取消勾选 Sign-up enabled 然后保存 第四步、退出验证 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/gitlab/gitlab知识点/2.设置gitlab分支保护.html":{"url":"linux/自动化运维平台/gitlab/gitlab知识点/2.设置gitlab分支保护.html","title":"设置gitlab分支保护","keywords":"","body":"[toc] 设置gitlab分支保护 第一步、新建组、项目、用户 gitlab中以root用户新建一个组，名称任意；一个项目，名称任意；新建测试用户，名称任意 这里新建组 test ，新建项目 test，新建用户 xiaoming 并设置密码 第二步、配置公钥及权限 选择另外一台机器，做为测试机，使用如下命令生成密钥对，这里把测试用户 xiaoming 的公钥添加gitlab账户中 #生成密钥 ssh-keygen -t rsa -f ~/.ssh/id_rsa -P '' -q #注册密钥 cat .ssh/id_rsa.pub >> ~/.ssh/authorized_keys 添加测试机 root 用户的公钥到 xiaoming 用户中 gitlab root用户端把测试用户 xiaoming 添加到测试项目中并设置权限为 Developer 第三步、测试机克隆gitlab端创建的项目并创建文件提交至master分支 测试机克隆项目 ⚠️这里自定义了一个域名 ⚠️如果gitlab是docker启动的，并且把宿主机的任意ssh端口映射到docker容器中，则在克隆的时候需要注意修改克隆远程仓库的地址形式 错误地址：git remote add origin git@10.0.0.13:222/root/jenkins-test1.git 正确地址： git remote add origin ssh://git@10.0.0.13:222/root/jenkins-test.git git clone ssh://git@gitlab.my.com:222/test/test.git 测试机创建文件并推送代码 #配置用户、邮箱信息 $ git config --local user.name 'xiaoming' $ git config --local user.email 'xiaoming@163.com' $ touch branch-test $ git add . $ git commit -m 'touch branch-test' [master (root-commit) 2583ff0] touch branch-test 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 branch-test $ git push origin master Counting objects: 3, done. Writing objects: 100% (3/3), 212 bytes | 0 bytes/s, done. Total 3 (delta 0), reused 0 (delta 0) To ssh://git@gitlab.my.com:222/test/test.git * [new branch] master -> master 上述操作可以看到，普通用户 xiaoming 是可以直接推送代码到 master 分支的，接下来做分支保护，只允许项目的master的能推送到master分支 第四步、gitlab配置分支保护 进入项目， Settings --> Repository Protected Branches --> Expand Branch：你要保护的分支(master或者dev) Allowed to merge：谁有权限去跟这个分支合并 Allowed to push：允许往分支上去推送 设置完成后点击 Protect 测试机再次推送，可以看到会提示不被允许 $ git add . $ git commit -m 'master分支保护测试' [master ab7374e] master分支保护测试 1 file changed, 1 insertion(+) $ git push origin master Counting objects: 5, done. Delta compression using up to 2 threads. Compressing objects: 100% (2/2), done. Writing objects: 100% (3/3), 310 bytes | 0 bytes/s, done. Total 3 (delta 0), reused 0 (delta 0) remote: GitLab: You are not allowed to push code to protected branches on this project. To ssh://git@gitlab.my.com:222/test/test.git ! [remote rejected] master -> master (pre-receive hook declined) error: failed to push some refs to 'ssh://git@gitlab.my.com:222/test/test.git' 非master用户提交合并请求 xiaoming 用户无法推送代码到 master 分支，因为有 master 分支保护 点击 Merge requests 发起合并代码请求 点击 New merge request in test 选择源分支和合并分支 填写标题、描述、提交信息 提交完成 登陆到root用户查看合并请求 选择 Merge 合并完后就可以看到合并后的代码了 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/gitlab/gitlab知识点/3.gitlab离线官方文档.html":{"url":"linux/自动化运维平台/gitlab/gitlab知识点/3.gitlab离线官方文档.html","title":"使用gitlab离线文档","keywords":"","body":"gitlab离线官方文档 gitlab离线官方文档说明 gitlab提供了离线文档镜像，只需要启动相对应版本的容器即可 启动容器后访问 IP:4000 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/gitlab/gitlab知识点/4.gitlab备份.html":{"url":"linux/自动化运维平台/gitlab/gitlab知识点/4.gitlab备份.html","title":"gitlab备份与恢复","keywords":"","body":"[toc] gitlab备份 gitlab备份恢复官方文档 官方文档中写的很详细了，这里示例的版本是 10.6 ，只列出了几个比较常用的命令 ⚠️gitlab备份的时候gitlab必须处于运行状态 一、gitlab的备份目录路径设置 rpm包安装的gitlab配置文件是 /etc/gitlab/gitlab.rb，docker安装的gitlab配置文件是 volume path/config/gitlab.rb # 自定义备份路径 gitlab_rails['manage_backup_path'] = true # gitlab备份目录 gitlab_rails['backup_path'] = \"/var/opt/gitlab/backups\" # 生成的备份文件权限 gitlab_rails['backup_archive_permissions'] = 0644 # 备份保留天数为3个月（即90天，这里是7776000秒，单位是秒） gitlab_rails['backup_keep_time'] = 7776000 修改以上配置需要执行命令 gitlab-ctl reconfigure 生效 二、设置gitlab备份 gitLab 12.2或更高版本执行以下命令进行备份 gitlab-backup create gitLab 12.1和更早版本执行以下命令进行备份 gitlab-rake gitlab:backup:create CRON=1 环境变量CRON=1的作用是如果没有任何错误发生时， 隐藏备份脚本的所有进度输出 The CRON=1 environment setting directs the backup script to hide all progress output if there aren’t any errors. This is recommended to reduce cron spam. 如果gitlab是docker安装，则执行以下命令 docker exec -t gitlab /bin/sh -c 'gitlab-rake gitlab:backup:create' 使用以上命令会在 /var/opt/gitlab/backups 目录下(rpm包安装)创建一个名称类似为 1605454369_2020_11_15_10.6.1_gitlab_backup.tar 的压缩包 配合计划任务每天定时备份 00 1 * * * docker exec -t gitlab /bin/bash -c 'gitlab-rake gitlab:backup:create CRON=1' 还需要备份两个文件 # gitlab密钥文件 /etc/gitlab/gitlab-secrets.json # gitlab配置文件 /etc/gitlab/gitlab.rb 三、gitlab恢复 先停止相关数据连接服务 gitlab-ctl stop unicorn gitlab-ctl stop sidekiq 确认数据连接服务已经停止 gitlab-ctl status 恢复gitlab数据，中间需要输入两次 yes，BACKUP后面就是要恢复的备份压缩文件时间名称部分 gitlab的恢复操作会先将当前所有的数据清空，然后再根据备份数据进行恢复 gitlab-rake gitlab:backup:restore BACKUP=1605454369_2020_11_15_10.6.1 恢复完成后启动gitlab gitlab-ctl start 恢复命令完成后，可以check检查一下恢复情况 gitlab-rake gitlab:check SANITIZE=true 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/gitlab/gitlab知识点/5.关闭gitlab双重认证.html":{"url":"linux/自动化运维平台/gitlab/gitlab知识点/5.关闭gitlab双重认证.html","title":"gitlab关闭双重认证","keywords":"","body":"[toc] 关闭gitlab双重认证 root用户登陆gitlab后提示如下，按什么都不管用 解决方法：直接修改数据库 1.查看 /etc/passwd 文件里边gitlab对应的系统用户 其中 gitlab-psql 就是gitlab系统用户 $ grep gitlab /etc/passwd gitlab-www:x:995:992::/var/opt/gitlab/nginx:/bin/false git:x:994:991::/var/opt/gitlab:/bin/sh gitlab-redis:x:993:990::/var/opt/gitlab/redis:/bin/false gitlab-psql:x:992:989::/var/opt/gitlab/postgresql:/bin/sh gitlab-prometheus:x:991:988::/var/opt/gitlab/prometheus:/bin/sh 2.查看gitlab安装时PostgreSQL数据库的配置信息 database一行就是数据库名称 host一行就是pg数据库安装位置 $ cat /var/opt/gitlab/gitlab-rails/etc/database.yml # This file is managed by gitlab-ctl. Manual changes will be # erased! To change the contents below, edit /etc/gitlab/gitlab.rb # and run `sudo gitlab-ctl reconfigure`. production: adapter: postgresql encoding: unicode collation: database: gitlabhq_production pool: 10 username: \"gitlab\" password: host: \"/var/opt/gitlab/postgresql\" port: 5432 socket: sslmode: sslrootcert: sslca: load_balancing: {\"hosts\":[]} prepared_statements: false statements_limit: 1000 fdw: 3.登陆postgresql数据库，连接到 gitlabhq_production库 # 切换用户 su - gitlab-psql # 连接gitlabhq_production库 psql -h /var/opt/gitlab/postgresql -d gitlabhq_production pg数据库相关操作 # 查看数据库 \\l # 查看多表 \\dt # 查看单表，如users表 \\d users 4.修改数据库 查看users表中用户的关键信息，取4个字段 SELECT name,username,otp_required_for_login,two_factor_grace_period, require_two_factor_authentication_from_group FROM users; 修改数据库，将二次认证字段的值修改为 f UPDATE users set require_two_factor_authentication_from_group = 'f' WHERE username = 'root'; 5.重新登陆gitlab即可 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/gitlab/gitlab知识点/6.gitlab忘记root密码.html":{"url":"linux/自动化运维平台/gitlab/gitlab知识点/6.gitlab忘记root密码.html","title":"gitlab忘记root密码","keywords":"","body":"gitlab忘记root密码 [toc] 切换到 git 用户 su - git 登录gitLab的rails控制台 $ gitlab-rails console -------------------------------------------------------------------------------- GitLab: 12.10.1 (e658772bd63) FOSS GitLab Shell: 12.2.0 PostgreSQL: 11.7 -------------------------------------------------------------------------------- Loading production environment (Rails 6.0.2) irb(main):001:0> 执行命令定位到root用户 user = User.where(id: 1).first irb(main):003:0> user = User.where(id: 1).first => # 修改root密码 user.password='xxx' irb(main):004:0> user.password='xxx' => \"xxx\" 保存密码 user.save! irb(main):005:0> user.save! Enqueued ActionMailer::DeliveryJob (Job ID: 20ec0a5f-9762-473a-9db6-b2eb14b091c1) to Sidekiq(mailers) with arguments: \"DeviseMailer\", \"password_change\", \"deliver_now\", #> => true 保存密码重新使用root用户登陆即可 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/gitlab/gitlab知识点/7.gitlab迁移.html":{"url":"linux/自动化运维平台/gitlab/gitlab知识点/7.gitlab迁移.html","title":"gitlab迁移","keywords":"","body":"gitlab迁移 gitlab迁移的整体步骤为 1.在新的服务器上安装相同版本的gitlab 2.将旧服务器上的gitlab备份，然后把备份文件拷贝到新机器上，最后再导入备份文件 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/gitlab/gitlab知识点/8.gitlab集成ldap.html":{"url":"linux/自动化运维平台/gitlab/gitlab知识点/8.gitlab集成ldap.html","title":"gitlab集成ldap","keywords":"","body":"[toc] gitlab集成ldap gitlab官方文档总地址 gitlab配置ldap官方文档 官方文档配置示例 gitlab_rails['ldap_enabled'] = true gitlab_rails['prevent_ldap_sign_in'] = false gitlab_rails['ldap_servers'] = { 'main' => { 'label' => 'LDAP', 'host' => 'ldap.mydomain.com', 'port' => 389, 'uid' => 'sAMAccountName', 'encryption' => 'simple_tls', 'verify_certificates' => true, 'bind_dn' => '_the_full_dn_of_the_user_you_will_bind_with', 'password' => '_the_password_of_the_bind_user', 'verify_certificates' => true, 'tls_options' => { 'ca_file' => '', 'ssl_version' => '', 'ciphers' => '', 'cert' => '', 'key' => '' }, 'timeout' => 10, 'active_directory' => true, 'allow_username_or_email_login' => false, 'block_auto_created_users' => false, 'base' => 'dc=example,dc=com', 'user_filter' => '', 'attributes' => { 'username' => ['uid', 'userid', 'sAMAccountName'], 'email' => ['mail', 'email', 'userPrincipalName'], 'name' => 'cn', 'first_name' => 'givenName', 'last_name' => 'sn' }, 'lowercase_usernames' => false, # EE Only 'group_base' => '', 'admin_group' => '', 'external_groups' => [], 'sync_ssh_keys' => false } } gitlab.rb 配置文件默认配置，默认是有主从配置的 ###! **remember to close this block with 'EOS' below** # gitlab_rails['ldap_servers'] = YAML.load 如果ldap是单点，则只需要修改为如下内容即可 gitlab_rails['ldap_enabled'] = true # 允许ldap，true为开启 gitlab_rails['prevent_ldap_sign_in'] = false # 拒绝ldap登陆，false为关闭 gitlab_rails['ldap_servers'] = YAML.load 配置完成后需要执行 gitlab-ctl reconfigure 使配置生效 基本配置项说明 配置项 说明 是否必须 示例 label ldap服务名称 ✅ 是 'Paris' or 'Acme, Ltd.' host ldap服务器地址 ✅ 是 'ldap.mydomain.com' port ldap服务器端口 ✅ 是 389 or 636 (for SSL) uid 用户名的LDAP属性。应该是属性，而不是映射到uid的值。 ✅ 是 'sAMAccountName' or 'uid' or 'userPrincipalName' bind_dn 绑定的用户的完整DN。 ⭕️ 否 'america\\momo' or 'CN=Gitlab,OU=Users,DC=domain,DC=com' password 绑定用户的密码。 ⭕️ 否 'your_great_password' encryption 加密方法 ✅ 是 'start_tls' or 'simple_tls' or 'plain' verify_certificates 当加密方法为start_tls或simple_tls时，启用SSL证书验证。默认值为true。 ⭕️ 否 boolean timeout 设置LDAP查询的超时时间，单位为秒。这有助于避免在LDAP服务器失去响应时阻塞请求。值为0表示没有超时。(默认值:10) ⭕️ 否 10 or 30 active_directory 此设置指定LDAP服务器是否为Active Directory LDAP服务器。对于非AD服务器，它会跳过特定于AD的查询。如果您的LDAP服务器不是AD，请将此设置为false。 ⭕️ 否 boolean allow_username_or_email_login 如果启用，GitLab将忽略用户在登录时提交的LDAP用户名中第一个@之后的所有内容。如果您在ActiveDirectory上使用uid: 'userPrincipalName'，您必须禁用此设置，因为userPrincipalName包含@。 ⭕️ 否 boolean block_auto_created_users 为了严格控制GitLab安装中计费用户的数量，可以启用此设置来阻止新用户，直到管理员清除他们(默认值:false)。 ⭕️ 否 boolean base 搜索用户规则 ✅ 是 'ou=people,dc=gitlab,dc=example' or 'DC=mydomain,DC=com' user_filter LDAP用户进行过滤。注意:GitLab不支持omniauth-ldap的自定义过滤器语法。 ⭕️ 否 For examples, read Examples of user filters lowercase_usernames 如果启用，GitLab会将名称转换为小写。 ⭕️ 否 boolean retry_empty_result_with_codes 如果结果/内容为空，则尝试重试操作的LDAP查询响应代码数组。对于谷歌安全LDAP，将该值设置为[80]。 ⭕️ 否 [80] SSL配置项说明 配置项 说明 是否必须 示例 ca_file 指定包含pem格式CA证书的文件的路径，例如，如果您需要一个内部CA。 ⭕️ 否 '/etc/ca.pem' ssl_version 如果OpenSSL的默认版本不合适，则指定OpenSSL使用的SSL版本。 ⭕️ 否 'TLSv1_1' ciphers 与LDAP服务器通信时使用的特定SSL密码。 ⭕️ 否 'ALL:!EXPORT:!LOW:!aNULL:!eNULL:!SSLv2' cert 客户端证书。 ⭕️ 否 '-----BEGIN CERTIFICATE----- -----END CERTIFICATE -----' key 客户端私钥。 ⭕️ 否 '-----BEGIN PRIVATE KEY----- -----END PRIVATE KEY -----' 属性配置项说明 GitLab用于为LDAP用户创建帐户的LDAP属性。指定的属性可以是字符串形式的属性名(例如，'mail')，也可以是要按顺序尝试的属性名数组(例如，['mail'，'email'])。用户的LDAP登录是上面作为uid指定的属性。 配置项 说明 是否必须 示例 username 用户名用于用户自己的项目的路径(如 gitlab.example.com/username/project )，当在问题中提到它们时，合并请求和评论(如 @username )。如果为 username 指定的属性包含电子邮件地址，则GitLab用户名是 @ 之前的电子邮件地址的一部分。 ⭕️ 否 ['uid', 'userid', 'sAMAccountName'] email 用户邮箱的LDAP属性。 ⭕️ 否 ['mail', 'email', 'userPrincipalName'] name 用户显示名的LDAP属性。如果 name 为空，则全名取自 first_name 和last_name。 ⭕️ 否 属性'cn'或'displayName'通常带有全名。或者，您可以通过指定一个不存在的属性(如'somethingNonExistent')来强制使用first_name和last_name。 first_name 用户名的LDAP属性。当为name配置的属性不存在时使用。 ⭕️ 否 'givenName' last_name 用户姓的LDAP属性。当为name配置的属性不存在时使用。 ⭕️ 否 'sn' LDAP同步配置项说明 配置项 说明 是否必须 示例 group_base 用于搜索组的基数。 ⭕️ 否 'ou=groups,dc=gitlab,dc=example' admin_group 包含GitLab管理员的组的CN。注意:不是cn=administrators或完整DN。 ⭕️ 否 'administrators' external_groups 包含应被视为外部用户的组的CNs数组。注意:不是cn=interns或完整的DN。 ⭕️ 否 ['interns', 'contractors'] sync_ssh_keys The LDAP attribute containing a user’s public SSH key. ⭕️ 否 'sshPublicKey'，如果没有设置则为false 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/gitlab/gitlab知识点/9.gitlab配置邮件.html":{"url":"linux/自动化运维平台/gitlab/gitlab知识点/9.gitlab配置邮件.html","title":"gitlab配置邮件","keywords":"","body":"[toc] gitlab配置邮件 gitlab配置邮件官方文档 QQ exmail (腾讯企业邮箱) 配置示例 gitlab_rails['smtp_enable'] = true gitlab_rails['smtp_address'] = \"smtp.exmail.qq.com\" gitlab_rails['smtp_port'] = 465 gitlab_rails['smtp_user_name'] = \"xxxx@xx.com\" gitlab_rails['smtp_password'] = \"password\" gitlab_rails['smtp_authentication'] = \"login\" gitlab_rails['smtp_enable_starttls_auto'] = true gitlab_rails['smtp_tls'] = true gitlab_rails['gitlab_email_from'] = 'xxxx@xx.com' gitlab_rails['smtp_domain'] = \"exmail.qq.com\" NetEase Free Enterprise Email (网易免费企业邮) 配置示例 gitlab_rails['smtp_enable'] = true gitlab_rails['smtp_address'] = \"smtp.ym.163.com\" gitlab_rails['smtp_port'] = 465 gitlab_rails['smtp_user_name'] = \"xxxx@xx.com\" gitlab_rails['smtp_password'] = \"password\" gitlab_rails['smtp_authentication'] = \"login\" gitlab_rails['smtp_enable_starttls_auto'] = true gitlab_rails['smtp_tls'] = true gitlab_rails['gitlab_email_from'] = 'xxxx@xx.com' gitlab_rails['smtp_domain'] = \"smtp.ym.163.com\" 开启smtp gitlab_rails['smtp_enable'] = true 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/gitlab/gitlab知识点/gitlab Server hooks.html":{"url":"linux/自动化运维平台/gitlab/gitlab知识点/gitlab Server hooks.html","title":"gitlab Server hooks","keywords":"","body":"gitlab Server hooks gitlab 文档地址 gitlab文档归档地址 在文档中搜索 Server hooks 即可 1.简介 Git支持在不同的操作上执行钩子。这些钩子运行在服务器上，可用于强制特定的提交策略或基于存储库状态执行其他任务。 gitlab支持以下的 hooks pre-receive post-receive update 2.配置 背景说明 公司使用gitlab作为代码仓库，有些开发配置本地git仓库时不配置个人企业邮箱，因此配置Server hooks来强制使用特定的邮箱格式 gitlab的配置文件 /etc/gitlab/gitlab.rd 中有这样一行默认配置 gitlab_shell['custom_hooks_dir'] = \"/opt/gitlab/embedded/service/gitlab-shell/hooks\" ，这个目录定义了Server hooks 脚本存放路径，在这个路径下可以新建 pre-receive.d 、 post-receive.d 、update.d，然后在这些目录中存放相应的脚本 新建目录，默认没有 hooks 目录 mkdir /opt/gitlab/embedded/service/gitlab-shell/hooks 新建子目录 mkdir /opt/gitlab/embedded/service/gitlab-shell/hooks/pre-receive.d 新建脚本 脚本内容为判断本地git仓库中配置的邮箱是否为要求的格式，如不是则报错并提示如何修改为正确格式 脚本名称任意 cat > pre-receive *} temp=${log#*@} email_suffix=${temp%>*} if [[ ${email_suffix} != '163.com' ]];then echo -e '\\033[31m you commit code use email is: \"'$email'\" the suffix of this email is: '${email_suffix}' Email format error: \"'$email'\" is not formal XXOO LDAP email can not commit your code unless follow these steps to modify your email to LDAP email\\033[0m \\033[32msteps: 1. git config --global --replace-all user.email xxx@163.com 2. git commit --amend --author \"xxx \" (modify author email to LDAP email(xxx@163.com) in your commit infos) 3. :wq\\033[0m \\033[33mattention: if your commit code use email is different from your LDAP email (xxx@163.com),your code will not statistical\\033[0m' exit 1 else echo -e '\\033[32myour email is right\\033[0m' exit 0 fi EOF 修改脚本所有者及赋予可执行权限 chown git.git pre-receive && chmod u+x pre-receive git本地仓库配置的邮箱是126邮箱，在进行提交代码的时候就会报错如下 修改为163格式邮箱 git config --global --replace-all user.email xxx@163.com git commit --amend --author \"xxx \" 修改完成后再提交就可以了 遇到的报错 在有合并操作的时候遇到了如下报错，gitlab中日志也没有有用的信息，但是查看提交信息，显示还是规则外的邮箱，所以这里有地方影响了合并操作 解决方法就是合并人修改一下commit邮箱就可以了 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/gitlab/gitlab知识点/gitlab文档.html":{"url":"linux/自动化运维平台/gitlab/gitlab知识点/gitlab文档.html","title":"gitlab文档","keywords":"","body":"gitlab文档 gitlab官方文档 gitlab官方文档归档地址 gitlab github地址 gitlab官方提供了启动一个容器即可查看相应版本文档的方式 docker run -it --rm -p 4000:4000 registry.gitlab.com/gitlab-org/gitlab-docs:14.0 浏览器访问 IP:4000 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/jenkins/jenkins安装/CentOS7.5安装jenkins-2.176.html":{"url":"linux/自动化运维平台/jenkins/jenkins安装/CentOS7.5安装jenkins-2.176.html","title":"CentOS7.5安装jenkins-2.176","keywords":"","body":"[toc] CentOS7.5安装jenkins-2.176 jenkins中文官网 jenkins官网 1.安装jdk8 jenkins运行依赖jdk 自行到oracle官网下载jdk 1.解压缩包 [root@jenkins ~]# tar xf jdk-8u211-linux-x64.tar.gz -C /usr/local 2.导出环境变量 [root@jenkins ~]# cat >/etc/profile.d/jdk8.sh 2.安装jenkins，这里安装长期支持版 #安装LTS(长期支持版) LTS (长期支持) 版本每12周从常规版本流中选择，作为该时间段的稳定版本。 [root@jenkins ~]# curl -o /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo [root@jenkins ~]# rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key [root@jenkins ~]# yum -y install jenkins #安装每周更新版 每周都会发布一个新版本，为用户和插件开发人员提供错误修复和功能。 [root@jenkins ~]# curl -o /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat/jenkins.repo [root@jenkins ~]# rpm --import https://pkg.jenkins.io/redhat/jenkins.io.key [root@jenkins ~]# yum -y install jenkins #安装指定版本 https://pkg.jenkins.io/redhat-stable/ 3.修改jenkins配置文件，让jenkins以root用户运行 [root@jenkins ~]# sed -i.bak '29cJENKINS_USER=\"root\"' /etc/sysconfig/jenkins 4.启动jenkins #启动jenins并加入开机自启 [root@jenkins ~]# systemctl enable jenkins && systemctl start jenkins 5.浏览器访问jenkins jenkins刚启动比较慢，等待启动完成 从/var/lib/jenkins/secrets/initialAdminPassword文件按中获取密码 是否安装插件，自行选择 选择插件进行安装，必须选择Locale插件，修改jenkins语言 等待安装完成 插件安装完成后创建管理员用户，输入密码、用户全名、邮件地址 jenkins URL默认为主机IP地址加8080端口 jenkins首界面 6.设置jenkins默认语言为中文 选择jenkins管理 配置系统 默认语言填写zh_CN 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/jenkins/jenkins安装/docker安装jenkins.html":{"url":"linux/自动化运维平台/jenkins/jenkins安装/docker安装jenkins.html","title":"docker安装jenkins","keywords":"","body":"docker安装jenkins docker jenkins 中国定制版说明 jenkins安装插件的时候由于源在国外，因此安装会非常慢甚至失败，当然也有国内源可以使用，但是需要自行修改文件，jenkins中文社区推出了中国定制版的jenkins docker镜像，只需要下载相应的docker镜像并启动容器即可，不需要做任何修改操作，并且安装后jenkins插件下载速度有明显的提升 docker jenkins中国定制版 dockerhub地址 docker jenkins中国定制版 github地址 jenkins中文社区维护的中文简体插件 jenkins插件下载地址 直接使用jenkins中国定制版镜像启动容器 docker run \\ -u root \\ -d \\ -h jenkins \\ --name jenkins \\ -p 8081:8080 \\ -p 50000:50000 \\ -v /usr/local/jenkins:/var/jenkins_home \\ -v /var/run/docker.sock:/var/run/docker.sock \\ jenkinszh/jenkins-zh:2.239 登陆jenkins后会提示jenkins反向代理配置错误，参考 官方提供的nginx配置 即可 upstream jenkins { keepalive 32; # keepalive connections server 127.0.0.1:8081; # jenkins ip and port } # Required for Jenkins websocket agents map $http_upgrade $connection_upgrade { default upgrade; '' close; } server { listen 80; server_name jenkins.abc.com; return 302 https://$server_name$request_uri; } server { listen 443 ssl; server_name jenkins.abc.com; # this is the jenkins web root directory # (mentioned in the /etc/default/jenkins file) root /var/run/jenkins/war/; access_log /var/log/jenkins/jenkins.abc.com.access.log; error_log /var/log/jenkins/jenkins.abc.com.error.log; ssl_certificate ssl_key/aldwx/1_abc.com_bundle.crt; ssl_certificate_key ssl_key/aldwx/2_abc.com.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # pass through headers from Jenkins that Nginx considers invalid ignore_invalid_headers off; location ~ \"^/static/[0-9a-fA-F]{8}\\/(.*)$\" { # rewrite all static files into requests to the root # E.g /static/12345678/css/something.css will become /css/something.css rewrite \"^/static/[0-9a-fA-F]{8}\\/(.*)\" /$1 last; } location /userContent { # have nginx handle all the static requests to userContent folder # note : This is the $JENKINS_HOME dir root /var/lib/jenkins/; if (!-f $request_filename){ # this file does not exist, might be a directory or a /**view** url rewrite (.*) /$1 last; break; } sendfile on; } location / { sendfile off; proxy_pass http://jenkins; proxy_redirect default; proxy_http_version 1.1; # Required for Jenkins websocket agents proxy_set_header Connection $connection_upgrade; proxy_set_header Upgrade $http_upgrade; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_max_temp_file_size 0; #this is the maximum upload size client_max_body_size 10m; client_body_buffer_size 128k; proxy_connect_timeout 90; proxy_send_timeout 90; proxy_read_timeout 90; proxy_buffering off; proxy_request_buffering off; # Required for HTTP CLI commands proxy_set_header Connection \"\"; # Clear for keepalive } } 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/jenkins/jenkins知识点/jenkins修改数据存放路径.html":{"url":"linux/自动化运维平台/jenkins/jenkins知识点/jenkins修改数据存放路径.html","title":"jenkins修改数据存放路径","keywords":"","body":"jenkins修改数据存放路径 jenkins数据存放路径默认是 /root/.jenkins war包安装 如果是用tomcat做容器的话，则在./bin/catalina.sh文件添加以下语句即可： export JENKINS_HOME=\"存放路径\" tomcat安装路径为 /data/app/tomcat7 jenkins war包路径为 /data/app/tomcat7/webapps/jenkins.war 修改文件 /data/app/tomcat7/bin/catalina.sh ，找到 export JENKINS_HOME=，指定路径即可 rpm包安装 jenkins默认安装路径是 /var/lib/jenkins 修改配置文件 /etc/sysconfig/jenkins ，找到 JENKINS_HOME=\"/var/lib/jenkins\"，重新指定路径即可 docker安装 docker安装jenkins的目录为 /var/jenkins_home，这里要看你指定的挂载卷路径，从挂载卷的路径中找到 docker run -p 8080:8080 -p 50000:50000 -v /your/home:/var/jenkins_home jenkins 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/jenkins/jenkins知识点/jenkins备份.html":{"url":"linux/自动化运维平台/jenkins/jenkins知识点/jenkins备份.html","title":"jenkins备份","keywords":"","body":"jenkins备份 一、安装备份插件 jenkins备份需要安装插件 thinBackup，如果没有配置加速或者下载比较慢 jenkins插件下载地址 thinBackup插件下载地址 插件安装完成后会在系统管理中出现如下 ThinBackup 二、备份设置 创建备份目录并设置目录权限为jenkins [ -f /backup/jenkins-bak ] || mkdir -p /backup/jenkins-bak && chown jenkins.jenkins /backup/jenkins-bak 手动备份 jenkins页面中选择 Manage Jenkins --> ThinBackup --> Backup Now(手动备份) 备份完成后会在设定的备份目录下生成日期格式的目录 FULL-2020-11-17_14-12 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/jenkins/jenkins知识点/jenkins从构建生成进程.html":{"url":"linux/自动化运维平台/jenkins/jenkins知识点/jenkins从构建生成进程.html","title":"jenkins从构建生成进程","keywords":"","body":"jenkins从构建生成进程 参考文章 从构建生成进程 有时，您希望从比构建本身寿命更长的构建中生成一个进程。例如，构建的一部分可能是使用构建结果启动新的应用程序服务器。执行此操作时，您经常会遇到构建未终止的问题；你会看到 shell script/ant/maven 按预期终止，但 Jenkins 只是坚持等待，好像它没有注意到构建已经结束。 从 Jenkins 1.136 开始，Jenkins 检测到这种情况，而不是导致无限阻塞，它只会打印出警告并让您继续。但是您仍然应该了解导致这种情况的原因。 以上为 官方文章 对于这个问题的描述，现在结合实际使用场景再描述一下当前的问题，比如说我们在jenkins项目构建中使用的是shell命令，并且又涉及到启动后台服务命令，例如 gitbook serve & ，当我们在jenkins项目构建中执行此命令，jenkins的构建会报错，大致原因是不能执行后台进程，但是当把 & 去掉让进程前台执行，jenkins的构建会无法结束，jenkins会一直显示构建中 造成此现象的原因 发生此问题的原因是文件描述符泄漏以及它们如何从一个进程继承到另一个进程。Jenkins 和子进程通过三个管道（stdin/stdout/stderr）相连。这使得 Jenkins 可以捕获子进程的输出。由于子进程可能会向管道写入大量数据并在此之后立即退出，因此 Jenkins 需要确保它在认为构建结束之前排空管道。詹金斯通过等待 EOF 来做到这一点。 当进程因任何原因终止时，操作系统会关闭它拥有的所有文件描述符。因此，即使该过程没有关闭 stdout/stderr，Jenkins 仍将获得 EOF。 当这些文件描述符被继承给其他进程时，就会出现复杂情况。假设子进程将另一个进程派生到后台。后台进程（AKA 守护进程）继承父进程的所有文件描述符，包括连接子进程和 Jenkins 的 stdout/stderr 管道的写入端。如果守护进程忘记关闭它们，即使子进程退出，Jenkins 也不会获得管道的 EOF，因为守护进程仍然打开这些描述符。这个问题就是这样发生的。 一个好的守护程序会关闭所有文件描述符以避免出现这样的问题，但通常会有不遵守规则的坏文件描述符。 解决方法 在linux中，可以使用如下方法执行，⚠️在 Jenkins Pipeline 的情况下使用 JENKINS_NODE_COOKIE 而不是 BUILD_ID daemonize -E BUILD_ID=dontKillMe /path/to/your/command 使用示例 daemonize需要执行命令 yum -y install daemonize 安装 # 原先执行方式，jenkins构建不会停止 cd /gitbook && gitbook serve # 修改为 cd /gitbook && daemonize -E BUILD_ID=dontKillMe gitbook serve 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/jenkins/jenkins知识点/jenkins用户权限管理.html":{"url":"linux/自动化运维平台/jenkins/jenkins知识点/jenkins用户权限管理.html","title":"jenkins用户权限管理","keywords":"","body":"[toc] jenkins用户权限管理 1.安装插件 Role-based Authorization Strategy 在jenkins插件中心搜索 Role-based 2.开启插件 Manage Jenkins --> Configure Global Security --> Authorization --> Role-Based Strategy 勾选 Role-Based Strategy 后就会出现 Manage and Assign Roles 3.环境准备 3.1 新建项目 新建2个文件夹 online_web 和 test_web，其中 online_web 模拟生产web项目，test_web 模拟测试web项目 online_web 下有2个项目 abc.baidu.com 、online.baidu.com test_web 下有2个项目 abctest.baidu.com 、test.baidu.com 3.2 新建用户 新建 online 和 test 用户，其中 online 用户模拟生产用户、test 模拟测试用户 4.角色权限分配 4.1 新建角色 在 Manage Roles 管理角色选项中新建角色 新建2个角色 测试 和 生产 ，必须勾选 Overall 中的 Read 选项，表示角色对全局可读，否则用户登陆后无任何查看权限 新建角色之后，开始给角色匹配项目，在 Item roles 中匹配项目，主要用到的就是正则表达式，例如在 Pattern 中填写 test_web.* 表示的就是对 test_web 目录下的所有项目有权限 4.2 分配角色 在 Global roles 中配置test用户对测试角色有权限，online用户对生产角色有权限 4.3 测试验证 用test用户登陆，可以看到test用户只对test_web下的所有项目有权限，online用户同理 4.4 Role-based Authorization Strategy 插件授权步骤 1.在 Manage Roles 中新建角色，必须创建全局角色设置可读，然后在角色下边利用正则匹配项目 2.在 Assign Roles 中分配角色，是针对 Manage Roles 中设置的角色，在 Assign Roles 设置哪个用户对 Manage Roles 中设置的角色有什么权限 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/jenkins/jenkins知识点/jenkins配置构建邮件通知.html":{"url":"linux/自动化运维平台/jenkins/jenkins知识点/jenkins配置构建邮件通知.html","title":"jenkins配置构建邮件通知","keywords":"","body":"[toc] jenkins配置构建邮件通知 本文严重抄袭于互联网 1.安装邮件插件 Email Extension Plugin 2.配置管理员邮件地址 Manage Jenkins -> Jenkins Location -> System Admin e-mail address 3.配置发件人信息 Manage Jenkins -> Extended E-mail Notification ⚠️SMTP Password 处填写的是邮箱授权码 4.配置邮件内容模板 Default Content 默认内容 $PROJECT_NAME - Build # $BUILD_NUMBER - $BUILD_STATUS: Check console output at $BUILD_URL to view the results. 邮件内容模板 ${ENV, var=\"JOB_NAME\"}-第${BUILD_NUMBER}次构建日志 本邮件由系统自动发出，无需回复！ 各位同事，大家好，以下为${PROJECT_NAME }项目构建信息 构建结果 - ${BUILD_STATUS} 构建信息 项目名称 ： ${PROJECT_NAME} 构建编号 ： 第${BUILD_NUMBER}次构建 触发原因： ${CAUSE} 构建状态： ${BUILD_STATUS} 构建日志： ${BUILD_URL}console 构建 Url ： ${BUILD_URL} 工作目录 ： ${PROJECT_URL}ws 项目 Url ： ${PROJECT_URL} 失败用例 $FAILED_TESTS 最近提交(#$SVN_REVISION) ${CHANGES_SINCE_LAST_SUCCESS, reverse=true, format=\"%c\", changesFormat=\"%d [%a] %m\"} 详细提交: ${PROJECT_URL}changes 5.配置邮件触发机制 在 Default Triggers 处配置触发机制 6.配置项目发送邮件 在项目下 Post-build Actions -> Editable Email Notification 相关信息配置 配置触发 Post-build Actions -> Advanced settings 默认只有 Failuer - Any ，点击 Add 增加 Success ，既构建失败和成功后发送邮件通知 选择发送至 Recipient List 是全局配置，如果想针对单个项目发送指定的邮箱则需要去掉 Recipient List 选项，在下方的 Recipient List 中单独进行配置，单独配置适用场景为针对某个项目只给相关开发发送邮件，而全局配置 Recipient List 配置运维组邮箱，这样就可以运维接受所有构建邮件，而开发只需要接受自己组的项目的邮件 7.构建项目测试 构建项目，可以看到最后会有 Email was triggered for: Success 收到的邮件 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/jenkins/jenkins知识点/jenkins配置构建企业微信通知.html":{"url":"linux/自动化运维平台/jenkins/jenkins知识点/jenkins配置构建企业微信通知.html","title":"jenkins配置构建企业微信通知","keywords":"","body":"[toc] jenkins配置构建企业微信通知 1.安装插件 Qy Wechat Notification 2.配置构建后企业微信通知 Post-build Actions -> Add post-build action -> 企业微信通知 填写企业微信机器人webhook地址，设置触发的条件，通知的用户和手机号码插件中说明的已经很清楚了 3.构建测试 构建成功后，相应的企业微信群中就会收到信息 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/jenkins/jenkins知识点/jenkins修改主目录.html":{"url":"linux/自动化运维平台/jenkins/jenkins知识点/jenkins修改主目录.html","title":"jenkins修改主目录","keywords":"","body":"jenkins修改主目录 1.rpm包安装 rpm包安装的jenkins默认主目录是 /var/lib/jenkins 安装后的默认目录结构 $ rpm -ql jenkinsls /etc/init.d/jenkins /etc/logrotate.d/jenkins /etc/sysconfig/jenkins /usr/lib/jenkins /usr/lib/jenkins/jenkins.war /usr/sbin/rcjenkins /var/cache/jenkins /var/lib/jenkins /var/log/jenkins 修改配置文件/etc/sysconfig/jenkins 修改 JENKINS_HOME=\"/var/lib/jenkins\" 修改为 JENKINS_HOME=\"xxx\" 修改完成后重启jenkins即可 2.war包安装 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/jenkins/jenkins知识点/jenkins使用nginx反向代理报错.html":{"url":"linux/自动化运维平台/jenkins/jenkins知识点/jenkins使用nginx反向代理报错.html","title":"jenkins使用nginx反向代理报错","keywords":"","body":"jenkins使用nginx反向代理报错 jenkins官方说明文档 jenkins使用nginx做反向代理，jenkins中提示如下 原先nginx配置如下 server { listen 80; server_name jenkins.xxx.com; rewrite ^(.*) https://$server_name$1 permanent; } server { listen 443 ssl; server_name jenkins.xxx.com; ssl_certificate ssl/xxx.com.pem; ssl_certificate_key ssl/xxx.com.key; ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; location / { proxy_pass http://127.0.0.1:8080; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-NginX-Proxy true; proxy_hide_header Server; proxy_redirect off; } } 修改nginx配置为如下 注意 root /var/lib/jenkins/; 配置，如果修改了jenkins主目录地址则需要修改为相应地址 upstream jenkins { keepalive 32; # keepalive connections server 127.0.0.1:8080; # jenkins ip and port } # Required for Jenkins websocket agents map $http_upgrade $connection_upgrade { default upgrade; '' close; } server { listen 80; server_name jenkins.xxx.com; return 302 https://$server_name$request_uri; } server { listen 443 ssl; server_name jenkins.xxx.com; # this is the jenkins web root directory # (mentioned in the /etc/default/jenkins file) root /var/run/jenkins/war/; access_log /var/log/jenkins/jenkins.xxx.com.access.log; error_log /var/log/jenkins/jenkins.xxx.com.error.log; ssl_certificate ssl/xxx.com.pem; ssl_certificate_key ssl/xxx.com.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # pass through headers from Jenkins that Nginx considers invalid ignore_invalid_headers off; location ~ \"^/static/[0-9a-fA-F]{8}\\/(.*)$\" { # rewrite all static files into requests to the root # E.g /static/12345678/css/something.css will become /css/something.css rewrite \"^/static/[0-9a-fA-F]{8}\\/(.*)\" /$1 last; } location /userContent { # have nginx handle all the static requests to userContent folder # note : This is the $JENKINS_HOME dir root /var/lib/jenkins/; if (!-f $request_filename){ # this file does not exist, might be a directory or a /**view** url rewrite (.*) /$1 last; break; } sendfile on; } location / { sendfile off; proxy_pass http://jenkins; proxy_redirect default; proxy_http_version 1.1; # Required for Jenkins websocket agents proxy_set_header Connection $connection_upgrade; proxy_set_header Upgrade $http_upgrade; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_max_temp_file_size 0; #this is the maximum upload size client_max_body_size 10m; client_body_buffer_size 128k; proxy_connect_timeout 90; proxy_send_timeout 90; proxy_read_timeout 90; proxy_buffering off; proxy_request_buffering off; # Required for HTTP CLI commands proxy_set_header Connection \"\"; # Clear for keepalive } } 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/jenkins/jenkins知识点/webhook/gitlab webhook 报错.html":{"url":"linux/自动化运维平台/jenkins/jenkins知识点/webhook/gitlab webhook 报错.html","title":"gitlab webhook 报错 ","keywords":"","body":"gitlab webhook 报错 报错1 Hook executed successfully but returned HTTP 422 gitlab+jenkins 配置webhook自动触发，在gitlab项目中配置好webhook后点击 Test Push events 报错如下 原因：jenkins url回调地址写错了，复制 Build when a change is pushed to GitLab. GitLab webhook URL 后面的url即可 报错2 Hook executed successfully but returned HTTP 403 gitlab+jenkins 配置webhook自动触发，在gitlab项目中配置好webhook后点击 Test Push events 报错如下 需要在jenkins中 系统管理 -> 系统配置 -> Gitlab 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/jenkins/jenkins知识点/webhook/gitea+jenkins实现webhook.html":{"url":"linux/自动化运维平台/jenkins/jenkins知识点/webhook/gitea+jenkins实现webhook.html","title":"gitea+jenkins实现webhook","keywords":"","body":"gitea+jenkins实现webhook 1.jenkins安装插件 Generic Webhook Trigger 2.jenkins项目配置 在jenkins项目中构建触发器下勾选 Generic Webhook Trigger，然后在 Token 处填写一个任意名称的token名 3.gitea项目配置 在项目中点击 设置 在 Web钩子 处点击 添加 Web 钩子 配置webhook，目标url填写如下格式，只需要替换 JENKINS_URL 和 token=xxx http://JENKINS_URL/generic-webhook-trigger/invoke?token=xxx 添加后的webhook 如果推送有问题我们可以点击 测试推送 来查看问题 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/jenkins/jenkins知识点/webhook/gitlab+jenkins实现webhook.html":{"url":"linux/自动化运维平台/jenkins/jenkins知识点/webhook/gitlab+jenkins实现webhook.html","title":"gitlab+jenkins实现webhook","keywords":"","body":"jenkins+gitlab实现webhook 1.jenkins安装插件 Gitlab Hook Plugin 2.获取jenkins项目回调url地址 在jenkins项目中构建触发器下勾选 Build when a change is pushed to GitLab. GitLab webhook URL ，然后复制后边的url地址，这个地址后续是需要填写在gitlab webhook配置中的，并且在下边可以选择相应触发事件 3.gitlab项目配置webhook 项目 Settings -> Webhooks 在url处填写jenkins项目中的回调url地址 配置完成后保存，然后在 Test 下点击 Push events 进行测试 测试提示如下即为成功 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/jenkins/jenkins故障/jenkins报错.html":{"url":"linux/自动化运维平台/jenkins/jenkins故障/jenkins报错.html","title":"jenkins报错","keywords":"","body":"[toc] 一、jenkins报错 Unable to create the home directory. This is most likely a permission problem 浏览器中访问 jenkins 报错如下 \u0011 生产中我们的jenkins是以war包形式放在tomcat的 webapps目录下启动的，tomcat设置的端口为8080 浏览器中报错如图后，登陆服务器查看，tomcat进程、监听的8080端口都是存在的，但是还是报错如图所示，尝试重启tomcat问题没有解决，但是有一个很奇怪的问题，当执行 ./bin/shutdown.sh 停止tomcat后，8080端口进程依然存在 参考stackoverflow文章，文章中也有人提到了需要设置jenkins家目录 JENKINS_HOME 变量，并且设置目录所有者为jenkins用户，这里我们的jenkins家目录设置的是 /root/.jenkins，是在 /etc/profile 中设置为 export JENKINS_HOME=\"/root/.jenkins\"，并且 /root/.jenkins 目录所有者就是jenkins用户，也没有改动，查看日志也没有找到有用的信息 尝试重启tomcat无效后，最终解决方法是直接kill掉8080端口对应的进程，然后执行 ./bin/startup.sh 启动tomcat，浏览器刷新等待jenkins重启完毕就可以了 二、jenkins报错 Jenkins detected that you appear to be running more than one instance of Jenkins that share the same home directory Error Jenkins detected that you appear to be running more than one instance of Jenkins that share the same home directory '/root/.jenkins’. This greatly confuses Jenkins and you will likely experience strange behaviors, so please correct the situation. This Jenkins: 158454546 contextPath=\"\" at 1541@localhost Other Jenkins: 1455232034 contextPath=\"/jenkins20190612\" at 1541@localhost jenkins出现 Unable to create the home directory. This is most likely a permission problem 问题后，kill掉进程后重启，运行一段时间后报错如图，参考网上文章是jenkins启动之后会在jenkins home目录($JENKINS_HOME)下生成一个\".owner\"文件，里面标识了本次jenkins实例的唯一标识，把这个文件删除就可以了 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/jenkins/jenkins故障/jenkins无限构建问题.html":{"url":"linux/自动化运维平台/jenkins/jenkins故障/jenkins无限构建问题.html","title":"jenkins无限构建问题","keywords":"","body":"jenkins无限构建问题 参考链接 问题说明 jenkins在配置完任务后点击build之后，发现任务列表中自动添加了第二个任务，然后就是第三个、第四个、第五个。。。无限循环 问题 在 console 输出中，可以看到有2个 git rev-parse，jenkins在pull代码的时候，发现了两个符合要求的分支，因此自动创建了另一个任务来pull另一个符合要求的任务 在项目git地址配置中我写的是 */test，而git仓库中有test和origin/test两个分支 至此，可以确认原因，就是我们在配置项目时，指定的分支，jenkins在git地址中找到了符合要求的多个分支，多余的分支，会自动创建一个新的任务去运行，运行的时候，又识别到了2个分支，又创建了新的分支，进入死循环 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/jenkins/jenkins故障/jenkins配置企业微信通知报错.html":{"url":"linux/自动化运维平台/jenkins/jenkins故障/jenkins配置企业微信通知报错.html","title":"jenkins配置企业微信通知报错","keywords":"","body":"jenkins配置企业微信通知报错 jenkins版本：2.303 jenkins安装方式：rpm包 系统jdk版本：oracle jdk1.8.0_261 企业微信插件 Qy Wechat Notification Plugin 版本：1.0.2 构建通知报错 通知异常javax.net.ssl.SSLHandshakeException: No appropriate protocol (protocol is disabled or cipher suites are inappropriate) 参考网上文章修改 java.security 文件，修改安装的jdk1.8的文件后重启jenkins不生效，查看jenkins系统信息才发现使用的是openjdk，原因未知，于是修改openjdk的 java.security ，重启jenkins 在 Manage Jenkins -> System Properties 下查看系统使用的jdk信息，不知为何使用的是openjdk 查找当前系统的 java.security 文件，发现有2个，一个是jdk的，另外一个是openjdk的 $ find / -name \"java.security\" /usr/local/jdk1.8.0_261/jre/lib/security/java.security /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.302.b08-0.el7_9.x86_64/jre/lib/security/java.security 修改 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.302.b08-0.el7_9.x86_64/jre/lib/security/java.security 以下内容 jdk.tls.disabledAlgorithms=SSLv3, TLSv1, TLSv1.1, RC4, DES, MD5withRSA, \\ DH keySize 修改为 jdk.tls.disabledAlgorithms=RC4, DES, MD5withRSA, \\ DH keySize 修改完成后重启jenkins再构建就会成功收到企业微信通知了 问题的原因为jenkins系统使用了openjdk1.8，按照网上的文章修改 java.security 文件重启jenkins生效，但是不知为何jenkins使用的是openjdk，自己的jenkins和公司的jenkins都是自己装的，并且安装方式都是rpm包 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/jenkins/jenkins故障/jenkins前端项目打包报错问题.html":{"url":"linux/自动化运维平台/jenkins/jenkins故障/jenkins前端项目打包报错问题.html","title":"jenkins前端项目打包报错问题","keywords":"","body":"jenkins前端项目打包报错问题 问题描述 项目相同的配置，并没有做任何改动，同一天时间，上午还没有问题，到了晚上打包就报错了 打包命令如下 yarn install yarn build:prod 报错信息如下，前端说是因为部分依赖包没有安装成功 排查步骤 以root用户在jenkins服务器上手动拉取代码打包，没有问题，以jenkins运行用户(有sudo权限)在jenkins服务器上手动拉取代码打包，也没有问题，这样就排除了权限问题，因为之前也出现过因为权限问题而导致打包失败的情况 解决方式 架构师给了个解决思路，即把项目目录清除后再打包，然后我们尝试执行后问题解决 在项目中构建环境下勾选 Delete workspace before build starts 这样就会在项目构建前把项目目录删除后再构建 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible好文/这样理解Ansible更容易.html":{"url":"linux/自动化运维平台/ansible/ansible好文/这样理解Ansible更容易.html","title":"这样理解ansible更容易","keywords":"","body":"[toc] 这样理解Ansible更容易 原文链接 这篇文章是 Jenkins 2.x实践指南 作者翟志军博客中的一篇内容，看完之后觉得写的非常好，所以收藏一下 滚滚长江东逝水，浪花淘尽英雄。是非成败转头空。青山依旧在，几度夕阳红。—— 《临江仙》 电脑店 从前，有一家电脑店。原来你即是老板，又是店员时，拿到清单，你就必须亲自动手采购，然后一个个零件组装。每天都做着即重复又辛苦的活。 虽说你的组装技术已经很娴熟了，但是偶尔还发生装错的情况（大概是那天和老板娘吵架了），把一个客人要求的 CPU i5 装成了 CPU i7。结果是你亏本或者赚得少了。 后来，你采购了一个自动组装电脑的机器人。你只要告诉它电脑的配置，并把零件放到指定的箱子中。接着启动这台机器，它就自动帮你组装好电脑了。它每天都干重复的活也不会叫辛苦。 最重要的是准确，它不会因为心情不好，而装错。因为它根本不会闹情绪。 这样，老板就可以从重复的工作解放出来。然后将多出来的时间花在与人的沟通上，为有不同需求的人设计更合适的电脑配置清单。毕竟游戏发烧友和办公小白领的需求是不一样的。 在运维领域，不少运维人都干着即是老板又是店员的工作。如果在运维领域也能有这样的“机器人”该多好。事实上，Ansible、Puppet、Check 就是这样的机器人。 为什么要从零设计一个运维机器人 本文并不想生硬地罗列 Ansible 的各个知识点。因为那样，大家不如直接看 Ansible 官方文档就好。 笔者采用从零设计一个运维机器人的方式来告诉你，为什么 Ansible 会是现在这个样子。当然，现实中的 Ansible 不会像本文所写的那样一步步设计。 为什么要这样呢？因为笔者觉得只有知道一个工具背后的设计原理，真正用这个工具才会得心应手。 运维机器人的最终模样 首先，需要确定一下实现这个运维机器人的目的是什么。我们并不是希望所有的运维工作都交给运维机器人，而是希望运维工作中重复的那部分尽可能的交给机器人，把创造性的工作全部交给人。如下图所示。 以终为始是一种非常有效的实现目标的思考模型。根据此思考模型，我们首先必须探讨运维机器人的最终模样。然后，再讨论可能的解决方案。 那么，什么样的运维机器人能帮助我们实现上述的自动化运维目标呢？想像一下。是不是只要我们对着运维机器人说一句：“我要部署一个 Nginx 到 192.168.12.11”。它就可以帮我们完成了？ 但是它怎么知道如何连接到 192.168.12.11 呢？是使用用户名密码的方式，还是使用私钥？它又怎么知道 Nginx 需要什么样的配置呢？一问下来，其实，语音运维只适用于启动一些预定义的动作。就像汽车的一键启动。你不可能使用语音来对 Nginx 进行大量的配置。 而纯文本才是进行大量配置的最好媒介。 所以，运维机器人的最终模样是：我们将部署的主机 IP、登录方式、Nginx 的配置放在一个文本文件中，然后运维机器人读取这个文本文件，然后根据配置进行部署。如果部署的是业务系统，我们还需要准备该业务系统的二进制包。如下图所示。 那么，我们在文本文件中使用何种语言描述我们的配置需求呢？可以分成两种。一种是利于人类学习的自然语言（如英语）。另一种是利于机器读取的结构化数据（如YAML、JSON）。 按当前的技术实现的可能性，不论是运维机器人，还是交给其它程序，都需要将自然语言转到结构化的数据。就像程序员，需要将业务知识翻译成编程语言；像编译器将编程语言翻译成机器真正能识别的二进制代码。 运维机器人的真正核心不是将自然语言转成结构化的数据。所以，文本文件中，我们直接写结构化数据。同时，我们决定使用 YAML 格式作为结构化数据的载体。因为它是非常流行的配置文件格式。降低人们写结构化数据的难度。 当然，好的设计不应该与具体配置文件格式耦合。 实现运维机器人要解决哪些问题 以上只是确定了运维机器人的最终模样及使用方式，解决的是用户的问题。但是因为我们是运维机器人的设计者，我们必须考虑如何实现它。 回想一下，平时我们的运维人员是如何实现自动化的？是不是写好了 bash 脚本后，然后将脚本上传到目标机器，最后在目标机器上执行该脚本。 这个 bash 脚本其实也可以算是一种结构化的数据格式，而且是一种不需要再做编译，目标机器能直接运行的格式。 在平时的自动化方式基础上进行抽象。我们觉得要实现运维机器人要解决的关键问题有： 需要将 YAML 转成目标机器可执行的程序（或脚本）。 需要将可执行的程序上传到目标机器，并执行。 为什么一定要将 YAML 转成目标机器可执行的程序呢？直接写 bash 不就可以了？ 因为运维机器人要解决的不止是 Linux 系统的自动化运维，还有 Windows，甚至路由器的。所以，我们需要使用一种独立于目标机器的语言来描述我们的运维需求。 问题 2 我们先放一放，因为上传代码到目标不是运维机器人的关键问题。 实现 YAML 转成 ？ 虽然咱们希望运维机器人能运维所有类型的机器，但是本文重点不是要在一篇文章内实现所有类型机器的自动化运维。接下我们只针对 Linux 机器进行讲解。 现在我们遇到的问题是要将 YAML 转成什么程序以实现在目标机器上执行？ 如果我们将 YAML 转成 Java 程序，那么目标机器就必须装有 JDK，这是不现实的。你不可能让所有的 Linux 机器都安装 JDK。所以，YAML 最好是能转成所有 Linux 都支持程序。目前笔者能想到的就是：bash 和 Python。 P.S. 现实中，很多运维工具要求所有的目标机器必须安装特定的客户端的。而 Ansible 却不需要。如果在使用自动化运维工具前，你要为所有的机器安装特定的客户端，那么，你怎么自动化为所有的机器自动安装客户端呢？留着读者思考。 从 bash 和 Python 之间做选择，没有什么好讨论的，选择 Python。 那么，怎么将 YAML 转成 Python 代码呢？这要看我们怎么设计 YAML 内的描述语言了。其实就是设计自动化运维机器人的领域特定语言（DSL）。为方便讨论，我们称之为：OPL 语言。 OPL 语言1.0：基本要素 现在咱们在为一台机器部署一个 Nginx 作为切入点，来设计我们的 OPL 语言。以下就是1.0版本： --- - host: \"192.168.35.10\" ansible_ssh_user: vagrant ansible_ssh_pass: vagrant tasks: - name: install nginx yum: name: nginx action: install host：代表部署的目标机器。 ansible_ssh_*：开头的 key 是指定 ssh 连接的用户名和密码。 tasks：是一个数组，包含一系列任务。 name：任务名称，方便人阅读。 yum：要执行的任务的类型，也就是要执行 yum 操作。yum 任务下的 name 属性代表要安装的软件名称，action 属性代表执行的是安装操作。 1.0版的 OPL 语言包含了运维领域最基本要素： 目标主机的描述 连接目标主机的必要信息 任务描述 为方便沟通，我们暂将些 YAML 文件命名为 playbook.yml。 OPL 语言2.0：采用声明式的任务描述方式 注意到 OPL 语言1.0中的任务描述方式很像现实中执行shell：yum install nginx。我们称这种方式为脚本式的。脚本式的描述方式与传统的运维方式更像。 但是声明式的描述方式更适合 OPL 语言所要解决的问题。我们期望的是描述我们期望的结果，而不是换一种方式写脚本。 采用声明式的描述方式，还有一个很重要的原因：幂等的。在概念上，声明式的描述被执行了多少次，结果都应该是我们所声明的。而第二次执行脚本式的代码时，你会问，结果还会和我第一次执行的一样吗？ 所以，2.0中，任务描述方式改成声明式： - name: Ensure nginx present yum: name: nginx state: present state 属性的值为 present 形容词。 P.S. 识别声明式与脚本式的简单办法是看它在描述目标状态时，是用动词，还是用形容词。 除了 yum 任务，接下来要实现的所有任务也将采用声明式。 OPL 语言3.0：主机清单 在 2.0 版本中，我们只实现了一台目标机器的部署。但是现实中，我们常常要针对多主机进行部署。我们需要一种更好的方式去描述目标主机。 主机清单的内容，也需要一个文件来存放。关于文件的格式，我们采用一种类 INI （INI-like）的格式。文件名暂命名为：inventory。以下是 inventory 文件的内容： [nginx] 192.168.35.10 192.168.35.11 192.168.35.12 [springboot] 192.168.35.20 192.168.35.21 192.168.35.22 [] 括号中是主机分组的名称，接下来是就是这个组内目标机器的 IP 列表。 而上一版本中的 playbook.yml 中的 host key 是单数，所以不合适了。我们改成 hosts 复数，同时值变成了在主机清单中的组名，而不是具体某台主机的 IP。 playbook.yml 文件内容如下： - hosts: \"nginx\" ansible_ssh_user: vagrant ansible_ssh_pass: vagrant tasks: - name: install nginx yum: name: nginx state: present 细心的读者朋友应该发现了，目标机器的连接方式是各不相同的，有些用用户密码的方式，有些用密钥的方式。所以，我们再将 ansible_ssh_* 写在 playbook.yml 中已经不合适了。 笔者只能告诉你，这是剧情需要。OPL 语言的后续版本会解决此问题。 现在咱们为运维机器人准备的内容已经不再只是一个 playbook.yml 文件，它应该是一个目录了，目录内容如下： ├── inventory └── playbook.yml OPL 语言4.0：include 任务 在 OPL 1.0 版本，已经考虑到了一次部署将会包括很多子任务。所以使用了 tasks 这个复数词。 而现实中一次部署任务往往包含几十个子任务，playbook.yml 的文件内容一定会膨胀。这样的源代码非常难维护。所以，在 4.0 版本，我们决定为 OPL 增加一个 include_tasks 任务类型。用户可以通过 include_tasks 任务类型将另一个包含任务描述的文件（这里我们称为子任务集）引入到当前的 playbook.yml。 - hosts: \"nginx\" tasks: - name: config firewalld include_tasks: firewalld.yml - name: install and config nginx include_tasks: nginx.yml 这时的目录结构如下： ├── firewalld.yml ├── inventory ├── nginx.yml └── playbook.yml 所有的子任务文件都与 playbook.yml 同级，会显得很乱，不利于区分哪个文件是 OPL 的执行入口。我们是不是可以建了一个 tasks 目录来专门存放呢？事实上就应该这么做。 所以，经过重构，得到了4.1 版本。目录结构调整如下： ├── inventory ├── playbook.yml └── tasks ├── firewalld.yml └── nginx.yml playbook.yml 中 include_tasks 任务的文件路径做相应的调整，改成： - name: install and config nginx include_tasks: tasks/nginx.yml OPL 语言5.0：丰富任务类型 5.0 之前的版本，已经实现了一个基本框架。5.0 版本中我们希望加入更多的任务类型，以满足不同的运维需求。 copy 任务 部署过程中，常常需要将一些文件从本地 copy 到目标机器。copy 任务的代码样例如下： - name: \"ensure nginx package exists\" copy: src: \"./files/nginx.tar.gz\" dest: \"/tmp\" 为方便管理，我们将所有 copy 任务用到的文件放在 files 目录中。此时目录结构调整如下： ├── inventory ├── playbook.yml └── tasks ├── files │ └── nginx.tar.gz ├── firewalld.yml └── nginx.yml file 任务 设置文件夹的权限是非常常见的操作，所以就有了 file 任务。 - name: \"ensure folder /app/nginx is created\" file: path: \"/app/nginx\" owner: \"nginx\" group: \"nginx\" mode: \"0700\" state: \"directory\" owner：指定文件的所属用户。 group：指定文件的所属用户组。 state 属性的值可以为： absent：不存在。可以理解为删除该文件或文件夹。 directory：文件夹。如果该文件夹不存在，则创建。 file：文件。如果不存在，则创建。 touch：与 linux 的 touch 实现相同的效果。 service 任务 在服务安装完成后，最常用的操作就是启动服务了。同时，它会根据不同的操作决定使用何种 service 实现。支持：BSD init, OpenRC, SysV, Solaris SMF, systemd, upstart。这就是封装的强大。用户只需要描述他的期望，剩下的机器能解决的，都由机器解决。 - name: ensure svn service started service: name: svnserver state: started enabled: true enabled 属性值为 true 代表开机自动启动。state 属性值可以为： reloaded：服务是被重新加载过的。 restarted：服务是被重启过的。 started：服务是启动的。 stopped：服务是停止的。 小结 如果 OPL 语言设计得足够好，它应该可以轻松地进行扩展。此处举的几个例子已经达到目的，就不再举更多的例子。 P.S. OPL 语言的子任务在 Ansible 中称为模块（module）。 OPL 语言6.0：模块化任务 在 5.0 版本中，我们为 OPL 增加了一些的任务类型。在写了一段时间 OPL 语言后，发现采用 include_tasks 对大规模 playbook.yml 进行拆分的方式，设计上的存在不足：不够内聚。具体表现如下： include_tasks 任务不利于分享给其他人使用。 nginx.yml 中的 copy 任务中，我们约定从 files 目录中读取文件。但是其它子任务中的 copy 又从哪里读取文件呢？这就是子任务之间会相互影响。 那么如何让子任务更内聚呢？将“子任务”的集合进行封装，并命名为 role。这样每个 role 都被看作成一个模块。 目录结构调整为： ├── inventory ├── playbook.yml └── roles ## 存放 playbook 使用到的 role ├── firewalld │ ├── files │ └── tasks │ └── main.yml └── nginx ├── files │ └── nginx.tar.gz └── tasks └── main.yml playbook.yml 内容调整如下： - hosts: \"nginx\" ansible_ssh_user: vagrant ansible_ssh_pass: vagrant roles: - firewalld - nginx 每个 role 只需要管理自己内部的逻辑，比如，每个 role 都会有一个 files 目录。copy 任务默认从所在 role 目录中的 files 目录中读取。上文介绍的 copy 任务（注意 src 属性值）改为： - name: \"ensure nginx package exists\" copy: src: \"nginx.tar.gz\" dest: \"/tmp\" 今后设计的所有任务类型默认都从 role 自身目录开始。 每个 role 的执行入口约定为 tasks/main.yml。include_tasks 任务仍然可以使用，只不过，默认从 main.yml 同级目录获取子任务集合。比如 nginx/tasks/main.yml 包含任务： - name: \"Config nginx\" includes_tasks: config.yml nginx role 的 tasks 目录内容如下： └── nginx ├── files │ └── nginx.tar.gz └── tasks ├── config.yml └── main.yml OPL 语言7.0：支持变量 6.0 版本中，我们如何将 role 分享给其他人使用呢？目前能想到的成本最低的方式是直接将 role 目录拷贝一份，并 push 到 GitHub 上。其他人将 role 下载到自己的 roles 目录即可。 可是，其他人下载 role 后，也需要查看 role 的内部逻辑，然后修改其中的逻辑，才能为自己所用。因为并不是每个人的 nginx 配置都是一样的。这说明咱们当的 OPL 的设计违反了程序设计的开闭原则： 开闭原则规定“软件中的对象（类，模块，函数等等）应该对于扩展是开放的，但是对于修改是封闭的”，这意味着一个实体是允许在不改变它的源代码的前提下变更它的行为。 —— 维基百科 那如何设计才能符合开闭原则呢？方法是将经常变化的与基本不用变化的逻辑分离。 以上文中的 nginx role 为例讲解。nginx 的安装部署，整个过程对于所有人来说都是大体相同的。不相同的是 nginx 的配置。 OPL 语言7.0版本将这些“配置”抽象出来，也就是变量。role 根据自身需要在 role 内部定义变量，用户在 role 外部可重新设置变量的值，即可定义 role 中的变化的部分。比如重新定义 nginx 的配置。 那么，具体如何定义及使用变量呢？ 区分默认变量和用户定义变量 对于“具体如何定义及使用变量呢”的问题，我们第一步是要将默认变量和用户定义变量区分开。 以上文的 copy 子任务为例： - name: \"ensure nginx package exists\" copy: src: \"nginx.tar.gz\" dest: \"/tmp\" copy 的目的地的属性为 dest，它的值是“写死”的。但是并不是所有人都希望 copy 到 /tmp 目录。/tmp 目录是 role 本身的默认值，如果用户不满意这个默认值，可以在使用 role 时，修改 dest 的值。 这又引出另一个问题：role 内部如何定义默认值？ 在 role 目录下，我们新建一个 defaults 目录，并放一个 main.yml 。defaults/main.yml 文件内容定义变量的默认值。role 的目录结构调整为： ├── defaults │ └── main.yml ├── files │ └── nginx.tar.gz └── tasks ├── config.yml └── main.yml nginx/defaults/main.yml 的内容如下： --- nginx_package_tmp_dir: \"/tmp\" # 其它变量，此处省略 copy 子任务的描述改成： - name: \"ensure nginx package exists\" copy: src: \"nginx.tar.gz\" dest: \"{{ nginx_package_tmp_dir }}\" {{ }} 是变量的占位符。nginx_package_tmp_dir 会被实际值所替换。 template 子任务 nginx 的配置是一个 nginx.conf 文件。nginx role 中的 nginx.conf 应该是一个模板，模板内的变量占位符会被变量实际值替换。模板引擎我们选择：Jinja2，一个 Python 的模板引擎。 这样想来，我们需要一个新的子任务类型 template： - name: ensure nginx config template: src: \"nginx.conf\" dest: \"/usr/local/nginx/conf/nginx.conf\" # 目标路径 与 copy 任务类似，从哪里拿 nginx.conf 模板文件呢？为区分模板文件与一般文件，我们在 role 目录下创建一个 templates 目录。最终 nginx role 的目录结构看起来是这样的： ├── defaults │ └── main.yml ├── files │ └── nginx.tar.gz ├── tasks │ ├── config.yml │ └── main.yml └── templates └── nginx.conf nginx.conf 部分内容如下： ## 省略 server { location / { proxy_pass http://127.0.0.1:/; } } 文件中 {{apps_port}} 最终会被实际变量的值替换。 在哪里定义变量？ 目前所有的默认变量值都定义在 defaults/main.yml 中，那么，role 的使用者如何定义自己的变量值呢？ 我们支持多种方式： playbook.yml 文件中 playbook.yml 文件中，又分为两种情况：1. 在 roles 级别；2. 在 playbook 级别的。代码样例如下： --- - hosts: \"nginx\" ## 1. playbook 级别变量 vars: apps_port: 9090 roles: - firewalld ## 2. roles 级别变量 - { role: nginx, apps_port: 9091 } inventory 文件中 针对每台机器，同一变量可能需要设置成不同的值。 [springboot] 192.168.35.20 apps_port=9091 ansible_ssh_user=vagrant ansible_ssh_pass=vagrant 192.168.35.21 apps_port=9092 ansible_ssh_user=vagrant1 ansible_ssh_pass=vagrant1 192.168.35.22 apps_port=9093 ansible_ssh_user=vagrant2 ansible_ssh_pass=vagrant2 在 OPL3.0 版本中的遗留问题，终于解决了。ansible\\_ssh\\_\\* 类的变量可以关联到具体某台机器了。也就是每台主机的 ansible\\_ssh\\_\\* 的值都可以是不一样的。 命令行转入 在命令行中加入参数：--extra-vars 即可。 ansible-playbook release.yml --extra-vars \"version=1.23.45 other_variable=foo\" ​ 当变量可以在多处定义时，随之而来的就是变量的优先级问题。变量的优先级应该如何设计呢？问题留给读者思考。 OPL 语言8.0：支持条件判断 8.0 版本决定加入条件判断：when。用法如下： - name: Ensure nginx exists yum: name: nginx state: present when: ansible_os_family == \"CentOS\" 当目标机器不是 CentOS 时，执行 yum 操作一定是失败的。所以，只有 ansible_os_family == \"CentOS\" 为 true 时才执行该子任务。 上例中，只是单条件。如果多条件呢？如何实现“与”和“或”呢？ “与”的示例如下： - name: \"shut down CentOS 6 and Debian 7 systems\" command: /sbin/shutdown -t now when: ansible_distribution == \"CentOS\" and ansible_distribution_major_version == \"6\" ​ 注：command 是一个执行命令的子任务类型。 “与”还有另一种写法： - name: \"shut down CentOS 6 systems\" command: /sbin/shutdown -t now when: - ansible_distribution == \"CentOS\" - ansible_distribution_major_version == \"6\" “或”的示例如下： - name: \"shut down CentOS 6 and Debian 7 systems\" command: /sbin/shutdown -t now when: ansible_distribution == \"CentOS\" or ansible_distribution == \"Debian\" ) 当然，你可以结合“与”和“或”来使用： - name: \"shut down CentOS 6 and Debian 7 systems\" command: /sbin/shutdown -t now when: (ansible_distribution == \"CentOS\" and ansible_distribution_major_version == \"6\") or (ansible_distribution == \"Debian\" and ansible_distribution_major_version == \"7\") 你可以把 when 想像成编程语言中的 if语句。它与具体的子任务的类型无关，任务子类型下都可以使用 when。 OPL 语言8.1：支持遍历 当某个任务需要根据数组中的数据进行重复执行时，OPL 语言就要考虑支持遍历了。 - name: Ensure soft exists yum: name: \"{{ item }}\" state: present when: ansible_os_family == \"CentOS\" with_items: - gcc - gcc-c++ 可以将 with_items 理解成编程语言中的 for语句。{{ item }} 中的 item 约定代表遍历的元素。 OPL 语言9.0：子任务支持返回值 用户在使用 OPL 语言时，写出以下代码： - name: ensure nginx config template: src: \"nginx.conf\" dest: \"/usr/local/nginx/conf\" - name: restart nginx service: name: nginx state: restarted 代码问题出现在哪里呢？它不是幂等的。不论 nginx.conf 文件是否有更新，每执行一次 OPL 它都会重启一次 nginx 服务。 我们期望的是当 nginx.conf 有更新时才执行启动 nginx 服务。 我们应该如何设计 OPL 以规避此类问题呢？ 我们从问题本身开始。当子任务设计完成后，我们需要根据子任务的执行结果去执行另一个子任务。用通用编程语言来表达，很简单： boolean changed = template(\"nginx.conf\", \"/usr/local/nginx/conf\") if(changed){ service(\"nginx\",\"restarted\") } 在通用编程语言中，我们很容易就实现的功能，在 YAML 中如何实现呢？其实类似的，让所有的子任务支持返回值，然后在后续子任务做判断就好了。代码如下： - name: ensure nginx config template: src: \"nginx.conf\" dest: \"/usr/local/nginx/conf\" register: nginx_config_result - name: restart nginx service: name: nginx state: restarted when: nginx_config_result.changed register 是新的语句，用于定义子任务返回结果名称。nginx_config_result 是一个对象，其中 changed 就是它的属性。changed 为 true 时，代表 ensure nginx config 子任务有变化。 OPL 语言9.1：支持延迟处理 在使用 9.0 版本一段时间后，用户开始报怨以下代码写起来太哆嗦，而且容易出错： - name: ensure nginx config template: src: \"nginx.conf\" dest: \"/usr/local/nginx/conf\" register: nginx_conf_result - name: ensure nginx upstream config template: src: \"upstream.conf\" dest: \"/usr/local/nginx/conf/upstream.conf\" register: nginx_upstream_result - name: ensure nginx vhosts config template: src: \"vhosts.conf\" dest: \"/usr/local/nginx/conf/vhosts.conf\" register: nginx_vhosts_result - name: restarted nginx service service: name: nginx state: restarted when: nginx_conf_result.changed or nginx_upstream_result.changed or nginx_vhosts_result.changed 用户写成这么哆嗦的语句，并不是用户的问题。而是我们的设计有没考虑到的地方。也就是有些任务是需要被另外一些任务触发执行的。我们当前不支持此种场景。 为实现此类场景，需要做两件事情： 主动触发子任务，使用 notify 语句指定要触发的另一个任务的任务名。 集中保存被动触发的任务，以区分主动执行的任务和被动执行的任务。约定被动触发的任务放在 role 目录下的 handlers/main.yml 文件。 接下来，我们具体看下如何重构以上哆嗦的写法。 第一步，对 nginx role 目录进行调整： ├── defaults │ └── main.yml ├── files │ └── nginx.tar.gz ├── handlers │ └── main.yml ├── tasks │ ├── config.yml │ └── main.yml └── templates └── nginx.conf 第二步，重构 tasks/config.yml： - name: ensure nginx config template: src: \"nginx.conf\" dest: \"/usr/local/nginx/conf\" notify: - restart nginx - name: ensure nginx upstream config template: src: \"upstream.conf\" dest: \"/usr/local/nginx/conf/upstream.conf\" notify: - restart nginx - name: ensure nginx vhosts config template: src: \"vhosts.conf\" dest: \"/usr/local/nginx/conf/vhosts.conf\" notify: - restart nginx 第三步，向 handlers/main.yml 文件加入被动执行的任务： --- - name: restart nginx service: name: nginx state: restarted OPL 语言之后 以上只是 OPL 语言的雏形，我们还需要根据现实的情况不断的扩展和完善。OPL 语言的设计只是实现运维机器人的一部分工作。我们还需要做的工作包括：实现一个程序，它会对 OPL 语言进行编译。并将编译后的 python 脚本上传到目标服务器。python 脚本运行后，这个程序将运行结果反馈给用户。 这个程序就是我们在命令行中敲入的 ansible-playbook 了。 至于 ansible-playbook 的更多细节，已不属于本文的内容，不作讨论。 总结 本文首先从电脑店引出现代自动化运维工具的基本模型。接着讨论应该如何设计一款自动化运维工具（上文称为运维机器人）。从而得知需要解决两个关键问题： 需要将 YAML 转成目标机器可执行的程序（或脚本）； 需要将可执行的程序上传到目标机器，并执行。 然后我们花了大篇幅介绍 OPL 语言的设计（实际上是介绍 Ansible 的 YAML 为什么要像现在这样写）。最后简单介绍 OPL 语言后要做的事情。 老实说，以上并不是真正意义上的从零设计 Ansible。也不存在什么 OPL 语言。笔者只是希望通过 OPL 语言的设计过程，尝试让读者了解 Ansible 的设计思路。期望读者因为本文，学习 Ansible 变得更轻松。 最后，文末有 OPL 语言各版本的样例。 附 从零设计 Ansible 代码样例：https://github.com/zacker330/design-ansible Jinja2 模板语言：http://docs.jinkan.org/docs/jinja2/ End 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible基础/ansible安装.html":{"url":"linux/自动化运维平台/ansible/ansible基础/ansible安装.html","title":"ansible安装","keywords":"","body":"ansible安装 ansible官网 ansible github ansible官网文档总地址 ansible官方安装文档 1.yum安装 需要epel源 yum -y install ansible yum安装的ansible二进制命令路径是 /usr/bin/ansible $ which ansible /usr/bin/ansible 默认读取的配置文件是 /etc/ansible/ansible.cfg $ ansible --version ansible 2.9.25 config file = /etc/ansible/ansible.cfg configured module search path = [u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules'] ansible python module location = /usr/lib/python2.7/site-packages/ansible executable location = /usr/bin/ansible python version = 2.7.5 (default, Oct 14 2020, 14:45:30) [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] 2.pip安装 安装pip yum -y install python-pip 安装最新版ansible pip install ansible 指定版本安装 pip install ansible==2.9.25 pip安装的ansible二进制命令路径是 /usr/local/bin/ansible $ which ansible /usr/local/bin/ansible pip安装的ansible默认没有配置文件路径，需要手动创建目录和文件，和yum安装的ansible默认文件路径一致，都是在 /etc/ansible 下 ansible --version ansible 2.9.25 config file = None configured module search path = ['/root/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules'] ansible python module location = /usr/local/lib/python3.6/site-packages/ansible executable location = /usr/local/bin/ansible python version = 3.6.8 (default, Nov 16 2020, 16:55:22) [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible基础/ansible基础知识.html":{"url":"linux/自动化运维平台/ansible/ansible基础/ansible基础知识.html","title":"ansible基础知识","keywords":"","body":"[toc] Ansible基础知识 1.Ansible基本概述 Ansible是一个配置管理系统configuration management system你只需要可以使用ssh访问你的服务器或设备就行 1.1 Ansible能做什么 ansible可以帮助我们完成一些批量任务，或者完成一些需要经常重复的工作。 比如：同时在100台服务器上安装nginx服务，并在安装后启动服务。 比如：将某个文件一次性拷贝到100台服务器上。 比如：每当有新服务器加入工作环境时，你都要为新服务器部署某个服务，也就是说你需要经常重复的完成相同的工作。 这些场景中我们都可以使用到ansible。 1.2 Ansible软件特点 ansible不需要单独安装客户端，SSH相当于ansible客户端。 ansible不需要启动任何服务，仅需安装对应工具即可。 ansible依赖大量的python模块来实现批量管理。 ansible默认配置文件是 /etc/ansible/ansible.cfg 1.3 Ansible基础架构 连接插件(connectior plugins) 用于连接主机 用来连接被管理端 核心模块(core modules) 连接主机实现操作， 它依赖于具体的模块来做具体的事情 自定义模块(custom modules) 根据自己的需求编写具体的模块 插件(plugins) 完成模块功能的补充 剧本(playbooks)ansible的配置文件,将多个任务定义在剧本中，由ansible自动执行 主机清单(host inventory)定义ansible需要操作主机的范围 最重要的一点是 ansible是模块化的 它所有的操作都依赖于模块 2. Ansible安装配置 ⚠️所有的受控主机必须与ansible服务端做ssh免密登陆 2.1 安装ansible(需要配置epel源) yum -y install ansible centos7.9安装的ansible版本为2.9.21 $ ansible --version ansible 2.9.21 2.2 配置ansible 编辑主机清单文件 cat >> /etc/ansible/hosts 2.3 验证ansible与受控机是否通信 # ansible是通过ssh端口探测通信 $ ansible all -m ping 10.0.0.101 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } 10.0.0.100 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } 2.4 ansible语法格式 命令 主机模块名 指定模块参数 模块名称 指定利用模块执行的动作选项 批量执行操作动作 $ ansible all -m command -a \"hostname\" 10.0.0.101 | CHANGED | rc=0 >> jenkins 10.0.0.100 | CHANGED | rc=0 >> gitlab-server # 说明 all 模块名 -m 指定模块 command command模块，完成基础命令 -a 指定执行动作 \"hostname\" 执行hostname命令 3.Ansible系列命令 3.1 ansiblie系列命令1：ansible 使用场景： 非固化需求 临时一次性操作 二次开发接口调用 使用示例 检查服务器存活状态 ansible all -m ping 3.2 ansiblie系列命令2：ansible-galaxy 命令作用： 根据下载量和关注量等信息，查找和安装优秀的roles 命令格式： ansible-galaxy [init|info|install|list|remove] [ --help] [options] ... 命令分为3部分 1⃣️ 选项 选项 说明 init 初始化本地的roles配置，以备上传roles至galaxy info 列表指定role的详细信息 install 下载并安装galaxy指定的roles到本地 list 列出本地已经下载的roles remove 删除本地已经下载的roles 2⃣️ help用法显示[--help] ansible-galaxy init --help 3⃣️ 参数 ansible-galaxy init [options] role_name 3.3 ansiblie系列命令3：ansible-doc 命令作用： 模块文档说明 命令格式： ansible-doc [options] [module] 示例： # 列出ansible支持的 模块 ansible-doc -l # 模块功能说明 ansible-doc ping 3.4ansiblie系列命令4：ansible-playbook 命令作用： 读取预先编写好的playbook文件实现批量管理 命令格式： ansible-playbook xxx.yaml 示例： # 执行http_install.yaml这个playbook中定义的所有任务集 ansible-playbook http_install.yaml 3.5ansiblie系列命令5：ansible-vault 命令作用： 用于配置文件加密 命令格式： ansible-vault [encrypt|decrypt|create|edit|rekey|view] [--help] [options] file 示例： a.yaml文件内容如下 # 安装apache - hosts: web tasks: - name: install httpd yum: name=httpd state=installed 加密文件 ansible-vault encrypt a.yaml $ ansible-vault encrypt a.yaml New Vault password: Confirm New Vault password: Encryption successful 加密后查看a.yaml文件就会显示乱码 $ cat a.yaml $ANSIBLE_VAULT;1.1;AES256 33666435656365396237363533616365346662373963393835376261333031356162373934383363 3633656532336436653261613539393532646131623433370a353865303931356131353065666261 35613738333338356635613337396565616663653366663134373537663935633134643734376333 6539333733353163380a356232663636343766313930636639383835656136623632393935636330 32386235313566383135386465613338346566623435363035646262356236393231353933396261 62626263613438313865666433323363636261616634613830623936393866616135663937386139 31636631313665613933393638663163393836386261316430353935363166633166383466363630 39333238623933613965333362396438303534363237393936393133636539633931306237366466 63303336336662346135356462316134616266366162316239373733636265633432 解密文件 ansible-vault decrypt a.yaml $ ansible-vault decrypt a.yaml Vault password: # 这里输入密码 Decryption successful 查看解密后的文件 $ cat a.yaml # 安装apache - hosts: web tasks: - name: install httpd yum: name=httpd state=installed 4.Ansible正则 4.1 ALL全量匹配 all或* 匹配所有主机，all 与 * 号功能相同，但是*号需要用 \"\" 引起来 ansible all -m ping $ ansible all -m ping 10.0.0.101 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } 10.0.0.100 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } ansible \"*\" -m ping $ ansible \"*\" -m ping 10.0.0.101 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } 10.0.0.100 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } 4.2 逻辑或匹配 : 同时对多台主机或多个组同时执行，相互之间用 : 分割，例如 jenking:gitlab $ ansible jenkins:gitlab -m ping jenkins | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } gitlab | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } 4.3 逻辑非匹配 ! 逻辑非用 ! 表示，主要针对多重条件的匹配规则 # 所有在a组但不在b组的主机 ansible a:!b -m ping 4.4 逻辑与匹配 & 逻辑与用 & 表示 # a组和b组中同时存在的主机 ansible a:&b -m ping 4.5 模糊匹配 * * 通配符在ansible中表示0个或多个任意字符 # 所有以www开头.com结尾的主机 ansible www*.com -m ping 4.6 正则匹配 ~ ~ 在ansible中表示正则匹配 //匹配www.a.com和www.b.com ⚠️注意 ~ 要在最前边，一定要加双引号 # 不加引号，会报错语法错误 $ ansible ~www\\.(a|b)\\.com -m ping bash: syntax error near unexpected token `(' # 加引号，没有问题 $ ansible \"~www\\.(a|b)\\.com\" -m ping www.b.com | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } www.a.com | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } 5.Ansible清单管理 inventory文件通常用于定义要管理主机的认证信息， 例如ssh登录用户名、密码以及key相关信息。 主机 支持主机名通配以及正则表达式，例如 web[1:3].abc.com 支持基于非标准的ssh端口，例如 web1.abc.com:6666 支持指定变量，可对个别主机的特殊配置，如登陆用户，密码等 主机组 支持嵌套组，例如 [game:children]，那么在game模块下面的组都会被game所包含 支持指定变量，例如 [game:vars] 在下面指定变量 主机清单文件内容官方示例模板 # This is the default ansible 'hosts' file. # # It should live in /etc/ansible/hosts # # - Comments begin with the '#' character # - Blank lines are ignored # - Groups of hosts are delimited by [header] elements # - You can enter hostnames or ip addresses # - A hostname/ip can be a member of multiple groups # Ex 1: Ungrouped hosts, specify before any group headers. ## green.example.com ## blue.example.com ## 192.168.100.1 ## 192.168.100.10 # Ex 2: A collection of hosts belonging to the 'webservers' group ## [webservers] ## alpha.example.org ## beta.example.org ## 192.168.1.100 ## 192.168.1.110 # If you have multiple hosts following a pattern you can specify # them like this: ## www[001:006].example.com # Ex 3: A collection of database servers in the 'dbservers' group ## [dbservers] ## ## db01.intranet.mydomain.net ## db02.intranet.mydomain.net ## 10.25.1.56 ## 10.25.1.57 # Here's another example of host ranges, this time there are no # leading 0s: ## db-[99:101]-node.example.com 常规写法 # 添加三台主机至webservers组 [webservers] web1.abc.com web2.abc.com web3.abc.com # 上边的写法可以简写成这样 [webservers] web[1:3].abc.com 带密码写法 # 添加三台主机至webservers组 [webservers] web1.abc.com ansible_ssh_pass='1' web2.abc.com ansible_ssh_pass='1' web3.abc.com ansible_ssh_pass='1' # 上边的写法可以简写成这样 [webservers] web[1:3].abc.com ansible_ssh_pass='1' # 也可以写成如下形式 [webservers] web1.abc.com web2.abc.com web3.abc.com [webservers:vars] ansible_ssh_pass='1' 多组写法 # 定义多组，多组汇总整合 [apache] web1.abc.com web2.abc.com web3.abc.com [apache:vars] ansible_ssh_pass='1' [nginx] 10.0.0.1 10.0.0.2 10.0.0.3 [nginx:vars] ansible_ssh_pass='1' # webservers组包括两个子组[apapche,nginx] [webservers:children] apache nginx ansible nginx --list-hosts ansible apache --list-hosts ansible websers --list-hosts Ansible内置变量 参数 用途 示例 ansible_ssh_host 定义hosts ssh地址 ansible_ssh_host=192.168.1.10 ansible_ssh_port 定义hosts ssh端口 ansible_ssh_port=2222 ansible_ssh_user 定义hosts ssh认证用户 ansible_ssh_user=user ansible_ssh_pass 定义hosts ssh认证密码 ansible_ssh_pass=pass ansible_sudo 定义hosts sudo用户 ansible_sudo=www ansible_sudo_pass 定义hosts sudo密码 ansible_sudo_pass=pass ansible_sudo_exe 定义hosts sudo路径 ansible_sudo_exe=/usr/bin/sudo ansible_connection 定义hosts 连接方式 ansible_connection=local ansible_ssh_private_key_file 定义hosts 私钥 ansible_ssh_private_key_file=/root/key ansible_ssh_shell_type 定义hosts shell类型 ansible_ssh_shell_type=bash ansible_python_interpreter 定义hosts 任务执行python路径 ansible_python_interpreter=/usr/bin/python2.7 ansible_*_interpreter 定义hosts 其他语言解析路径 ansible_* _interpreter=/usr/bin/ruby 6.Ansible Playbook playbook是由一个或多个模块组成的，使用多个不同的模块，完成一件事情 playbook通过yaml语法识别描述的状态文件。扩展名是yaml或yml 6.1 YAML三要素 缩进 YAML使用一个固定的缩进风格表示层级结构,每个缩进由两个空格组成, 不能使用tabs 冒号 YAML使用一个固定的缩进风格表示层级结构,每个缩进由两个空格组成, 不能使用tabs 短横线 表示列表项，使用一个短横杠加一个空格 多个项使用同样的缩进级别作为同一列表 6.2 ansible playbook安装Apache示例 notify 表示当配置文件发生改变时，触发 handlers 中name与notify名称相同的操作 handlers 表示当有notify触发时执行 # 安装apache - hosts: web tasks: - name: install httpd yum: name=autoconf state=installed - name: configure httpd copy: src=./httpd.conf dest=/etc/httpd/conf/httpd.conf notify: restart httpd - name: start httpd service: name=httpd state=started enabled=yes handlers: - name: restart httpd service: name=httpd state=restarted 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible模块/ansible常用模块/1.ansible模块-命令模块.html":{"url":"linux/自动化运维平台/ansible/ansible模块/ansible常用模块/1.ansible模块-命令模块.html","title":"命令模块","keywords":"","body":"ansible模块-命令模块 ansible查看帮助方法 # 查看所有模块帮助信息 ansible-doc -l # 指定查看某个模块参数用法 ansible-doc copy 命令模块 1 command模块 不支持管道 不支持通配符 默认模块 # 默认模块，执行命令，以下两个命令本质相同 ansible all -m command -a \"hostname\" ansible all -a hostname command模块不支持管道 $ ansible all -m command -a \"ifconfig|grep eth0\" devops01 | FAILED | rc=2 >> [Errno 2] No such file or directory devops02 | FAILED | rc=2 >> [Errno 2] No such file or directory devops03 | FAILED | rc=2 >> [Errno 2] No such file or directory 2 shell模块 支持管道 支持通配符 $ ansible all -m shell -a \"ifconfig|grep eth0\" devops01 | CHANGED | rc=0 >> eth0: flags=4163 mtu 1500 devops02 | CHANGED | rc=0 >> eth0: flags=4163 mtu 1500 devops03 | CHANGED | rc=0 >> eth0: flags=4163 mtu 1500 3 raw/branch模块 支持管道 支持通配符 $ ansible all -m raw/branch -a \"ifconfig|grep eth0\" devops01 | CHANGED | rc=0 >> eth0: flags=4163 mtu 1500 Shared connection to devops01 closed. devops03 | CHANGED | rc=0 >> eth0: flags=4163 mtu 1500 Shared connection to devops03 closed. devops02 | CHANGED | rc=0 >> eth0: flags=4163 mtu 1500 Shared connection to devops02 closed. 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible模块/ansible常用模块/2.ansible模块-脚本模块.html":{"url":"linux/自动化运维平台/ansible/ansible模块/ansible常用模块/2.ansible模块-脚本模块.html","title":"脚本模块","keywords":"","body":"ansible模块-脚本模块 ansible查看帮助方法 # 查看所有模块帮助信息 ansible-doc -l # 指定查看某个模块参数用法 ansible-doc copy 1 script执行脚本模块 编辑测试脚本 # 创建目录 [ -d /script ] || mkdir /script # 编辑测试脚本 cat > /script/test.sh 在ansible本机运行，等同于在远程主机运行，不需要将脚本文件分别拷贝到目标主机执行 ansible all -m script -a \"/script/test.sh\" 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible模块/ansible常用模块/3.ansible模块-安装模块.html":{"url":"linux/自动化运维平台/ansible/ansible模块/ansible常用模块/3.ansible模块-安装模块.html","title":"安装模块","keywords":"","body":"ansible模块-安装模块 ansible查看帮助方法 # 查看所有模块帮助信息 ansible-doc -l # 指定查看某个模块参数用法 ansible-doc copy 1 yum安装软件模块 ansible all -m yum -a \"name=httpd state=installed\" yum模块参数 参数 说明 path 指定远程主机目录或文件信息 recurse 递归授权 latest 安装最新软件包 state installed，present 安装软件包 removed，absent 移除软件包 table{ border-collapse:collapse } tr,td{ border:1px solid #333; } 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible模块/ansible常用模块/4.ansible模块-文件模块.html":{"url":"linux/自动化运维平台/ansible/ansible模块/ansible常用模块/4.ansible模块-文件模块.html","title":"文件模块","keywords":"","body":"ansible模块-文件模块 ansible查看帮助方法 # 查看所有模块帮助信息 ansible-doc -l # 指定查看某个模块参数用法 ansible-doc copy 1 copy拷贝文件模块 1.1 拷贝文件 # 将ansible本机/etc/hosts文件拷贝到目标机/tmp下并命名为test.txt ansible all -m copy -a \"src=/etc/hosts dest=/tmp/test.txt\" 1.2 拷贝文件并备份 ansible all -m copy -a \"src=/etc/hosts dest=/tmp/test.txt backup=yes\" ⚠️只有当ansible本地文件内容变化之后，受控机本地才会有备份文件 $ ls test.txt test.txt.8600.2021-07-01@21:10:49~ 1.3 直接被控端文件内容 # 直接向远端文件内写入数据信息，并且会覆盖远端文件内原有数据信息，文件不存在会自动创建 ansible all -m copy -a \"content='abc' dest=/tmp/abc mode=777\" copy模块参数 参数 说明 src 源文件路径 dest 目标路径 backup 对传输过去的文件进行备份 content 直接批量在被管理端文件中添加内容 group 将本地文件推送到远端，指定文件属组信息 owner 将本地文件推送到远端，指定文件属主信息 mode 将本地文件推送到远端，指定文件权限信息 2 file文件配置模块 2.1 创建文件 # path中的远端路径必须存在 ansible all -m file -a \"path=/tmp/tt state=touch mode=555 owner=root group=root\" 2.2 创建目录 ansible all -m file -a \"path=/tmp/abc state=directory\" 2.3 创建软连接 ansible all -m file -a \"src=/tmp/tt path=/tmp/tt_link state=link\" file模块参数 参数 说明 path 指定远程主机目录或文件信息 recurse 递归授权 state directory 在远端创建目录 touch 在远端创建文件 link link或hard表示创建链接文件 absent 表示删除文件或目录 mode 设置文件或目录权限 owner 设置文件或目录属主信息 group 设置文件或目录属组信息 table{ border-collapse:collapse } tr,td{ border:1px solid #333; } 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible模块/ansible常用模块/5.ansible模块-服务模块.html":{"url":"linux/自动化运维平台/ansible/ansible模块/ansible常用模块/5.ansible模块-服务模块.html","title":"服务模块","keywords":"","body":"ansible模块-服务模块 ansible查看帮助方法 # 查看所有模块帮助信息 ansible-doc -l # 指定查看某个模块参数用法 ansible-doc copy 1 service服务模块 ⚠️是stopped，不是stoped ansible all -m service -a \"name=httpd state=stopped enabled=yes\" service模块参数 参数 说明 name 定义要启动服务的名称 enabled 是否让服务开机自启动 state started 启动 stopped 停止 restarted 重启 reloaded 重载 table{ border-collapse:collapse } tr,td{ border:1px solid #333; } 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible模块/ansible常用模块/6.ansible模块-用户和组模块.html":{"url":"linux/自动化运维平台/ansible/ansible模块/ansible常用模块/6.ansible模块-用户和组模块.html","title":"用户和组模块","keywords":"","body":"ansible模块-用户和组模块 ansible查看帮助方法 # 查看所有模块帮助信息 ansible-doc -l # 指定查看某个模块参数用法 ansible-doc copy 1.user用户模块 ansible all -m user -a \"name=hehe uid=888 group=888 shell=/sbin/nologin create_home=no\" user模块参数 参数 说明 name 指定要创建的用户名称 uid 指定用户uid create_home 是否创建用户家目录 group 指定用户组名称 groups 用于指定用户所在的附加组。注意，如果说用户已经存在并且已经拥有多个附加组，那么如果想要继续添加新的附加组，需要结合 append 参数使用，否则在默认情况下，当再次使用 groups 参数设置附加组时，用户原来的附加组会被覆盖。 append 如果用户原本就存在多个附加组，那么当使用 groups 参数设置附加组时，当前设置会覆盖原来的附加组设置，如果不想覆盖原来的附加组设置，需要结合 append 参数，将 append 设置为 yes，表示追加附加组到现有的附加组设置，append 默认值为 no shell 指定用户登陆shell expires 此参数用于指定用户的过期时间，相当于设置 /etc/shadow 文件中的的第8列，比如，你想要设置用户的过期日期为2018年12月31日，那么你首先要获取到2018年12月31日的 unix 时间戳，使用命令 \"date -d 2018-12-31 +%s\" 获取到的时间戳为1546185600，所以，当设置 expires=1546185600 时，表示用户的过期时间为2018年12月31日0点0分，设置成功后，查看远程主机的 /etc/shadow 文件，对应用户的第8八列的值将变成17895（表示1970年1月1日到2018年12月31日的天数，unix 时间戳的值会自动转换为天数，我们不用手动的进行换算），目前此参数只支持在 Linux 和 FreeBSD 系统中使用 password 指定用户的密码，此密码为哈希加密后的密码 generate_ssh_key 此参数默认值为 no，如果设置为 yes，表示为对应的用户生成 ssh 密钥对，默认在用户家目录的 ./ssh 目录中生成名为 id_rsa 的私钥和名为 id_rsa.pub 的公钥，如果同名的密钥已经存在与对应的目录中，原同名密钥并不会被覆盖(不做任何操作) ssh_key_file 当 generate_ssh_key 参数的值为 yes 时，使用此参数自定义生成 ssh 私钥的路径和名称，对应公钥会在同路径下生成，公钥名以私钥名开头，以\".pub\"结尾 ssh_key_comment 当 generate_ssh_key 参数的值为 yes 时，在创建证书时，使用此参数设置公钥中的注释信息。但是如果同名的密钥对已经存在，则并不会修改原来的注释信息，即不做任何操作。当不指定此参数时，默认的注释信息为\"ansible-generated on 远程主机的主机名\" ssh_key_passphrase 当 generate_ssh_key 参数的值为 yes 时，在创建证书时，使用此参数设置私钥的密码。但是如果同名的密钥对已经存在，则并不会修改原来的密码，即不做任何操作 ssh_key_type 当 generate_ssh_key 参数的值为 yes 时，在创建证书时，使用此参数设置密钥对的类型。默认密钥类型为 rsa，但是如果同名的密钥对已经存在，并不会对同名密钥做任何操作 remove 当 state 的值设置为 absent 时，表示要删除远程主机中的用户。但是在删除用户时，不会删除用户的家目录等信息，这是因为 remove 参数的默认值为 no，如果设置为yes，在删除用户的同时，会删除用户的家目录。当 state=absent 并且 remove=yes 时，相当于执行 \"userdel --remove\" 命令 state present 默认为present，创建用户 absent 删除用户 table{ border-collapse:collapse } tr,td{ border:1px solid #333; } 使用示例 ad-hoc 在命令行中不能使用明文密码，执行后虽然会提示 CHANGED ，但是实际上是不生效的 $ ansible all -m user -a 'name=root password=123' [WARNING]: The input password appears not to have been hashed. The 'password' argument must be encrypted for this module to work properly. 使用如下方法 参考链接 # 先在命令行中指定一个变量 password='1' # 然后再执行即可 ansible 10.0.0.25 -m user -a \"name=root update_password=always password=\\\"{{ \\\"$password\\\" | password_hash('sha512') }}\\\"\" playbook 修改多个用户 cat > change_users_pwd.yml ansible-playbook change_users_pwd.yml 修改单个用户 cat > change_user_pwd.yml 使用 -e 参数传递变量到playbook中 ansible-playbook change_user_pwd.yml -e \"name1=root chpass=123\" 2.group组模块 ansible all -m group -a \"name=hehe gid=888\" group模块参数 参数 说明 name 要创建的组名称 gid 指定组gid state present 默认为present，创建组 absent 删除组 table{ border-collapse:collapse } tr,td{ border:1px solid #333; } 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible模块/ansible常用模块/7.ansible模块-计划任务模块.html":{"url":"linux/自动化运维平台/ansible/ansible模块/ansible常用模块/7.ansible模块-计划任务模块.html","title":"计划任务模块","keywords":"","body":"ansible模块-计划任务模块 ansible查看帮助方法 # 查看所有模块帮助信息 ansible-doc -l # 指定查看某个模块参数用法 ansible-doc copy 1 cron计划任务模块 1.1 新增计划任务 正常使用crond服务 $ crontab -l * * * * * /bin/sh /server/scripts/yum.sh 使用ansible添加一条计划任务 ansible all -m cron -a \"minute=* hour=* day=* month=* weekday=* job='/bin/sh /server/scripts/test.sh'\" 等同于 ansible all -m cron -a \"job='/bin/sh /server/scripts/test.sh'\" 执行完后查看，可以看到，当不指定 name 参数时，默认的注释为 #Ansible: None $ crontab -l #Ansible: None * * * * * /bin/sh /server/scripts/test.sh 设置计划任务注释信息，需要使用 name 参数 ansible all -m cron -a \"name='cron01' job='/bin/sh /server/scripts/test.sh'\" 执行完后查看，可以看到，当指定 name 参数时，会有指定名称的注释 $ crontab -l #Ansible: cron01 * * * * * /bin/sh /server/scripts/test.sh 指定用户 ansible all -m cron -a \"name='cron01' job='/bin/sh /server/scripts/test.sh' user=hehe\" 查看 hehe 用户的计划任务 $ crontab -l -u hehe #Ansible: cron01 * * * * * /bin/sh /server/scripts/test.sh 1.2 删除计划任务 删除注释名称为 cron01 的计划任务 ansible all -m cron -a \"name=cron01 state=absent\" 1.3 注释计划任务 查看计划任务 $ crontab -l #Ansible: cron01 * * * * * /bin/sh /server/scripts/test.sh 注释计划任务 注释之后想要取消注视需要指定 disabled=no ansible all -m cron -a \"name='cron01' minute=* hour=* day=* month=* weekday=* job='/bin/sh /server/scripts/test.sh' disabled=yes\" 再次查看 $ crontab -l #Ansible: cron01 #* * * * * /bin/sh /server/scripts/test.sh 1.4 修改计划任务 查看计划任务 $ crontab -l #Ansible: cron01 * * * * * /bin/sh /server/scripts/test.sh 修改计划任务 因为已经存在注释名称为 cron01 的计划任务，因此再次执行会修改，添加 backup 参数备份计划任务 $ ansible all -m cron -a \"name='cron01' minute=10 hour=10 day=* month=* weekday=* job='/bin/sh /server/scripts/test.sh' backup=yes\" devops02 | CHANGED => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"backup_file\": \"/tmp/crontab7HtoRG\", \"changed\": true, \"envs\": [], \"jobs\": [ \"cron01\" ] } 查看备份 $ cat /tmp/crontabeXW_Jf #Ansible: cron01 * * * * * /bin/sh /server/scripts/test.sh cron模块参数 参数 说明 minute 分钟 hour 小时 day 日期 month 月份 weekday 星期 job 要执行的命令或脚本 name 指定计划任务名称 disabled 是否注视 state absent表示删除计划任务 user 指定运行用户 disabled 注释计划任务，必须指定名称、时间、job backup 如果为yes，则修改或者删除计划任务前会先备份 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible模块/ansible常用模块/8.ansible模块-挂载模块.html":{"url":"linux/自动化运维平台/ansible/ansible模块/ansible常用模块/8.ansible模块-挂载模块.html","title":"挂载模块","keywords":"","body":"ansible模块-挂载模块 ansible查看帮助方法 # 查看所有模块帮助信息 ansible-doc -l # 指定查看某个模块参数用法 ansible-doc copy 1 mount模块 [root@ansible ~]# ansible all -m mount -a \"src=172.16.1.31:/data path=/data fstype=nfs opts=defaults state=present\" [root@ansible ~]# ansible web -m mount -a \"src=172.16.1.31:/data path=/data fstype=nfs opts=defaults state=mounted\" [root@ansible ~]# ansible web -m mount -a \"src=172.16.1.31:/data path=/data fstype=nfs opts=defaults state=unmounted\" [root@ansible ~]# ansible web -m mount -a \"src=172.16.1.31:/data path=/data fstype=nfs opts=defaults state=absent\" mount模块参数 参数 说明 src 指定要挂载的内容 path 本地挂载点 fstype 挂载类型 opts 挂载权限 state present 开机挂载，仅将挂载配置写入/etc/fstab mounted 挂载设备，并将配置写入/etc/fstab unmounted 卸载设备，不会清除/etc/fstab写入的配置 absent 卸载设备，会清理/etc/fstab写入的配置 table{ border-collapse:collapse } tr,td{ border:1px solid #333; } 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible模块/ansible常用模块/9.ansible模块-解压缩模块.html":{"url":"linux/自动化运维平台/ansible/ansible模块/ansible常用模块/9.ansible模块-解压缩模块.html","title":"解压缩模块","keywords":"","body":"ansible模块-解压缩模块 ansible查看帮助方法 # 查看所有模块帮助信息 ansible-doc -l # 指定查看某个模块参数用法 ansible-doc copy 1 unarchive模块 在ansbile本机执行解压缩 ansible all -m unarchive -a \"src=/tmp/hehe.tar.gz dest=/tmp\" 在受控机执行解压缩 如果是在受控机执行解压缩，则受控机上压缩文件必须存在 ansible all -m unarchive -a \"src=/tmp/hehe.tar.gz dest=/tmp copy=no\" unarchive模块参数 参数 说明 copy 默认为yes，即先在本地解压然后再传输到受控机，如果为no，则解压缩在受控机上执行 creates 当文件存在时，不再进行解压 mode 指定解压缩文件的权限 list_files 是否列出文件列表，默认no remote_src 表示文件已经在受控机上，相当于copy =no 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible模块/ansible常用模块/10.ansible模块-下载模块.html":{"url":"linux/自动化运维平台/ansible/ansible模块/ansible常用模块/10.ansible模块-下载模块.html","title":"下载模块","keywords":"","body":"ansible模块-下载模块 ansible查看帮助方法 # 查看所有模块帮助信息 ansible-doc -l # 指定查看某个模块参数用法 ansible-doc copy 1 get_url模块 ansible all -m get_url -a \"url=https://mirrors4.tuna.tsinghua.edu.cn/zabbix/zabbix/5.0/rhel/7/x86_64/zabbix-agent-5.0.0-1.el7.x86_64.rpm dest=/opt\" get_url模块参数 参数 说明 url 指定要下载的url地址 dest 指定将url下载至哪 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible模块/ansible常用模块/11.ansible模块-替换模块.html":{"url":"linux/自动化运维平台/ansible/ansible模块/ansible常用模块/11.ansible模块-替换模块.html","title":"替换模块","keywords":"","body":"ansible模块-替换模块 ansible查看帮助方法 # 查看所有模块帮助信息 ansible-doc -l # 指定查看某个模块参数用法 ansible-doc copy 1 lineinfile模块 ansible all -m lineinfile -a \"path=/root/hehe regexp='^user admin' line='user hehe'\" lineinfile模块参数 参数 说明 path 文件路径 regexp 匹配的规则，即要替换的内容 line 替换为什么 state absent 删除 2 replace模块 ansible all -m replace -a \"path=/root/hehe regexp='^user admin' replace='hehe'\" replace模块参数 参数 说明 path 文件路径 regexp 匹配的规则，即要替换的内容 replace 替换为什么 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible变量/ansible定义变量.html":{"url":"linux/自动化运维平台/ansible/ansible变量/ansible定义变量.html","title":"ansible定义变量","keywords":"","body":"[toc] ansible定义变量 ansible2.9变量官方文档 1.ansible定义变量 1.1 在playbook中的play进行定义 在yml文件中通过 vars 关键字定义变量，引用变量使用 {{}} vars: - 变量名1： 变量值1 - 变量名2： 变量值2 使用示例 - hosts: all vars: - pkg_name1: httpd - pkg_name2: nginx tasks: - name: install httpd yum: name: - \"{{ pkg_name1 }}\" - \"{{ pkg_name2 }}\" state: present 在playbook中还可以通过 vars_files 关键字引用变量文件 vars_files: 变量文件 编辑一个变量文件 pkg_name1: httpd pkg_name2: nginx 在yml文件使用关键字 vars_files 引用变量文件 - hosts: devops02 vars_files: ./vars_pub.yml tasks: - name: install httpd nginx yum: name: - \"{{ pkg_name1 }}\" - \"{{ pkg_name2 }}\" state: present 1.2 通过inventory主机清单进行定义 在inventory主机清单中通过 [组名:vars] 定义变量，这个变量既可以在inventory中引用，也可以在playbook中引用 [all_server:vars] ansible_ssh_user=ops ansible_ssh_port=2233 ansible_ssh_private_key_file=/home/ops/.ssh/id_rsa_ops ansible_become=true ansible_become_method=sudo ansible_become_user=root [all_server] devops01 devops02 1.2.1 group_vars 与 host_vars 官方推荐在项目目录下，创建两个变量目录 host_vars、group_vars ，在这2个目录下存放变量的文件 在组(group_vars)下面创建一个和inventory中组名相同的变量文件，那么inventory中某个组下面的主机就会引用 group_vars 目录下与组名同名的变量文件中的变量 在主机(host_vars)下面创建一个和inventory中主机名相同的变量文件，那么invertory中的某个主机就会引用 host_vars 目录下和主机同名的变量文件中的变量 1.2.1.1 group_vars 使用示例 在项目下创建 group_vars 目录 mkdir group_vars 项目下hosts文件内容如下 [devops] devops01 devops02 在 group_vars 目录下创建与组名同名的文件并写入变量 cat > group_vars/devops 编辑yml文件 cat > vars01.yml 执行playbook，可以看到在yml文件中没有指定变量文件的情况下会自动读取 group_vars 目录下与invertory中组名同名的变量文件中的变量 $ ansible-playbook vars01.yml -i hosts PLAY [devops] ****************************************************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************************************* ok: [devops02] ok: [devops01] TASK [install httpd nginx] ***************************************************************************************************************************** ok: [devops02] ok: [devops01] PLAY RECAP ********************************************************************************************************************************************* devops01 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 devops02 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 1.2.1.2 host_vars 使用示例 在项目下创建 host_vars 目录 mkdir host_vars 项目下hosts文件内容如下 [devops] devops01 在 host_vars 目录下创建与主机同名的文件并写入变量 cat > host_vars/devops01 编辑yml文件 cat > vars02.yml 执行playbook，可以看到在yml文件中没有指定变量文件的情况下会自动读取 host_vars 目录下与invertory中主机名同名的变量文件中的变量 $ ansible-playbook vars02.yml -i hosts PLAY [devops01] **************************************************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************************************* ok: [devops01] TASK [install httpd nginx] ***************************************************************************************************************************** ok: [devops01] PLAY RECAP ********************************************************************************************************************************************* devops01 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 1.2.2 特殊组 all group_vars 与 host_vars 有一个共同的缺点，如果有多个组或者多台主机的话，那就得编辑多个变量文件，即使变量文件内容一致也需要编辑多个与组名相同的变量文件，这样的话就会有很多重复的工作 例如在 group_vars 目录下有2个与组名相同的变量文件v1与v2，v1与v2文件内容是相同的，但是v1与v2不能互相调用对方的变量，此时使用 all 这个组就可以解决这个问题 项目下 hosts 文件内容如下 [d1] devops01 [d2] devops02 在 host_vars 目录下创建 all 组并写入变量 cat > group_vars/all 编辑yml文件d1 cat > d1.yml 执行playbook，可以从 group_vars 目录下读取 all 组中的变量 $ ansible-playbook d1.yml -i hosts PLAY [d1] ********************************************************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************************************* ok: [devops01] TASK [install httpd nginx] ***************************************************************************************************************************** ok: [devops01] PLAY RECAP ********************************************************************************************************************************************* devops01 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 编辑yml文件d2 cat > d2.yml 执行playbook，可以从 group_vars 目录下读取 all 组中的变量 $ ansible-playbook d2.yml -i hosts PLAY [d2] ********************************************************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************************************* ok: [devops02] TASK [install httpd nginx] ***************************************************************************************************************************** ok: [devops02] PLAY RECAP ********************************************************************************************************************************************* devops02 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 playbook指定执行单个主机的时候，会先从 host_vars 下查找变量，如果没有就从 group_vars 中查找，最后从 group_vars/all 中查找 1.3 通过执行playbook时进行定义 执行playbook通过 -e 参数指定变量 项目下hosts文件内容如下 [prod] devops01 [test] devops02 编辑yml文件，主机处定义一个变量，在执行playbook的时候通过 -e 参数指定要执行的主机组 cat > var.yml 执行playbook的时候，通过 -e 指定要执行的主机组 $ ansible-playbook -i hosts var.yml -e \"hosts=prod\" [WARNING]: Found variable using reserved name: hosts PLAY [prod] ******************************************************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************************************* ok: [devops01] TASK [install nginx] *********************************************************************************************************************************** ok: [devops01] PLAY RECAP ********************************************************************************************************************************************* devops01 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 2.ansible变量优先级 -e 传参 > vars_files(playbook) > vars(playbook) > host_vars(inventory) > group_vars(inventory)> 特殊组all 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible变量/ansible变量注册register.html":{"url":"linux/自动化运维平台/ansible/ansible变量/ansible变量注册register.html","title":"ansible变量注册register","keywords":"","body":"ansible变量注册register ansible变量注册register就是在执行的playbook中如果想获取某些信息，但是plabybook又不支持输出的情况下使用，这个时候使用register把想要获取的输入的信息放到一个变量中，然后通过这个变量再输出 示例：安装启动httpd的同时想要查看httpd的启动状态 这里通过shell模块执行 ps aux 命令 - hosts: devops tasks: - name: install httpd yum: name: - httpd state: present - name: start httpd service: name: httpd state: started - name: check httpd status shell: ps aux|grep httpd 但是没有得到我们想要的输出 $ ansible-playbook install_httpd.yml PLAY [devops] ****************************************************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************************************* ok: [devops02] ok: [devops03] ok: [devops01] TASK [install httpd] *********************************************************************************************************************************** ok: [devops02] ok: [devops03] ok: [devops01] TASK [start httpd] ************************************************************************************************************************************* ok: [devops02] ok: [devops03] ok: [devops01] TASK [check httpd status] ****************************************************************************************************************************** changed: [devops02] changed: [devops03] changed: [devops01] PLAY RECAP ********************************************************************************************************************************************* devops01 : ok=4 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 devops02 : ok=4 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 devops03 : ok=4 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 修改一下yml文件 - hosts: devops tasks: - name: install httpd yum: name: - httpd state: present - name: start httpd service: name: httpd state: started - name: check httpd status shell: ps aux|grep httpd # 这里增加一个register关键字，下边是变量的名称 # register的作用是接收上一步shell模块的输出 register: check_httpd - name: print output debug: msg: \"{{ check_httpd }}\" 可以看到有很多的输出，如果不想看到所有的输出，则需要指定关键字输出 $ ansible-playbook install_httpd.yml PLAY [devops] ****************************************************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************************************* ok: [devops02] ok: [devops03] ok: [devops01] TASK [install httpd] *********************************************************************************************************************************** ok: [devops02] ok: [devops03] ok: [devops01] TASK [start httpd] ************************************************************************************************************************************* ok: [devops02] ok: [devops03] ok: [devops01] TASK [check httpd status] ****************************************************************************************************************************** changed: [devops02] changed: [devops03] changed: [devops01] TASK [print output] ************************************************************************************************************************************ ok: [devops01] => { \"msg\": { \"changed\": true, \"cmd\": \"ps aux|grep httpd\", \"delta\": \"0:00:00.042416\", \"end\": \"2021-09-12 23:11:20.304771\", \"failed\": false, \"rc\": 0, \"start\": \"2021-09-12 23:11:20.262355\", \"stderr\": \"\", \"stderr_lines\": [], \"stdout\": \"root 25808 0.0 0.1 224080 5036 ? Ss 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\\napache 25810 0.0 0.0 226164 3092 ? S 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\\napache 25811 0.0 0.0 226164 3092 ? S 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\\napache 25812 0.0 0.0 226164 3092 ? S 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\\napache 25813 0.0 0.0 226164 3092 ? S 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\\napache 25814 0.0 0.0 226164 3092 ? S 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\\nroot 30483 17.4 1.1 392188 44092 pts/0 Sl+ 23:11 0:00 /usr/bin/python2 /usr/bin/ansible-playbook install_httpd.yml\\nroot 31578 6.0 1.1 395864 41904 pts/0 S+ 23:11 0:00 /usr/bin/python2 /usr/bin/ansible-playbook install_httpd.yml\\nroot 31580 14.0 1.3 403088 49884 pts/0 S+ 23:11 0:00 /usr/bin/python2 /usr/bin/ansible-playbook install_httpd.yml\\nroot 31591 7.0 1.0 394384 40928 pts/0 S+ 23:11 0:00 /usr/bin/python2 /usr/bin/ansible-playbook install_httpd.yml\\nroot 31840 0.0 0.0 113280 1196 pts/4 S+ 23:11 0:00 /bin/sh -c ps aux|grep httpd\\nroot 31844 0.0 0.0 112812 948 pts/4 S+ 23:11 0:00 grep httpd\", \"stdout_lines\": [ \"root 25808 0.0 0.1 224080 5036 ? Ss 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\", \"apache 25810 0.0 0.0 226164 3092 ? S 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\", \"apache 25811 0.0 0.0 226164 3092 ? S 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\", \"apache 25812 0.0 0.0 226164 3092 ? S 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\", \"apache 25813 0.0 0.0 226164 3092 ? S 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\", \"apache 25814 0.0 0.0 226164 3092 ? S 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\", \"root 30483 17.4 1.1 392188 44092 pts/0 Sl+ 23:11 0:00 /usr/bin/python2 /usr/bin/ansible-playbook install_httpd.yml\", \"root 31578 6.0 1.1 395864 41904 pts/0 S+ 23:11 0:00 /usr/bin/python2 /usr/bin/ansible-playbook install_httpd.yml\", \"root 31580 14.0 1.3 403088 49884 pts/0 S+ 23:11 0:00 /usr/bin/python2 /usr/bin/ansible-playbook install_httpd.yml\", \"root 31591 7.0 1.0 394384 40928 pts/0 S+ 23:11 0:00 /usr/bin/python2 /usr/bin/ansible-playbook install_httpd.yml\", \"root 31840 0.0 0.0 113280 1196 pts/4 S+ 23:11 0:00 /bin/sh -c ps aux|grep httpd\", \"root 31844 0.0 0.0 112812 948 pts/4 S+ 23:11 0:00 grep httpd\" ] } } ok: [devops02] => { \"msg\": { \"changed\": true, \"cmd\": \"ps aux|grep httpd\", \"delta\": \"0:00:00.037817\", \"end\": \"2021-09-12 23:11:20.254390\", \"failed\": false, \"rc\": 0, \"start\": \"2021-09-12 23:11:20.216573\", \"stderr\": \"\", \"stderr_lines\": [], \"stdout\": \"root 3190 0.0 0.4 224084 5036 ? Ss 22:33 0:00 /usr/sbin/httpd -DFOREGROUND\\napache 3191 0.0 0.3 226168 3092 ? S 22:33 0:00 /usr/sbin/httpd -DFOREGROUND\\napache 3192 0.0 0.3 226168 3092 ? S 22:33 0:00 /usr/sbin/httpd -DFOREGROUND\\napache 3193 0.0 0.3 226168 3092 ? S 22:33 0:00 /usr/sbin/httpd -DFOREGROUND\\napache 3194 0.0 0.3 226168 3092 ? S 22:33 0:00 /usr/sbin/httpd -DFOREGROUND\\napache 3195 0.0 0.3 226168 3092 ? S 22:33 0:00 /usr/sbin/httpd -DFOREGROUND\\nroot 9953 0.0 0.1 113280 1196 pts/1 S+ 23:11 0:00 /bin/sh -c ps aux|grep httpd\\nroot 9955 0.0 0.0 112808 952 pts/1 S+ 23:11 0:00 grep httpd\", \"stdout_lines\": [ \"root 3190 0.0 0.4 224084 5036 ? Ss 22:33 0:00 /usr/sbin/httpd -DFOREGROUND\", \"apache 3191 0.0 0.3 226168 3092 ? S 22:33 0:00 /usr/sbin/httpd -DFOREGROUND\", \"apache 3192 0.0 0.3 226168 3092 ? S 22:33 0:00 /usr/sbin/httpd -DFOREGROUND\", \"apache 3193 0.0 0.3 226168 3092 ? S 22:33 0:00 /usr/sbin/httpd -DFOREGROUND\", \"apache 3194 0.0 0.3 226168 3092 ? S 22:33 0:00 /usr/sbin/httpd -DFOREGROUND\", \"apache 3195 0.0 0.3 226168 3092 ? S 22:33 0:00 /usr/sbin/httpd -DFOREGROUND\", \"root 9953 0.0 0.1 113280 1196 pts/1 S+ 23:11 0:00 /bin/sh -c ps aux|grep httpd\", \"root 9955 0.0 0.0 112808 952 pts/1 S+ 23:11 0:00 grep httpd\" ] } } ok: [devops03] => { \"msg\": { \"changed\": true, \"cmd\": \"ps aux|grep httpd\", \"delta\": \"0:00:00.037784\", \"end\": \"2021-09-12 23:11:20.266617\", \"failed\": false, \"rc\": 0, \"start\": \"2021-09-12 23:11:20.228833\", \"stderr\": \"\", \"stderr_lines\": [], \"stdout\": \"root 25572 0.0 0.4 224084 5036 ? Ss 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\\napache 25573 0.0 0.3 226168 3092 ? S 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\\napache 25574 0.0 0.3 226168 3092 ? S 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\\napache 25575 0.0 0.3 226168 3092 ? S 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\\napache 25576 0.0 0.3 226168 3092 ? S 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\\napache 25577 0.0 0.3 226168 3092 ? S 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\\nroot 30803 0.0 0.1 113280 1196 pts/0 S+ 23:11 0:00 /bin/sh -c ps aux|grep httpd\\nroot 30805 0.0 0.0 112808 952 pts/0 S+ 23:11 0:00 grep httpd\", \"stdout_lines\": [ \"root 25572 0.0 0.4 224084 5036 ? Ss 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\", \"apache 25573 0.0 0.3 226168 3092 ? S 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\", \"apache 25574 0.0 0.3 226168 3092 ? S 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\", \"apache 25575 0.0 0.3 226168 3092 ? S 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\", \"apache 25576 0.0 0.3 226168 3092 ? S 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\", \"apache 25577 0.0 0.3 226168 3092 ? S 22:31 0:00 /usr/sbin/httpd -DFOREGROUND\", \"root 30803 0.0 0.1 113280 1196 pts/0 S+ 23:11 0:00 /bin/sh -c ps aux|grep httpd\", \"root 30805 0.0 0.0 112808 952 pts/0 S+ 23:11 0:00 grep httpd\" ] } } PLAY RECAP ********************************************************************************************************************************************* devops01 : ok=5 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 devops02 : ok=5 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 devops03 : ok=5 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 msg输出的关键字说明 关键字 说明 changed 是否改变 cmd register上一步执行的命令 delta 执行花费的时间 end 结束时间 failed 是否失败 rc 返回0表明执行成功，返回1表明执行失败 start 开始时间 stderr 错误输出 stderr_lines 错误输出，以行展示 stdout 输出信息 stdout_lines 输出信息，以行展示 再次修改yml文件，这次我们只获取rc信息 - hosts: devops tasks: - name: install httpd yum: name: - httpd state: present - name: start httpd service: name: httpd state: started - name: check httpd status shell: ps aux|grep httpd # 这里增加一个register关键字，下边是变量的名称 # register的作用是接收上一步shell模块的输出 register: check_httpd - name: print output debug: msg: # 变量.关键字 就是输出指定的信息 \"{{ check_httpd.rc }}\" 可以看到，只输出了rc的相关信息 $ ansible-playbook install_httpd.yml PLAY [devops] ****************************************************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************************************* ok: [devops02] ok: [devops03] ok: [devops01] TASK [install httpd] *********************************************************************************************************************************** ok: [devops02] ok: [devops03] ok: [devops01] TASK [start httpd] ************************************************************************************************************************************* ok: [devops02] ok: [devops03] ok: [devops01] TASK [check httpd status] ****************************************************************************************************************************** changed: [devops02] changed: [devops03] changed: [devops01] TASK [print output] ************************************************************************************************************************************ ok: [devops01] => { \"msg\": \"0\" } ok: [devops02] => { \"msg\": \"0\" } ok: [devops03] => { \"msg\": \"0\" } PLAY RECAP ********************************************************************************************************************************************* devops01 : ok=5 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 devops02 : ok=5 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 devops03 : ok=5 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 ansible变量注册register的使用步骤如下 1.ansible playbook在某些情况下无法直接输出，因此使用register来获取我们想要的输出结果，例如想要获取shell模块执行的命令的输出 2.register的作用就是将命令的输出保存到一个变量中，变量名任意 3.通过debug模块的msg方法输出全部结果，如果想要指定某些字段的输出，则需要使用 变量名.关键字 的方法来获取指定的输出 - name: check httpd status shell: ps aux|grep httpd # 这里增加一个register关键字，下边是变量的名称 # register的作用是接收上一步shell模块的输出 register: check_httpd - name: print output debug: msg: # 变量.关键字 就是输出指定的信息 \"{{ check_httpd.rc }}\" 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible变量/ansible Facts变量.html":{"url":"linux/自动化运维平台/ansible/ansible变量/ansible Facts变量.html","title":"ansible Facts变量","keywords":"","body":"[toc] ansible Facts变量 ⚠️在yml文件中通过 gather_facts: no 来关闭facts变量 1.Facts变量作用 Facts变量作用：用来采集被控端主机信息，例如cpu、内存、硬盘等 2.使用setup模块来获取被控机相关信息 可以使用setup模块来获取被控机相关信息 ansible devops02 -m setup 以下为执行setup模块输出的单个被控机全部内容，所看到的信息全部都是变量 devops02 | SUCCESS => { \"ansible_facts\": { \"ansible_all_ipv4_addresses\": [ \"10.0.0.61\" ], \"ansible_all_ipv6_addresses\": [ \"fe80::21c:42ff:fed4:711d\" ], \"ansible_apparmor\": { \"status\": \"disabled\" }, \"ansible_architecture\": \"x86_64\", \"ansible_bios_date\": \"07/31/2020\", \"ansible_bios_version\": \"16.0.0 (48916)\", \"ansible_cmdline\": { \"BOOT_IMAGE\": \"/vmlinuz-3.10.0-1160.el7.x86_64\", \"LANG\": \"en_US.UTF-8\", \"net.ifnames\": \"0\", \"quiet\": true, \"rd.lvm.lv\": \"centos/swap\", \"rhgb\": true, \"ro\": true, \"root\": \"/dev/mapper/centos-root\", \"spectre_v2\": \"retpoline\" }, \"ansible_date_time\": { \"date\": \"2021-09-15\", \"day\": \"15\", \"epoch\": \"1631707228\", \"hour\": \"20\", \"iso8601\": \"2021-09-15T12:00:28Z\", \"iso8601_basic\": \"20210915T200028186193\", \"iso8601_basic_short\": \"20210915T200028\", \"iso8601_micro\": \"2021-09-15T12:00:28.186193Z\", \"minute\": \"00\", \"month\": \"09\", \"second\": \"28\", \"time\": \"20:00:28\", \"tz\": \"CST\", \"tz_offset\": \"+0800\", \"weekday\": \"Wednesday\", \"weekday_number\": \"3\", \"weeknumber\": \"37\", \"year\": \"2021\" }, \"ansible_default_ipv4\": { \"address\": \"10.0.0.61\", \"alias\": \"eth0\", \"broadcast\": \"10.0.0.255\", \"gateway\": \"10.0.0.1\", \"interface\": \"eth0\", \"macaddress\": \"00:1c:42:d4:71:1d\", \"mtu\": 1500, \"netmask\": \"255.255.255.0\", \"network\": \"10.0.0.0\", \"type\": \"ether\" }, \"ansible_default_ipv6\": {}, \"ansible_device_links\": { \"ids\": { \"dm-0\": [ \"dm-name-centos-root\", \"dm-uuid-LVM-MDpDUifPb6kJUr9fDyJ3m16lXE23SScm2hWpbl8n7I5MqclSkfSLRtLVhwqIZP5B\" ], \"dm-1\": [ \"dm-name-centos-swap\", \"dm-uuid-LVM-MDpDUifPb6kJUr9fDyJ3m16lXE23SScmUhtOfTUBKWPnHn8VklaeXfnR6LhzeZUO\" ], \"sda\": [ \"ata-Virtual_HDD_V65X1HR75TK4SFVV1XTD\" ], \"sda1\": [ \"ata-Virtual_HDD_V65X1HR75TK4SFVV1XTD-part1\" ], \"sda2\": [ \"ata-Virtual_HDD_V65X1HR75TK4SFVV1XTD-part2\", \"lvm-pv-uuid-Z75Sgb-wZGs-IthV-5Bhm-hn5m-9VeB-24PIOp\" ], \"sr0\": [ \"ata-Virtual_DVD-ROM__1__-_31415B265\" ] }, \"labels\": {}, \"masters\": { \"sda2\": [ \"dm-0\", \"dm-1\" ] }, \"uuids\": { \"dm-0\": [ \"0fe73be4-eeb3-4995-9d5b-bfc0cc576a83\" ], \"dm-1\": [ \"1c605635-105b-4fc5-aa39-633d867bf1d7\" ], \"sda1\": [ \"544d152e-4e60-41f1-b1f9-d976a1983ac3\" ] } }, \"ansible_devices\": { \"dm-0\": { \"holders\": [], \"host\": \"\", \"links\": { \"ids\": [ \"dm-name-centos-root\", \"dm-uuid-LVM-MDpDUifPb6kJUr9fDyJ3m16lXE23SScm2hWpbl8n7I5MqclSkfSLRtLVhwqIZP5B\" ], \"labels\": [], \"masters\": [], \"uuids\": [ \"0fe73be4-eeb3-4995-9d5b-bfc0cc576a83\" ] }, \"model\": null, \"partitions\": {}, \"removable\": \"0\", \"rotational\": \"0\", \"sas_address\": null, \"sas_device_handle\": null, \"scheduler_mode\": \"\", \"sectors\": \"98549760\", \"sectorsize\": \"512\", \"serial\": \"V65X1HR75TK4SFVV1XTD\", \"size\": \"46.99 GB\", \"support_discard\": \"4096\", \"vendor\": null, \"virtual\": 1 }, \"dm-1\": { \"holders\": [], \"host\": \"\", \"links\": { \"ids\": [ \"dm-name-centos-swap\", \"dm-uuid-LVM-MDpDUifPb6kJUr9fDyJ3m16lXE23SScmUhtOfTUBKWPnHn8VklaeXfnR6LhzeZUO\" ], \"labels\": [], \"masters\": [], \"uuids\": [ \"1c605635-105b-4fc5-aa39-633d867bf1d7\" ] }, \"model\": null, \"partitions\": {}, \"removable\": \"0\", \"rotational\": \"0\", \"sas_address\": null, \"sas_device_handle\": null, \"scheduler_mode\": \"\", \"sectors\": \"4194304\", \"sectorsize\": \"512\", \"serial\": \"V65X1HR75TK4SFVV1XTD\", \"size\": \"2.00 GB\", \"support_discard\": \"4096\", \"vendor\": null, \"virtual\": 1 }, \"sda\": { \"holders\": [], \"host\": \"SATA controller: Intel Corporation 82801HR/HO/HH (ICH8R/DO/DH) 6 port SATA Controller [AHCI mode] (rev 02)\", \"links\": { \"ids\": [ \"ata-Virtual_HDD_V65X1HR75TK4SFVV1XTD\" ], \"labels\": [], \"masters\": [], \"uuids\": [] }, \"model\": \"Virtual HDD\", \"partitions\": { \"sda1\": { \"holders\": [], \"links\": { \"ids\": [ \"ata-Virtual_HDD_V65X1HR75TK4SFVV1XTD-part1\" ], \"labels\": [], \"masters\": [], \"uuids\": [ \"544d152e-4e60-41f1-b1f9-d976a1983ac3\" ] }, \"sectors\": \"2097152\", \"sectorsize\": 512, \"size\": \"1.00 GB\", \"start\": \"2048\", \"uuid\": \"544d152e-4e60-41f1-b1f9-d976a1983ac3\" }, \"sda2\": { \"holders\": [ \"centos-root\", \"centos-swap\" ], \"links\": { \"ids\": [ \"ata-Virtual_HDD_V65X1HR75TK4SFVV1XTD-part2\", \"lvm-pv-uuid-Z75Sgb-wZGs-IthV-5Bhm-hn5m-9VeB-24PIOp\" ], \"labels\": [], \"masters\": [ \"dm-0\", \"dm-1\" ], \"uuids\": [] }, \"sectors\": \"102758400\", \"sectorsize\": 512, \"size\": \"49.00 GB\", \"start\": \"2099200\", \"uuid\": null } }, \"removable\": \"0\", \"rotational\": \"0\", \"sas_address\": null, \"sas_device_handle\": null, \"scheduler_mode\": \"deadline\", \"sectors\": \"104857600\", \"sectorsize\": \"512\", \"serial\": \"V65X1HR75TK4SFVV1XTD\", \"size\": \"50.00 GB\", \"support_discard\": \"4096\", \"vendor\": \"ATA\", \"virtual\": 1 }, \"sr0\": { \"holders\": [], \"host\": \"SATA controller: Intel Corporation 82801HR/HO/HH (ICH8R/DO/DH) 6 port SATA Controller [AHCI mode] (rev 02)\", \"links\": { \"ids\": [ \"ata-Virtual_DVD-ROM__1__-_31415B265\" ], \"labels\": [], \"masters\": [], \"uuids\": [] }, \"model\": \"Virtual DVD-ROM\", \"partitions\": {}, \"removable\": \"1\", \"rotational\": \"1\", \"sas_address\": null, \"sas_device_handle\": null, \"scheduler_mode\": \"deadline\", \"sectors\": \"2097151\", \"sectorsize\": \"512\", \"size\": \"1024.00 MB\", \"support_discard\": \"0\", \"vendor\": null, \"virtual\": 1 } }, \"ansible_distribution\": \"CentOS\", \"ansible_distribution_file_parsed\": true, \"ansible_distribution_file_path\": \"/etc/redhat-release\", \"ansible_distribution_file_variety\": \"RedHat\", \"ansible_distribution_major_version\": \"7\", \"ansible_distribution_release\": \"Core\", \"ansible_distribution_version\": \"7.9\", \"ansible_dns\": { \"nameservers\": [ \"223.5.5.5\", \"114.114.114.114\" ], \"search\": [ \"localdomain\" ] }, \"ansible_domain\": \"\", \"ansible_effective_group_id\": 0, \"ansible_effective_user_id\": 0, \"ansible_env\": { \"HISTTIMEFORMAT\": \"%Y-%m-%d %H:%M:%S \", \"HOME\": \"/root\", \"LANG\": \"C\", \"LC_ALL\": \"C\", \"LC_NUMERIC\": \"C\", \"LESSOPEN\": \"||/usr/bin/lesspipe.sh %s\", \"LOGNAME\": \"root\", \"LS_COLORS\": \"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:\", \"MAIL\": \"/var/mail/root\", \"PATH\": \"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin\", \"PWD\": \"/root\", \"SHELL\": \"/bin/bash\", \"SHLVL\": \"2\", \"SSH_CLIENT\": \"10.0.0.60 60520 22\", \"SSH_CONNECTION\": \"10.0.0.60 60520 10.0.0.61 22\", \"SSH_TTY\": \"/dev/pts/2\", \"TERM\": \"xterm\", \"USER\": \"root\", \"XDG_RUNTIME_DIR\": \"/run/user/0\", \"XDG_SESSION_ID\": \"7504\", \"_\": \"/usr/bin/python\" }, \"ansible_eth0\": { \"active\": true, \"device\": \"eth0\", \"features\": { \"busy_poll\": \"off [fixed]\", \"fcoe_mtu\": \"off [fixed]\", \"generic_receive_offload\": \"on\", \"generic_segmentation_offload\": \"on\", \"highdma\": \"on [fixed]\", \"hw_tc_offload\": \"off [fixed]\", \"l2_fwd_offload\": \"off [fixed]\", \"large_receive_offload\": \"off [fixed]\", \"loopback\": \"off [fixed]\", \"netns_local\": \"off [fixed]\", \"ntuple_filters\": \"off [fixed]\", \"receive_hashing\": \"off [fixed]\", \"rx_all\": \"off [fixed]\", \"rx_checksumming\": \"off [fixed]\", \"rx_fcs\": \"off [fixed]\", \"rx_gro_hw\": \"off [fixed]\", \"rx_udp_tunnel_port_offload\": \"off [fixed]\", \"rx_vlan_filter\": \"off [fixed]\", \"rx_vlan_offload\": \"off [fixed]\", \"rx_vlan_stag_filter\": \"off [fixed]\", \"rx_vlan_stag_hw_parse\": \"off [fixed]\", \"scatter_gather\": \"on\", \"tcp_segmentation_offload\": \"on\", \"tx_checksum_fcoe_crc\": \"off [fixed]\", \"tx_checksum_ip_generic\": \"on\", \"tx_checksum_ipv4\": \"off [fixed]\", \"tx_checksum_ipv6\": \"off [fixed]\", \"tx_checksum_sctp\": \"off [fixed]\", \"tx_checksumming\": \"on\", \"tx_fcoe_segmentation\": \"off [fixed]\", \"tx_gre_csum_segmentation\": \"off [fixed]\", \"tx_gre_segmentation\": \"off [fixed]\", \"tx_gso_partial\": \"off [fixed]\", \"tx_gso_robust\": \"off [fixed]\", \"tx_ipip_segmentation\": \"off [fixed]\", \"tx_lockless\": \"off [fixed]\", \"tx_nocache_copy\": \"off\", \"tx_scatter_gather\": \"on\", \"tx_scatter_gather_fraglist\": \"off [fixed]\", \"tx_sctp_segmentation\": \"off [fixed]\", \"tx_sit_segmentation\": \"off [fixed]\", \"tx_tcp6_segmentation\": \"on\", \"tx_tcp_ecn_segmentation\": \"on\", \"tx_tcp_mangleid_segmentation\": \"off\", \"tx_tcp_segmentation\": \"on\", \"tx_udp_tnl_csum_segmentation\": \"off [fixed]\", \"tx_udp_tnl_segmentation\": \"off [fixed]\", \"tx_vlan_offload\": \"off [fixed]\", \"tx_vlan_stag_hw_insert\": \"off [fixed]\", \"udp_fragmentation_offload\": \"off [fixed]\", \"vlan_challenged\": \"off [fixed]\" }, \"hw_timestamp_filters\": [], \"ipv4\": { \"address\": \"10.0.0.61\", \"broadcast\": \"10.0.0.255\", \"netmask\": \"255.255.255.0\", \"network\": \"10.0.0.0\" }, \"ipv6\": [ { \"address\": \"fe80::21c:42ff:fed4:711d\", \"prefix\": \"64\", \"scope\": \"link\" } ], \"macaddress\": \"00:1c:42:d4:71:1d\", \"module\": \"virtio_net\", \"mtu\": 1500, \"pciid\": \"virtio0\", \"promisc\": false, \"timestamping\": [ \"rx_software\", \"software\" ], \"type\": \"ether\" }, \"ansible_fibre_channel_wwn\": [], \"ansible_fips\": false, \"ansible_form_factor\": \"Laptop\", \"ansible_fqdn\": \"devops02\", \"ansible_hostname\": \"devops02\", \"ansible_hostnqn\": \"\", \"ansible_interfaces\": [ \"lo\", \"eth0\" ], \"ansible_is_chroot\": false, \"ansible_iscsi_iqn\": \"\", \"ansible_kernel\": \"3.10.0-1160.el7.x86_64\", \"ansible_kernel_version\": \"#1 SMP Mon Oct 19 16:18:59 UTC 2020\", \"ansible_lo\": { \"active\": true, \"device\": \"lo\", \"features\": { \"busy_poll\": \"off [fixed]\", \"fcoe_mtu\": \"off [fixed]\", \"generic_receive_offload\": \"on\", \"generic_segmentation_offload\": \"on\", \"highdma\": \"on [fixed]\", \"hw_tc_offload\": \"off [fixed]\", \"l2_fwd_offload\": \"off [fixed]\", \"large_receive_offload\": \"off [fixed]\", \"loopback\": \"on [fixed]\", \"netns_local\": \"on [fixed]\", \"ntuple_filters\": \"off [fixed]\", \"receive_hashing\": \"off [fixed]\", \"rx_all\": \"off [fixed]\", \"rx_checksumming\": \"on [fixed]\", \"rx_fcs\": \"off [fixed]\", \"rx_gro_hw\": \"off [fixed]\", \"rx_udp_tunnel_port_offload\": \"off [fixed]\", \"rx_vlan_filter\": \"off [fixed]\", \"rx_vlan_offload\": \"off [fixed]\", \"rx_vlan_stag_filter\": \"off [fixed]\", \"rx_vlan_stag_hw_parse\": \"off [fixed]\", \"scatter_gather\": \"on\", \"tcp_segmentation_offload\": \"on\", \"tx_checksum_fcoe_crc\": \"off [fixed]\", \"tx_checksum_ip_generic\": \"on [fixed]\", \"tx_checksum_ipv4\": \"off [fixed]\", \"tx_checksum_ipv6\": \"off [fixed]\", \"tx_checksum_sctp\": \"on [fixed]\", \"tx_checksumming\": \"on\", \"tx_fcoe_segmentation\": \"off [fixed]\", \"tx_gre_csum_segmentation\": \"off [fixed]\", \"tx_gre_segmentation\": \"off [fixed]\", \"tx_gso_partial\": \"off [fixed]\", \"tx_gso_robust\": \"off [fixed]\", \"tx_ipip_segmentation\": \"off [fixed]\", \"tx_lockless\": \"on [fixed]\", \"tx_nocache_copy\": \"off [fixed]\", \"tx_scatter_gather\": \"on [fixed]\", \"tx_scatter_gather_fraglist\": \"on [fixed]\", \"tx_sctp_segmentation\": \"on\", \"tx_sit_segmentation\": \"off [fixed]\", \"tx_tcp6_segmentation\": \"on\", \"tx_tcp_ecn_segmentation\": \"on\", \"tx_tcp_mangleid_segmentation\": \"on\", \"tx_tcp_segmentation\": \"on\", \"tx_udp_tnl_csum_segmentation\": \"off [fixed]\", \"tx_udp_tnl_segmentation\": \"off [fixed]\", \"tx_vlan_offload\": \"off [fixed]\", \"tx_vlan_stag_hw_insert\": \"off [fixed]\", \"udp_fragmentation_offload\": \"on\", \"vlan_challenged\": \"on [fixed]\" }, \"hw_timestamp_filters\": [], \"ipv4\": { \"address\": \"127.0.0.1\", \"broadcast\": \"\", \"netmask\": \"255.0.0.0\", \"network\": \"127.0.0.0\" }, \"ipv6\": [ { \"address\": \"::1\", \"prefix\": \"128\", \"scope\": \"host\" } ], \"mtu\": 65536, \"promisc\": false, \"timestamping\": [ \"rx_software\", \"software\" ], \"type\": \"loopback\" }, \"ansible_local\": {}, \"ansible_lsb\": {}, \"ansible_lvm\": { \"lvs\": { \"root\": { \"size_g\": \"46.99\", \"vg\": \"centos\" }, \"swap\": { \"size_g\": \"2.00\", \"vg\": \"centos\" } }, \"pvs\": { \"/dev/sda2\": { \"free_g\": \"0.00\", \"size_g\": \"49.00\", \"vg\": \"centos\" } }, \"vgs\": { \"centos\": { \"free_g\": \"0.00\", \"num_lvs\": \"2\", \"num_pvs\": \"1\", \"size_g\": \"49.00\" } } }, \"ansible_machine\": \"x86_64\", \"ansible_machine_id\": \"0af8c41dd588fb4f8f96a48e78beb674\", \"ansible_memfree_mb\": 142, \"ansible_memory_mb\": { \"nocache\": { \"free\": 734, \"used\": 252 }, \"real\": { \"free\": 142, \"total\": 986, \"used\": 844 }, \"swap\": { \"cached\": 0, \"free\": 0, \"total\": 0, \"used\": 0 } }, \"ansible_memtotal_mb\": 986, \"ansible_mounts\": [ { \"block_available\": 223727, \"block_size\": 4096, \"block_total\": 259584, \"block_used\": 35857, \"device\": \"/dev/sda1\", \"fstype\": \"xfs\", \"inode_available\": 523962, \"inode_total\": 524288, \"inode_used\": 326, \"mount\": \"/boot\", \"options\": \"rw,relatime,attr2,inode64,noquota\", \"size_available\": 916385792, \"size_total\": 1063256064, \"uuid\": \"544d152e-4e60-41f1-b1f9-d976a1983ac3\" }, { \"block_available\": 11527392, \"block_size\": 4096, \"block_total\": 12312705, \"block_used\": 785313, \"device\": \"/dev/mapper/centos-root\", \"fstype\": \"xfs\", \"inode_available\": 24533001, \"inode_total\": 24637440, \"inode_used\": 104439, \"mount\": \"/\", \"options\": \"rw,relatime,attr2,inode64,noquota\", \"size_available\": 47216197632, \"size_total\": 50432839680, \"uuid\": \"0fe73be4-eeb3-4995-9d5b-bfc0cc576a83\" } ], \"ansible_nodename\": \"devops02\", \"ansible_os_family\": \"RedHat\", \"ansible_pkg_mgr\": \"yum\", \"ansible_proc_cmdline\": { \"BOOT_IMAGE\": \"/vmlinuz-3.10.0-1160.el7.x86_64\", \"LANG\": \"en_US.UTF-8\", \"net.ifnames\": \"0\", \"quiet\": true, \"rd.lvm.lv\": [ \"centos/root\", \"centos/swap\" ], \"rhgb\": true, \"ro\": true, \"root\": \"/dev/mapper/centos-root\", \"spectre_v2\": \"retpoline\" }, \"ansible_processor\": [ \"0\", \"GenuineIntel\", \"Intel(R) Core(TM) i7-8750H CPU @ 2.20GHz\", \"1\", \"GenuineIntel\", \"Intel(R) Core(TM) i7-8750H CPU @ 2.20GHz\" ], \"ansible_processor_cores\": 2, \"ansible_processor_count\": 1, \"ansible_processor_threads_per_core\": 1, \"ansible_processor_vcpus\": 2, \"ansible_product_name\": \"Parallels Virtual Platform\", \"ansible_product_serial\": \"Parallels-1D C4 F8 0A 88 D5 4F FB 8F 96 A4 8E 78 BE B6 74\", \"ansible_product_uuid\": \"0AF8C41D-D588-FB4F-8F96-A48E78BEB674\", \"ansible_product_version\": \"None\", \"ansible_python\": { \"executable\": \"/usr/bin/python\", \"has_sslcontext\": true, \"type\": \"CPython\", \"version\": { \"major\": 2, \"micro\": 5, \"minor\": 7, \"releaselevel\": \"final\", \"serial\": 0 }, \"version_info\": [ 2, 7, 5, \"final\", 0 ] }, \"ansible_python_version\": \"2.7.5\", \"ansible_real_group_id\": 0, \"ansible_real_user_id\": 0, \"ansible_selinux\": { \"status\": \"disabled\" }, \"ansible_selinux_python_present\": true, \"ansible_service_mgr\": \"systemd\", \"ansible_ssh_host_key_ecdsa_public\": \"AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBCSlkb1znSaz1isLTp/A+1gAUSHuH72VkAUu40IovJZSZ4IbCkSWPunoghZrF9SX29XOfauFUK3hZvdiLXMQwD8=\", \"ansible_ssh_host_key_ed25519_public\": \"AAAAC3NzaC1lZDI1NTE5AAAAIFqE/3imUpdPXOqCkpjQkEAp8vMQOX/Tmoe9HYcVFTYK\", \"ansible_ssh_host_key_rsa_public\": \"AAAAB3NzaC1yc2EAAAADAQABAAABAQDmJK1wwE9wLsL/5uqqtB2RfkSwCQJ4X8uc4+F6fqNMkrvnfsicT2MiIoC4LIRc3MCaVCSYjtJdFb7ExDICIMHuno6/DhnfUCHwQxAJoupE59YgeCZyh1I+pbDy86RKIMybpJpDCMvFp/fPWubrlLX29dXa64Ytmxgu9/gqCKhXIJw9+ZetRJrvDyiv0Fhot8YzaxBwWBSTI+pgAo6o81QO1HkbkENluK9WKrJkWZYnD6FC2VFzjMLHFw+EIf5HJ0Ll+X1YfE9z7Pch2jOoFLKCXa3vz2Bhxt+v71MfXv+j+u8qs7twh5WYaT5ZQoqh39WB1B9FWLg3Fs3xGT12sy/J\", \"ansible_swapfree_mb\": 0, \"ansible_swaptotal_mb\": 0, \"ansible_system\": \"Linux\", \"ansible_system_capabilities\": [ \"cap_chown\", \"cap_dac_override\", \"cap_dac_read_search\", \"cap_fowner\", \"cap_fsetid\", \"cap_kill\", \"cap_setgid\", \"cap_setuid\", \"cap_setpcap\", \"cap_linux_immutable\", \"cap_net_bind_service\", \"cap_net_broadcast\", \"cap_net_admin\", \"cap_net_raw\", \"cap_ipc_lock\", \"cap_ipc_owner\", \"cap_sys_module\", \"cap_sys_rawio\", \"cap_sys_chroot\", \"cap_sys_ptrace\", \"cap_sys_pacct\", \"cap_sys_admin\", \"cap_sys_boot\", \"cap_sys_nice\", \"cap_sys_resource\", \"cap_sys_time\", \"cap_sys_tty_config\", \"cap_mknod\", \"cap_lease\", \"cap_audit_write\", \"cap_audit_control\", \"cap_setfcap\", \"cap_mac_override\", \"cap_mac_admin\", \"cap_syslog\", \"35\", \"36+ep\" ], \"ansible_system_capabilities_enforced\": \"True\", \"ansible_system_vendor\": \"Parallels Software International Inc.\", \"ansible_uptime_seconds\": 359912, \"ansible_user_dir\": \"/root\", \"ansible_user_gecos\": \"root\", \"ansible_user_gid\": 0, \"ansible_user_id\": \"root\", \"ansible_user_shell\": \"/bin/bash\", \"ansible_user_uid\": 0, \"ansible_userspace_architecture\": \"x86_64\", \"ansible_userspace_bits\": \"64\", \"ansible_virtualization_role\": \"guest\", \"ansible_virtualization_type\": \"parallels\", \"discovered_interpreter_python\": \"/usr/bin/python\", \"gather_subset\": [ \"all\" ], \"module_setup\": true }, \"changed\": false } 3.使用debug模块来获取被控机相关变量信息 示例1：获取被控机ip地址 方法1：使用命令 $ ansible devops02 -m setup -a \"filter=ansible_default_ipv4\" devops02 | SUCCESS => { \"ansible_facts\": { \"ansible_default_ipv4\": { \"address\": \"10.0.0.61\", \"alias\": \"eth0\", \"broadcast\": \"10.0.0.255\", \"gateway\": \"10.0.0.1\", \"interface\": \"eth0\", \"macaddress\": \"00:1c:42:d4:71:1d\", \"mtu\": 1500, \"netmask\": \"255.255.255.0\", \"network\": \"10.0.0.0\", \"type\": \"ether\" }, \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false } 方法2: 使用playbook 编辑yml文件 $ cat > facts.yml 执行yml文件 $ ansible-playbook facts.yml PLAY [all] ********************************************************************************************************************************************* TASK [Gathering Facts] ********************************************************************************************************************************* ok: [devops02] ok: [devops03] ok: [devops01] TASK [output hosts info] ******************************************************************************************************************************* ok: [devops01] => { \"msg\": \"10.0.0.60\" } ok: [devops02] => { \"msg\": \"10.0.0.61\" } ok: [devops03] => { \"msg\": \"10.0.0.62\" } PLAY RECAP ********************************************************************************************************************************************* devops01 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 devops02 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 devops03 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 4.Facts变量使用示例 4.1 示例1 拷贝文件，不同主机信息不同 典型的例子就是zabbix agent文件 zabbix_agentd.conf 中有 Hostname 参数，默认为 Zabbix server 现在我们批量在被控机上安装zabbix agent，想实现的效果是不同主机，zabbix_agentd.conf 文件中 Hostname 参数为当前主机的主机名 这里我们用一个文件测试即可，编辑一个测试文件 ansible Facts变量中有一个固定变量 ansible_fqdn ，这个变量就是获取当前主机的主机名 # 编辑一个文件，模拟zabbix agent文件，这里主要是验证Facts变量 cat > zabbix_agentd.conf 编辑yml文件 这里使用的是template模块而不是copy模块，template模块会解析facts变量 cat > facts_zabbix.yml 执行yml文件 $ ansible-playbook facts_zabbix.yml PLAY [all] ********************************************************************************************************************************************* TASK [Gathering Facts] ********************************************************************************************************************************* ok: [devops03] ok: [devops02] ok: [devops01] TASK [copy zabbix agent conf] ************************************************************************************************************************** changed: [devops02] changed: [devops03] changed: [devops01] PLAY RECAP ********************************************************************************************************************************************* devops01 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 devops02 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 devops03 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 登陆其中一台机器验证 $ cat /etc/zabbix_agentd.conf Hostname=devops02 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible任务控制/ansible任务控制-判断语句when.html":{"url":"linux/自动化运维平台/ansible/ansible任务控制/ansible任务控制-判断语句when.html","title":"ansible任务控制-判断语句when","keywords":"","body":"ansible任务控制-判断语句when ansible任务控制中的判断语句when使用场景为依据不同的环境执行不同的动作，例如操作系统是centos与ubuntu，我们可能需要在不同的操作系统上执行不同的操作，这个时候就会用到when 使用示例，在2个主机上执行不同的命令 编辑yml文件，判断当主机明是devops01时才操作 cat > task_when.yml 执行yml文件，可以看到只有主机名为devops01的机器执行了，其余主机均为skip状态 $ ansible-playbook task_when.yml PLAY [all] ********************************************************************************************************************************************* TASK [Gathering Facts] ********************************************************************************************************************************* ok: [devops02] ok: [devops03] ok: [devops01] TASK [devops01 test] *********************************************************************************************************************************** skipping: [devops02] skipping: [devops03] changed: [devops01] PLAY RECAP ********************************************************************************************************************************************* devops01 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 devops02 : ok=1 changed=0 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 devops03 : ok=1 changed=0 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible任务控制/ansible任务控制-循环语句with_items.html":{"url":"linux/自动化运维平台/ansible/ansible任务控制/ansible任务控制-循环语句with_items.html","title":"ansible任务控制-循环语句with_items","keywords":"","body":"ansible任务控制-循环语句with_items ansible任务控制-循环语句with_items使用场景为当我们要对某一些任务需要批量处理，例如重启服务，如果使用原先的service模块是只能写一个服务的，再比如使用copy或者template模块拷贝文件或目录，单个模块只能拷贝一个文件，这个时候就需要用到循环了 使用示例 使用示例1，使用service模块同时重启2个服务 编辑yml文件 cat > task_with_items.yml EOF - hosts: devops01 tasks: - name: restart nginx mysql service: name: \"{{ item }}\" state: restarted with_items: - nginx - mysqld EOF 执行yml文件 $ ansible-playbook task_with_items.yml PLAY [devops01] **************************************************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************************************* ok: [devops01] TASK [restart nginx mysql] ***************************************************************************************************************************** changed: [devops01] => (item=nginx) changed: [devops01] => (item=mysqld) PLAY RECAP ********************************************************************************************************************************************* devops01 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 使用示例2，批量创建多个用户和组 编辑yml文件 cat > task_with_items_adduser.yml EOF - hosts: devops01 tasks: - name: add user1-5 group1-5 user: name: \"{{ item.username }}\" groups: \"{{ item.groupname }}\" state: present with_items: - { username: 'user1', groupname: 'root' } - { username: 'user2', groupname: 'root' } - { username: 'user3', groupname: 'root' } - { username: 'user4', groupname: 'root' } - { username: 'user5', groupname: 'root' } EOF 执行yml文件 $ ansible-playbook task_with_items_adduser.yml PLAY [devops01] **************************************************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************************************* ok: [devops01] TASK [add user1-5 group1-5] **************************************************************************************************************************** changed: [devops01] => (item={u'username': u'user1', u'groupname': u'root'}) changed: [devops01] => (item={u'username': u'user2', u'groupname': u'root'}) changed: [devops01] => (item={u'username': u'user3', u'groupname': u'root'}) changed: [devops01] => (item={u'username': u'user4', u'groupname': u'root'}) changed: [devops01] => (item={u'username': u'user5', u'groupname': u'root'}) PLAY RECAP ********************************************************************************************************************************************* devops01 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible任务控制/ansible任务控制-handlers.html":{"url":"linux/自动化运维平台/ansible/ansible任务控制/ansible任务控制-handlers.html","title":"ansible任务控制-handlers","keywords":"","body":"ansible任务控制-handlers handlers作用 handlers用于notify监控发生变更后的动作触发，例如当某一个服务配置文件发生变更时重启服务 handlers流程 1.配置notify监控，例如监控nginx的配置文件 2.发送通知到handlers 3.handlers触发动作 示例：当nginx配置文件发生变更时重启服务 编辑yml文件 ⚠️notify后的内容可以任意，但是handlers后的name下的内容必须与notify后定义的名称一致 handlers会在所有任务正确执行完成后执行，只会执行一次，并且只有当tasks改变后才会触发handlers cat > task_handlers.yml 执行yml文件 $ ansible-playbook task_handlers.yml PLAY [devops02] **************************************************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************************************* ok: [devops02] TASK [copy nginx conf] ********************************************************************************************************************************* changed: [devops02] RUNNING HANDLER [restart nginx] ************************************************************************************************************************ changed: [devops02] PLAY RECAP ********************************************************************************************************************************************* devops02 : ok=3 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible任务控制/ansible任务控制-标签tags.html":{"url":"linux/自动化运维平台/ansible/ansible任务控制/ansible任务控制-标签tags.html","title":"ansible任务控制-标签tags","keywords":"","body":"ansible任务控制-标签tags tags使用场景为当有多个tasks的时候，想要指定某一个tasks执行，这个时候就可以利用为tasks打标签然后指定标签执行 tasks打标签有以下3种方式 一个tasks指定一个tags 一个tasks指定多个tags 多个tasks指定一个tags 编辑yml文件 cat > task_tags.yml 执行yml文件，使用 -t 选项指定tags，这里只执行标签为 install nginx 的tasks 使用 --skip-tags 选项排除要执行的tags $ ansible-playbook task_tags.yml -t \"install nginx\" PLAY [devops03] **************************************************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************************************* ok: [devops03] TASK [install nginx] *********************************************************************************************************************************** changed: [devops03] PLAY RECAP ********************************************************************************************************************************************* devops03 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible任务控制/ansible任务控制-包含include.html":{"url":"linux/自动化运维平台/ansible/ansible任务控制/ansible任务控制-包含include.html","title":"ansible任务控制-包含include","keywords":"","body":"ansible任务控制-包含include include的使用场景比较简单，就是多个playbook需要执行相同的操作，例如有5个playbook，这5个playbook都是安装不同的服务，但是都会安装nginx，同时也都会重启nginx，这个时候就可以使用include 首先编辑一个没有play的yml，其他的yml中使用include来包含这个yml cat > task_include_tem.yml 编辑a.yml 这里我们仅仅使用shell模块echo一句话，然后重启nginx cat > a.yml 执行a.yml $ ansible-playbook a.yml PLAY [all] ********************************************************************************************************************************************* TASK [Gathering Facts] ********************************************************************************************************************************* ok: [devops03] ok: [devops02] ok: [devops01] TASK [print test a] ************************************************************************************************************************************ changed: [devops03] changed: [devops02] changed: [devops01] TASK [restart nginx] *********************************************************************************************************************************** changed: [devops02] changed: [devops03] changed: [devops01] PLAY RECAP ********************************************************************************************************************************************* devops01 : ok=3 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 devops02 : ok=3 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 devops03 : ok=3 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 编辑b.yml cat > b.yml 执行b.yml $ ansible-playbook b.yml PLAY [all] ********************************************************************************************************************************************* TASK [Gathering Facts] ********************************************************************************************************************************* ok: [devops02] ok: [devops03] ok: [devops01] TASK [print test b] ************************************************************************************************************************************ changed: [devops03] changed: [devops02] changed: [devops01] TASK [restart nginx] *********************************************************************************************************************************** changed: [devops03] changed: [devops02] changed: [devops01] PLAY RECAP ********************************************************************************************************************************************* devops01 : ok=3 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 devops02 : ok=3 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 devops03 : ok=3 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 yml文件中的 include 还可以写成 include_tasks cat > b.yml 执行的时候会多了一个 included: /root/yml/task_include_tem.yml for devops01, devops02, devops03 提示 $ ansible-playbook b.yml PLAY [all] ********************************************************************************************************************************************* TASK [Gathering Facts] ********************************************************************************************************************************* ok: [devops03] ok: [devops02] ok: [devops01] TASK [print test b] ************************************************************************************************************************************ changed: [devops02] changed: [devops03] changed: [devops01] TASK [restat nginx] ************************************************************************************************************************************ included: /root/yml/task_include_tem.yml for devops01, devops02, devops03 TASK [restart nginx] *********************************************************************************************************************************** changed: [devops03] changed: [devops02] changed: [devops01] PLAY RECAP ********************************************************************************************************************************************* devops01 : ok=4 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 devops02 : ok=4 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 devops03 : ok=4 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible任务控制/ansible任务控制-忽略错误ignore_errors.html":{"url":"linux/自动化运维平台/ansible/ansible任务控制/ansible任务控制-忽略错误ignore_errors.html","title":"ansible任务控制-忽略错误ignore_errors","keywords":"","body":"ansible任务控制-忽略错误ignore_errors 编辑yml文件 这里故意执行一个返回值永远是错误的命令false cat > task_ingore_errors.yml 执行yml文件，因为 /bin/false 永远是错误的返回，因此不能继续往下执行 $ ansible-playbook task_ingore_errors.yml PLAY [devops03] **************************************************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************************************* ok: [devops03] TASK [exec error cmd] ********************************************************************************************************************************** fatal: [devops03]: FAILED! => {\"changed\": true, \"cmd\": \"/bin/false\", \"delta\": \"0:00:00.031517\", \"end\": \"2021-09-21 23:51:30.811885\", \"msg\": \"non-zero return code\", \"rc\": 1, \"start\": \"2021-09-21 23:51:30.780368\", \"stderr\": \"\", \"stderr_lines\": [], \"stdout\": \"\", \"stdout_lines\": []} PLAY RECAP ********************************************************************************************************************************************* devops03 : ok=1 changed=0 unreachable=0 failed=1 skipped=0 rescued=0 ignored=0 如果想要忽略错误不影响后边的task，就需要用到 ignore_errors cat > task_ingore_errors.yml 再次执行yml就会忽略错误继续执行了 $ ansible-playbook task_ingore_errors.yml PLAY [devops03] **************************************************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************************************* ok: [devops03] TASK [exec error cmd] ********************************************************************************************************************************** fatal: [devops03]: FAILED! => {\"changed\": true, \"cmd\": \"/bin/false\", \"delta\": \"0:00:00.029109\", \"end\": \"2021-09-21 23:54:38.138763\", \"msg\": \"non-zero return code\", \"rc\": 1, \"start\": \"2021-09-21 23:54:38.109654\", \"stderr\": \"\", \"stderr_lines\": [], \"stdout\": \"\", \"stdout_lines\": []} ...ignoring TASK [touch file] ************************************************************************************************************************************** changed: [devops03] PLAY RECAP ********************************************************************************************************************************************* devops03 : ok=3 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=1 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible任务控制/ansible任务控制-错误处理changed_when.html":{"url":"linux/自动化运维平台/ansible/ansible任务控制/ansible任务控制-错误处理changed_when.html","title":"ansible任务控制-错误处理changed_when","keywords":"","body":"ansible任务控制-错误处理changed_when force_handlers:yes 强制执行handlers 默认情况下，当task失败后，play会终止，后续的handlers就不会被执行，可是使用参数 force_handlers:yes 强制执行handlers 编辑yml文件 cat >tasks_changed_when.yml 执行yml文件 可以看到，当我们执行一个错误的命令abc后，后边重启httpd的handlers没有被执行 $ ansible-playbook tasks_changed_when.yml PLAY [all] ************************************************************************************************************************** TASK [Gathering Facts] ************************************************************************************************************** ok: [devops02] ok: [devops01] TASK [touch test file] ************************************************************************************************************** changed: [devops02] changed: [devops01] TASK [exec error command] *********************************************************************************************************** fatal: [devops02]: FAILED! => {\"changed\": true, \"cmd\": \"abc\", \"delta\": \"0:00:00.087827\", \"end\": \"2021-09-25 15:28:19.873285\", \"msg\": \"non-zero return code\", \"rc\": 127, \"start\": \"2021-09-25 15:28:19.785458\", \"stderr\": \"/bin/sh: abc: command not found\", \"stderr_lines\": [\"/bin/sh: abc: command not found\"], \"stdout\": \"\", \"stdout_lines\": []} fatal: [devops01]: FAILED! => {\"changed\": true, \"cmd\": \"abc\", \"delta\": \"0:00:00.087584\", \"end\": \"2021-09-25 15:28:19.941478\", \"msg\": \"non-zero return code\", \"rc\": 127, \"start\": \"2021-09-25 15:28:19.853894\", \"stderr\": \"/bin/sh: abc: command not found\", \"stderr_lines\": [\"/bin/sh: abc: command not found\"], \"stdout\": \"\", \"stdout_lines\": []} RUNNING HANDLER [restart httpd] ***************************************************************************************************** PLAY RECAP ************************************************************************************************************************** devops01 : ok=2 changed=1 unreachable=0 failed=1 skipped=0 rescued=0 ignored=0 devops02 : ok=2 changed=1 unreachable=0 failed=1 skipped=0 rescued=0 ignored=0 在yml文件中添加参数 force_handlers: yes 强制执行handlers cat >tasks_changed_when.yml 再次执行yml文件，可以看到handlers被强制执行了 $ ansible-playbook tasks_changed_when.yml PLAY [all] ************************************************************************************************************************** TASK [Gathering Facts] ************************************************************************************************************** ok: [devops01] ok: [devops02] TASK [touch test file] ************************************************************************************************************** changed: [devops02] changed: [devops01] TASK [exec error command] *********************************************************************************************************** fatal: [devops01]: FAILED! => {\"changed\": true, \"cmd\": \"abc\", \"delta\": \"0:00:00.080787\", \"end\": \"2021-09-25 15:34:15.667841\", \"msg\": \"non-zero return code\", \"rc\": 127, \"start\": \"2021-09-25 15:34:15.587054\", \"stderr\": \"/bin/sh: abc: command not found\", \"stderr_lines\": [\"/bin/sh: abc: command not found\"], \"stdout\": \"\", \"stdout_lines\": []} fatal: [devops02]: FAILED! => {\"changed\": true, \"cmd\": \"abc\", \"delta\": \"0:00:00.078859\", \"end\": \"2021-09-25 15:34:15.747691\", \"msg\": \"non-zero return code\", \"rc\": 127, \"start\": \"2021-09-25 15:34:15.668832\", \"stderr\": \"/bin/sh: abc: command not found\", \"stderr_lines\": [\"/bin/sh: abc: command not found\"], \"stdout\": \"\", \"stdout_lines\": []} RUNNING HANDLER [restart httpd] ***************************************************************************************************** changed: [devops01] changed: [devops02] PLAY RECAP ************************************************************************************************************************** devops01 : ok=3 changed=2 unreachable=0 failed=1 skipped=0 rescued=0 ignored=0 devops02 : ok=3 changed=2 unreachable=0 failed=1 skipped=0 rescued=0 ignored=0 使用 changed_when 检查任务返回的结果 例如，我们修改了某一个服务的配置文件，在重启服务之前做一下相应的检测(例如nginx服务的 nginx -t 命令)，如果检测成功我们就重启服务，否则就不重启服务，这个时候就需要用到changed_when 检查任务返回结果 编辑yml文件 cat > tasks_changed_when.yml 执行yml文件 $ ansible-playbook tasks_changed_when.yml PLAY [devops] *********************************************************************************************************************** TASK [Gathering Facts] ************************************************************************************************************** ok: [devops01] ok: [devops02] TASK [install nginx] **************************************************************************************************************** ok: [devops02] ok: [devops01] TASK [copy nginx conf] ************************************************************************************************************** changed: [devops01] changed: [devops02] RUNNING HANDLER [restart nginx] ***************************************************************************************************** changed: [devops01] changed: [devops02] PLAY RECAP ************************************************************************************************************************** devops01 : ok=4 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 devops02 : ok=4 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 上边的yml文件是没有做配置文件是否正确校验的，如果我们不做配置文件语法校验，则推送错误文件后重启服务会导致服务重启失败，此时我们就需要对配置文件做一下校验，如果正确就重启服务 故意把nginx配置文件改错，然后再执行yml文件，可以看到重启nginx服务会失败，此时nginx服务是挂掉的 $ ansible-playbook tasks_changed_when.yml PLAY [devops] *********************************************************************************************************************** TASK [Gathering Facts] ************************************************************************************************************** ok: [devops01] ok: [devops02] TASK [install nginx] **************************************************************************************************************** ok: [devops01] ok: [devops02] TASK [copy nginx conf] ************************************************************************************************************** changed: [devops01] changed: [devops02] RUNNING HANDLER [restart nginx] ***************************************************************************************************** fatal: [devops01]: FAILED! => {\"changed\": false, \"msg\": \"Unable to restart service nginx: Job for nginx.service failed because the control process exited with error code. See \\\"systemctl status nginx.service\\\" and \\\"journalctl -xe\\\" for details.\\n\"} fatal: [devops02]: FAILED! => {\"changed\": false, \"msg\": \"Unable to restart service nginx: Job for nginx.service failed because the control process exited with error code. See \\\"systemctl status nginx.service\\\" and \\\"journalctl -xe\\\" for details.\\n\"} NO MORE HOSTS LEFT ****************************************************************************************************************** PLAY RECAP ************************************************************************************************************************** devops01 : ok=3 changed=1 unreachable=0 failed=1 skipped=0 rescued=0 ignored=0 devops02 : ok=3 changed=1 unreachable=0 failed=1 skipped=0 rescued=0 ignored=0 如果想避免因为配置文件拷贝错误导致服务重启失败的问题，则需要对配置文件做语法校验 再次编辑yml文件 cat > tasks_changed_when.yml 还是拷贝错误的配置文件，可以看到由于检测失败并没有执行重启服务的操作 $ ansible-playbook tasks_changed_when.yml PLAY [devops] *********************************************************************************************************************** TASK [Gathering Facts] ************************************************************************************************************** ok: [devops01] ok: [devops02] TASK [install nginx] **************************************************************************************************************** ok: [devops01] ok: [devops02] TASK [copy nginx conf] ************************************************************************************************************** ok: [devops02] ok: [devops01] TASK [check nginx conf] ************************************************************************************************************* fatal: [devops01]: FAILED! => {\"changed\": true, \"cmd\": \"/usr/sbin/nginx -t\", \"delta\": \"0:00:00.099378\", \"end\": \"2021-09-25 16:06:50.395384\", \"msg\": \"non-zero return code\", \"rc\": 1, \"start\": \"2021-09-25 16:06:50.296006\", \"stderr\": \"nginx: [emerg] unknown directive \\\"ser\\\" in /etc/nginx/nginx.conf:5\\nnginx: configuration file /etc/nginx/nginx.conf test failed\", \"stderr_lines\": [\"nginx: [emerg] unknown directive \\\"ser\\\" in /etc/nginx/nginx.conf:5\", \"nginx: configuration file /etc/nginx/nginx.conf test failed\"], \"stdout\": \"\", \"stdout_lines\": []} fatal: [devops02]: FAILED! => {\"changed\": true, \"cmd\": \"/usr/sbin/nginx -t\", \"delta\": \"0:00:00.101067\", \"end\": \"2021-09-25 16:06:50.474854\", \"msg\": \"non-zero return code\", \"rc\": 1, \"start\": \"2021-09-25 16:06:50.373787\", \"stderr\": \"nginx: [emerg] unknown directive \\\"ser\\\" in /etc/nginx/nginx.conf:5\\nnginx: configuration file /etc/nginx/nginx.conf test failed\", \"stderr_lines\": [\"nginx: [emerg] unknown directive \\\"ser\\\" in /etc/nginx/nginx.conf:5\", \"nginx: configuration file /etc/nginx/nginx.conf test failed\"], \"stdout\": \"\", \"stdout_lines\": []} PLAY RECAP ************************************************************************************************************************** devops01 : ok=3 changed=0 unreachable=0 failed=1 skipped=0 rescued=0 ignored=0 devops02 : ok=3 changed=0 unreachable=0 failed=1 skipped=0 rescued=0 ignored=0 我们把配置文件改为正确的再次执行，可以看到当检测到配置文件正确后才触发重启服务操作 $ ansible-playbook tasks_changed_when.yml PLAY [devops] *********************************************************************************************************************** TASK [Gathering Facts] ************************************************************************************************************** ok: [devops01] ok: [devops02] TASK [install nginx] **************************************************************************************************************** ok: [devops02] ok: [devops01] TASK [copy nginx conf] ************************************************************************************************************** changed: [devops02] changed: [devops01] TASK [check nginx conf] ************************************************************************************************************* changed: [devops01] changed: [devops02] RUNNING HANDLER [restart nginx] ***************************************************************************************************** changed: [devops02] changed: [devops01] PLAY RECAP ************************************************************************************************************************** devops01 : ok=5 changed=3 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 devops02 : ok=5 changed=3 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 ansible每次检测配置文件都会提示changed，如果想要关闭显示可以使用参数 chenged_when: false ，使用这个参数的前提是当前操作并没有产生实际的操作(这里执行的 nginx -t只是检测nginx配置文件语法并没有真正的做操作) - name: check nginx conf shell: /usr/sbin/nginx -t register: check_nginx_conf changed_when: ( check_nginx_conf.stdout.find('successful')) 修改yml文件 cat > tasks_changed_when.yml 再次执行yml文件就会看到 TASK [check nginx conf] 处显示的不是 changed状态了 $ ansible-playbook tasks_changed_when.yml PLAY [devops] *********************************************************************************************************************** TASK [Gathering Facts] ************************************************************************************************************** ok: [devops02] ok: [devops01] TASK [install nginx] **************************************************************************************************************** ok: [devops02] ok: [devops01] TASK [copy nginx conf] ************************************************************************************************************** changed: [devops02] changed: [devops01] TASK [check nginx conf] ************************************************************************************************************* ok: [devops01] ok: [devops02] RUNNING HANDLER [restart nginx] ***************************************************************************************************** changed: [devops01] changed: [devops02] PLAY RECAP ************************************************************************************************************************** devops01 : ok=5 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 devops02 : ok=5 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible知识点/ansible只输出错误信息.html":{"url":"linux/自动化运维平台/ansible/ansible知识点/ansible只输出错误信息.html","title":"ansible只输出错误信息","keywords":"","body":"ansible只输出错误信息 参考文章 ansible2.9官方文档 本文示例ansible版本 $ ansible --version ansible 2.9.17 config file = /etc/ansible/ansible.cfg configured module search path = [u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules'] ansible python module location = /usr/lib/python2.7/site-packages/ansible executable location = /usr/bin/ansible python version = 2.7.5 (default, Apr 2 2020, 13:16:51) [GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] 有时主机较多时，我们只想关注有问题的主机。 Ansible callback 插件中有一个 actionable，官方描述为： actionable - shows only items that need attention 即只输出需要关注的部分。 但是 callback 插件只对 playbook 生效，如何对 Ad-hoc 起作用呢？ 可以通过修改配置文件 [defaults] bin_ansible_callbacks=True 或者修改环境变量来实现 export ANSIBLE_LOAD_CALLBACK_PLUGINS=1 这里我们通过环境变量的方式来进行，这样比较轻量，无需修改文件 在运行 Ad-hoc 命令时，前面加上两个参数即可： ANSIBLE_LOAD_CALLBACK_PLUGINS=1 ANSIBLE_STDOUT_CALLBACK=actionable 完整命令如下： ANSIBLE_LOAD_CALLBACK_PLUGINS=1 ANSIBLE_STDOUT_CALLBACK=actionable ansible all -m ping 例如，在机器数量多的时候，执行 ping 模块，就想要查看无法 ping 通的主机 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible知识点/ansible使用sudo.html":{"url":"linux/自动化运维平台/ansible/ansible知识点/ansible使用sudo.html","title":"ansible使用sudo","keywords":"","body":"ansible使用sudo 场景 在生产环境中，禁止root登陆，使用ops运维用户密钥登陆，在ansible主控机上只有ops用户的密钥，在root用户下执行ansible命令或者playbook时，就需要使用sudo来执行了 示例 /etc/ansible/hosts 文件内容如下 [test] 172.16.0.12 ansible_ssh_port=222 ansible_ssh_user=ops ansible_ssh_private_key_file=/home/ops/.ssh/id_rsa_ops 在ansible主控机上用到的密钥是有sudo权限的 ops 用户，执行新建目录操作，报错权限拒绝 $ whoami root $ ansible 172.16.0.12 -m shell -a \"mkdir /opt/test\" [WARNING]: Consider using the file module with state=directory rather than running 'mkdir'. If you need to use command because file is insufficient you can add 'warn: false' to this command task or set 'command_warnings=False' in ansible.cfg to get rid of this message. 172.16.0.12 | FAILED | rc=1 >> mkdir: cannot create directory ‘/opt/test’: Permission deniednon-zero return code 解决方法 生产中禁止root远程登陆，因此使用拥有sudo权限的用户进行操作，此时就需要使用sudo来执行(避免一些操作无权限) # 编辑 /etc/ansible/hosts 文件，写入以下内容 [test] 172.16.0.12 [test:vars] ansible_ssh_user=ops ansible_ssh_port=222 ansible_ssh_private_key_file=/home/ops/.ssh/id_rsa_ops ansible_become=true ansible_become_method=sudo ansible_become_user=root ansible sudo 官方文档 参数 说明 ansible_ssh_user ssh用户 ansible_ssh_port ssh端口 ansible_ssh_private_key_file ssh用户密钥 ansible_ssh_pass ssh用户密码 ansible_sudo_pass ssh sudo 用户密码 ansible_become 为true表示启用sudo ansible_become_method 常用方法为sudo ansible_become_user 提权用户 写法优化 # test组下边可能有多个主机，如果每个主机后边都单独写参数会导致阅读性变差，同时还会有重复的参数项 [test] 172.16.0.12 ansible_ssh_port=222 ansible_ssh_user=ops ansible_ssh_private_key_file=/home/ops/.ssh/id_rsa_ops 172.16.0.13 ansible_ssh_port=222 ansible_ssh_user=ops ansible_ssh_private_key_file=/home/ops/.ssh/id_rsa_ops # 优化写法，在相同组名后指定变量，例如 [test:vars] ，这样以后只需要在这个组下边增加主机即可，而不用在每一个主机后都增加变量 [test] 172.16.0.12 [test:vars] ansible_ssh_user=ops ansible_ssh_port=222 ansible_ssh_private_key_file=/home/ops/.ssh/id_rsa_ops ansible_become=true ansible_become_method=sudo ansible_become_user=root 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible playbook/ansible playbook.html":{"url":"linux/自动化运维平台/ansible/ansible playbook/ansible playbook.html","title":"ansible playbook","keywords":"","body":"ansible playbook ansible2.9官方文档 ansible playbook使用 -C 选项进行模拟执行，和sed命令 -i 选项类似 ansible playbook 官方示例，单task --- - hosts: webservers vars: http_port: 80 max_clients: 200 remote_user: root tasks: - name: ensure apache is at the latest version yum: name: httpd state: latest - name: write the apache config file template: src: /srv/httpd.j2 dest: /etc/httpd.conf notify: - restart apache - name: ensure apache is running service: name: httpd state: started handlers: - name: restart apache service: name: httpd state: restarted ansible playbook 官方示例，多task --- - hosts: webservers remote_user: root tasks: - name: ensure apache is at the latest version yum: name: httpd state: latest - name: write the apache config file template: src: /srv/httpd.j2 dest: /etc/httpd.conf - hosts: databases remote_user: root tasks: - name: ensure postgresql is at the latest version yum: name: postgresql state: latest - name: ensure that postgresql is started service: name: postgresql state: started 1.playbook基础 1.1 主机与用户 hosts 行的内容是一个或多个组或主机的 patterns,以逗号为分隔符 remote_user 就是账户名 --- - hosts: webservers remote_user: root 还可以在每一个task中定义自己的远程用户 --- - hosts: webservers remote_user: root tasks: - name: test connection ping: remote_user: yourname 支持以另一个用户的身份运行， ansible2.9 becom官方文档 --- - hosts: webservers remote_user: yourname become: yes 可以在一个特定的task中使用关键字become --- - hosts: webservers remote_user: yourname tasks: - service: name: nginx state: started become: yes become_method: sudo 你也可以以你的身份登录，然后变成一个不同于root的用户 --- - hosts: webservers remote_user: yourname become: yes become_user: postgres 还可以使用其他提权方法，如 su --- - hosts: webservers remote_user: yourname become: yes become_method: su 如果你需要在使用 sudo 时指定密码,可在运行 ansible-playbook 命令时加上选项 --ask-sudo-pass (-K). 如果使用 sudo 时,playbook 疑似被挂起,可能是在 sudo prompt 处被卡住,这时可执行 Control-C 杀死卡住的任务,再重新运行一次. 当使用 become_user 切换到 非root 用户时,模块的参数会暂时写入/tmp 目录下的一个随机临时文件. 当命令执行结束后,临时文件立即删除.这种情况发生在普通用户的切换时,比如从 ‘bob’ 切换到 ‘timmy’, 切换到 root 账户时,不会发生,如从 ‘bob’ 切换到 ‘root’,直接以普通用户或root身份登录也不会发生. 如果你不希望这些数据在短暂的时间内可以被读取（不可写）,请避免在 sudo_user 中传递未加密的密码. 其他情况下,/tmp 目录不被使用,这种情况不会发生.Ansible 也有意识的在日志中不记录密码参数. 1.2 Tasks列表 基本tasks示例 tasks: - name: make sure apache is running service: name: httpd state: started copy模块使用示例 tasks: - name: Copy ansible inventory file to client copy: src=/etc/ansible/hosts dest=/etc/ansible/hosts owner=root group=root mode=0644 也可以写成这样 tasks: - name: Copy ansible inventory file to client copy: src: /etc/ansible/hosts dest: /etc/ansible/hosts owner: root group: root mode: 0644 还可以使用变量 tasks: - name: create a virtual host file for {{ vhost }} template: src: somefile.j2 dest: /etc/httpd/conf.d/{{ vhost }} 1.3 Handlers Handlers 最佳的应用场景是用来重启服务，或者触发系统重启操作，除此以外很少用到了 handlers:在发生改变时执行的操作 notify 动作会在playbook的每一个task结束时触发，而且即使有多个不同的task通知改变的发生，notify 动作只会被触发一次 举例来说，比如多个 resources(资源) 因为一个配置文件被改动，所以 apache 需要重新启动，但是重新启动的操作只会被执行一次 如下的例子是当配置文件修改时，重启memcache和apache - name: template configuration file template: src: template.j2 dest: /etc/foo.conf notify: - restart memcached - restart apache notify 动作下就是 handlers ，即 notify 和 handlers 是一一对应的 notify定义一个动作，比如叫 restart apache ，在 handlers 中使用 - name: restart apache 引用，然后接着执行后续的操作，例如进行服务重启 handlers: - name: restart memcached service: name: memcached state: restarted - name: restart apache service: name: apache state: restarted 如果想要handlers使用变量，例如，如果服务的名称因为分布而略有不同，想显示每个目标机器重新启动的服务的确切名称。避免在handlers中放置变量。由于handlers很早就被模块化了，ansible可能没有这样的处理handlers的可用值 handlers: # this handler name may cause your play to fail! - name: restart \"{{ web_service_name }}\" 如果handlers中使用的变量是不可用的，则会导致整个play失败，在play中更改该变量不会影响handlers，相反，将变量放在handlers的任务参数重，可以使用 include_vars tasks: - name: Set host variables based on distribution include_vars: \"{{ ansible_facts.distribution }}.yml\" handlers: - name: restart web service service: name: \"{{ web_service_name | default('httpd') }}\" state: restarted 从ansible2.2开始，handlers还可以使用 listen ，task可以按如下方式通知这些主题 handlers: - name: restart memcached service: name: memcached state: restarted listen: \"restart web services\" - name: restart apache service: name: apache state: restarted listen: \"restart web services\" tasks: - name: restart everything command: echo \"this task will restart the web services\" notify: \"restart web services\" 1.4 Ansible-Pull(拉取配置而非推送配置) ansible pull 官方文档(ansible2.9) 1.4.1 Ansible-Pull简介 我们可不可以将 ansible 的体系架构颠倒过来，让托管节点从一个 central location 做 check in 获取配置信息，而不是推送配置信息到所有的托管节点？这样是可以的。 Ansible-pull 是一个小脚本，它从 git 上 checkout 一个关于配置指令的 repo，然后以这个配置指令来运行 ansible-playbook 也有一个叫做 clever playbook 的东西: clever playbook ， 这个可以通过 crontab 来配置 ansible-pull（from push mode） 1.4.2 ansible pull模式 ansible模式使用的是push模式，即只需要在ansible主控端编排playbook，然后push到远程主机即可，pull模式则正好和push相反，pull模式适用于以下场景 被控节点在配置时不可用，比如自动伸缩的场景 被控节点较多，ansible控制机资源有限无法实现高并发操作 ansible基于pull模式的工作流程 每台被控端需要安装ansible和git 所有的配置及playbook yaml文件都存放在git仓库 被控端的 ansible-pull 计划任务会定期检查给定的git的tag或者分支 如果ansible主控端上传到git上特定的文件发生了变更，则执行相应的动作 1.4.2.1 ansible pull 模式测试 ansible主控制机编辑yaml文件并上传至git，⚠️yml文件中的 - hosts 参数必须为 127.0.0.1 cat > local.yml 被控机手动执行验证 如果ansible控制端上传到git的yml文件名称不是 local.yml ，则需要在命令后边手动指定yml文件名称，不需要参数，在git地址后直接写上文件名即可 ansible-pull -o -C master -d /opt -i /etc/ansible/hosts -U git@gitee.com:abc/ansible_pull.git ansible-pull 参数说明 参数 说明 -C 指定git分支 -d git仓库检出的目录 -i 主机hosts路径 -U git仓库地址 -e 添加参数 -o 只有git仓库发生改变才执行playbook 完成输出，可以看到当git仓库中的yml文件发生改变时，被控端拉取yml文件后会自动执行 $ ansible-pull -o -C master -d /opt -U git@gitee.com:abc/ansible_pull.git Starting Ansible Pull at 2021-08-06 17:42:37 /usr/bin/ansible-pull -o -C master -d /opt -U git@gitee.com:abc/ansible_pull.git [WARNING]: Could not match supplied host pattern, ignoring: devops02 [WARNING]: Your git version is too old to fully support the depth argument. Falling back to full checkouts. localhost | CHANGED => { \"after\": \"da5e7724ecefbe59ccb308ff1bbd38903803142e\", \"before\": null, \"changed\": true } [WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all' [WARNING]: Could not match supplied host pattern, ignoring: devops02 PLAY [127.0.0.1] ************************************************************************************************************************************************************* TASK [Gathering Facts] ******************************************************************************************************************************************************* ok: [localhost] TASK [touch file] ************************************************************************************************************************************************************ changed: [localhost] PLAY RECAP ******************************************************************************************************************************************************************* localhost : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 在被控机查看执行结果，在 /opt 下创建了test文件 $ ll /opt/test -rw-r--r-- 1 root root 0 Aug 6 17:42 /opt/test 配置计划任务 计划任务成功执行后就会在 opt 下自动pull下 local.yml 以及自动执行 playbook */1 * * * * ansible-pull -o -C master -d /opt -U git@gitee.com:pptfz/ansible_pull.git 1.5 Linting playbooks 在执playbook之前，可以使用命令 ansible-lint 对其进行详细检查 以下检测结果表明相应的行后边有多余的空格(不过没有影响，只是检测会有提示)，如果没有任何输出则表明playbook没有任何问题 $ ansible-lint copy.yaml [201] Trailing whitespace copy.yaml:6 copy: [201] Trailing whitespace copy.yaml:9 owner: root [201] Trailing whitespace copy.yaml:10 group: root 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/自动化运维平台/ansible/ansible报错记录.html":{"url":"linux/自动化运维平台/ansible/ansible报错记录.html","title":"ansible报错记录","keywords":"","body":"ansible报错记录 ansible报错一 ansible报错 /usr/lib/python2.7/site-packages/ansible/parsing/vault/__init__.py:44: CryptographyDeprecationWarning: Python 2 is no longer supported by the Python core team. Support for it is now deprecated in cryptography, and will be removed in the next release. $ ansible --version /usr/lib/python2.7/site-packages/ansible/parsing/vault/__init__.py:44: CryptographyDeprecationWarning: Python 2 is no longer supported by the Python core team. Support for it is now deprecated in cryptography, and will be removed in the next release. from cryptography.exceptions import InvalidSignature ansible 2.9.25 config file = /etc/ansible/ansible.cfg configured module search path = [u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules'] ansible python module location = /usr/lib/python2.7/site-packages/ansible executable location = /bin/ansible python version = 2.7.5 (default, Jun 20 2019, 20:27:34) [GCC 4.8.5 20150623 (Red Hat 4.8.5-36)] 解决方法 # 过滤crypto包 $ rpm -qa |grep crypto python2-cryptography-1.7.2-2.el7.x86_64 # 卸载python2-cryptography yum -y remove python2-cryptography # 重新安装ansible yum -y instal ansible 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/zabbix/zabbix问题记录/zabbix数据库迁移的一个小问题.html":{"url":"linux/monitor/zabbix/zabbix问题记录/zabbix数据库迁移的一个小问题.html","title":"zabbix数据库迁移问题","keywords":"","body":"zabbix数据库迁移的一个小问题 背景： 原先zabbix5.0是部署在1c1g的华为云主机上，但是因为有其他服务运行因此会导致mysql经常被系统干掉，选择把数据库迁移至1c4g的腾讯云主机上 过程： 导出zabbix库，拷贝至腾讯云主机然后导入 修改zabbix-server配置文件/etc/zabbix/zabbix_server.conf中的DBHost为新主机IP或域名 腾讯云主机mysql中重新授权zabbix，即只允许华为云主机连接 问题： zabbix报错如下 排查过程： zabbix服务端日志中没有有用的信息 刚导入成功后zabbix是没有问题的，也可以获取数据，但是隔了一段时间报错Unknown database 'zabbix'，原因是在华为云机器上执行了删除zabbix数据库的操作，但是配置文件/etc/zabbix/web/zabbix.conf.php中的数据库指向还是本机，因此会报错未知的数据库 解决方法： 修改配置文件/etc/zabbix/web/zabbix.conf.php中$DB['SERVER']字段中的localhost为新mysql主机的IP或域名 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/zabbix/zabbix问题记录/ubuntu启动zabbix-agent报错.html":{"url":"linux/monitor/zabbix/zabbix问题记录/ubuntu启动zabbix-agent报错.html","title":"ubuntu启动zabbix-agent报错","keywords":"","body":"ubuntu启动zabbix-agent报错 ubuntu16中重新安装zabbix-agent，重新启动后报错如下 $ systemctl start zabbix-agent Failed to start zabbix-agent.service: Unit zabbix-agent.service is masked. 百度半天没有答案，看了一堆博客基本上都是一样的，谷歌找到答案，先执行以下命令，然后再重新启动zabbix-agent就可以了 $ systemctl unmask zabbix-agent.service Removed symlink /etc/systemd/system/zabbix-agent.service. 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/zabbix/zabbix3.4/1.CentOS7.5安装zabbix-3.4.html":{"url":"linux/monitor/zabbix/zabbix3.4/1.CentOS7.5安装zabbix-3.4.html","title":"zabbix3.4安装","keywords":"","body":"[toc] CentOS7.5安装zabbix-3.4 1.实验环境 操作系统 IP地址 域名 内存 CentOS7.5 10.0.0.200 my.zabbix.com 1G 2.安装步骤 zabbix3.4 中文手册地址 2.1 配置zabbix仓库 rpm -ivh https://mirrors.aliyun.com/zabbix/zabbix/3.4/rhel/7/x86_64/zabbix-release-3.4-2.el7.noarch.rpm 2.2 安装Zabbix程序包，以及MySQL、Zabbix-agent yum -y install zabbix-server-mysql zabbix-web-mysql zabbix-agent mariadb-server 2.3 创建Zabbix数据库以及用户授权 启动mariadb systemctl start mariadb && systemctl enable mariadb 创建数据库并授权 mysql -uroot -e \"create database zabbix character set utf8 collate utf8_bin;\" mysql -uroot -e \"grant all privileges on zabbix.* to zabbix@localhost identified by 'zabbix';\" mysql -uroot -e \"flush privileges;\" 2.4导入Zabbix数据至数据库中 zcat /usr/share/doc/zabbix-server-mysql-3.4.15/create.sql.gz |mysql -uroot zabbix 2.5 编辑 /etc/zabbix/zabbix_server.conf文件，修改数据库配置 修改 /etc/zabbix/zabbix_server.conf 文件 取消 DBHost=localhost注释 修改 DBPassword=zabbix 用以下命令修改 sed -i.bak '/# DBHost=localhost/c DBHost=localhost' /etc/zabbix/zabbix_server.conf sed -i '/# DBPassword=/c DBPassword=zabbix' /etc/zabbix/zabbix_server.conf 2.6 启动Zabbix服务进程，并加入开机自启 systemctl start zabbix-server && systemctl enable zabbix-server 2.7 配置Apache的配置文件 /etc/httpd/conf.d/zabbix.conf，修改时区 修改 Europe/Riga 为 Asia/Shanghai sed -i.bak '/Riga$/c php_value date.timezone Asia/Shanghai' /etc/httpd/conf.d/zabbix.conf 2.8 整合nginx，让zabbix可以以域名访问 因为本文用到了apache来展示zabbix web界面，因此设置apache监听8080端口，nginx监听80端口 2.8.1 安装nginx yum -y install nginx 2.8.2 配置apache主配置文件，修改监听端口 sed -i.bak '/Listen 80/c Listen 8080' /etc/httpd/conf/httpd.conf 2.8.3 配置nginx虚拟主机配置文件 cat >> /etc/nginx/conf.d/my.zabbix.com.conf 2.8.4 启动nginx systemctl enable nginx && systemctl start nginx 绑定windows hosts文件，能以域名访问 mac编辑 /etc/hosts 文件 C:\\Windows\\System32\\drivers\\etc\\hosts 10.0.0.200 my.zabbix.com 2.9 启动Apache Web服务器 systemctl enable httpd && systemctl start httpd 2.10 浏览器输入地址 my.zabbix.com/zabbix 开始安装 第一步 第二步，全部为OK才可以 第三步 配置数据库连接 第四步 第五步，确认信息 第六步，完成安装 完成安装后会生成一个配置信息文件 第七步，登陆 第八步，修改zabbix语言为中文 完成安装后首界面 2.11 解决图形中文乱码问题 2.11.1 从windows找到楷体字体simkai，搜索楷体即可 windows路径 c盘-->Windows-->Fonts mac路径/Library/Fonts 2.11.2 在zabbix-server上备份zabbix默认字体并且上传新字体 # 切换目录 cd /usr/share/fonts/dejavu/ # 上传字体，修改名称为DejaVuSans.ttf mv DejaVuSans.ttf DejaVuSans.ttf.bak mv simkai.ttf DejaVuSans.ttf # 修改字体的权限要让zabbix用户可以读 chmod 644 DejaVuSans.ttf 2.11.3 验证效果 监测中-->图形 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/zabbix/zabbix3.4/2.配置zabbix基于apache、nginx用户认证.html":{"url":"linux/monitor/zabbix/zabbix3.4/2.配置zabbix基于apache、nginx用户认证.html","title":"zabbix基于apache、nginx用户认证","keywords":"","body":"[toc] 配置zabbix基于apache、nginx用户认证 一、基于apache 1.修改Apache的配置文件/etc/httpd/conf/httpd.conf 对需要认证的资源所在的目录进行配置,在文件最后一行加入以下内容，其中Allowoverride authconfig一行表示允许对/etc/zabbix/web目录下的内容进行用户认证 [root@zabbix-server ~]# vim /etc/httpd/conf/httpd.conf Options Indexes FollowSymLinks Allowoverride AuthConfig Order allow,deny Allow from all 2.在限制访问目录/usr/share/zabbix下创建文件.htaccess，并写入以下内容 [root@zabbix-server ~]# cat > /usr/share/zabbix/.htaccess 3.创建一个用户名为admin,密码为123456的登陆认证用户，同时将密码存放于/etc/zabbix/auth_conf [root@zabbix-server ~]# yum -y install httpd-tools [root@zabbix-server ~]# htpasswd -b -c /usr/share/zabbix/auth_file admin 123456 Adding password for user admin 4.重启apache服务 [root@zabbix-server ~]# systemctl restart httpd 二、基于nginx 1.安装包 [root@zabbix-server ~]# yum -y install httpd-tools 2.创建认证文件、配置nginx 这里指定了认证文件是/etc/nginx/auth_file，认证的用户名是admin，密码是123456，文件的所有者为root，权限是644 [root@zabbix-server ~]# htpasswd -b -c /etc/nginx/auth_file admin 123456 #nginx配置文件写入auth_basic和auth_basic_user_file location / { auth_basic \"Auth access down Input your Passwd!\"; auth_basic_user_file /etc/nginx/auth_file; } 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/zabbix/zabbix3.4/3.zabbix3.4监控一台主机.html":{"url":"linux/monitor/zabbix/zabbix3.4/3.zabbix3.4监控一台主机.html","title":"zabbix3.4监控一台主机","keywords":"","body":"[toc] zabbix3.4监控一台主机 zabbix-serverIP地址：10.0.0.200 1.安装zabbix-agent rpm -ivh https://mirrors4.tuna.tsinghua.edu.cn/zabbix/zabbix/3.4/rhel/7/x86_64/zabbix-agent-3.4.15-1.el7.x86_64.rpm 2.配置zabbix-agent # 指定zabbix-server地址 sed -i.bak 's/Server=127.0.0.1/Server=10.0.0.200/' /etc/zabbix/zabbix_agentd.conf 3.启动zabbix-agent # 启动zabbix-agent并设置开机自起 systemctl start zabbix-agent && systemctl enable zabbix-agent 4.在zabbix-server端web界面，点击配置-->主机-->创建主机 5.点击配置-->主机-->模板-->链接指示器 添加后的主机 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/zabbix/zabbix3.4/4.zabbix3.4创建自定义触发器.html":{"url":"linux/monitor/zabbix/zabbix3.4/4.zabbix3.4创建自定义触发器.html","title":"zabbix3.4创建自定义触发器","keywords":"","body":"[toc] zabbix3.4创建自定义触发器 触发器：trigger 当监控项超过阈值时，产生告警 第一步，配置-->主机-->主机列表处的触发器-->创建触发器 第二步，添加表达式 第三步，添加表达式后测试表达式 第四步，查看触发器列表中刚添加的触发器 第五步，查看监控项中刚添加的触发器 配置-->主机-->主机列表中的主机-->监控项 第六步，开启zabbix监控面板告警声音 右上角-->小人头-->正在发送消息-->勾选要接受的告警 第七步，查看zabbix监控首页 在用一个终端登陆zabbix-agent主机，此时root登陆数为2，就会触发告警 第八步，查看键值 添加完成后，可以在zabbix-server端获取监控项的值 配置-->主机-->主机列表-->监控项-->键值 zabbix-server端取值 #zabbix-server端安装zabbix-get包 [root@zabbix-server ~]# yum -y install zabbix-get #对agent端取键值，结果为2表明root用户登陆数为2 [root@zabbix-server ~]# zabbix_get -s 10.0.0.10 -k system.users.num 2 -s 指定agent端IP地址 -k 键值 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/zabbix/zabbix3.4/5.zabbix3.4自定义监控项.html":{"url":"linux/monitor/zabbix/zabbix3.4/5.zabbix3.4自定义监控项.html","title":"zabbix3.4自定义监控项","keywords":"","body":"[toc] zabbix3.4自定义监控项 一、实验环境 服务器角色 ip 主机名 zabbix-server 10.0.0.200 zabbix-server zabbix-agent 10.0.0.10 test1 二、zabbix自定义监控项整体过程 2.1在对应的agent主机上编写自定义监控TCP的11种状态（在agent本地进行取值） #进到/etc/zabbix/zabbix_agentd.d [root@test1 zabbix_agentd.d]# pwd /etc/zabbix/zabbix_agentd.d #编写自定义取值文件 [root@test1 zabbix_agentd.d]# cat > tcp_state.conf 2.2在server上使用zabbix_get获取对应主机的值 [root@test1 zabbix-server]# zabbix_get -s 10.0.0.10 -k tcp_state[estb] 2.3在web界面添加 监控项 将监控项制作了一个图形 将主机关联该 TCP状态的模板 三、创建zabbix自定义监控项 3.1自定义监控tcp11种状态(传参方式，在本地取值) //创建自定义监控项文件 [root@test ~]# cd /etc/zabbix/zabbix_agentd.d/ [root@test ~]# cat > tcp_state.conf 3.2在zabbix-web端先创建模板 配置-->模板-->创建模板 添加模板后可以在配置-->模板中看到刚新建的模板 3.3点击创建的模板中的监控项，不要点主机中的监控项，然后点击创建监控项 点击添加后就可以看到刚创建的监控项 3.4利用克隆快速添加监控项 点击刚创建的监控项 点击克隆 然后填写信息，再点击最下方添加 其余tcp状态依照克隆方法依次添加 tcp12种状态，tcp_state.conf 为在agent端 /etc/zabbix/zabbix_agentd.d 路径下创建的文件中自定义的 [root@test1 zabbix_agentd.d]# cat tcp_state.conf UserParameter=tcp_state[*],ss -an|awk '{print $2}'|grep -i \"$1\"|wc -l tcp_state[ESTABLISHED] tcp_state[SYN-SENT] tcp_state[SYN-RECV] tcp_state[FIN-WAIT1] tcp_state[FIN-WAIT2] tcp_state[TIME-WAIT] tcp_state[CLOSE] tcp_state[CLOSE-WAIT] tcp_state[LAST-ACK] tcp_state[LISTEN] tcp_state[CLOSING] tcp_state[UNKNOWN] 在配置-->模板中可以看新建的模板的信息，此时的模板是新建的，与主机没有任何关系，除非主机链接这个模板，这里可以看到有刚创建的12项监控项，但是没有图形，需要手动再创建图形 3.5创建图形 配置-->模板-->创建的模板(这里为Linux TCP Status)-->图形-->创建图形 添加后的图形 配置-->模板-->新建的模板 可以看到刚创建的图形 配置-->主机-->模板 查看图形，监测中-->最新数据 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/zabbix/zabbix3.4/6.zabbix3.4自定义告警内容.html":{"url":"linux/monitor/zabbix/zabbix3.4/6.zabbix3.4自定义告警内容.html","title":"zabbix3.4自定义告警内容","keywords":"","body":"[toc] zabbix3.4自定义告警内容 1.启动默认告警项 配置-->动作 2.修改告警内容 默认告警信息 #告警标题 Problem: {TRIGGER.NAME} #告警内容 Problem started at {EVENT.TIME} on {EVENT.DATE} Problem name: {TRIGGER.NAME} Host: {HOST.NAME} Severity: {TRIGGER.SEVERITY} Original problem ID: {EVENT.ID} {TRIGGER.URL} 修改告警默认标题如下 服务器:{HOST.NAME}发生: {TRIGGER.NAME} 故障! 修改告警内容如下 服务器:{HOST.NAME}发生: {TRIGGER.NAME} 故障! { 告警主机:{HOST.NAME} 告警地址:{HOST.IP} 监控项目:{ITEM.NAME} 监控取值:{ITEM.LASTVALUE} 告警等级:{TRIGGER.SEVERITY} 当前状态:{TRIGGER.STATUS} 告警信息:{TRIGGER.NAME} 告警时间:{EVENT.DATE} {EVENT.TIME} 事件ID:{EVENT.ID} } 3.修改恢复内容 默认恢复信息 #恢复标题 Resolved: {TRIGGER.NAME} #恢复内容 Problem has been resolved at {EVENT.RECOVERY.TIME} on {EVENT.RECOVERY.DATE} Problem name: {TRIGGER.NAME} Host: {HOST.NAME} Severity: {TRIGGER.SEVERITY} Original problem ID: {EVENT.ID} {TRIGGER.URL} 修改恢复信息标题如下 服务器:{HOST.NAME}: {TRIGGER.NAME} 已恢复! 修改恢复信息如下 服务器:{HOST.NAME}: {TRIGGER.NAME} 已恢复! { 告警主机:{HOST.NAME} 告警地址:{HOST.IP} 监控项目:{ITEM.NAME} 监控取值:{ITEM.LASTVALUE} 告警等级:{TRIGGER.SEVERITY} 当前状态:{TRIGGER.STATUS} 告警信息:{TRIGGER.NAME} 告警时间:{EVENT.DATE} {EVENT.TIME} 恢复时间:{EVENT.RECOVERY.DATE} {EVENT.RECOVERY.TIME} 持续时间:{EVENT.AGE} 事件ID:{EVENT.ID} } 4.修改确认操作 默认确认信息 #确认标题 Acknowledged: {TRIGGER.NAME} #确认内容 {USER.FULLNAME} acknowledged problem at {ACK.DATE} {ACK.TIME} with the following message: {ACK.MESSAGE} Current problem status is {EVENT.STATUS} 修改确认标题如下 服务器:{HOST.NAME}: 报警确认 修改确认内容如下 { 确认人:{USER.FULLNAME} 时间:{ACK.DATE} {ACK.TIME} 确认信息如下: \"{ACK.MESSAGE}\" 问题服务器IP:{HOSTNAME1} 问题ID:{EVENT.ID} 当前的问题是: {TRIGGER.NAME} } 修改完后点击更新 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/zabbix/zabbix3.4/7.zabbix3.4配置触发zabbix-agent端shell脚本、命令.html":{"url":"linux/monitor/zabbix/zabbix3.4/7.zabbix3.4配置触发zabbix-agent端shell脚本、命令.html","title":"zabbix3.4配置触发zabbix-agent端shell脚本、命令","keywords":"","body":"[toc] zabbix3.4配置触发zabbix-agent端shell脚本、命令 一、说明 1.zabbix-server端配置了触发器，如果监控项的阀值达到触发器的值，会发出告警信息。我们还想增加一个功能，如磁盘使用空间达到80%阀值的时候，触发一个自动清理磁盘空间的脚本，达到所谓的“自愈”功能 2.要配置好监控项、触发器，要确保触发器在监控项达到设置的阀值的时候能正常触发，这个是实现触发zabbix-agent脚本任务执行的前提，此处以磁盘可用空间低于30%为例 二、执行步骤 2.1zabbix-agent端开启允许远程执行命令 //zabbix-agent端编辑配置文件/etc/zabbix/zabbix_agentd.conf第73行，取消# EnableRemoteCommands=0注释，并把值修改为1 EnableRemoteCommands=0 用以下命令修改 [root@zabbix-agent ~]# sed -i.bak '/# EnableRemoteCommands=0/a EnableRemoteCommands=1' /etc/zabbix/zabbix_agentd.conf //重启zabbix-agent [root@zabbix-agent ~]# systemctl restart zabbix-agent 2.2在zabbix-agent端编写脚本，交给触发器触发时执行 //创建存放脚本目录，在zabbix-server端配置触发脚本时要写脚本的绝对路径 [root@zabbix-agent ~]# mkdir /etc/zabbix/scripts //编写脚本,此处仅仅是为了演示触发效果 [root@zabbix-agent ~]# mkdir /etc/zabbix/scripts [root@zabbix-agent ~]# cat >/etc/zabbix/scripts/test.sh > /tmp/test.txt EOF 2.3在zabbix-agent端将zabbix用户加入到sudo中 visudo编辑文件，在93行新加入abbix ALL=(ALL) NOPASSWD: ALL viduso编辑文件，注释53行Defaults !visiblepw //用以下命令修改 [root@zabbix-agent ~]# sed -i.bak '/^root/a zabbix ALL=(ALL) NOPASSWD: ALL' /etc/sudoers && sed -i '/Defaults !visiblepw/c#Defaults !visiblepw' /etc/sudoers 2.4zabbix-server创建动作，并指定只让某个主机组执行命令 配置-->动作-->创建动作 操作-->新的 进行相关配置 在zabbix-agent端做如下操作 #创建一个8G的大文件让系统磁盘使用率大于80%，这样就会触发告警，从而执行我们自定义的脚本 dd if=/dev/zero of=/opt/bigfile bs=1M count=8192 #等待server端web界面告警后，就会在我们自定的脚本中设定的路径/tmp下创建一个test.txt文件 cat /tmp/test.txt 2018-01-10-22:28:13磁盘使用率超过80% 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/zabbix/zabbix3.4/8.zabbix3.4设置邮件告警.html":{"url":"linux/monitor/zabbix/zabbix3.4/8.zabbix3.4设置邮件告警.html","title":"zabbix3.4设置邮件告警","keywords":"","body":"[toc] zabbix3.4设置邮件告警 第一步、配置-->动作-->启用动作 第二步、管理-->报警媒介类型-->选择Email 第三步、设置服务器发件人，使用邮箱账户和授权密码，授权密码在qq邮箱官网中设置 第四步、设置收件人邮箱，右上角小人头-->报警媒介--添加 第五步、收件人类型选择Email，填写收件人邮箱，接受报警级别，添加 第六步、确认没有问题，点击更新 第七步、验证是否发送邮件 第八步、qq邮箱查看邮件 邮件发送失败 发送邮件失败原因1：qq邮箱服务器地址写错，正确为smtp.qq.com，而不是mail.qq.com 发送邮件失败原因2：填写发件人信息后没有保存 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/zabbix/zabbix3.4/9.zabbix3.4设置邮件告警升级.html":{"url":"linux/monitor/zabbix/zabbix3.4/9.zabbix3.4设置邮件告警升级.html","title":"zabbix3.4设置邮件告警升级","keywords":"","body":"[toc] zabbix3.4设置邮件告警升级 一、邮件告警升级过程 这里仅作示例 1.首先发给运维组，持续10分钟 2.运维组没有解决，发给经理组，持续10分钟 3.经理组没有解决，发给总监组 二、邮件告警升级过程配置 2.1配置-->动作-->选择动作(这里选择默认) 2.2操作-->点击 新的 2.3填写第一步发送设置 2.4填写第二步发送设置 2.5填写第三步发送设置 2.6添加完成后页面 操作步骤写1-2，2-3，3-4 1-2，3-4，5-6都可以 2.7添加完成后在相应用户填写收件地址即可，管理-->用户--报警媒介 2.8给创建的用户群组赋予读写权限 管理-->用户群组-->选择创建的用户群组-->权限 2.9验证 经过测试 1.管理员即运维组先收到告警，对应下图中问题下方1 2.规定时间没有完成发送给经理组，对应下图中问题下方2 3.经理组在规定时间内没有完成处理发送给总监组，对应下图中问题下方3，应为本文中设置总监接受1分钟，因此总监只接受一次 4.如果恢复，运维组、经理组、总监组都会收到恢复信息 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/zabbix/zabbix3.4/10.zabbix3.4使用企业微信告警.html":{"url":"linux/monitor/zabbix/zabbix3.4/10.zabbix3.4使用企业微信告警.html","title":"zabbix3.4使用企业微信告警","keywords":"","body":"[toc] zabbix3.4使用企业微信告警 第一步、注册企业微信，个人就可以注册，然后按照提示填写相关信息 企业微信注册地址 第二步、登陆企业微信 企业微信首界面 应用与小程序-->创建应用，根据提示填写相关信息，这里已经创建好了一个zabbix监控 点击创建的应用，后续python脚本会用到Agentld和Secret 点击我的企业，后续python脚本会用到企业ID 第三步、zabbix-server端配置python脚本 3.1环境准备 #安装依赖包 [root@zabbix-server ~]# yum -y install python-pip [root@zabbix-server ~]# pip install requests [root@zabbix-server ~]# cd /usr/lib/zabbix/alertscripts [root@zabbix-server alertscripts]# ls weixin.py #给脚本赋予执行权限 [root@zabbix-server alertscripts]# chmod +x weixin.py #一定要修改/tmp/weixin.log这个文件的权限为zabbix，因为zabbix程序在执行weixin.py的时候是以zabbix用户执行的，而这个文件默认是root，不修改权限会报错 [root@zabbix-server alertscripts]# chown zabbix.zabbix /tmp/weixin.log 3.2 编写企业微信告警脚本 3.2.1 使用管理员id [root@zabbix-server alertscripts]# pwd /usr/lib/zabbix/alertscripts [root@zabbix-server alertscripts]# cat >weixin.py 成员信息中查看 [root@zabbix-server alertscripts]# ./weixin.py 企业微信号 test test 3.2.2 企业微信机器人告警 编辑脚本 cat > /usr/lib/zabbix/alertscripts/enterprise_wechat.py 测试，机器人能收到告警即可 python enterprise_wechat.py 告警测试 企业微信收到信息即为正确 第四步、zabbix-server端配置 4.1管理-->报警媒介类型-->创建媒体类型 4.2填写相关信息 脚本名称为自己定义的weixin.py，路径为在zabbix-server端/etc/zabbix/zabbix_server.conf中定义 AlertScriptsPath=/usr/lib/zabbix/alertscripts 在下图中脚本参数的地方写入以下3个参数，注意脚本参数一定要填写正确 {ALERT.SENDTO} {ALERT.SUBJECT} {ALERT.MESSAGE} 4.3设置收件人 右上角小人头-->报警媒介-->添加 第五步、修改告警相关信息 5.1修改告警操作 修改操作标题如下 服务器:{HOST.NAME}发生: {TRIGGER.NAME} 故障! 修改消息内容如下 服务器:{HOST.NAME}发生: {TRIGGER.NAME} 故障! { 告警主机:{HOST.NAME} 告警地址:{HOST.IP} 监控项目:{ITEM.NAME} 监控取值:{ITEM.LASTVALUE} 告警等级:{TRIGGER.SEVERITY} 当前状态:{TRIGGER.STATUS} 告警信息:{TRIGGER.NAME} 告警时间:{EVENT.DATE} {EVENT.TIME} 事件ID:{EVENT.ID} } 5.2修改恢复操作 修改恢复标题如下 服务器:{HOST.NAME}: {TRIGGER.NAME} 已恢复! 修改恢复内容如下 服务器:{HOST.NAME}: {TRIGGER.NAME} 已恢复! { 告警主机:{HOST.NAME} 告警地址:{HOST.IP} 监控项目:{ITEM.NAME} 监控取值:{ITEM.LASTVALUE} 告警等级:{TRIGGER.SEVERITY} 当前状态:{TRIGGER.STATUS} 告警信息:{TRIGGER.NAME} 告警时间:{EVENT.DATE} {EVENT.TIME} 恢复时间:{EVENT.RECOVERY.DATE} {EVENT.RECOVERY.TIME} 持续时间:{EVENT.AGE} 事件ID:{EVENT.ID} } 5.3修改确认信息 修改确认标题如下 服务器:{HOST.NAME}: 报警确认 修改确认内容如下 服务器:{HOST.NAME}: 报警确认 { 确认人:{USER.FULLNAME} 时间:{ACK.DATE} {ACK.TIME} 确认信息如下: \"{ACK.MESSAGE}\" 问题服务器IP:{HOSTNAME1} 问题ID:{EVENT.ID} 当前的问题是: {TRIGGER.NAME} } 第六步、验证 企业微信中创建的zabbix监控应用能收到信息即为正确 pc端企业微信 手机端企业微信 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/zabbix/zabbix3.4/11.zabbix3.4使用钉钉告警.html":{"url":"linux/monitor/zabbix/zabbix3.4/11.zabbix3.4使用钉钉告警.html","title":"zabbix3.4使用钉钉告警","keywords":"","body":"[toc] zabbix3.4使用钉钉告警 第一步、钉钉机器人创建 打开windows钉钉客户端-->右上角头像处-->机器人管理 选择自定义 选择添加 填写机器人名字和要添加到的群组 这里的webhook值需要写在后续python脚本中 第二步、zabbix-server端编写python脚本 #在zabbix-server端编辑python脚本 [root@zabbix-server alertscripts]# pwd /usr/lib/zabbix/alertscripts [root@zabbix-server alertscripts]# cat >/usr/lib/zabbix/alertscripts/dingding.py 第三步、zabbix web界面设置 3.1创建钉钉告警 管理-->报警媒介类型-->创建媒体类型 填写相关信息，注意脚本参数写 {ALERT.MESSAGE} 右上角小人头-->报警媒介-->添加 3.2修改告警操作 修改操作标题如下 服务器:{HOST.NAME}发生: {TRIGGER.NAME} 故障! 修改消息内容如下 服务器:{HOST.NAME}发生: {TRIGGER.NAME} 故障! { 告警主机:{HOST.NAME} 告警地址:{HOST.IP} 监控项目:{ITEM.NAME} 监控取值:{ITEM.LASTVALUE} 告警等级:{TRIGGER.SEVERITY} 当前状态:{TRIGGER.STATUS} 告警信息:{TRIGGER.NAME} 告警时间:{EVENT.DATE} {EVENT.TIME} 事件ID:{EVENT.ID} } 3.3修改恢复操作 修改恢复标题如下 服务器:{HOST.NAME}: {TRIGGER.NAME} 已恢复! 修改恢复内容如下 服务器:{HOST.NAME}: {TRIGGER.NAME} 已恢复! { 告警主机:{HOST.NAME} 告警地址:{HOST.IP} 监控项目:{ITEM.NAME} 监控取值:{ITEM.LASTVALUE} 告警等级:{TRIGGER.SEVERITY} 当前状态:{TRIGGER.STATUS} 告警信息:{TRIGGER.NAME} 告警时间:{EVENT.DATE} {EVENT.TIME} 恢复时间:{EVENT.RECOVERY.DATE} {EVENT.RECOVERY.TIME} 持续时间:{EVENT.AGE} 事件ID:{EVENT.ID} } 3.4修改确认信息 修改确认标题如下 服务器:{HOST.NAME}: 报警确认 修改确认内容如下 服务器:{HOST.NAME}: 报警确认 { 确认人:{USER.FULLNAME} 时间:{ACK.DATE} {ACK.TIME} 确认信息如下: \"{ACK.MESSAGE}\" 问题服务器IP:{HOSTNAME1} 问题ID:{EVENT.ID} 当前的问题是: {TRIGGER.NAME} } 第四步、钉钉验证 pc端钉钉 手机端钉钉 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/zabbix/zabbix3.4/12.zabbix3.4默认键值与自定义告警变量.html":{"url":"linux/monitor/zabbix/zabbix3.4/12.zabbix3.4默认键值与自定义告警变量.html","title":"zabbix3.4默认键值与自定义告警变量","keywords":"","body":"zabbix3.4默认键值与自定义告警变量 zabbix3.4默认键值 在zabbix-server端使用zabbix_get命令获取对应主机键值结果 zabbix_get -s 客户端IP -k 键值名称 键值 含义 agent.hostname 客户端主机名。返回字符串 agent.ping 客户端可达性检查。返回 nothing - 不可达；1 - 可达 agent.version zabbix客户端（agent）的版本。返回字符串 kernel.maxfiles 操作系统最大的文件打开数量。返回整数 kernel.maxproc 操作系统最大的进程数。返回整数 net.dns[,name,,,,] 检查 DNS 服务是否开启。返回 0 - DNS 服务关闭（服务未响应或DNS解析失败）；1 - DNS 服务开启 net.dns.record[,name,,,,] 执行DNS查询。返回字符串信息 net.if.collisions[if] 网络冲突数量。返回整型 net.if.in[if,] 网络接口上传流量统计。返回 整数 net.if.list 网络接口列表（包括接口类型，状态，IPv4地址，说明）。返回文本 net.if.out[if,] 上行流量统计。返回整数 net.if.total[if,] 网络接口上传下载的流量总和。返回整数 net.tcp.listen[port] 检查 TCP 端口 是否处于侦听状态。返回 0 - 未侦听；1 - 正在侦听 net.tcp.port[,port] 检查是否能建立 TCP 连接到指定端口。返回 0 - 不能连接；1 - 可以连接 net.tcp.service[service,,] 检查服务是否运行并接受 TCP 连接。返回 0 - 服务关闭；1 - 服务运行 net.tcp.service.perf[service,,] 检查 TCP 服务的性能，当服务 down 时返回 0，否则返回连接服务花费的秒数 net.udp.listen[port] 检查 UDP 端口 是否处于侦听状态。返回 0 - 未侦听；1 - 正在侦听 net.udp.service[service,,] 检查服务是否运行并响应 UDP 请求。返回 0 - 服务关闭；1 - 服务运行 net.udp.service.perf[service,,] 检查 UDP 服务的性能，当服务 down 时返回 0，否则返回连接到服务花费的秒数 perf_counter[counter,] 所有Windows计数器值。返回 整形、浮点、字符串、文本 proc.cpu.util[,,,,,] CPU进程百分比。返回浮点值 proc.mem[,,,,] 进程内存，以字节为单位。返回整数 proc.num[,,,] 进程数。返回整数 proc_info[process,,] Various information about specific process(es). Returns float sensor[device,sensor,] 硬件传感器读数。返回浮点型 service.info[service,] Information about a service. Returns integer with param as state, startup; string - with param as displayname, path, user; text - with param as description; Specifically for state: 0 - running, 1 - paused, 2 - start pending, 3 - pause pending, 4 - continue pending, 5 - stop pending, 6 - stopped, 7 - unknown, 255 - no such service; Specifically for startup: 0 - automatic, 1 - automatic delayed, 2 - manual, 3 - disabled, 4 - unknown services[,,] 列表服务。返回0表示空，如果是列表则是每行一个内容 system.boottime 系统启动时间。返回时间戳 system.cpu.intr 设备的中断数。返回整数 system.cpu.load[,] CPU 负载。返回浮点数 system.cpu.num[] CPU 数量，返回整数 system.cpu.switches 上下文的数量进行切换。它返回一个整数值。 system.cpu.util[,,] CPU 使用率。返回浮点数 system.hostname[] 系统主机名。返回字符串 system.hw.chassis[] 机架信息。返回字符串 system.hw.cpu[,] CPU 信息。返回字符串或整数 system.hw.devices[] PCI或者USB设备列表。返回文本 system.hw.macaddr[,] MAC地址。返回字符串 system.localtime[] 系统时间。返回的是UTC整数。是服务器本地的时间 system.run[command,] 即在主机上指定的命令的执行。返回命令执行结果的文本值。如果指定NOWAIT的模式，这将返回执行命令的结果1。 system.stat[resource,] 系统统计数据。返回整数值或者浮点值 system.sw.arch 软件架构信息。返回字符串 system.sw.os[] 操作系统信息。返回字符串 system.sw.packages[,,] 安装包列表。返回文本 system.swap.in[,] 在交换分区(swap)（从设备到内存）统计数据。返回整数 system.swap.out[,] 交换分区（从内存到设备）的统计数据。返回整数 system.swap.size[,] 交换分区空间大小，字节或从总百分比。从字节返回整数;对于浮动比例 system.uname 识别系统。返回 字符串 system.uptime 系统启动时间。返回整数 system.users.num 已登录的用户数量，返回整数 vfs.dev.read[,,] 磁盘读取数据。类型是sectors, operations, bytes;返回整数，类型是 sps, ops, bps则返回浮点。 vfs.dev.write[,,] 磁盘写入数据。类型是sectors, operations, bytes;返回整数，类型是 sps, ops, bps则返回浮点。 vfs.dir.size[dir,,,,] Directory size (in bytes). Returns integer vfs.file.cksum[file] 文件效验，unix标准算法。返回整数 vfs.file.contents[file,] 搜索文件内容。返回文本 vfs.file.exists[file] 检查文件是否存在。返回 0 - 未找到文件；1 - 常规文件或链接（软/硬）存在 vfs.file.md5sum[file] 文件的MD5校验。返回字符串（该文件的MD5哈希值） vfs.file.regexp[file,regexp,,,,] 查找文件中的字符串，返回内容是被匹配内容的整行字符串，或者其他可选参数 vfs.file.regmatch[file,regexp,,,] 查找文件中的字符串，如果有则返回1，没有则返回0。 vfs.file.size[file] 文件大小（单位bytes）。返回整数 vfs.file.time[file,] 文件事件信息，返回的是时间戳 整数 vfs.fs.inode[fs,] 数或inode的百分比。返回数字，如果是浮点则是百分比 vfs.fs.size[fs,] 磁盘容量。如果返回的是字节则是整数，如果返回的是百分比则是浮点。 vm.memory.size[] 从字节或总百分比的内存大小。它返回一个整数值，如果字节，只要百分比返回浮点值。 vm.vmemory.size[] 虚拟空间大小（以字节计）或百分比（总计）。 返回整型字节; 浮点百分比 web.page.get[host,,] 获取网页。返回信息为网页源码或者TXT web.page.perf[host,,] 全网页加载时间（秒）。返回浮点型 web.page.regexp[host,,,,,] 查找网页中的字符串，返回内容是被匹配内容的整行字符串，或者其他可选参数 wmi.get[,] 执行 WMI 查询返回第一个对象。返回整形、浮点、字符串或者文本内容 abbix3.4自定义告警变量 zabbix3.4自定义告警变量 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/zabbix/zabbix3.4/14.zabbix3.4自动化监控.html":{"url":"linux/monitor/zabbix/zabbix3.4/14.zabbix3.4自动化监控.html","title":"zabbix3.4自动化监控","keywords":"","body":"[toc] zabbix3.4自动化监控 1.zabbix主动被动模式区别 1.1主动模式与被动模式针对的是agent 主动模式 (Zabbix-agent主动上报给Zabbix-server) agent向server主动请求对应监控项列表，在本机收集对应的监控数据，提交给server/proxy 主动模式不启用10050端口 被动模式 (Zabbix-server轮询检测zabbix-agent) agent打开一个端口，默认为10050，等待server来获取数据，然后agent进行数据收集再发送到server 1.2主动模式与被动模式选择如何选择 当Queue里有大量延迟的监控项 当监控主机超过300+, 建议使用主动模式 2.zabbix被动模式: zabbix默认是被动模式,假如有100个监控, 需要100个回合 3.zabbix主动模式 3.1修改zabbix为主动模式 第一步、修改agent端端配置文件/etc/zabbix/zabbix_agentd.conf #修改以下两项 ServerActive=zabbix-server的IP地址 Hostname=zabbix-agent的主机名 第二步、zabbix web界面修改agent基础模版为Active 全克隆被动模式模版Template OS Linux 配置-->模版-->搜索Template OS Linux 点击全克隆 修改名称为Template OS Linux Active 点击克隆后的Active模版-->监控项-->全选-->批量更新-->类型-->选择主动式-->更新 更新后的类型为zabbix客户端主动式 第三步、agent主机引用主动模式的模版 配置-->主机-->模版-->清理之前的默认模版Template OS Linux选择主动模式的模版Template OS Linux Active-->更新 修改agent主机配置文件/etc/zabbix/zabbix_agentd.conf //注释Server= #Server=10.0.0.200 //修改ServerActive一项为server端IP地址 ServerActive=10.0.0.200 //修改Hostname 这里修改为server web端主机的名称 Hostname=test1-10.0.0.10 //将StartAgents一项修改为0，表示启用主动模式 StartAgents=0 //重启agent systemctl restart zabbix-agent #⚠️启用主动模式后，agent端就不会监听本地10050端口 4.zabbix自动注册 4.1zabbix自动注册说明 zabbix3.4自动注册官方手册 zabbix-agent可以自动注册到服务器进行监控。这种方式无需在服务器上手动配置它们。 4.2配置步骤 第一步、配置zabbix-agent指定zabbix-server 修改zabbix-agent配置文件/etc/zabbix/zabbix_agentd.conf，将Server和ServerActive都指定为zabbix-server 的IP，修改Hostname为zabbix-agent的主机名 [root@zabbix-agent ~]# vim /etc/zabbix/zabbix_agentd.conf Server= ServerActive= Hostname= [root@zabbix-agent ~]# systemctl restart zabbix-agent ServerActive即为主动模式 注意： 必须指定hostname，否则无法发现主机 第二步、单击配置->动作，选择自动注册为事件源，然后单击创建动作 第三步、添加动作条件 添加后的自动注册操作 稍等几分钟，主机名似k8s的主机就会自动注册到server端 另外一种方式，不修改agent端配置文件/etc/zabbix/zabbix_agentd.conf中的hostname，触发条件写主机名称似IP地址，但是这样在web界面显示的主机名称是默认的Zabbix Server，感觉有些low 5.zabbix自动发现 5.1自动发现说明 zabbix3.4网络发现官方手册 网络发现由两个阶段组成:发现discovery和动作actions ⚠️自动发现的缺点 自动发现中填写的IP地址范围是根据zabbix-server端的子网掩码去扫对应的主机，但是这样有缺陷，这样只能去扫跟zabbix-server端子网掩码相同段的主机，如果agent划分了子网掩码，这样就无法扫全主机 5.2配置步骤 第一步、配置-->自动发现-->启用默认的发现规则(或者新建发现规则、克隆默认规则) 这里选择克隆系统默认规则并改名 克隆后的自动发现规则 第二步、配置-->动作-->创建动作 操作信息 #默认标题 自动发现主机IP:{DISCOVERY.DEVICE.IPADDRESS} #消息内容 客户端名称:{DISCOVERY.SERVICE.NAME} 客户端端口:{DISCOVERY.SERVICE.PORT} 客户端状态:{DISCOVERY.SERVICE.STATUS} 稍等几分钟就会自动发主机 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/zabbix/zabbix5.0/安装/标准安装.html":{"url":"linux/monitor/zabbix/zabbix5.0/安装/标准安装.html","title":"标准安装","keywords":"","body":"[toc] CentOS7安装zabbix5.0 zabbix5.0标准安装官方文档 一、标准安装 1.1 添加yum源 rpm -Uvh https://repo.zabbix.com/zabbix/5.0/rhel/7/x86_64/zabbix-release-5.0-1.el7.noarch.rpm 1.2 安装zabbix-server和zabbix-agent yum -y install zabbix-server-mysql zabbix-agent 1.3 安装Zabbix frontend 启用红帽软件集合 yum -y install centos-release-scl 1.4 编辑配置文件/etc/yum.repos.d/zabbix.repo以使用zabbix-frontend库 [zabbix-frontend] enabled=1 #使用如下命令修改 sed -i '11s/0/1/' /etc/yum.repos.d/zabbix.repo 1.5 安装Zabbix frontend包 yum -y install zabbix-web-mysql-scl zabbix-apache-conf-scl 1.6 创建数据库 mysql -e \"create database zabbix character set utf8 collate utf8_bin;\" mysql -e \"create user zabbix@localhost identified by 'zabbix';\" mysql -e \"grant all privileges on zabbix.* to zabbix@localhost;\" 1.7 导入数据库 zcat /usr/share/doc/zabbix-server-mysql*/create.sql.gz | mysql -uzabbix -p zabbix 1.8 为Zabbix server配置数据库 编辑配置文件/etc/zabbix/zabbix_server.conf //配置数据库密码 DBPassword=zabbix #使用如下命令 sed -i.bak '/# DBPassword=/c DBPassword=zabbix' /etc/zabbix/zabbix_server.conf 1.9 为Zabbix前端配置PHP 编辑配置文件/etc/opt/rh/rh-php72/php-fpm.d/zabbix.conf设置时区 sed -i.bak '/^;/c php_value[date.timezone] = Asia/Shanghai' /etc/opt/rh/rh-php72/php-fpm.d/zabbix.conf 1.10 给/etc/zabbix所有文件权限设置为zabbix chown -R zabbix.zabbix /etc/zabbix/ 1.11 启动服务 ⚠️Apache默认监听80端口，如果本机80端口被占用(例如已经安装nginx)，则需要修改apache配置文件 /etc/httpd/conf/httpd.conf sed -i.bak 's/^Listen 80/Listen 81/' /etc/httpd/conf/httpd.conf systemctl restart zabbix-server zabbix-agent httpd rh-php72-php-fpm systemctl enable zabbix-server zabbix-agent httpd rh-php72-php-fpm 1.12 完成安装 ⚠️如果安装的时候选择的是Apache，则访问的url中需要加 /zabbix，即 IP:端口/zabbix 如果选择的是Nginx则不需要，即 IP:端口 第一步、浏览器访问 IP:端口/zabbix 点击Next step 第二步、检查配置，要全部OK才可以 第三步、配置数据库信息 第四步、zabbix-server名称配置 第五步、确认配置信息 如果遇到这个则是权限问题，点击链接下载配置文件并存放至/etc/zabbix/web目录下，并且权限设置为644，属主和属组都是zabbix 第七步、完成安装 第八步、登陆zabbix 用户名Admin 密码zabbix 登陆后首界面 设置中文 点击左下角User settings，Language处选择Chinese(zh_CN)，然后点击Update 1.13 修改中文乱码问题 点击主机-->图形，会看到有乱码 解决方法 从windows找到楷体字体simkai，搜索楷体即可 windows路径 c盘-->Windows-->Fonts mac路径/Library/Fonts 在zabbix-server上备份zabbix默认字体并且上传新字体 [root@zabbix-server ~]# cd /usr/share/fonts/dejavu/ [root@zabbix-server dejavu]# ls DejaVuSans-BoldOblique.ttf DejaVuSansCondensed-BoldOblique.ttf DejaVuSansCondensed-Oblique.ttf DejaVuSans-ExtraLight.ttf DejaVuSans.ttf DejaVuSans-Bold.ttf DejaVuSansCondensed-Bold.ttf DejaVuSansCondensed.ttf DejaVuSans-Oblique.ttf #然后上传字体，修改名称为DejaVuSans.ttf [root@zabbix-server dejavu]# mv DejaVuSans.ttf DejaVuSans.ttf.bak [root@zabbix-server dejavu]# mv simkai.ttf DejaVuSans.ttf #注意字体的权限要让zabbix用户可以读 [root@zabbix-server dejavu]# chmod 644 DejaVuSans.ttf [root@zabbix-server dejavu]# ll DejaVuSans.ttf -rw-r--r-- 1 root root 19647736 Jan 13 16:29 DejaVuSans.ttf 浏览器刷新验证 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/zabbix/zabbix5.0/安装/docker-compose安装.html":{"url":"linux/monitor/zabbix/zabbix5.0/安装/docker-compose安装.html","title":"docker-compose安装","keywords":"","body":"[toc] CentOS7安装zabbix5.2 二、docker-compose安装 zabbix官方安装文档 zabbix github docker地址 2.1 下载代码 git clone https://github.com.cnpmjs.org/zabbix/zabbix-docker.git 2.2 检出分支，这里选择最新版5.2 cd zabbix-docker/ git checkout 5.2 2.2.1 yaml文件说明 一共有12个yaml文件，每个yaml文件官方都有说明 文件中带 _latest 的是从dockerhub拉取镜像 文件中带 _local 的是在本地构建镜像 File name Description Translation docker-compose_v3_alpine_mysql_latest.yaml The compose file runs the latest version of Zabbix 5.2 components on Alpine Linux with MySQL database support. compose文件在支持MySQL数据库的Alpine Linux上运行最新版本的Zabbix 5.2组件。 docker-compose_v3_alpine_mysql_local.yaml The compose file locally builds the latest version of Zabbix 5.2 and runs Zabbix components on Alpine Linux with MySQL database support. compose文件在本地构建最新版本的Zabbix 5.2，并在支持MySQL数据库的Alpine Linux上运行Zabbix组件。 docker-compose_v3_alpine_pgsql_latest.yaml The compose file runs the latest version of Zabbix 5.2 components on Alpine Linux with PostgreSQL database support. compose文件在支持PostgreSQL数据库的Alpine Linux上运行最新版本的Zabbix 5.2组件。 docker-compose_v3_alpine_pgsql_local.yaml The compose file locally builds the latest version of Zabbix 5.2 and runs Zabbix components on Alpine Linux with PostgreSQL database support. compose文件在本地构建最新版本的Zabbix 5.2，并在支持PostgreSQL数据库的Alpine Linux上运行Zabbix组件。 docker-compose_v3_centos_mysql_latest.yaml The compose file runs the latest version of Zabbix 5.2 components on CentOS 8 with MySQL database support. compose文件在CentOS 8上运行最新版本的Zabbix 5.2组件，支持MySQL数据库。 docker-compose_v3_centos_mysql_local.yaml The compose file locally builds the latest version of Zabbix 5.2 and runs Zabbix components on CentOS 8 with MySQL database support. compose文件在本地构建Zabbix 5.2的最新版本，并在CentOS 8上运行Zabbix组件，支持MySQL数据库。 docker-compose_v3_centos_pgsql_latest.yaml The compose file runs the latest version of Zabbix 5.2 components on CentOS 8 with PostgreSQL database support. compose文件在CentOS 8上运行最新版本的Zabbix 5.2组件，支持PostgreSQL数据库。 docker-compose_v3_centos_pgsql_local.yaml The compose file locally builds the latest version of Zabbix 5.2 and runs Zabbix components on CentOS 8 with PostgreSQL database support. compose文件在本地构建最新版本的Zabbix 5.2，并在CentOS 8上运行带有PostgreSQL数据库支持的Zabbix组件。 docker-compose_v3_ubuntu_mysql_latest.yaml The compose file runs the latest version of Zabbix 5.2 components on Ubuntu 20.04 with MySQL database support. compose文件在支持MySQL数据库的Ubuntu 20.04上运行Zabbix 5.2组件的最新版本。 docker-compose_v3_ubuntu_mysql_local.yaml The compose file locally builds the latest version of Zabbix 5.2 and runs Zabbix components on Ubuntu 20.04 with MySQL database support. compose文件在本地构建Zabbix 5.2的最新版本，并在支持MySQL数据库的Ubuntu 20.04上运行Zabbix组件。 docker-compose_v3_ubuntu_pgsql_latest.yaml The compose file runs the latest version of Zabbix 5.2 components on Ubuntu 20.04 with PostgreSQL database support. compose文件在支持PostgreSQL数据库的Ubuntu 20.04上运行Zabbix 5.2组件的最新版本。 docker-compose_v3_ubuntu_pgsql_local.yaml The compose file locally builds the latest version of Zabbix 5.2 and runs Zabbix components on Ubuntu 20.04 with PostgreSQL database support. compose文件在本地构建Zabbix 5.2的最新版本，并在支持PostgreSQL数据库的Ubuntu 20.04上运行Zabbix组件。 选择任意一个都可以，这里以 docker-compose_v3_alpine_mysql_latest.yaml为例，包含的镜像一共有11个，实际只需要7个 服务名 镜像 作用 是否必须 zabbix-server zabbix/zabbix-server-mysql:alpine-5.2-latest Zabbix软件的核心进程，执行监控操作，与Zabbix proxies和Agents进行交互、触发器计算、发送告警通知；也是数据的中央存储库 必须 zabbix-proxy-sqlite3 zabbix/zabbix-proxy-sqlite3:alpine-5.2-latest 代替Zabbix Server采集数据，从而分担Zabbix Server负载的进程 2个zabbix-proxy选择其中1个 zabbix-proxy-mysql zabbix/zabbix-proxy-mysql:alpine-5.2-latest 代替Zabbix Server采集数据，从而分担Zabbix Server负载的进程 2个zabbix-proxy选择其中1个 zabbix-web-apache-mysql zabbix/zabbix-web-apache-mysql:alpine-5.2-latest web界面 2个zabbix-web选择其中1个 zabbix-web-nginx-mysql zabbix/zabbix-web-nginx-mysql:alpine-5.2-latest web界面 2个zabbix-web选择其中1个 zabbix-agent zabbix/zabbix-agent:alpine-5.2-latest 部署在被监控目标上，用于主动监控本地资源和应用程序，并将收集的数据发送给 Zabbix server 必须 zabbix-java-gateway zabbix/zabbix-java-gateway:alpine-5.2-latest 以 Zabbix 守护进程方式原生支持监控 JMX 应用程序 必须 zabbix-snmptraps zabbix/zabbix-snmptraps:alpine-5.2-latest 用于设备发生故障时的主动通知的监控 必须 mysql-server mysql:8.0 数据库 必须 db_data_mysql busybox 不太清楚 非必须，启动后停止 elasticsearch elasticsearch 没有用到 非必须，默认注释 docker-compose_v3_alpine_mysql_latest.yaml文件中的镜像 2.3 启动 2.3.1 修改yaml文件 任意选择一个yaml文件，需要做一些修改，zabbix-proxy、zabbix-web选择一个即可，修改后的docker-compose_v3_alpine_mysql_latest.yaml如下 在文件中zabbix-server的持久化配置处添加了 - ./zbx_env/etc/conf:/etc/zabbix:rw，目的是把zabbix-server的主配置文件映射到宿主机 文件中的镜像，官方默认最新版都是latest结尾的，这里修改为具体的版本，可在 dockerhub官网查看 官方默认镜像 修改为 zabbix/zabbix-server-mysql:alpine-5.2-latest zabbix/zabbix-server-mysql:alpine-5.2.5 zabbix/zabbix-proxy-sqlite3:alpine-5.2-latest - zabbix/zabbix-proxy-mysql:alpine-5.2-latest zabbix/zabbix-proxy-mysql:alpine-5.2.5 zabbix/zabbix-web-apache-mysql:alpine-5.2-latest - zabbix/zabbix-web-nginx-mysql:alpine-5.2-latest zabbix/zabbix-web-nginx-mysql:alpine-5.2.5 zabbix/zabbix-agent:alpine-5.2-latest zabbix/zabbix-agent:alpine-5.2.5 zabbix/zabbix-java-gateway:alpine-5.2-latest zabbix/zabbix-java-gateway:alpine-5.2.5 zabbix/zabbix-snmptraps:alpine-5.2-latest zabbix/zabbix-snmptraps:alpine-5.2.5 mysql:8.0 - busybox - elasticsearch - 这里遇到一个问题，就是zabbix-server处增加了 - ./zbx_env/etc/conf:/etc/zabbix:rw，是想把zabbix_server容器中 /etc/zabbix/zabbix_server.conf主配置文件映射到本地，但是始终报错 zabbix_server [7]: cannot open config file \"/etc/zabbix/zabbix_server.conf\": [2] No such file or directory，原因未知，无法解决 version: '3.5' services: zabbix-server: image: zabbix/zabbix-server-mysql:alpine-5.2-latest ports: - \"10051:10051\" volumes: - /etc/localtime:/etc/localtime:ro - /etc/timezone:/etc/timezone:ro - ./zbx_env/usr/lib/zabbix/alertscripts:/usr/lib/zabbix/alertscripts:ro - ./zbx_env/usr/lib/zabbix/externalscripts:/usr/lib/zabbix/externalscripts:ro - ./zbx_env/var/lib/zabbix/export:/var/lib/zabbix/export:rw - ./zbx_env/var/lib/zabbix/modules:/var/lib/zabbix/modules:ro - ./zbx_env/var/lib/zabbix/enc:/var/lib/zabbix/enc:ro - ./zbx_env/var/lib/zabbix/ssh_keys:/var/lib/zabbix/ssh_keys:ro - ./zbx_env/var/lib/zabbix/mibs:/var/lib/zabbix/mibs:ro # 这里添加后会有问题，无法持久化zabbix-server主配置文件 # - ./zbx_env/etc/conf:/etc/zabbix:rw - snmptraps:/var/lib/zabbix/snmptraps:rw links: - mysql-server:mysql-server - zabbix-java-gateway:zabbix-java-gateway ulimits: nproc: 65535 nofile: soft: 20000 hard: 40000 deploy: resources: limits: cpus: '0.70' memory: 1G reservations: cpus: '0.5' memory: 512M env_file: - .env_db_mysql - .env_srv secrets: - MYSQL_USER - MYSQL_PASSWORD - MYSQL_ROOT_PASSWORD # - client-key.pem # - client-cert.pem # - root-ca.pem depends_on: - mysql-server - zabbix-java-gateway - zabbix-snmptraps networks: zbx_net_backend: aliases: - zabbix-server - zabbix-server-mysql - zabbix-server-alpine-mysql - zabbix-server-mysql-alpine zbx_net_frontend: # devices: # - \"/dev/ttyUSB0:/dev/ttyUSB0\" stop_grace_period: 30s sysctls: - net.ipv4.ip_local_port_range=1024 65000 - net.ipv4.conf.all.accept_redirects=0 - net.ipv4.conf.all.secure_redirects=0 - net.ipv4.conf.all.send_redirects=0 labels: com.zabbix.description: \"Zabbix server with MySQL database support\" com.zabbix.company: \"Zabbix LLC\" com.zabbix.component: \"zabbix-server\" com.zabbix.dbtype: \"mysql\" com.zabbix.os: \"alpine\" #zabbix-proxy-sqlite3: # image: zabbix/zabbix-proxy-sqlite3:alpine-5.2-latest # ports: # - \"10061:10051\" # volumes: # - /etc/localtime:/etc/localtime:ro # - /etc/timezone:/etc/timezone:ro # - ./zbx_env/usr/lib/zabbix/externalscripts:/usr/lib/zabbix/externalscripts:ro # - ./zbx_env/var/lib/zabbix/modules:/var/lib/zabbix/modules:ro # - ./zbx_env/var/lib/zabbix/enc:/var/lib/zabbix/enc:ro # - ./zbx_env/var/lib/zabbix/ssh_keys:/var/lib/zabbix/ssh_keys:ro # - ./zbx_env/var/lib/zabbix/mibs:/var/lib/zabbix/mibs:ro # - snmptraps:/var/lib/zabbix/snmptraps:rw # links: # - zabbix-server:zabbix-server # - zabbix-java-gateway:zabbix-java-gateway # ulimits: # nproc: 65535 # nofile: # soft: 20000 # hard: 40000 # deploy: # resources: # limits: # cpus: '0.70' # memory: 512M # reservations: # cpus: '0.3' # memory: 256M # env_file: # - .env_prx # - .env_prx_sqlite3 # depends_on: # - zabbix-java-gateway # - zabbix-snmptraps # networks: # zbx_net_backend: # aliases: # - zabbix-proxy-sqlite3 # - zabbix-proxy-alpine-sqlite3 # - zabbix-proxy-sqlite3-alpine # zbx_net_frontend: # stop_grace_period: 30s # labels: # com.zabbix.description: \"Zabbix proxy with SQLite3 database support\" # com.zabbix.company: \"Zabbix LLC\" # com.zabbix.component: \"zabbix-proxy\" # com.zabbix.dbtype: \"sqlite3\" # com.zabbix.os: \"alpine\" zabbix-proxy-mysql: image: zabbix/zabbix-proxy-mysql:alpine-5.2-latest ports: - \"10071:10051\" volumes: - /etc/localtime:/etc/localtime:ro - /etc/timezone:/etc/timezone:ro - ./zbx_env/usr/lib/zabbix/externalscripts:/usr/lib/zabbix/externalscripts:ro - ./zbx_env/var/lib/zabbix/modules:/var/lib/zabbix/modules:ro - ./zbx_env/var/lib/zabbix/enc:/var/lib/zabbix/enc:ro - ./zbx_env/var/lib/zabbix/ssh_keys:/var/lib/zabbix/ssh_keys:ro - ./zbx_env/var/lib/zabbix/mibs:/var/lib/zabbix/mibs:ro - snmptraps:/var/lib/zabbix/snmptraps:rw links: - zabbix-server:zabbix-server - zabbix-java-gateway:zabbix-java-gateway ulimits: nproc: 65535 nofile: soft: 20000 hard: 40000 deploy: resources: limits: cpus: '0.70' memory: 512M reservations: cpus: '0.3' memory: 256M env_file: - .env_db_mysql_proxy - .env_prx - .env_prx_mysql depends_on: - mysql-server - zabbix-java-gateway - zabbix-snmptraps secrets: - MYSQL_USER - MYSQL_PASSWORD - MYSQL_ROOT_PASSWORD # - client-key.pem # - client-cert.pem # - root-ca.pem networks: zbx_net_backend: aliases: - zabbix-proxy-mysql - zabbix-proxy-alpine-mysql - zabbix-proxy-mysql-alpine zbx_net_frontend: stop_grace_period: 30s labels: com.zabbix.description: \"Zabbix proxy with MySQL database support\" com.zabbix.company: \"Zabbix LLC\" com.zabbix.component: \"zabbix-proxy\" com.zabbix.dbtype: \"mysql\" com.zabbix.os: \"alpine\" # zabbix-web-apache-mysql: # image: zabbix/zabbix-web-apache-mysql:alpine-5.2-latest # ports: # - \"80:8080\" # - \"443:8443\" # links: # - mysql-server:mysql-server # - zabbix-server:zabbix-server # volumes: # - /etc/localtime:/etc/localtime:ro # - /etc/timezone:/etc/timezone:ro # - ./zbx_env/etc/ssl/apache2:/etc/ssl/apache2:ro # - ./zbx_env/usr/share/zabbix/modules/:/usr/share/zabbix/modules/:ro # deploy: # resources: # limits: # cpus: '0.70' # memory: 512M # reservations: # cpus: '0.5' # memory: 256M # env_file: # - .env_db_mysql # - .env_web # secrets: # - MYSQL_USER # - MYSQL_PASSWORD ## - client-key.pem ## - client-cert.pem ## - root-ca.pem # depends_on: # - mysql-server # - zabbix-server # healthcheck: # test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8080/\"] # interval: 10s # timeout: 5s # retries: 3 # start_period: 30s # networks: # zbx_net_backend: # aliases: # - zabbix-web-apache-mysql # - zabbix-web-apache-alpine-mysql # - zabbix-web-apache-mysql-alpine # zbx_net_frontend: # stop_grace_period: 10s # sysctls: # - net.core.somaxconn=65535 # labels: # com.zabbix.description: \"Zabbix frontend on Apache web-server with MySQL database support\" # com.zabbix.company: \"Zabbix LLC\" # com.zabbix.component: \"zabbix-frontend\" # com.zabbix.webserver: \"apache2\" # com.zabbix.dbtype: \"mysql\" # com.zabbix.os: \"alpine\" zabbix-web-nginx-mysql: image: zabbix/zabbix-web-nginx-mysql:alpine-5.2-latest ports: - \"8081:8080\" - \"8443:8443\" links: - mysql-server:mysql-server - zabbix-server:zabbix-server volumes: - /etc/localtime:/etc/localtime:ro - /etc/timezone:/etc/timezone:ro - ./zbx_env/etc/ssl/nginx:/etc/ssl/nginx:ro - ./zbx_env/usr/share/zabbix/modules/:/usr/share/zabbix/modules/:ro deploy: resources: limits: cpus: '0.70' memory: 512M reservations: cpus: '0.5' memory: 256M env_file: - .env_db_mysql - .env_web secrets: - MYSQL_USER - MYSQL_PASSWORD # - client-key.pem # - client-cert.pem # - root-ca.pem depends_on: - mysql-server - zabbix-server healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8080/\"] interval: 10s timeout: 5s retries: 3 start_period: 30s networks: zbx_net_backend: aliases: - zabbix-web-nginx-mysql - zabbix-web-nginx-alpine-mysql - zabbix-web-nginx-mysql-alpine zbx_net_frontend: stop_grace_period: 10s sysctls: - net.core.somaxconn=65535 labels: com.zabbix.description: \"Zabbix frontend on Nginx web-server with MySQL database support\" com.zabbix.company: \"Zabbix LLC\" com.zabbix.component: \"zabbix-frontend\" com.zabbix.webserver: \"nginx\" com.zabbix.dbtype: \"mysql\" com.zabbix.os: \"alpine\" zabbix-agent: image: zabbix/zabbix-agent:alpine-5.2-latest ports: - \"10050:10050\" volumes: - /etc/localtime:/etc/localtime:ro - /etc/timezone:/etc/timezone:ro - ./zbx_env/etc/zabbix/zabbix_agentd.d:/etc/zabbix/zabbix_agentd.d:ro - ./zbx_env/var/lib/zabbix/modules:/var/lib/zabbix/modules:ro - ./zbx_env/var/lib/zabbix/enc:/var/lib/zabbix/enc:ro - ./zbx_env/var/lib/zabbix/ssh_keys:/var/lib/zabbix/ssh_keys:ro links: - zabbix-server:zabbix-server deploy: resources: limits: cpus: '0.2' memory: 128M reservations: cpus: '0.1' memory: 64M mode: global env_file: - .env_agent privileged: true pid: \"host\" networks: zbx_net_backend: aliases: - zabbix-agent - zabbix-agent-passive - zabbix-agent-alpine stop_grace_period: 5s labels: com.zabbix.description: \"Zabbix agent\" com.zabbix.company: \"Zabbix LLC\" com.zabbix.component: \"zabbix-agentd\" com.zabbix.os: \"alpine\" zabbix-java-gateway: image: zabbix/zabbix-java-gateway:alpine-5.2-latest ports: - \"10052:10052\" deploy: resources: limits: cpus: '0.5' memory: 512M reservations: cpus: '0.25' memory: 256M env_file: - .env_java networks: zbx_net_backend: aliases: - zabbix-java-gateway - zabbix-java-gateway-alpine stop_grace_period: 5s labels: com.zabbix.description: \"Zabbix Java Gateway\" com.zabbix.company: \"Zabbix LLC\" com.zabbix.component: \"java-gateway\" com.zabbix.os: \"alpine\" zabbix-snmptraps: image: zabbix/zabbix-snmptraps:alpine-5.2-latest ports: - \"162:1162/udp\" volumes: - snmptraps:/var/lib/zabbix/snmptraps deploy: resources: limits: cpus: '0.5' memory: 256M reservations: cpus: '0.25' memory: 128M networks: zbx_net_frontend: aliases: - zabbix-snmptraps zbx_net_backend: stop_grace_period: 5s labels: com.zabbix.description: \"Zabbix snmptraps\" com.zabbix.company: \"Zabbix LLC\" com.zabbix.component: \"snmptraps\" com.zabbix.os: \"alpine\" mysql-server: image: mysql:8.0 command: - mysqld - --character-set-server=utf8 - --collation-server=utf8_bin - --default-authentication-plugin=mysql_native_password # - --require-secure-transport # - --ssl-ca=/run/secrets/root-ca.pem # - --ssl-cert=/run/secrets/server-cert.pem # - --ssl-key=/run/secrets/server-key.pem volumes: - ./zbx_env/var/lib/mysql:/var/lib/mysql:rw env_file: - .env_db_mysql secrets: - MYSQL_USER - MYSQL_PASSWORD - MYSQL_ROOT_PASSWORD # - server-key.pem # - server-cert.pem # - root-ca.pem stop_grace_period: 1m networks: zbx_net_backend: aliases: - mysql-server - zabbix-database - mysql-database db_data_mysql: image: busybox volumes: - ./zbx_env/var/lib/mysql:/var/lib/mysql:rw # elasticsearch: # image: elasticsearch # environment: # - transport.host=0.0.0.0 # - discovery.zen.minimum_master_nodes=1 # networks: # zbx_net_backend: # aliases: # - elasticsearch networks: zbx_net_frontend: driver: bridge driver_opts: com.docker.network.enable_ipv6: \"false\" ipam: driver: default config: - subnet: 172.16.238.0/24 zbx_net_backend: driver: bridge driver_opts: com.docker.network.enable_ipv6: \"false\" internal: true ipam: driver: default config: - subnet: 172.16.239.0/24 volumes: snmptraps: secrets: MYSQL_USER: file: ./.MYSQL_USER MYSQL_PASSWORD: file: ./.MYSQL_PASSWORD MYSQL_ROOT_PASSWORD: file: ./.MYSQL_ROOT_PASSWORD # client-key.pem: # file: ./.ZBX_DB_KEY_FILE # client-cert.pem: # file: ./.ZBX_DB_CERT_FILE # root-ca.pem: # file: ./.ZBX_DB_CA_FILE # server-cert.pem: # file: ./.DB_CERT_FILE # server-key.pem: # file: ./.DB_KEY_FILE 2.3.2 拷贝相关目录、文件 下载的官方代码中不是所有文件都是必须的 以下隐藏文件是 docker-compose.yaml 文件中各服务需要用到的环境变量文件 .env_agent .env_db_mysql .env_db_mysql_proxy .env_db_pgsql .env_java .env_prx .env_prx_mysql .env_prx_sqlite3 .env_srv .MYSQL_PASSWORD .MYSQL_ROOT_PASSWORD .MYSQL_USER .POSTGRES_PASSWORD .POSTGRES_USER .gitignore 现在把这些文件(包括环境变量文件和yaml文件)拷贝到一个目录，这里拷贝到 /data/docker-project/zabbix # 创建目录 [ -d /data/docker-project/zabbix ] || mkdir -p /data/docker-project/zabbix # 拷贝环境变量文件 cp -p .env_web .env_agent .env_db_mysql .env_db_mysql_proxy .env_db_pgsql .env_java .env_prx .env_prx_mysql .env_prx_sqlite3 .env_srv .gitignore .MYSQL_PASSWORD .MYSQL_ROOT_PASSWORD .MYSQL_USER .POSTGRES_PASSWORD .POSTGRES_USER /data/docker-project/zabbix # 拷贝yaml文件 cp docker-compose_v3_alpine_mysql_latest.yaml /data/docker-project/zabbix/docker-compose.yaml 2.3.3 启动 cd /data/docker-project/zabbix docker-compose up -d docker-compose安装遇到如下问题 官方默认的yaml文件中没有把zabbix-server的主配置文件持久化，在yaml文件中 zabbix-server服务下配置持久化 - ./zbx_env/etc/conf:/etc/zabbix:rw会导致 zabbix_zabbix-server_1容器退出，并报错 **** Configuration file '/etc/zabbix/zabbix_server.conf' does not exist ，zabbix_server [7]: cannot open config file \"/etc/zabbix/zabbix_server.conf\": [2] No such file or directory，无法解决 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/zabbix/zabbix5.0/安装/zabbix源码安装.html":{"url":"linux/monitor/zabbix/zabbix5.0/安装/zabbix源码安装.html","title":"源码安装","keywords":"","body":"zabbix源码安装 zabbix github地址 zabbix官网 zabbix源码官方下载地址 zabbix源码安装官方文档 1.下载源码 从 zabbix源码官方下载地址 下载源码包 wget https://cdn.zabbix.com/zabbix/sources/stable/5.4/zabbix-5.4.1.tar.gz 解压缩源码包 tar xf zabbix-5.4.1.tar.gz 2.创建用户 groupadd --system zabbix useradd --system -g zabbix -d /usr/lib/zabbix -s /sbin/nologin -c \"Zabbix Monitoring System\" zabbix ⚠️官方特别说明 Zabbix 进程不需要主目录，这就是我们不建议创建它的原因。但是，如果您正在使用某些需要它的功能（例如将 MySQL 凭据存储在 $HOME/.my.cnf 中），您可以使用以下命令自由创建它。 mkdir -m u=rwx,g=rwx,o= -p /usr/lib/zabbix chown zabbix:zabbix /usr/lib/zabbix Zabbix 前端安装不需要单独的用户帐户。 如果 Zabbix服务器和代理在同一台机器上运行，建议使用不同的用户来运行服务器而不是运行代理。否则，如果两者都以同一用户身份运行，则代理可以访问服务器配置文件，Zabbix 中的任何管理员级别用户都可以很容易地检索，例如，数据库密码。 ⚠️以root、bin或任何其他具有特殊权限的帐户运行 Zabbix存在安全风险。 3.创建ZABBIX数据库 3.1 安装mysql ⚠️源码安装的zabbix5.4中提示mysql版本需要5.7.28以上 3.1.1 下载MySQL-5.7.32二进制包 wget https://cdn.mysql.com/archives/mysql-5.7/mysql-5.7.32-linux-glibc2.12-x86_64.tar.gz 3.1.2 解压缩mysql二进制包到/usr/local tar xf mysql-5.7.32-linux-glibc2.12-x86_64.tar.gz -C /usr/local 3.1.3 修改名称、做软连接 mv /usr/local/mysql-5.7.32-linux-glibc2.12-x86_64 /usr/local/mysql-5.7.32 && ln -s /usr/local/mysql-5.7.32 /usr/local/mysql 3.1.4 创建mysql用户 useradd -M -s /bin/nologin mysql 3.1.5 编辑主配置文件，myql-5.7.32二进制包默认没有mysql配置文件 ⚠️如果指定了mysql的socket文件位置，则必须添加[client]标签并同时指定socket文件位置，否则客户端会从/tmp下找socket文件 # 备份/etc/my.cnf mv /etc/my.cnf /etc/my.cnf.old # 以下配置为最精简版，可根据实际情况进行相应设置 cat > /etc/my.cnf 3.1.6 创建socket文件目录 mkdir -p /var/lib/mysql 3.1.7 相关目录、文件授权 chown -R mysql.mysql /usr/local/mysql* /var/lib/mysql chown mysql.mysql /etc/my.cnf 3.1.8 拷贝启动脚本 cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysqld 3.1.9 初始化mysql /usr/local/mysql/bin/mysqld --initialize-insecure --user=mysql --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data ⚠️mysql5.7初始化没有提示！！！ 参数 说明 --user 指定mysql用户 --basedir 指定mysql安装目录 --datadir 指定mysql数据目录 --initialize-insecure 不生成随机密码 如遇初始化报错 /usr/local/mysql/bin/mysqld: error while loading shared libraries: libnuma.so.1: cannot open shared object file: No such file or directory，安装 numactl 包解决 yum -y install numactl 3.1.10 添加mysql命令环境变量 # 导出mysql命令环境变量 echo \"export PATH=/usr/local/mysql/bin:$PATH\" > /etc/profile.d/mysql.sh # 使配置生效 source /etc/profile 3.1.11 配置systemd管理mysql cat >> /etc/systemd/system/mysqld.service 3.1.12 启动mysql、检查启动 启动mysql # 重新加载systemd系统服务 systemctl daemon-reload # 启动mysql并加入开机自启 systemctl start mysqld && systemctl enable mysqld 查看 # 查看mysql端口 $ netstat -ntpl | grep 3306 tcp6 0 0 :::3306 :::* LISTEN 31349/mysqld 3.2 创建zabbix数据库 官方安装文档 mysql -uroot -e \"create database zabbix character set utf8 collate utf8_bin;\" mysql -uroot -e \"create user 'zabbix'@'localhost' identified by 'zabbix';\" mysql -uroot -e \"grant all privileges on zabbix.* to 'zabbix'@'localhost';\" 3.3 导入数据 如果您是从源安装 Zabbix，请继续将数据导入数据库。对于 Zabbix 代理数据库，只有schema.sql应该导入（没有 images.sql 和 data.sql） zabbix数据库文件在zabbix源码包的 cd zabbix-5.4.1/database/mysql/ 路径下 # 注意导入顺序，依次为 schema.sql、images.sql、data.sql，否则会有外键报错 cd zabbix-5.4.1/database/mysql/ mysql -uzabbix -pzabbix zabbix 4.编译安装 4.1 安装依赖 yum -y install libxml2-devel net-snmp-devel libevent-devel curl-devel 4.2 编译安装 使用命令 ./configure --help 查看所有支持的选项 解决依赖关系 ./configure --enable-server --enable-agent --with-mysql --enable-ipv6 --with-net-snmp --with-libcurl --with-libxml2 --prefix=/usr/local/zabbix ⚠️运行make install将默认在 /usr/local/sbin 中安装守护程序二进制文件（zabbix_server、zabbix_agentd、zabbix_proxy），在 /usr/local/bin 中安装客户端二进制文件（zabbix_get、zabbix_sender）。如果要指定与 /usr/local 不同的位置，需要添加参数 --prefix 来指定安装目录，例如 --prefix=/usr/local/zabbix。在这种情况下，守护程序二进制文件将安装在 /sbin 下，而实用程序将安装在 /bin 下。手册页将安装在 /share 下。 关于编译选项的说明： 如果使用 --enable-agent 选项，则编译命令行实用程序 zabbix_get 和 zabbix_sender。 虚拟机监控需要--with-libcurl 和--with-libxml2 配置选项；SMTP 身份验证和web.page.*Zabbix 代理项也需要 --with-libcurl 。请注意，使用 --with-libcurl 配置选项需要cURL 7.20.0 或更高版本。 Zabbix 总是使用 PCRE 库进行编译（从 3.4.0 版本开始）；安装它不是可选的。--with-libpcre=[DIR] 只允许指向特定的基本安装目录，而不是搜索 libpcre 文件的许多常见位置。 您可以使用 --enable-static 标志来静态链接库。如果您计划在不同的服务器之间分发已编译的二进制文件，则必须使用此标志使这些二进制文件无需所需的库即可工作。请注意， --enable-static 在Solaris 中不起作用。 构建服务器时不建议使用 --enable-static 选项。为了静态构建服务器，您必须拥有所需的每个外部库的静态版本。在配置脚本中没有严格检查。 在 MySQL 配置文件中添加可选路径 --with-mysql=//mysql_config 以在需要使用不在默认位置的 MySQL 客户端库时选择所需的 MySQL 客户端库。当在同一系统上安装了多个版本的 MySQL 或与 MySQL 一起安装了 MariaDB 时，它很有用。 使用 --with-oracle 标志指定 OCI API 的位置。 开始编译安装 ⚠️zabbix只需要运行 make install即可，不需要运行 make make install 5.查看、编辑配置文件 5.1 配置文件说明 使用 tree 命令查看目录结构 $ pwd /usr/local/zabbix $ tree . ├── bin │ ├── zabbix_get │ ├── zabbix_js │ └── zabbix_sender ├── etc │ ├── zabbix_agentd.conf │ ├── zabbix_agentd.conf.d │ ├── zabbix_server.conf │ └── zabbix_server.conf.d ├── lib │ └── modules ├── sbin │ ├── zabbix_agentd │ └── zabbix_server └── share ├── man │ ├── man1 │ │ ├── zabbix_get.1 │ │ └── zabbix_sender.1 │ └── man8 │ ├── zabbix_agentd.8 │ └── zabbix_server.8 └── zabbix ├── alertscripts └── externalscripts zabbix_server配置文件路径为 /usr/local/zabbix/etc/zabbix_server.conf zabbix_agent配置文件路径为 /usr/local/zabbix/etc/zabbix_agentd.conf zabbix_server命令路径为 /usr/local/zabbix/sbin/zabbix_server zabbix_agentd命令路径为 /usr/local/zabbix/sbin/zabbix_agentd 5.2 编辑配置文件 5.2.1 编辑 zabbix_server 配置文件 编辑 zabbix_server 配置文件 /usr/local/zabbix/etc/zabbix_server.conf ，指定zabbix数据库用户名和密码，修改 # DBPassword= 一行，可以使用如下命令修改(DBName和DBUser默认均为zabbix) # 修改zabbix数据库密码 sed -i '/# DBPassword=/c DBPassword=zabbix' /usr/local/zabbix/etc/zabbix_server.conf # 修改日志文件位置，默认为 /tmp/zabbix_server.log，这里修改为 /var/log/zabbix/zabbix_server.log sed -i '/^LogFile/cLogFile=/var/log/zabbix/zabbix_server.log' /usr/local/zabbix/etc/zabbix_server.conf # 创建日志目录并设置目录所有者为zabbix mkdir /var/log/zabbix && chown zabbix.zabbix /var/log/zabbix # 如果使用的是mysql，zabbix会默认去/tmp下找mysql.sock，本文的mysql.sock在/var/lib/mysql/mysql.sock，因此需要指定mysql.sock的路径 sed -i '/# DBSocket=/c DBSocket=\\/var\\/lib\\/mysql\\/mysql.sock' /usr/local/zabbix/etc/zabbix_server.conf 如果想要优化zabbix相关性能，可以参考 zabbix5.4官方性能调优文档 zabbix_server 配置文件内容如下 egrep -v '^$|#' /usr/local/zabbix/etc/zabbix_server.conf LogFile=/var/log/zabbix/zabbix_server.log DBName=zabbix DBUser=zabbix DBPassword=zabbix DBSocket=/var/lib/mysql/mysql.sock Timeout=4 LogSlowQueries=3000 StatsAllowedIP=127.0.0.1 5.2.2 编辑 zabbix_agent 配置文件 编辑 zabbix_agent 配置文件 /usr/local/zabbix/etc/zabbix_agentd.conf ，指定 zabbix_server 服务器地址，修改 Server=127.0.0.1 一行，默认为127.0.0.1，可以使用如下命令修改 # 修改zabbix_server地址 sed -i '/Server=127.0.0.1/cServer=10.0.0.11' /usr/local/zabbix/etc/zabbix_agentd.conf # 修改日志文件位置，默认为 /tmp/zabbix_agentd.log，这里修改为 /var/log/zabbix/zabbix_agentd.log sed -i '/^LogFile/cLogFile=/var/log/zabbix/zabbix_agentd.log' /usr/local/zabbix/etc/zabbix_agentd.conf zabbix_agentd 配置文件内容如下 $ egrep -v '^$|#' /usr/local/zabbix/etc/zabbix_agentd.conf LogFile=/var/log/zabbix/zabbix_agentd.log Server=10.0.0.11 ServerActive=127.0.0.1 Hostname=Zabbix server 6.启动zabbix 6.1 标准启动 启动 zabbix_server /usr/local/zabbix/sbin/zabbix_server -c /usr/local/zabbix/etc/zabbix_server.conf 启动 zabbix_agentd /usr/local/zabbix/sbin/zabbix_agentd -c /usr/local/zabbix/etc/zabbix_agentd.conf 6.2 使用supervisor管理zabbix 6.2.1 安装supervisor 执行以下一键安装脚本 #!/usr/bin/env bash TMP_FILE=/usr/lib/tmpfiles.d/tmp.conf # 安装supervisor最新版 yum -y install python2-pip && pip install supervisor # 创建目录 [ -d /etc/supervisor ] || mkdir /etc/supervisor # 创建supervisor配置文件 cat >/etc/supervisor/supervisord.conf /etc/logrotate.d/supervisor > $TMP_FILE fi # 将supervisor加入systemd cat >/usr/lib/systemd/system/supervisord.service 6.2.2 使用supervisor管理zabbix 如下配置实际上是可以成功启动服务的，但是命令行却报错，原因未知 编辑zabbix_server配置文件 cat > /etc/supervisor/config.d/zabbix_server.ini 编辑zabbix_agentd配置文件 cat > /etc/supervisor/config.d/zabbix_agentd.ini 6.3 验证zabbix启动 # 查看zabbix_server $ netstat -ntpl|grep 10051 tcp 0 0 0.0.0.0:10051 0.0.0.0:* LISTEN 9748/zabbix_server tcp6 0 0 :::10051 :::* LISTEN 9748/zabbix_server # 查看zabbix_agentd $netstat -ntpl|grep 10050 tcp 0 0 0.0.0.0:10050 0.0.0.0:* LISTEN 9902/zabbix_agentd tcp6 0 0 :::10050 :::* LISTEN 9902/zabbix_agent 7.安装php 7.1 添加第三方yum源 安装epel源并添加第三方yum源 yum -y install epel-release && \\ yum -y install https://rpms.remirepo.net/enterprise/remi-release-7.rpm 7.2 选择要安装的php版本 export phpversion=php73 yum -y install $phpversion-php-fpm $phpversion-php-cli $phpversion-php-bcmath $phpversion-php-gd $phpversion-php-json $phpversion-php-mbstring $phpversion-php-mcrypt $phpversion-php-mysqlnd $phpversion-php-opcache $phpversion-php-pdo $phpversion-php-pecl-crypto $phpversion-php-pecl-mcrypt $phpversion-php-pecl-geoip $phpversion-php-recode $phpversion-php-snmp $phpversion-php-soap $phpversion-php-xml 通过以下命令来获取更多安装信息 yum search php73 安装后的php配置文件路径 /etc/opt/remi/php73 7.3 启动php并设置开机自启 systemctl enable php73-php-fpm && systemctl start php73-php-fpm 8.配置zabbix web 8.1 安装nginx yum -y install nginx systemctl enable nginx && systemctl start nginx 8.2 拷贝文件 zabbix前端是php编写的，需要把zabbix源码目录下的ui目录下的所有文件拷贝到 /var/www/html/zabbix # 创建zabbix目录 mkdir -p /var/www/html/zabbix # 拷贝所有文件到/var/www/html/zabbix cp -rp zabbix-5.4.1/ui/* /var/www/html/zabbix # 修改文件权限为zabbix所有 chown -R zabbix.zabbix /var/www/html/zabbix/* 8.3 配置nginx 这里需要做一下hosts解析 zabbix.test.com cat > /etc/nginx/conf.d/zabbix.conf 9.安装zabbix 浏览器访问 zabbix.test.com 9.1 默认语言选择中文 9.2 必要条件检测 如检测失败则需要修改php的相关配置，本文安装的php的配置文件路径为 /etc/opt/remi/php73 以下选项只是为警告，不修改也可以 使用如下命令修改相关参数 sed -i.bak -e '/^post_max_size/cpost_max_size = 16M' \\ -e '/^max_execution_time/cmax_execution_time = 300' \\ -e '/^max_input_time/cmax_input_time = 300' \\ /etc/opt/remi/php73/php.ini 修改完成后重启php systemctl restart php73-php-fpm.service 重新检测，都为ok即可 9.3 配置数据库连接 9.4 配置zabbix服务器详细信息 9.5 设置时区 9.6 确认安装信息 9.7 开始安装 如遇如下界面，则需要按照提示下载配置文件 zabbix.conf.php 并上传至 /var/www/html/zabbix/conf 注意修改 zabbix.conf.php 文件所有者为zabbix chown zabbix.zabbix zabbix.conf.php 再次检测 10.登陆zabbix 用户名 Admin 密码 zabbix 登陆后首页面 前端有问题！ 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/zabbix/zabbix5.0/知识点/zabbix5.0配置企业微信webhook告警.html":{"url":"linux/monitor/zabbix/zabbix5.0/知识点/zabbix5.0配置企业微信webhook告警.html","title":"zabbix5.0配置企业微信webhook告警","keywords":"","body":"[toc] zabbix5.0配置企业微信webhook告警 本文严重抄袭于此 Zabbix 5.0对于告警（报警媒介）进行了扩展和优化，可以直接支持 WebHook 类型的报警媒介。我们再开发企业微信机器人可以直接通过 JavaScript 语言编写脚本，因为得到了 Zabbix 的原生支持，告警脚本通用性强且更加灵活。本文将分享如何通过 Zabbix 报警媒介在企业微信发送告警信息。 1.创建报警媒介类型 管理 -> 报警媒介类型 -> 创建媒介类型 1.1 编辑报警媒介类型相关信息 以下为脚本内容 var Qiyeweixin = { key: null, message: null, msgtype: \"markdown\", proxy: null, sendMessage: function () { var params = { msgtype: Qiyeweixin.msgtype, markdown: { content: Qiyeweixin.message, }, }, data, response, request = new CurlHttpRequest(), url = \"https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=\" + Qiyeweixin.key; if (Qiyeweixin.proxy) { request.setProxy(Qiyeweixin.proxy); } request.AddHeader(\"Content-Type: application/json\"); data = JSON.stringify(params); // Remove replace() function if you want to see the exposed key in the log file. Zabbix.Log( 4, \"[Qiyeweixin Webhook] URL: \" + url.replace(Qiyeweixin.key, \"\") ); Zabbix.Log(4, \"[Qiyeweixin Webhook] params: \" + data); response = request.Post(url, data); Zabbix.Log(4, \"[Qiyeweixin Webhook] HTTP code: \" + request.Status()); try { response = JSON.parse(response); } catch (error) { response = null; } if (request.Status() !== 200 || response.errcode !== 0) { if (typeof response.errmsg === \"string\") { throw response.errmsg; } else { throw \"Unknown error. Check debug log for more information.\"; } } }, }; try { var params = JSON.parse(value); if (typeof params.Key === \"undefined\") { throw 'Incorrect value is given for parameter \"Key\": parameter is missing'; } Qiyeweixin.key = params.Key; if (params.HTTPProxy) { Qiyeweixin.proxy = params.HTTPProxy; } Qiyeweixin.to = params.To; Qiyeweixin.message = params.Subject + \"\\n\" + params.Message; Qiyeweixin.sendMessage(); return \"OK\"; } catch (error) { Zabbix.Log(4, \"[Qiyeweixin Webhook] notification failed: \" + error); throw \"Sending failed: \" + error + \".\"; } 1.2 编辑消息模板 1.2.1 默认内容 Message template 问题 默认内容 主题 Problem: {EVENT.NAME} 消息 Problem started at {EVENT.TIME} on {EVENT.DATE} Problem name: {EVENT.NAME} Host: {HOST.NAME} Severity: {EVENT.SEVERITY} Operational data: {EVENT.OPDATA} Original problem ID: {EVENT.ID} {TRIGGER.URL} Message template Problem recovery 默认内容 主题 Resolved in {EVENT.DURATION}: {EVENT.NAME} 消息 Problem has been resolved at {EVENT.RECOVERY.TIME} on {EVENT.RECOVERY.DATE} Problem name: {EVENT.NAME} Problem duration: {EVENT.DURATION} Host: {HOST.NAME} Severity: {EVENT.SEVERITY} Original problem ID: {EVENT.ID} {TRIGGER.URL} Message template Problem update 默认内容 主题 Updated problem in {EVENT.AGE}: {EVENT.NAME} 消息 {USER.FULLNAME} {EVENT.UPDATE.ACTION} problem at {EVENT.UPDATE.DATE} {EVENT.UPDATE.TIME}. {EVENT.UPDATE.MESSAGE} Current problem status is {EVENT.STATUS}, age is {EVENT.AGE}, acknowledged: {EVENT.ACK.STATUS}. 1.2.2 自定义内容 在模板中是支持 markdown 语法的。目前支持的 markdown 语法是如下的子集。这是由企业微信机器人开发文档定义的。 标题 （支持 1 至 6 级标题，注意#与文字中间要有空格） 加粗 链接 行内代码段（不支持跨行） 引用 字体颜色（有三种内置颜色） 1.2.2.1 问题 修改主题内容如下 普通写法 服务器: {HOST.NAME} 发生: {TRIGGER.NAME} 故障! markdown写法 # 服务器: `{HOST.NAME}` 发生: `{TRIGGER.NAME}` 故障! 修改消息内容如下 普通写法 告警主机: {HOST.NAME} 告警地址: {HOST.IP} 监控项目: {ITEM.NAME} 监控取值: {ITEM.LASTVALUE} 告警等级: {TRIGGER.SEVERITY} 当前状态: {TRIGGER.STATUS} 告警信息: {TRIGGER.NAME} 告警时间: {EVENT.DATE} {EVENT.TIME} 事件ID: {EVENT.ID} markdown写法 >**告警主机**: `{HOST.NAME}` >**告警地址**: `{HOST.IP}` >**监控项目**: `{ITEM.NAME}` >**监控取值**: `{ITEM.LASTVALUE}` >**告警等级**: `{TRIGGER.SEVERITY}` >**当前状态**: `{TRIGGER.STATUS}` >**告警信息**: `{TRIGGER.NAME}` >**告警时间**: `{EVENT.DATE} {EVENT.TIME}` >**事件ID**: `{EVENT.ID}` 1.2.2.2 恢复 修改主题内容如下 普通写法 服务器: {HOST.NAME}: 故障 {TRIGGER.NAME} 已恢复! markdown写法 # 服务器: `{HOST.NAME}` 故障: `{TRIGGER.NAME}` 已恢复! 修改消息内容如下 普通写法 告警主机: {HOST.NAME} 告警地址: {HOST.IP} 监控项目: {ITEM.NAME} 监控取值: {ITEM.LASTVALUE} 告警等级: {TRIGGER.SEVERITY} 当前状态: {TRIGGER.STATUS} 告警信息: {TRIGGER.NAME} 告警时间: {EVENT.DATE} {EVENT.TIME} 恢复时间: {EVENT.RECOVERY.DATE} {EVENT.RECOVERY.TIME} 持续时间: {EVENT.AGE} 事件ID: {EVENT.ID} markdown写法 >**告警主机**: `{HOST.NAME}` >**告警地址**: `{HOST.IP}` >**监控项目**: `{ITEM.NAME}` >**监控取值**: `{ITEM.LASTVALUE}` >**告警等级**: `{TRIGGER.SEVERITY}` >**当前状态**: `{TRIGGER.STATUS}` >**告警信息**: `{TRIGGER.NAME}` >**告警时间**: `{EVENT.DATE} {EVENT.TIME}` >**恢复时间**: `{EVENT.RECOVERY.DATE} {EVENT.RECOVERY.TIME}` >**持续时间**: `{EVENT.AGE}` >**事件ID**: `{EVENT.ID}` 1.2.2.3 更新 修改主题内容如下 普通写法 服务器: {HOST.NAME}: 报警更新 markdown写法 # 服务器: `{HOST.NAME}:` 报警更新 修改消息内容如下 普通写法 更新人: {USER.FULLNAME} 时间: {ACK.DATE} {ACK.TIME} 更新信息如下: \"{ACK.MESSAGE}\" 问题服务器IP: {HOSTNAME1} 问题ID: {EVENT.ID} 当前的问题是: {TRIGGER.NAME} markdown写法 >**更新人**: `{USER.FULLNAME}` >**时间**: `{ACK.DATE} {ACK.TIME}` >**更新信息如下**: `\"{ACK.MESSAGE}\"` >**问题服务器IP**: `{HOSTNAME1}` >**问题ID**: `{EVENT.ID}` >**当前的问题是**: `{TRIGGER.NAME}` 2.创建触发动作 配置 -> 动作 -> 创建动作 依次编辑 操作、恢复操作、更新操作，需要配置的就是发送的组和发送的报警媒介 3.创建接受人 点击左下角的 User settings 添加报警媒介收件人 创建完成后如果发生告警则相应的企业微信群就会收到告警了，这里效果图就不展示了 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/zabbix/zabbix5.0/知识点/zabbix5.0修改磁盘告警阈值.html":{"url":"linux/monitor/zabbix/zabbix5.0/知识点/zabbix5.0修改磁盘告警阈值.html","title":"zabbix5.0修改磁盘告警阈值","keywords":"","body":"zabbix5.0修改磁盘告警阈值 背景说明 zabbix5.0磁盘告警阈值默认为大于80%就告警 在 配置 -> 模板 -> 找到对应模板，这里以 Template OS Linux by Zabbix agent active 为例 点击模板中的 自动发现 选择 Template Module Linux filesystems by Zabbix agent active: Mounted filesystem discovery 中的 触发器类型 点击 Template Module Linux filesystems by Zabbix agent active: {#FSNAME}: Disk space is critically low (used > {$VFS.FS.PUSED.MAX.CRIT:\"{#FSNAME}\"}%) 中的 Template Module Linux filesystems by Zabbix agent active: 点击 {#FSNAME}: Disk space is critically low (used > {$VFS.FS.PUSED.MAX.CRIT:\"{#FSNAME}\"}%) 修改表达式 默认表达式如下 {Template Module Linux filesystems by Zabbix agent active:vfs.fs.size[{#FSNAME},pused].last()}>{$VFS.FS.PUSED.MAX.CRIT:\"{#FSNAME}\"} and (({Template Module Linux filesystems by Zabbix agent active:vfs.fs.size[{#FSNAME},total].last()}-{Template Module Linux filesystems by Zabbix agent active:vfs.fs.size[{#FSNAME},used].last()}) 删除原先的表达式，然后点击 添加 点击 选择原型 设置阈值 设置完成 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/zabbix/zabbix5.0/知识点/zabbix5.0配置web监控并告警.html":{"url":"linux/monitor/zabbix/zabbix5.0/知识点/zabbix5.0配置web监控并告警.html","title":"zabbix5.0配置web监控并告警","keywords":"","body":"zabbix5.0配置web监控并告警 zabbix5.0 web监控官方文档 1.配置web监控 1.1 选择web监测 创建web监测有2种方式 1.直接在单台主机上创建 2.直接在模板上创建 建议是新建一个模板，然后在这个模板上单独新建web监测 1.2 创建web场景 配置web监测 名称、应用集(可选)、客户端(默认是zabbix) 在 步骤 中配置要监控的域名，名称 和 URL 最好写成一样的，便于识别，勾选 跟随跳转，这里主要是网站可能涉及到 301 和 302 条转，超时默认为 15s，要求的状态码多个以 , 分割 2.配置web监控告警 2.1 web场景信息说明 这里提前创建了一个模板，名称为 web site monitor 其中的web场景名称为 www.baidu.com ⚠️这里的操作方法是一个域名对应一个web场景，当然一个web场景是可以包含多个域名的，但是有一个问题没有解决，就是在告警的时候告警内容只能包含到web场景名称，不能包含web场景中的具体域名 web场景中的域名只有一个，其中期望的状态码故意写成 502，方便后续告警验证 2.2 配置web监控告警 2.2.1 创建触发器 在新建的模板中创建触发器 定义触发器名称，监控项选择 web site monitor: Failed step of scenario \"www.baidu.com\". 触发器条件设置监控项 web site monitor: Failed step of scenario \"www.baidu.com\". 结果不等于0 Failed step of scenario 为web场景监测失败的返回码，如果等于0则说明网站返回码和期望的返回状态码相同，非0则说明网站返回状态码与期望值不相同 恢复表达式和问题表达式触发条件相反 创建后的触发器 2.2.2 告警测试 因为期望的状态码故意写成了 502 ，因此触发器一旦创建就会触发告警 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/zabbix/zabbix问题记录/zabbix-server无法启动.html":{"url":"linux/monitor/zabbix/zabbix问题记录/zabbix-server无法启动.html","title":"zabbix-server无法启动","keywords":"","body":"zabbix-server无法启动 zabbix5.0配置了主机自动注册后，主机注册到一半的时候(总数150+)，zabbix server挂了，然后日志信息如下，各种查，在参考链接中有提到可能是由于 CacheSize 的问题，调大 zabbix server中 CacheSize的值，然后重启zabbix server就可以了 12362:20210315:162445.365 Starting Zabbix Server. Zabbix 5.0.9 (revision 4d07aaafe2). 12362:20210315:162445.365 ****** Enabled features ****** 12362:20210315:162445.365 SNMP monitoring: YES 12362:20210315:162445.365 IPMI monitoring: YES 12362:20210315:162445.365 Web monitoring: YES 12362:20210315:162445.365 VMware monitoring: YES 12362:20210315:162445.365 SMTP authentication: YES 12362:20210315:162445.365 ODBC: YES 12362:20210315:162445.365 SSH support: YES 12362:20210315:162445.365 IPv6 support: YES 12362:20210315:162445.365 TLS support: YES 12362:20210315:162445.365 ****************************** 12362:20210315:162445.365 using configuration file: /etc/zabbix/zabbix_server.conf 12362:20210315:162445.369 current database version (mandatory/optional): 05000000/05000002 12362:20210315:162445.369 required mandatory version: 05000000 12362:20210315:162445.377 server #0 started [main process] 12364:20210315:162445.378 server #1 started [configuration syncer #1] 12364:20210315:162445.802 __mem_malloc: skipped 0 asked 72 skip_min 18446744073709551615 skip_max 0 12364:20210315:162445.802 [file:dbconfig.c,line:96] __zbx_mem_malloc(): out of memory (requested 70 bytes) 12364:20210315:162445.802 [file:dbconfig.c,line:96] __zbx_mem_malloc(): please increase CacheSize configuration parameter 12364:20210315:162445.802 === memory statistics for configuration cache === 12364:20210315:162445.802 free chunks of size 24 bytes: 15 12364:20210315:162445.802 free chunks of size 64 bytes: 1 12364:20210315:162445.802 min chunk size: 24 bytes 12364:20210315:162445.802 max chunk size: 64 bytes 12364:20210315:162445.802 memory of total size 7228440 bytes fragmented into 72488 chunks 12364:20210315:162445.802 of those, 424 bytes are in 16 free chunks 12364:20210315:162445.802 of those, 7228016 bytes are in 72472 used chunks 12364:20210315:162445.802 of those, 1159792 bytes are used by allocation overhead 12364:20210315:162445.802 ================================ 12364:20210315:162445.802 === Backtrace: === 12364:20210315:162445.803 14: /usr/sbin/zabbix_server: configuration syncer [syncing configuration](zbx_backtrace+0x42) [0x555562a7ea97] 12364:20210315:162445.803 13: /usr/sbin/zabbix_server: configuration syncer [syncing configuration](__zbx_mem_malloc+0x18c) [0x555562a797e9] 12364:20210315:162445.803 12: /usr/sbin/zabbix_server: configuration syncer [syncing configuration](+0x18a609) [0x555562a40609] 12364:20210315:162445.803 11: /usr/sbin/zabbix_server: configuration syncer [syncing configuration](zbx_hashset_insert_ext+0x12e) [0x555562a8488e] 12364:20210315:162445.803 10: /usr/sbin/zabbix_server: configuration syncer [syncing configuration](+0x18b1b9) [0x555562a411b9] 12364:20210315:162445.803 9: /usr/sbin/zabbix_server: configuration syncer [syncing configuration](DCstrpool_replace+0x55) [0x555562a412b9] 12364:20210315:162445.803 8: /usr/sbin/zabbix_server: configuration syncer [syncing configuration](+0x1927c7) [0x555562a487c7] 12364:20210315:162445.803 7: /usr/sbin/zabbix_server: configuration syncer [syncing configuration](DCsync_configuration+0x1061) [0x555562a4d0fe] 12364:20210315:162445.803 6: /usr/sbin/zabbix_server: configuration syncer [syncing configuration](dbconfig_thread+0x135) [0x55556290f1b2] 12364:20210315:162445.803 5: /usr/sbin/zabbix_server: configuration syncer [syncing configuration](zbx_thread_start+0x37) [0x555562a8d5fa] 12364:20210315:162445.803 4: /usr/sbin/zabbix_server: configuration syncer [syncing configuration](MAIN_ZABBIX_ENTRY+0xa02) [0x5555628ff624] 12364:20210315:162445.803 3: /usr/sbin/zabbix_server: configuration syncer [syncing configuration](daemon_start+0x305) [0x555562a7e6e3] 12364:20210315:162445.803 2: /usr/sbin/zabbix_server: configuration syncer [syncing configuration](main+0x312) [0x5555628febd8] 12364:20210315:162445.803 1: /lib64/libc.so.6(__libc_start_main+0xf5) [0x7f9ebac64555] 12364:20210315:162445.803 0: /usr/sbin/zabbix_server: configuration syncer [syncing configuration](+0x47c59) [0x5555628fdc59] 12362:20210315:162445.805 One child process died (PID:12364,exitcode/signal:1). Exiting ... 12362:20210315:162445.806 syncing trend data... 12362:20210315:162445.806 syncing trend data done 12362:20210315:162445.807 Zabbix Server stopped. Zabbix 5.0.9 (revision 4d07aaafe2). 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/zabbix/zabbix各种监控/zabbix监控交换机.html":{"url":"linux/monitor/zabbix/zabbix各种监控/zabbix监控交换机.html","title":"zabbix监控交换机","keywords":"","body":"[toc] zabbix监控交换机 一、交换机开启SNMP 在交换机中执行如下命令，设置一个只读的团体名，名称唯一 snmp-agent community read public100 在zabbix-server中执行如下命令，会返回交换机中的所有配置信息 snmpwalk -v 2c -c public100 10.0.0.123 二、zabbix添加主机 SNMP接口处填写交换机的IP，端口默认161 交换机是华为的，所以模版选择 Template Net Huawei VRP SNMPv2 ⚠️这里的宏必须填写正确，要和交换机中设置的 snmp-agent community read public100 名称对应 宏： {$SNMP_COMMUNITY} 值：public100 效果示意图 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/grafana/安装/grafana安装.html":{"url":"linux/monitor/grafana/安装/grafana安装.html","title":"安装","keywords":"","body":"[toc] grafana安装 grafana官网 grafana官方下载地址 grafana官方管理配置文档 1.rpm包安装 1.1 下载安装包并安装 wget https://dl.grafana.com/oss/release/grafana-7.0.3-1.x86_64.rpm yum -y localinstall grafana-7.0.3-1.x86_64.rpm 1.2 启动服务并设置开机自启 grafana默认监听TCP/3000端口 systemctl enable grafana-server && systemctl start grafana-server 1.3 grafana配置文件路径说明 官方文档中对于grafana配置文件路径的说明 文件 路径 安装目录 /usr/share/grafana/ grafana-cli 路径 /usr/share/grafana/bin/grafana-cli 全局配置文件 /etc/grafana/grafana.ini 默认配置文件 /usr/share/grafana/conf/defaults.ini plugins 安装目录 /var/lib/grafana/plugins/ 默认数据存储文件路径 /var/lib/grafana/grafana.db 日志文件存储路径 /var/log/grafana/ 邮件默认发送模板路径 /usr/share/grafana/public/emails/ 2.yum安装 yum安装官方文档 2.1 编辑yum源 cat > /etc/yum.repos.d/grafana.repo 2.2 安装 # 默认安装最新版 yum -y install grafana # 指定版本安装 yum -y install grafana-7.5.0 2.3 启动 systemctl start grafana-server && systemctl enable grafana-server 3.docker安装 3.1 安装并启动容器 官方安装文档中安装的grafana并没有做持久化 docker run -d --name=grafana -p 3000:3000 grafana/grafana grafana数据存储路径为 /var/lib/grafana/grafana.db 文件，插件存储路径为 /var/lib/grafana/plugins 目录，因此需要持久化 /var/lib/grafana 这个目录 docker安装grafana持久化官方文档 ⚠️以挂载宿主机目录的形式启动grafana的时候必须指定用户为root，否则后续会报权限错误；而以volume形式挂载则没有问题 docker run \\ -d \\ --restart=always \\ --name=grafana \\ --user=root \\ -h grafana \\ -p 3000:3000 \\ -v /data/docker-volume/grafana:/var/lib/grafana \\ grafana/grafana:8.0.2 4.二进制安装 4.1 下载二进制包 wget https://dl.grafana.com/oss/release/grafana-8.0.2.linux-amd64.tar.gz 4.2 启动 # 解压缩二进制包 tar xf grafana-8.0.2.linux-amd64.tar.gz # 后台启动 cd grafana-8.0.2/ ./bin/grafana-server & 3.登陆grafana 浏览器访问 IP:3000 初始默认账户和密码都是admin 登陆成功后会提示修改密码，也可以选择跳过不修改 登陆后首界面 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/monitor/grafana/知识点/grafana接入zabbix数据源.html":{"url":"linux/monitor/grafana/知识点/grafana接入zabbix数据源.html","title":"grafana接入zabbix数据源","keywords":"","body":"[toc] grafana接入zabbix数据源 1.安装zabbix插件 grafana 官方插件下载地址 # 安装zabbix插件 grafana-cli plugins install alexanderzobnin-zabbix-app # 插件安装完成后要重启grafana systemctl restart grafana-server 2.启用zabbix插件 第一步、点击左侧设置按钮，然后点击Plugins 第二步、在最下边找到zabbix插件，红色警告表明插件是外部插件 第三步、允许zabbix插件 3.配置zabbix数据源 第一步、点击左侧设置按钮，然后选择Data Sources 第二步、选择Add data source 第三步、选择zabbix 第四步、配置zabbix用户密码信息 grafana会自动使用默认的zabbix api地址 配置完成后点击下方的 Save & Test zabbix api地址如下，如果localhost无法解析，则需要把localhost改成zabbix的域名 http://localhost/zabbix/api_jsonrpc.php 检测成功会提示如下 4.添加dashboard进行自定义监控 4.1 创建dashboard 点击左侧+号，选择Dashboard 选择右上角Dashboard settings 对dashboard进行重命名 确认dashboard名称 创建完成后的dashboard 4.2 创建监控图形 选择创建好的dashboard，然后点击右上角的Add panel，添加一个面板 示例：创建CPU负载监控图形 修改标题 4.3 保存dashboard 右上角点击Save 保存dashboard的时候会要求添加改变的描述信息，即修改了哪些内容 保存完后点击Apply 最终效果如下，左上角是新建的dashboard，最好以部门名称命名，这样便于区分不同部门的机器，这里是示例创建了CPU负载监控图形，其余监控创建方法一致，面板大小也可以调整 4.4 创建其他监控图形 如果想要创建其他监控项图形，先选择对应的dashboard，然后在创建面板，最后在面板中选择相应的监控信息即可 修改监控图形指标单位 在可用内存监控中，默认的单位不太好识别 修改指标单位 点击右侧的Axes(轴)，选择相对应的轴，比如Left Y(左Y轴)，然后在Unit(单位)选项卡处选择相应的单位，选择Data(Metric)下的bytes(Metric) 最终效果，这样看起来左侧显示单位就比较明了了 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/性能测试/文件删除效率测试/测试Linux下删除大量文件的效率.html":{"url":"linux/性能测试/文件删除效率测试/测试Linux下删除大量文件的效率.html","title":"文件删除效率测试","keywords":"","body":"[toc] 测试Linux下删除大量文件的效率 创建50万个测试文件 //创建50万个文件 mkdir /test && cd /test time for i in $(seq 1 500000);do echo text >>$i.txt;done real 1m9.177s user 0m9.731s sys 0m48.238s //总大小为2G du -sh /test/ 2.0G /test/ 1.rm删除 time rm -rf * -bash: /usr/bin/rm: Argument list too long real 0m1.709s user 0m1.580s sys 0m0.123s 文件太多，rm不起作用 2.find删除 time find ./ -type f -exec rm {} \\; real 10m56.698s user 2m13.203s sys 8m35.653s 用时10分钟 3.find with delete time find ./ -type f -delete real 0m26.757s user 0m1.222s sys 0m23.112s 用时26秒 4.rsync删除 //先建立一个空文件夹test-bak mkdir test-bak time rsync -a --delete test-bak/ /test/ real 0m25.440s user 0m1.364s sys 0m22.082s 用时25秒 5.python2.7 import os import timeit def main(): for pathname,dirnames,filenames in os.walk('/test'): for filename in filenames: file=os.path.join(pathname,filename) os.remove(file) if __name__=='__main__': t=timeit.Timer('main()','from __main__ import main') print t.timeit(1) 用时35秒 6.perl time perl -e 'for(){((stat)[9] 系统环境 ucloud 1c2g centos7.7 #使用dd命令测试磁盘读写速度为78.6MB/s dd if=/dev/zero of=/opt/bigfile bs=1M count=1024 1024+0 records in 1024+0 records out 1073741824 bytes (1.1 GB) copied, 13.668 s, 78.6 MB/s #使用hdparm测试磁盘读写速度为74.60MB/s hdparm -t --direct /dev/vda1 /dev/vda1: Timing O_DIRECT disk reads: 230 MB in 3.08 seconds = 74.60 MB/sec 50万个文件删除所用时间 rm删除：文件太多，无法删除 find删除：用时10分钟 find with delete删除：用时26秒 rsync删除：用时25秒 python2.7删除：用时35秒 perl删除：用时33秒 rsync删除最快 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux系统/centos8/CentOS8使用记录.html":{"url":"linux/linux系统/centos8/CentOS8使用记录.html","title":"centos8使用记录","keywords":"","body":"[toc] CentOS8使用记录 centos8的作者应该很喜欢腾讯的地下城与勇士(dnf)，否则踏🐎怎么会把安装命令yum尼玛改成dnf ？？？ 1.更换yum源 1.备份 mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup 2.下载阿里云yum源 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-8.repo 或 curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-8.repo 3.生成缓存 dnf clean all dnf makecache 4.安装epol源 yum install -y https://mirrors.aliyun.com/epel/epel-release-latest-8.noarch.rpm sed -i 's|^#baseurl=https://download.fedoraproject.org/pub|baseurl=https://mirrors.aliyun.com|' /etc/yum.repos.d/epel* sed -i 's|^metalink|#metalink|' /etc/yum.repos.d/epel* 2.同步时间 centos8不支持ntpdate了，改用chrony 1.安装 dnf -y install chrony 2.修改配置文件/etc/chrony.conf，注释pool开头一行，新增阿里云地址 #pool 2.centos.pool.ntp.org iburst server ntp.aliyun.com iburst sed -i.bak '3s/^/#&/g' /etc/chrony.conf && sed -i '4cserver ntp.aliyun.com iburst' /etc/chrony.conf server：指明时间服务器地址； allow NETADD/NETMASK allow all：允许所有客户端主机； deny NETADDR/NETMASK deny all：拒绝所有客户端； bindcmdaddress：命令管理接口监听的地址； local stratum 10：即使自己未能通过网络时间服务器同步到时间，也允许将本地时间作为标准时间授时给其它客户端； 3.启动服务 systemctl enable chronyd && systemctl start chronyd 4.检查端口 chronyd服务监听udp32端口 netstat -nupl|grep chronyd 5.验证同步 chronyc sources 210 Number of sources = 1 MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== ^* 203.107.6.88 2 6 17 44 -2379us[-3100us] +/- 43ms 个人实际使用，这个chrony不好用，绝对没有ntpdate好用，我手动更改了时间，尼玛5分钟了还没有同步，垃圾 还是直接使用命令来的快 chronyd -q \"server ntp.aliyun.com iburst\" centos8继续使用ntpdate 1.添加wlnmp源 rpm -ivh http://mirrors.wlnmp.com/centos/wlnmp-release-centos.noarch.rpm 2.安装ntp服务 dnf -y install wntp 3.时间同步 ntpdate ntp.aliyun.com 3.centos8网络服务 centos8使用的是NetworkManager管理网络，不能使用systemctld管理network systemctl enable NetworkManager nmcli c reload eth0 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux系统/ubuntu/ubuntu18.04使用记录.html":{"url":"linux/linux系统/ubuntu/ubuntu18.04使用记录.html","title":"ubuntu使用记录","keywords":"","body":"[toc] ubuntu18.04使用记录 1.设置静态IP //编辑文件 root@ubuntu18:~# cat /etc/netplan/01-network-manager-all.yaml # Let NetworkManager manage all devices on this system network: ethernets: enp0s5: #⚠️这里要看一下网卡的名称 dhcp4: false addresses: [10.0.0.15/24] gateway4: 10.0.0.1 nameservers: addresses: [223.5.5.5,114.114.114.114] version: 2 renderer: networkd //使设置生效 netplan apply 2.开启ssh服务 //ubuntu-18.04desktpo版默认只安装ssh-agent sudo apt -y install openssh-server //启动服务 sudo service ssh start 3.安装python3.6 //ubuntu18.04默认python版本是3.7 root@ubuntu18:~# python3 Python 3.7.5rc1 (default, Oct 8 2019, 16:47:45) [GCC 9.2.1 20191008] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. root@ubuntu18:~# python3.7 Python 3.7.5rc1 (default, Oct 8 2019, 16:47:45) [GCC 9.2.1 20191008] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. //安装依赖包 apt -y install python3-dev libffi-dev libssl-dev zlib* //源码编译安装python3.6 ./configure --enable-optimizations --prefix=/usr/local/python36 make make altinstall #防止覆盖原来的版本 //设置环境变量 echo \"PATH=/usr/local/python36/bin:$PATH\" >/etc/profile.d/python36.sh && source /etc/ //设置pip国内源 mkdir /root/.pip cat >/root/.pip/pip.conf ubuntu安装python3.6问题 4.更换国内源(ubuntu18.04) //备份原有文件 cp /etc/apt/sources.list{,.bak} //设置为阿里云源 cat >/etc/apt/sources.list 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux系统/kali/kali2020.1使用记录.html":{"url":"linux/linux系统/kali/kali2020.1使用记录.html","title":"kali使用记录","keywords":"","body":"[toc] kali2020.1使用记录 1.开启ssh kali默认ssh服务部开启 //启动ssh服务 service ssh start //开机自启 update-rc.d ssh enable 2.配置网卡 1.编辑文件/etc/network/interfaces #注释以下两行 #auto lo #iface lo inet loopback auto eth0 iface eth0 inet static address 10.0.0.17 netmask 255.255.255.0 gateway 10.0.0.1 2.重启网路服务 service networking restart 3.配置阿里云源 #编辑文件/etc/apt/sources.list，写入以下两行 deb https://mirrors.aliyun.com/kali kali-rolling main non-free contrib deb-src https://mirrors.aliyun.com/kali kali-rolling main non-free contrib #更新 apt update 4.设置图形界面 1.安装包 apt install x-window-system-core xfce4 -y 2.在命令行输入”dpkg-reconfigure locales”。进入图形化界面之后，（空格是选择，Tab是切换，*是选中），选中zh_CN.UTF-8，确定后，将h_CN.UTF-8选为默认。 接下来的几步就是选择zh_CN.UTF-8 3.安装中文字体 apt-get -y install xfonts-intl-chinese apt-get -y install ttf-wqy-microhei 安装完后如果没有显示中文重启即可 #开机设置进入图形界面 1.编辑/etc/default/grub 修改quite为text GRUB_CMDLINE_LINUX_DEFAULT=\"quiet\" 2.使配置生效 update-grub 5.kali设置服务开机自启 //设置开机自启 update-rc.d 服务 enable //禁止开机启动 update-rc.d 服务 disable // 删除开机启动 update-rc.d -f 服务 remove 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux系统/deepin/deepin15.11使用记录.html":{"url":"linux/linux系统/deepin/deepin15.11使用记录.html","title":"deepin使用记录","keywords":"","body":"[toc] deepin15.11使用记录 深度官方下载地址 深度系统管理官方文档 1.修改网卡名称为eth0 1.备份文件 cp /etc/default/grub{,.bak} 2.修改/etc/default/grub 修改GRUB_CMDLINE_LINUX=\"\" 修改为GRUB_CMDLINE_LINUX=\"net.ifnames=0 biosdevname=0\" 3.更新grub update-grub 重启系统 2.设置IP地址 1.备份网卡配置文件 cp /etc/network/interfaces{,.bak} #注释这行 #source-directory /etc/network/interfaces.d #写入以下配置信息 auto eth0 iface eth0 inet static address 10.0.0.18 netmask 255.255.255.0 gateway 10.0.0.1 2.重启网络服务 systemctl restart networking 3.安装ssh服务 1.安装 apt -y install openssh-server 2.启动 systemctl start sshd 3.设置开机自启，需要自行创建/etc/rc.local文件，然后把要启动的服务命令写上(此方式适用于systemd管理的服务) cat >/etc/rc.local 检测包是否安装 dpkg --list |grep 包名 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/网络协议/tcp:ip/TCP:IP 协议.html":{"url":"linux/网络协议/tcp:ip/TCP:IP 协议.html","title":"TCP/IP协议","keywords":"","body":"[toc] TCP/IP 协议 本文严重抄袭至互联网 总纲 一、 计算机网络体系结构分层 计算机网络体系结构分层 不难看出，TCP/IP 与 OSI 在分层模块上稍有区别。OSI 参考模型注重“通信协议必要的功能是什么”，而 TCP/IP 则更强调“在计算机上实现协议应该开发哪种程序”。 二、TCP/IP基础 2.1 TCP/IP 的具体含义 从字面意义上讲，有人可能会认为 TCP/IP 是指 TCP 和 IP 两种协议。实际生活当中有时也确实就是指这两种协议。然而在很多情况下，它只是利用 IP 进行通信时所必须用到的协议群的统称。具体来说，IP 或 ICMP、TCP 或 UDP、TELNET 或 FTP、以及 HTTP 等都属于 TCP/IP 协议。他们与 TCP 或 IP 的关系紧密，是互联网必不可少的组成部分。TCP/IP 一词泛指这些协议，因此，有时也称 TCP/IP 为网际协议群。 互联网进行通信时，需要相应的网络协议，TCP/IP 原本就是为使用互联网而开发制定的协议族。因此，互联网的协议就是 TCP/IP，TCP/IP 就是互联网的协议。 网际协议群 2.2 数据包 包、帧、数据包、段、消息 以上五个术语都用来表述数据的单位，大致区分如下： 包可以说是全能性术语； 帧用于表示数据链路层中包的单位； 数据包是 IP 和 UDP 等网络层以上的分层中包的单位； 段则表示 TCP 数据流中的信息； 消息是指应用协议中数据的单位。 每个分层中，都会对所发送的数据附加一个首部，在这个首部中包含了该层必要的信息，如发送的目标地址以及协议相关信息。通常，为协议提供的信息为包首部，所要发送的内容为数据。在下一层的角度看，从上一层收到的包全部都被认为是本层的数据。 数据包首部 网络中传输的数据包由两部分组成：一部分是协议所要用到的首部，另一部分是上一层传过来的数据。首部的结构由协议的具体规范详细定义。在数据包的首部，明确标明了协议应该如何读取数据。反过来说，看到首部，也就能够了解该协议必要的信息以及所要处理的数据。包首部就像协议的脸。 2.3 数据处理流程 下图以用户 a 向用户 b 发送邮件为例子： 数据处理流程 ① 应用程序处理 首先应用程序会进行编码处理，这些编码相当于 OSI 的表示层功能； 编码转化后，邮件不一定马上被发送出去，这种何时建立通信连接何时发送数据的管理功能，相当于 OSI 的会话层功能。 ② TCP 模块的处理 TCP 根据应用的指示，负责建立连接、发送数据以及断开连接。TCP 提供将应用层发来的数据顺利发送至对端的可靠传输。为了实现这一功能，需要在应用层数据的前端附加一个 TCP 首部。 ③ IP 模块的处理 IP 将 TCP 传过来的 TCP 首部和 TCP 数据合起来当做自己的数据，并在 TCP 首部的前端加上自己的 IP 首部。IP 包生成后，参考路由控制表决定接受此 IP 包的路由或主机。 ④ 网络接口（以太网驱动）的处理 从 IP 传过来的 IP 包对于以太网来说就是数据。给这些数据附加上以太网首部并进行发送处理，生成的以太网数据包将通过物理层传输给接收端。 ⑤ 网络接口（以太网驱动）的处理 主机收到以太网包后，首先从以太网包首部找到 MAC 地址判断是否为发送给自己的包，若不是则丢弃数据。 如果是发送给自己的包，则从以太网包首部中的类型确定数据类型，再传给相应的模块，如 IP、ARP 等。这里的例子则是 IP 。 ⑥ IP 模块的处理 IP 模块接收到 数据后也做类似的处理。从包首部中判断此 IP 地址是否与自己的 IP 地址匹配，如果匹配则根据首部的协议类型将数据发送给对应的模块，如 TCP、UDP。这里的例子则是 TCP。 另外吗，对于有路由器的情况，接收端地址往往不是自己的地址，此时，需要借助路由控制表，在调查应该送往的主机或路由器之后再进行转发数据。 ⑦ TCP 模块的处理 在 TCP 模块中，首先会计算一下校验和，判断数据是否被破坏。然后检查是否在按照序号接收数据。最后检查端口号，确定具体的应用程序。数据被完整地接收以后，会传给由端口号识别的应用程序。 ⑧ 应用程序的处理 接收端应用程序会直接接收发送端发送的数据。通过解析数据，展示相应的内容。 三、传输层中的 TCP 和 UDP TCP/IP 中有两个具有代表性的传输层协议，分别是 TCP 和 UDP。 TCP 是面向连接的、可靠的流协议。流就是指不间断的数据结构，当应用程序采用 TCP 发送消息时，虽然可以保证发送的顺序，但还是犹如没有任何间隔的数据流发送给接收端。TCP 为提供可靠性传输，实行“顺序控制”或“重发控制”机制。此外还具备“流控制（流量控制）”、“拥塞控制”、提高网络利用率等众多功能。 UDP 是不具有可靠性的数据报协议。细微的处理它会交给上层的应用去完成。在 UDP 的情况下，虽然可以确保发送消息的大小，却不能保证消息一定会到达。因此，应用有时会根据自己的需要进行重发处理。 TCP 和 UDP 的优缺点无法简单地、绝对地去做比较：TCP 用于在传输层有必要实现可靠传输的情况；而在一方面，UDP 主要用于那些对高速传输和实时性有较高要求的通信或广播通信。TCP 和 UDP 应该根据应用的目的按需使用。 3.1端口号 数据链路和 IP 中的地址，分别指的是 MAC 地址和 IP 地址。前者用来识别同一链路中不同的计算机，后者用来识别 TCP/IP 网络中互连的主机和路由器。在传输层也有这种类似于地址的概念，那就是端口号。端口号用来识别同一台计算机中进行通信的不同应用程序。因此，它也被称为程序地址。 3.1.1 根据端口号识别应用 一台计算机上同时可以运行多个程序。传输层协议正是利用这些端口号识别本机中正在进行通信的应用程序，并准确地将数据传输。 数据端口号识别应用 3.1.2 通过 IP 地址、端口号、协议号进行通信识别 仅凭目标端口号识别某一个通信是远远不够的。 通过端口号、IP地址、协议号进行通信识别 ① 和② 的通信是在两台计算机上进行的。它们的目标端口号相同，都是80。这里可以根据源端口号加以区分。 ③ 和 ① 的目标端口号和源端口号完全相同，但它们各自的源 IP 地址不同。 此外，当 IP 地址和端口号全都一样时，我们还可以通过协议号来区分（TCP 和 UDP） 3.1.3 端口号的确定 标准既定的端口号：这种方法也叫静态方法。它是指每个应用程序都有其指定的端口号。但并不是说可以随意使用任何一个端口号。例如 HTTP、FTP、TELNET 等广为使用的应用协议中所使用的端口号就是固定的。这些端口号被称为知名端口号，分布在 0~1023 之间；除知名端口号之外，还有一些端口号被正式注册，它们分布在 1024~49151 之间，不过这些端口号可用于任何通信用途。 时序分配法：服务器有必要确定监听端口号，但是接受服务的客户端没必要确定端口号。在这种方法下，客户端应用程序完全可以不用自己设置端口号，而全权交给操作系统进行分配。动态分配的端口号范围在 49152~65535 之间。 3.1.4 端口号与协议 端口号由其使用的传输层协议决定。因此，不同的传输层协议可以使用相同的端口号。 此外，那些知名端口号与传输层协议并无关系。只要端口一致都将分配同一种应用程序进行处理。 3.2 UDP UDP 不提供复杂的控制机制，利用 IP 提供面向无连接的通信服务。 并且它是将应用程序发来的数据在收到的那一刻，立即按照原样发送到网络上的一种机制。即使是出现网络拥堵的情况，UDP 也无法进行流量控制等避免网络拥塞行为。 此外，传输途中出现丢包，UDP 也不负责重发。 甚至当包的到达顺序出现乱序时也没有纠正的功能。 如果需要以上的细节控制，不得不交由采用 UDP 的应用程序去处理。 UDP 常用于一下几个方面：1.包总量较少的通信（DNS、SNMP等）；2.视频、音频等多媒体通信（即时通信）；3.限定于 LAN 等特定网络中的应用通信；4.广播通信（广播、多播）。 3.3 TCP TCP 与 UDP 的区别相当大。它充分地实现了数据传输时各种控制功能，可以进行丢包时的重发控制，还可以对次序乱掉的分包进行顺序控制。而这些在 UDP 中都没有。 此外，TCP 作为一种面向有连接的协议，只有在确认通信对端存在时才会发送数据，从而可以控制通信流量的浪费。 根据 TCP 的这些机制，在 IP 这种无连接的网络上也能够实现高可靠性的通信（ 主要通过检验和、序列号、确认应答、重发控制、连接管理以及窗口控制等机制实现）。 3.3.1 三次握手（重点） TCP 提供面向有连接的通信传输。面向有连接是指在数据通信开始之前先做好两端之间的准备工作。 所谓三次握手是指建立一个 TCP 连接时需要客户端和服务器端总共发送三个包以确认连接的建立。在socket编程中，这一过程由客户端执行connect来触发。 下面来看看三次握手的流程图： 第一次握手：客户端将标志位SYN置为1，随机产生一个值seq=J，并将该数据包发送给服务器端，客户端进入SYN_SENT状态，等待服务器端确认。 第二次握手：服务器端收到数据包后由标志位SYN=1知道客户端请求建立连接，服务器端将标志位SYN和ACK都置为1，ack=J+1，随机产生一个值seq=K，并将该数据包发送给客户端以确认连接请求，服务器端进入SYN_RCVD状态。 第三次握手：客户端收到确认后，检查ack是否为J+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=K+1，并将该数据包发送给服务器端，服务器端检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，客户端和服务器端进入ESTABLISHED状态，完成三次握手，随后客户端与服务器端之间可以开始传输数据了。 3.3.2 四次挥手（重点） 四次挥手即终止TCP连接，就是指断开一个TCP连接时，需要客户端和服务端总共发送4个包以确认连接的断开。在socket编程中，这一过程由客户端或服务端任一方执行close来触发。 由于TCP连接是全双工的，因此，每个方向都必须要单独进行关闭，这一原则是当一方完成数据发送任务后，发送一个FIN来终止这一方向的连接，收到一个FIN只是意味着这一方向上没有数据流动了，即不会再收到数据了，但是在这个TCP连接上仍然能够发送数据，直到这一方向也发送了FIN。首先进行关闭的一方将执行主动关闭，而另一方则执行被动关闭。 下面来看看四次挥手的流程图： 中断连接端可以是客户端，也可以是服务器端。 第一次挥手：客户端发送一个FIN=M，用来关闭客户端到服务器端的数据传送，客户端进入FIN_WAIT_1状态。意思是说\"我客户端没有数据要发给你了\"，但是如果你服务器端还有数据没有发送完成，则不必急着关闭连接，可以继续发送数据。 第二次挥手：服务器端收到FIN后，先发送ack=M+1，告诉客户端，你的请求我收到了，但是我还没准备好，请继续你等我的消息。这个时候客户端就进入FIN_WAIT_2 状态，继续等待服务器端的FIN报文。 第三次挥手：当服务器端确定数据已发送完成，则向客户端发送FIN=N报文，告诉客户端，好了，我这边数据发完了，准备好关闭连接了。服务器端进入LAST_ACK状态。 第四次挥手：客户端收到FIN=N报文后，就知道可以关闭连接了，但是他还是不相信网络，怕服务器端不知道要关闭，所以发送ack=N+1后进入TIME_WAIT状态，如果Server端没有收到ACK则可以重传。服务器端收到ACK后，就知道可以断开连接了。客户端等待了2MSL后依然没有收到回复，则证明服务器端已正常关闭，那好，我客户端也可以关闭连接了。最终完成了四次握手。 上面是一方主动关闭，另一方被动关闭的情况，实际中还会出现同时发起主动关闭的情况， 具体流程如下图： 3.3.3 通过序列号与确认应答提高可靠性 在 TCP 中，当发送端的数据到达接收主机时，接收端主机会返回一个已收到消息的通知。这个消息叫做确认应答（ACK）。当发送端将数据发出之后会等待对端的确认应答。如果有确认应答，说明数据已经成功到达对端。反之，则数据丢失的可能性很大。 在一定时间内没有等待到确认应答，发送端就可以认为数据已经丢失，并进行重发。由此，即使产生了丢包，仍然能够保证数据能够到达对端，实现可靠传输。 未收到确认应答并不意味着数据一定丢失。也有可能是数据对方已经收到，只是返回的确认应答在途中丢失。这种情况也会导致发送端误以为数据没有到达目的地而重发数据。 此外，也有可能因为一些其他原因导致确认应答延迟到达，在源主机重发数据以后才到达的情况也屡见不鲜。此时，源主机只要按照机制重发数据即可。 对于目标主机来说，反复收到相同的数据是不可取的。为了对上层应用提供可靠的传输，目标主机必须放弃重复的数据包。为此我们引入了序列号。 序列号是按照顺序给发送数据的每一个字节（8位字节）都标上号码的编号。接收端查询接收数据 TCP 首部中的序列号和数据的长度，将自己下一步应该接收的序列号作为确认应答返送回去。通过序列号和确认应答号，TCP 能够识别是否已经接收数据，又能够判断是否需要接收，从而实现可靠传输。 序列号和确认应答 3.3.4 重发超时的确定 重发超时是指在重发数据之前，等待确认应答到来的那个特定时间间隔。如果超过这个时间仍未收到确认应答，发送端将进行数据重发。最理想的是，找到一个最小时间，它能保证“确认应答一定能在这个时间内返回”。 TCP 要求不论处在何种网络环境下都要提供高性能通信，并且无论网络拥堵情况发生何种变化，都必须保持这一特性。为此，它在每次发包时都会计算往返时间及其偏差。将这个往返时间和偏差时间相加，重发超时的时间就是比这个总和要稍大一点的值。 在 BSD 的 Unix 以及 Windows 系统中，超时都以0.5秒为单位进行控制，因此重发超时都是0.5秒的整数倍。不过，最初其重发超时的默认值一般设置为6秒左右。 数据被重发之后若还是收不到确认应答，则进行再次发送。此时，等待确认应答的时间将会以2倍、4倍的指数函数延长。 此外，数据也不会被无限、反复地重发。达到一定重发次数之后，如果仍没有任何确认应答返回，就会判断为网络或对端主机发生了异常，强制关闭连接。并且通知应用通信异常强行终止。 3.3.5 以段为单位发送数据 在建立 TCP 连接的同时，也可以确定发送数据包的单位，我们也可以称其为“最大消息长度”（MSS）。最理想的情况是，最大消息长度正好是 IP 中不会被分片处理的最大数据长度。 TCP 在传送大量数据时，是以 MSS 的大小将数据进行分割发送。进行重发时也是以 MSS 为单位。 MSS 在三次握手的时候，在两端主机之间被计算得出。两端的主机在发出建立连接的请求时，会在 TCP 首部中写入 MSS 选项，告诉对方自己的接口能够适应的 MSS 的大小。然后会在两者之间选择一个较小的值投入使用。 3.3.6 利用窗口控制提高速度 TCP 以1个段为单位，每发送一个段进行一次确认应答的处理。这样的传输方式有一个缺点，就是包的往返时间越长通信性能就越低。 为解决这个问题，TCP 引入了窗口这个概念。确认应答不再是以每个分段，而是以更大的单位进行确认，转发时间将会被大幅地缩短。也就是说，发送端主机，在发送了一个段以后不必要一直等待确认应答，而是继续发送。如下图所示： 窗口控制 窗口大小就是指无需等待确认应答而可以继续发送数据的最大值。上图中窗口大小为4个段。这个机制实现了使用大量的缓冲区，通过对多个段同时进行确认应答的功能。 3.3.7 滑动窗口控制 上图中的窗口内的数据即便没有收到确认应答也可以被发送出去。不过，在整个窗口的确认应答没有到达之前，如果其中部分数据出现丢包，那么发送端仍然要负责重传。为此，发送端主机需要设置缓存保留这些待被重传的数据，直到收到他们的确认应答。 在滑动窗口以外的部分包括未发送的数据以及已经确认对端已收到的数据。当数据发出后若如期收到确认应答就可以不用再进行重发，此时数据就可以从缓存区清除。 收到确认应答的情况下，将窗口滑动到确认应答中的序列号的位置。这样可以顺序地将多个段同时发送提高通信性能。这种机制也别称为滑动窗口控制。 3.3.8 窗口控制中的重发控制 在使用窗口控制中， 出现丢包一般分为两种情况： ① 确认应答未能返回的情况。在这种情况下，数据已经到达对端，是不需要再进行重发的，如下图： ② 某个报文段丢失的情况。接收主机如果收到一个自己应该接收的序列号以外的数据时，会针对当前为止收到数据返回确认应答。如下图所示，当某一报文段丢失后，发送端会一直收到序号为1001的确认应答，因此，在窗口比较大，又出现报文段丢失的情况下，同一个序列号的确认应答将会被重复不断地返回。而发送端主机如果连续3次收到同一个确认应答，就会将其对应的数据进行重发。这种机制比之前提到的超时管理更加高效，因此也被称为高速重发控制。 四、网络层中的 IP 协议 IP（IPv4、IPv6）相当于 OSI 参考模型中的第3层——网络层。网络层的主要作用是“实现终端节点之间的通信”。这种终端节点之间的通信也叫“点对点通信”。 网络的下一层——数据链路层的主要作用是在互连同一种数据链路的节点之间进行包传递。而一旦跨越多种数据链路，就需要借助网络层。网络层可以跨越不同的数据链路，即使是在不同的数据链路上也能实现两端节点之间的数据包传输。 IP 大致分为三大作用模块，它们是 IP 寻址、路由（最终节点为止的转发）以及 IP 分包与组包。 4.1 IP 地址 4.1.1 IP 地址概述 在计算机通信中，为了识别通信对端，必须要有一个类似于地址的识别码进行标识。在数据链路中的 MAC 地址正是用来标识同一个链路中不同计算机的一种识别码。 作为网络层的 IP ,也有这种地址信息，一般叫做 IP 地址。IP 地址用于在“连接到网络中的所有主机中识别出进行通信的目标地址”。因此，在 TCP/IP 通信中所有主机或路由器必须设定自己的 IP 地址。 不论一台主机与哪种数据链路连接，其 IP 地址的形式都保持不变。 IP 地址（IPv4 地址）由32位正整数来表示。IP 地址在计算机内部以二进制方式被处理。然而，由于我们并不习惯于采用二进制方式，我们将32位的 IP 地址以每8位为一组，分成4组，每组以 “.” 隔开，再将每组数转换成十进制数。如下： 4.1.2 IP 地址由网络和主机两部分标识组成 如下图，网络标识在数据链路的每个段配置不同的值。网络标识必须保证相互连接的每个段的地址不相重复。而相同段内相连的主机必须有相同的网络地址。IP 地址的“主机标识”则不允许在同一个网段内重复出现。由此，可以通过设置网络地址和主机地址，在相互连接的整个网络中保证每台主机的 IP 地址都不会相互重叠。即 IP 地址具有了唯一性。 IP地址的主机标识 如下图，IP 包被转发到途中某个路由器时，正是利用目标 IP 地址的网络标识进行路由。因为即使不看主机标识，只要一见到网络标识就能判断出是否为该网段内的主机。 IP地址的网络标识 4.1.3 IP 地址的分类 IP 地址分为四个级别，分别为A类、B类、C类、D类。它根据 IP 地址中从第 1 位到第 4 位的比特列对其网络标识和主机标识进行区分。 A 类 IP 地址是首位以 “0” 开头的地址。从第 1 位到第 8 位是它的网络标识。用十进制表示的话，0.0.0.0~127.0.0.0 是 A 类的网络地址。A 类地址的后 24 位相当于主机标识。因此，一个网段内可容纳的主机地址上限为16,777,214个。 B 类 IP 地址是前两位 “10” 的地址。从第 1 位到第 16 位是它的网络标识。用十进制表示的话，128.0.0.0~191.255.0.0 是 B 类的网络地址。B 类地址的后 16 位相当于主机标识。因此，一个网段内可容纳的主机地址上限为65,534个。 C 类 IP 地址是前三位为 “110” 的地址。从第 1 位到第 24 位是它的网络标识。用十进制表示的话，192.0.0.0~223.255.255.0 是 C 类的网络地址。C 类地址的后 8 位相当于主机标识。因此，一个网段内可容纳的主机地址上限为254个。 D 类 IP 地址是前四位为 “1110” 的地址。从第 1 位到第 32 位是它的网络标识。用十进制表示的话，224.0.0.0~239.255.255.255 是 D 类的网络地址。D 类地址没有主机标识，常用于多播。 在分配 IP 地址时关于主机标识有一点需要注意。即要用比特位表示主机地址时，不可以全部为 0 或全部为 1。因为全部为 0 只有在表示对应的网络地址或 IP 地址不可以获知的情况下才使用。而全部为 1 的主机通常作为广播地址。因此，在分配过程中，应该去掉这两种情况。这也是为什么 C 类地址每个网段最多只能有 254（ 28 - 2 = 254）个主机地址的原因。 4.1.4 广播地址 广播地址用于在同一个链路中相互连接的主机之间发送数据包。将 IP 地址中的主机地址部分全部设置为 1，就成了广播地址。 广播分为本地广播和直接广播两种。在本网络内的广播叫做本地广播；在不同网络之间的广播叫做直接广播。 4.1.5 IP 多播 多播用于将包发送给特定组内的所有主机。由于其直接使用 IP 地址，因此也不存在可靠传输。 相比于广播，多播既可以穿透路由器，又可以实现只给那些必要的组发送数据包。请看下图： IP多播 多播使用 D 类地址。因此，如果从首位开始到第 4 位是 “1110”，就可以认为是多播地址。而剩下的 28 位可以成为多播的组编号。 此外， 对于多播，所有的主机（路由器以外的主机和终端主机）必须属于 224.0.0.1 的组，所有的路由器必须属于 224.0.0.2 的组。 1.6 子网掩码 现在一个 IP 地址的网络标识和主机标识已不再受限于该地址的类别，而是由一个叫做“子网掩码”的识别码通过子网网络地址细分出比 A 类、B 类、C 类更小粒度的网络。这种方式实际上就是将原来 A 类、B 类、C 类等分类中的主机地址部分用作子网地址，可以将原网络分为多个物理网络的一种机制。 子网掩码用二进制方式表示的话，也是一个 32 位的数字。它对应 IP 地址网络标识部分的位全部为 “1”，对应 IP 地址主机标识的部分则全部为 “0”。由此，一个 IP 地址可以不再受限于自己的类别，而是可以用这样的子网掩码自由地定位自己的网络标识长度。当然，子网掩码必须是 IP 地址的首位开始连续的 “1”。 对于子网掩码，目前有两种表示方式。第一种是，将 IP 地址与子网掩码的地址分别用两行来表示。以 172.20.100.52 的前 26 位是网络地址的情况为例，如下： 第二种表示方式是，在每个 IP 地址后面追加网络地址的位数用 “/ ” 隔开，如下： 另外，在第二种方式下记述网络地址时可以省略后面的 “0” 。例如：172.20.0.0/26 跟 172.20/26 其实是一个意思。 4.2 路由 发送数据包时所使用的地址是网络层的地址，即 IP 地址。然而仅仅有 IP 地址还不足以实现将数据包发送到对端目标地址，在数据发送过程中还需要类似于“指明路由器或主机”的信息，以便真正发往目标地址。保存这种信息的就是路由控制表。 该路由控制表的形成方式有两种：一种是管理员手动设置，另一种是路由器与其他路由器相互交换信息时自动刷新。前者也叫做静态路由控制，而后者叫做动态路由控制。 IP 协议始终认为路由表是正确的。然后，IP 本身并没有定义制作路由控制表的协议。即 IP 没有制作路由控制表的机制。该表示由一个叫做“路由协议”的协议制作而成。 4.2.1 IP 地址与路由控制 IP 地址的网络地址部分用于进行路由控制。 路由控制表中记录着网络地址与下一步应该发送至路由器的地址。 在发送 IP 包时，首先要确定 IP 包首部中的目标地址，再从路由控制表中找到与该地址具有相同网络地址的记录，根据该记录将 IP 包转发给相应的下一个路由器。如果路由控制表中存在多条相同网络地址的记录，就选择一个最为吻合的网络地址。 路由控制表与IP包发送 4.3 IP 分包与组包 每种数据链路的最大传输单元（MTU）都不尽相同，因为每个不同类型的数据链路的使用目的不同。使用目的不同，可承载的 MTU 也就不同。 任何一台主机都有必要对 IP 分片进行相应的处理。分片往往在网络上遇到比较大的报文无法一下子发送出去时才会进行处理。 经过分片之后的 IP 数据报在被重组的时候，只能由目标主机进行。路由器虽然做分片但不会进行重组。 4.3.1 路径 MTU 发现 分片机制也有它的不足。如路由器的处理负荷加重之类。因此，只要允许，是不希望由路由器进行 IP 数据包的分片处理的。 为了应对分片机制的不足，“路径 MTU 发现” 技术应运而生。路径 MTU 指的是，从发送端主机到接收端主机之间不需要分片是最大 MTU 的大小。即路径中存在的所有数据链路中最小的 MTU 。 进行路径 MTU 发现，就可以避免在中途的路由器上进行分片处理，也可以在 TCP 中发送更大的包。 4.4 IPv6 IPv6（IP version 6）是为了根本解决 IPv4 地址耗尽的问题而被标准化的网际协议。IPv4 的地址长度为 4 个 8 位字节，即 32 比特。而 IPv6 的地址长度则是原来的 4 倍，即 128 比特，一般写成 8 个 16 位字节。 4.4.1 IPv6 的特点 IP 得知的扩大与路由控制表的聚合。 性能提升。包首部长度采用固定的值（40字节），不再采用首部检验码。简化首部结构，减轻路由器负担。路由器不再做分片处理。 支持即插即用功能。即使没有DHCP服务器也可以实现自动分配 IP 地址。 采用认证与加密功能。应对伪造 IP 地址的网络安全功能以及防止线路窃听的功能。 多播、Mobile IP 成为扩展功能。 4.4.2 IPv6 中 IP 地址的标记方法 一般人们将 128 比特 IP 地址以每 16 比特为一组，每组用冒号（“：”）隔开进行标记。 而且如果出现连续的 0 时还可以将这些 0 省略，并用两个冒号（“：：”）隔开。但是，一个 IP 地址中只允许出现一次两个连续的冒号。 4.4.3 IPv6 地址的结构 IPv6 类似 IPv4，也是通过 IP 地址的前几位标识 IP 地址的种类。 在互联网通信中，使用一种全局的单播地址。它是互联网中唯一的一个地址，不需要正式分配 IP 地址。 4.4.4 全局单播地址 全局单播地址是指世界上唯一的一个地址。它是互联网通信以及各个域内部通信中最为常用的一个 IPv6 地址。 格式如下图所示，现在 IPv6 的网络中所使用的格式为，n = 48，m = 16 以及 128 - n - m = 64。即前 64 比特为网络标识，后 64 比特为主机标识。 全局单播地址 4.4.5 链路本地单播地址 链路本地单播地址是指在同一个数据链路内唯一的地址。它用于不经过路由器，在同一个链路中的通信。通常接口 ID 保存 64 比特版的 MAC 地址。 链路本地单播地址 4.4.6 唯一本地地址 唯一本地地址是不进行互联网通信时所用的地址。 唯一本地地址虽然不会与互联网连接，但是也会尽可能地随机生成一个唯一的全局 ID。 L 通常被置为 1 全局 ID 的值随机决定 子网 ID 是指该域子网地址 接口 ID 即为接口的 ID 唯一本地地址 4.4.7 IPv6 分段处理 IPv6 的分片处理只在作为起点的发送端主机上进行，路由器不参与分片。 IPv6 中最小 MTU 为 1280 字节，因此，在嵌入式系统中对于那些有一定系统资源限制的设备来说，不需要进行“路径 MTU 发现”，而是在发送 IP 包时直接以 1280 字节为单位分片送出。 4.4.8 IP 首部（暂略） 4.5 IP 协议相关技术 IP 旨在让最终目标主机收到数据包，但是在这一过程中仅仅有 IP 是无法实现通信的。必须还有能够解析主机名称和 MAC 地址的功能，以及数据包在发送过程中异常情况处理的功能。 4.5.1 DNS 我们平常在访问某个网站时不适用 IP 地址，而是用一串由罗马字和点号组成的字符串。而一般用户在使用 TCP/IP 进行通信时也不使用 IP 地址。能够这样做是因为有了 DNS （Domain Name System）功能的支持。DNS 可以将那串字符串自动转换为具体的 IP 地址。 这种 DNS 不仅适用于 IPv4，还适用于 IPv6。 4.5.2 ARP 只要确定了 IP 地址，就可以向这个目标地址发送 IP 数据报。然而，在底层数据链路层，进行实际通信时却有必要了解每个 IP 地址所对应的 MAC 地址。 ARP 是一种解决地址问题的协议。以目标 IP 地址为线索，用来定位下一个应该接收数据分包的网络设备对应的 MAC 地址。不过 ARP 只适用于 IPv4，不能用于 IPv6。IPv6 中可以用 ICMPv6 替代 ARP 发送邻居探索消息。 RARP 是将 ARP 反过来，从 MAC 地址定位 IP 地址的一种协议。 4.5.3 ICMP ICMP 的主要功能包括，确认 IP 包是否成功送达目标地址，通知在发送过程当中 IP 包被废弃的具体原因，改善网络设置等。 IPv4 中 ICMP 仅作为一个辅助作用支持 IPv4。也就是说，在 IPv4 时期，即使没有 ICMP，仍然可以实现 IP 通信。然而，在 IPv6 中，ICMP 的作用被扩大，如果没有 ICMPv6，IPv6 就无法进行正常通信。 4.5.4 DHCP 如果逐一为每一台主机设置 IP 地址会是非常繁琐的事情。特别是在移动使用笔记本电脑、只能终端以及平板电脑等设备时，每移动到一个新的地方，都要重新设置 IP 地址。 于是，为了实现自动设置 IP 地址、统一管理 IP 地址分配，就产生了 DHCP（Dynamic Host Configuration Protocol）协议。有了 DHCP，计算机只要连接到网络，就可以进行 TCP/IP 通信。也就是说，DHCP 让即插即用变得可能。 DHCP 不仅在 IPv4 中，在 IPv6 中也可以使用。 4.5.5 NAT NAT（Network Address Translator）是用于在本地网络中使用私有地址，在连接互联网时转而使用全局 IP 地址的技术。 除转换 IP 地址外，还出现了可以转换 TCP、UDP 端口号的 NAPT（Network Address Ports Translator）技术，由此可以实现用一个全局 IP 地址与多个主机的通信。 NAT（NAPT）实际上是为正在面临地址枯竭的 IPv4 而开发的技术。不过，在 IPv6 中为了提高网络安全也在使用 NAT，在 IPv4 和 IPv6 之间的相互通信当中常常使用 NAT-PT。 4.5.6 IP 隧道 夹着 IPv4 网络的两个 IPv6 网络 如上图的网络环境中，网络 A 与网络 B 之间无法直接进行通信，为了让它们之间正常通信，这时必须得采用 IP 隧道的功能。 IP 隧道可以将那些从网络 A 发过来的 IPv6 的包统合为一个数据，再为之追加一个 IPv4 的首部以后转发给网络 C。 一般情况下，紧接着 IP 首部的是 TCP 或 UDP 的首部。然而，现在的应用当中“ IP 首部的后面还是 IP 首部”或者“ IP 首部的后面是 IPv6 的首部”等情况与日俱增。这种在网络层的首部后面追加网络层首部的通信方法就叫做“ IP 隧道”。 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/网络协议/http/HTTP 协议.html":{"url":"linux/网络协议/http/HTTP 协议.html","title":"HTTP协议","keywords":"","body":"[toc] HTTP 协议 本文严重抄袭至互联网 总纲 一、概述 1.1 计算机网络体系结构分层 1.2 TCP/IP 通信传输流 利用 TCP/IP 协议族进行网络通信时，会通过分层顺序与对方进行通信。发送端从应用层往下走，接收端则从链路层往上走。如下： TCP/IP 通信传输流 首先作为发送端的客户端在应用层（HTTP 协议）发出一个想看某个 Web 页面的 HTTP 请求。 接着，为了传输方便，在传输层（TCP 协议）把从应用层处收到的数据（HTTP 请求报文）进行分割，并在各个报文上打上标记序号及端口号后转发给网络层。 在网络层（IP 协议），增加作为通信目的地的 MAC 地址后转发给链路层。这样一来，发往网络的通信请求就准备齐全了。 接收端的服务器在链路层接收到数据，按序往上层发送，一直到应用层。当传输到应用层，才能算真正接收到由客户端发送过来的 HTTP请求。 HTTP请求如下图所示： 在网络体系结构中，包含了众多的网络协议，这篇文章主要围绕 HTTP 协议（HTTP/1.1版本）展开。 HTTP协议（HyperText Transfer Protocol，超文本传输协议）是用于从WWW服务器传输超文本到本地浏览器的传输协议。它可以使浏览器更加高效，使网络传输减少。它不仅保证计算机正确快速地传输超文本文档，还确定传输文档中的哪一部分，以及哪部分内容首先显示(如文本先于图形)等。 HTTP是客户端浏览器或其他程序与Web服务器之间的应用层通信协议。在Internet上的Web服务器上存放的都是超文本信息，客户机需要通过HTTP协议传输所要访问的超文本信息。HTTP包含命令和传输信息，不仅可用于Web访问，也可以用于其他因特网/内联网应用系统之间的通信，从而实现各类应用资源超媒体访问的集成。 我们在浏览器的地址栏里输入的网站地址叫做URL (Uniform Resource Locator，统一资源定位符)。就像每家每户都有一个门牌地址一样，每个网页也都有一个Internet地址。当你在浏览器的地址框中输入一个URL或是单击一个超级链接时，URL就确定了要浏览的地址。浏览器通过超文本传输协议(HTTP)，将Web服务器上站点的网页代码提取出来，并翻译成漂亮的网页。 二、HTTP 工作过程 HTTP请求响应模型 HTTP通信机制是在一次完整的 HTTP 通信过程中，客户端与服务器之间将完成下列7个步骤： 1.建立 TCP 连接 在HTTP工作开始之前，客户端首先要通过网络与服务器建立连接，该连接是通过 TCP 来完成的，该协议与 IP 协议共同构建 Internet，即著名的 TCP/IP 协议族，因此 Internet 又被称作是 TCP/IP 网络。HTTP 是比 TCP 更高层次的应用层协议，根据规则，只有低层协议建立之后，才能进行高层协议的连接，因此，首先要建立 TCP 连接，一般 TCP 连接的端口号是80； 2.客户端向服务器发送请求命令 一旦建立了TCP连接，客户端就会向服务器发送请求命令； 例如：GET/sample/hello.jsp HTTP/1.1 3.客户端发送请求头信息 客户端发送其请求命令之后，还要以头信息的形式向服务器发送一些别的信息，之后客户端发送了一空白行来通知服务器，它已经结束了该头信息的发送； 4.服务器应答 客户端向服务器发出请求后，服务器会客户端返回响应； 例如： HTTP/1.1 200 OK 响应的第一部分是协议的版本号和响应状态码 5.服务器返回响应头信息 正如客户端会随同请求发送关于自身的信息一样，服务器也会随同响应向用户发送关于它自己的数据及被请求的文档； 6.服务器向客户端发送数据 服务器向客户端发送头信息后，它会发送一个空白行来表示头信息的发送到此为结束，接着，它就以 Content-Type 响应头信息所描述的格式发送用户所请求的实际数据； 7.服务器关闭 TCP 连接 一般情况下，一旦服务器向客户端返回了请求数据，它就要关闭 TCP 连接，然后如果客户端或者服务器在其头信息加入了这行代码 Connection:keep-alive ，TCP 连接在发送后将仍然保持打开状态，于是，客户端可以继续通过相同的连接发送请求。保持连接节省了为每个请求建立新连接所需的时间，还节约了网络带宽。 三、HTTP 协议基础 3.1 通过请求和响应的交换达成通信 应用 HTTP 协议时，必定是一端担任客户端角色，另一端担任服务器端角色。仅从一条通信线路来说，服务器端和客服端的角色是确定的。HTTP 协议规定，请求从客户端发出，最后服务器端响应该请求并返回。换句话说，肯定是先从客户端开始建立通信的，服务器端在没有接收到请求之前不会发送响应。 3.2 HTTP 是不保存状态的协议 HTTP 是一种无状态协议。协议自身不对请求和响应之间的通信状态进行保存。也就是说在 HTTP 这个级别，协议对于发送过的请求或响应都不做持久化处理。这是为了更快地处理大量事务，确保协议的可伸缩性，而特意把 HTTP 协议设计成如此简单的。 可是随着 Web 的不断发展，我们的很多业务都需要对通信状态进行保存。于是我们引入了 Cookie 技术。有了 Cookie 再用 HTTP 协议通信，就可以管理状态了。 3.3 使用 Cookie 的状态管理 Cookie 技术通过在请求和响应报文中写入 Cookie 信息来控制客户端的状态。Cookie 会根据从服务器端发送的响应报文内的一个叫做 Set-Cookie 的首部字段信息，通知客户端保存Cookie。当下次客户端再往该服务器发送请求时，客户端会自动在请求报文中加入 Cookie 值后发送出去。服务器端发现客户端发送过来的 Cookie 后，会去检查究竟是从哪一个客户端发来的连接请求，然后对比服务器上的记录，最后得到之前的状态信息。 Cookie 的流程 3.4 请求 URI 定位资源 HTTP 协议使用 URI 定位互联网上的资源。正是因为 URI 的特定功能，在互联网上任意位置的资源都能访问到。 3.5 告知服务器意图的 HTTP 方法（HTTP/1.1） 方法 描述 请求 响应 GET 用来请求访问已被URI识别的资源指定的资源经服务器端解析后返回响应内容 GET /index.html HTTP/1.1Host: www.baidu.com 返回index.html的页面资源 POST 用来传输实体的主体虽然用GET方法也可以传输实体的主体，但一般不用GET方法进行传输，而是用POST方法虽说POST的功能与GET相似，但POST的主要目的并不是获取响应的主体内容 POST /submit.cgi HTTP/1.1Host: www.baidu.comContent-Length: 1500(1500字节的数据) 返回submit.cgi接收数据的处理结果 PUT 用来传输文件就像FTP协议的文件上传一样，要求在请求报文的主体中包含文件的内容，然后保存到请求URI指定的位置 PUT /example.html HTTP/1.1Host: www.baidu.comContent-Type: text/htmlContent-Length: 1560(1560字节的数据) 响应返回状态吗 204 No Content(比如：该html已存在于服务器上) HEAD 用于确认URI的有效性及资源更新的日期时间等，和GET方法一样，只是不返回报文主体部分 HEAD /index.html HTTP/1.1Host: www.baidu.com 返回index.html有关的响应头部 DELETE 用来删除文件，是与PUT 相反的方法按请求URI删除指定的资源 DELETE /example.html HTTP/1.1Host: www.baidu.com 响应返回状态吗 204 No Content(比如：该html已存在于服务器上) OPTIONS 用来查询针对请求URI指定的资源支持的方法 OPTIONS * HTTP/1.1Host: www.baidu.com HTTP/1.1 200 OKAllow: GET,POST,HEAD,OPTIONS(返回服务器支持的方法) TRACE 让web服务器端将之前的请求通信环回给客户端的方法发送请求时，在Max-Forwards首部字段中填入数值，每经过一个服务端就将该数字减1，当数值刚好剑到0时就停止持续传输，最后接收到请求的服务端则返回状态码 200 OK 的响应 TRACE /HTTP/1.1Host: www.baidu.comMax-Forwards: 2 HTTP/1.1 200 OkContent-Type: message/httpContent-Length: 1024TRACE / HTTP/1.1Host: www.baidu.comMax-Forwards: 2(返回响应包含请求内容) CONNECT 要求在与代理服务器通信时建立隧道，实现用隧道协议进行TCP通信，主要使用SSL和TLS协议把通信内容加密后经网络隧道传输 CONNECT proxy.sample.com:8000 HTTP/1.1Host: proxy.sample.com HTTP/1.1 200 OK(之后进入网络隧道) 3.6 持久连接 HTTP 协议的初始版本中，每进行一个 HTTP 通信都要断开一次 TCP 连接。比如使用浏览器浏览一个包含多张图片的 HTML 页面时，在发送请求访问 HTML 页面资源的同时，也会请求该 HTML 页面里包含的其他资源。因此，每次的请求都会造成无畏的 TCP 连接建立和断开，增加通信量的开销。 为了解决上述 TCP 连接的问题，HTTP/1.1 和部分 HTTP/1.0 想出了持久连接的方法。其特点是，只要任意一端没有明确提出断开连接，则保持 TCP 连接状态。旨在建立一次 TCP 连接后进行多次请求和响应的交互。在 HTTP/1.1 中，所有的连接默认都是持久连接。 3.7 管线化 持久连接使得多数请求以管线化方式发送成为可能。以前发送请求后需等待并接收到响应，才能发送下一个请求。管线化技术出现后，不用等待亦可发送下一个请求。这样就能做到同时并行发送多个请求，而不需要一个接一个地等待响应了。 比如，当请求一个包含多张图片的 HTML 页面时，与挨个连接相比，用持久连接可以让请求更快结束。而管线化技术要比持久连接速度更快。请求数越多，时间差就越明显。 四、HTTP 协议报文结构 4.1 HTTP 报文 用于 HTTP 协议交互的信息被称为 HTTP 报文。请求端（客户端）的 HTTP 报文叫做请求报文；响应端（服务器端）的叫做响应报文。HTTP 报文本身是由多行（用 CR+LF 作换行符）数据构成的字符串文本。 4.2 HTTP 报文结构 HTTP 报文大致可分为报文首部和报文主体两部分。两者由最初出现的空行（CR+LF）来划分。通常，并不一定有报文主体。如下： HTTP 报文结构 4.2.1 请求报文结构 请求报文结构 请求报文的首部内容由以下数据组成： 请求行 —— 包含用于请求的方法、请求 URI 和 HTTP 版本。 首部字段 —— 包含表示请求的各种条件和属性的各类首部。（通用首部、请求首部、实体首部以及RFC里未定义的首部如 Cookie 等） 请求报文的示例，如下： 4.2.2 响应报文结构 响应报文的首部内容由以下数据组成： 状态行 —— 包含表明响应结果的状态码、原因短语和 HTTP 版本。 首部字段 —— 包含表示请求的各种条件和属性的各类首部。（通用首部、响应首部、实体首部以及RFC里未定义的首部如 Cookie 等） 响应报文的示例，如下： 五、HTTP 报文首部之请求行、状态行 5.1 请求行 举个栗子，下面是一个 HTTP 请求的报文： GET /index.htm HTTP/1.1 Host: sample.com 其中，下面的这行就是请求行， GET /index.htm HTTP/1.1 开头的 GET 表示请求访问服务器的类型，称为方法； 随后的字符串 /index.htm 指明了请求访问的资源对象，也叫做请求 URI； 最后的 HTTP/1.1，即 HTTP 的版本号，用来提示客户端使用的 HTTP 协议功能。 综合来看，大意是请求访问某台 HTTP 服务器上的 /index.htm 页面资源。 5.2 状态行 同样举个栗子，下面是一个 HTTP 响应的报文： HTTP/1.1 200 OK Date: Mon, 10 Jul 2017 15:50:06 GMT Content-Length: 256 Content-Type: text/html ... 其中，下面的这行就是状态行， HTTP/1.1 200 OK 开头的 HTTP/1.1 表示服务器对应的 HTTP 版本； 紧挨着的 200 OK 表示请求的处理结果的状态码和原因短语。 六、HTTP 报文首部之首部字段（重点分析） 6.1 首部字段概述 先来回顾一下首部字段在报文的位置，HTTP 报文包含报文首部和报文主体，报文首部包含请求行（或状态行）和首部字段。 在报文众多的字段当中，HTTP 首部字段包含的信息最为丰富。首部字段同时存在于请求和响应报文内，并涵盖 HTTP 报文相关的内容信息。使用首部字段是为了给客服端和服务器端提供报文主体大小、所使用的语言、认证信息等内容。 6.2 首部字段结构 HTTP 首部字段是由首部字段名和字段值构成的，中间用冒号“：”分隔。 另外，字段值对应单个 HTTP 首部字段可以有多个值。 当 HTTP 报文首部中出现了两个或以上具有相同首部字段名的首部字段时，这种情况在规范内尚未明确，根据浏览器内部处理逻辑的不同，优先处理的顺序可能不同，结果可能并不一致。 首部字段名 冒号 字段值 Content-Type ： text/html Keep-Alive ： timeout=30, max=120 6.3 首部字段类型 首部字段根据实际用途被分为以下4种类型： 类型 描述 通用首部字段 请求报文和响应报文两方都会使用的首部 请求首部字段 从客户端向服务器端发送请求报文时使用的首部。补充了请求的附加内容、客户端信息、响应内容相关优先级等信息 响应首部字段 从服务器端向客户端返回响应报文时使用的首部。补充了响应的附加内容，也会要求客户端附加额外的内容信息。 实体首部字段 针对请求报文和响应报文的实体部分使用的首部。补充了资源内容更新时间等与实体有关的的信息。 6.4 通用首部字段（HTTP/1.1） 首部字段名 说明 Cache-Control 控制缓存的行为 Connection 逐挑首部、连接的管理 Date 创建报文的日期时间 Pragma 报文指令 Trailer 报文末端的首部一览 Transfer-Encoding 指定报文主体的传输编码方式 Upgrade 升级为其他协议 Via 代理服务器的相关信息 Warning 错误通知 6.4.1 Cache-Control 通过指定首部字段 Cache-Control 的指令，就能操作缓存的工作机制。 6.4.1.1 可用的指令一览 可用的指令按请求和响应分类如下： 缓存请求指令 指令 参数 说明 no-cache 无 强制向服务器再次验证 no-store 无 不缓存请求或响应的任何内容 max-age = [秒] 必需 响应的最大Age值 max-stale( =[秒]) 可省略 接收已过期的响应 min-fresh = [秒] 必需 期望在指定时间内的响应仍有效 no-transform 无 代理不可更改媒体类型 only-if-cached 无 从缓存获取资源 cache-extension - 新指令标记（token） 缓存响应指令 指令 参数 说明 public 无 可向任意方提供响应的缓存 private 可省略 仅向特定用户返回响应 no-cache 可省略 缓存前必须先确认其有效性 no-store 无 不缓存请求或响应的任何内容 no-transform 无 代理不可更改媒体类型 must-revalidate 无 可缓存但必须再向源服务器进行确认 proxy-revalidate 无 要求中间缓存服务器对缓存的响应有效性再进行确认 max-age = [秒] 必需 响应的最大Age值 s-maxage = [秒] 必需 公共缓存服务器响应的最大Age值 cache-extension - 新指令标记（token） 6.4.1.2 表示能否缓存的指令 public 指令 Cache-Control: public 当指定使用 public 指令时，则明确表明其他用户也可利用缓存。 private 指令 Cache-Control: private 当指定 private 指令后，响应只以特定的用户作为对象，这与 public 指令的行为相反。缓存服务器会对该特定用户提供资源缓存的服务，对于其他用户发送过来的请求，代理服务器则不会返回缓存。 no-cache 指令 Cache-Control: no-cache 使用 no-cache 指令是为了防止从缓存中返回过期的资源。 客户端发送的请求中如果包含 no-cache 指令，则表示客户端将不会接收缓存过的响应。于是，“中间”的缓存服务器必须把客户端请求转发给源服务器。 如果服务器中返回的响应包含 no-cache 指令，那么缓存服务器不能对资源进行缓存。源服务器以后也将不再对缓存服务器请求中提出的资源有效性进行确认，且禁止其对响应资源进行缓存操作。 Cache-Control: no-cache=Location 由服务器返回的响应中，若报文首部字段 Cache-Control 中对 no-cache 字段名具体指定参数值，那么客户端在接收到这个被指定参数值的首部字段对应的响应报文后，就不能使用缓存。换言之，无参数值的首部字段可以使用缓存。只能在响应指令中指定该参数。 no-store 指令 Cache-Control: no-store 当使用 no-store 指令时，暗示请求（和对应的响应）或响应中包含机密信息。因此，该指令规定缓存不能在本地存储请求或响应的任一部分。 注意：no-cache 指令代表不缓存过期的指令，缓存会向源服务器进行有效期确认后处理资源；no-store 指令才是真正的不进行缓存。 6.4.1.3 指定缓存期限和认证的指令 s-maxage 指令 Cache-Control: s-maxage=604800（单位：秒） s-maxage 指令的功能和 max-age 指令的相同，它们的不同点是 s-maxage 指令只适用于供多位用户使用的公共缓存服务器（一般指代理）。也就是说，对于向同一用户重复返回响应的服务器来说，这个指令没有任何作用。 另外，当使用 s-maxage 指令后，则直接忽略对 Expires 首部字段及 max-age 指令的处理。 max-age 指令 Cache-Control: max-age=604800（单位：秒） 当客户端发送的请求中包含 max-age 指令时，如果判定缓存资源的缓存时间数值比指定的时间更小，那么客户端就接收缓存的资源。另外，当指定 max-age 的值为0，那么缓存服务器通常需要将请求转发给源服务器。 当服务器返回的响应中包含 max-age 指令时，缓存服务器将不对资源的有效性再作确认，而 max-age 数值代表资源保存为缓存的最长时间。 应用 HTTP/1.1 版本的缓存服务器遇到同时存在 Expires 首部字段的情况时，会优先处理 max-age 指令，并忽略掉 Expires 首部字段；而 HTTP/1.0 版本的缓存服务器则相反。 min-fresh 指令 Cache-Control: min-fresh=60（单位：秒） min-fresh 指令要求缓存服务器返回至少还未过指定时间的缓存资源。 max-stale 指令 Cache-Control: max-stale=3600（单位：秒） 使用 max-stale 可指示缓存资源，即使过期也照常接收。 如果指令未指定参数值，那么无论经过多久，客户端都会接收响应；如果指定了具体参数值，那么即使过期，只要仍处于 max-stale 指定的时间内，仍旧会被客户端接收。 only-if-cached 指令 Cache-Control: only-if-cached 表示客户端仅在缓存服务器本地缓存目标资源的情况下才会要求其返回。换言之，该指令要求缓存服务器不重新加载响应，也不会再次确认资源的有效性。 must-revalidate 指令 Cache-Control: must-revalidate 使用 must-revalidate 指令，代理会向源服务器再次验证即将返回的响应缓存目前是否仍有效。另外，使用 must-revalidate 指令会忽略请求的 max-stale 指令。 proxy-revalidate 指令 Cache-Control: proxy-revalidate proxy-revalidate 指令要求所有的缓存服务器在接收到客户端带有该指令的请求返回响应之前，必须再次验证缓存的有效性。 no-transform 指令 Cache-Control: no-transform 使用 no-transform 指令规定无论是在请求还是响应中，缓存都不能改变实体主体的媒体类型。这样做可防止缓存或代理压缩图片等类似操作。 6.4.1.4 Cache-Control 扩展 Cache-Control: private, community=\"UCI\" 通过 cache-extension 标记（token），可以扩展 Cache-Control 首部字段内的指令。上述 community 指令即扩展的指令，如果缓存服务器不能理解这个新指令，就会直接忽略掉。 6.4.2 Connection Connection 首部字段具备以下两个作用： 控制不再转发的首部字段 Connection: Upgrade 在客户端发送请求和服务器返回响应中，使用 Connection 首部字段，可控制不再转发给代理的首部字段，即删除后再转发（即Hop-by-hop首部）。 管理持久连接 Connection: close HTTP/1.1 版本的默认连接都是持久连接。当服务器端想明确断开连接时，则指定 Connection 首部字段的值为 close。 Connection: Keep-Alive HTTP/1.1 之前的 HTTP 版本的默认连接都是非持久连接。为此，如果想在旧版本的 HTTP 协议上维持持续连接，则需要指定 Connection 首部字段的值为 Keep-Alive。 6.4.3 Date 表明创建 HTTP 报文的日期和时间。 Date: Mon, 10 Jul 2017 15:50:06 GMT HTTP/1.1 协议使用在 RFC1123 中规定的日期时间的格式。 6.4.4 Pragma Pragma 首部字段是 HTTP/1.1 版本之前的历史遗留字段，仅作为与 HTTP/1.0 的向后兼容而定义。 Pragma: no-cache 该首部字段属于通用首部字段，但只用在客户端发送的请求中，要求所有的中间服务器不返回缓存的资源。 所有的中间服务器如果都能以 HTTP/1.1 为基准，那直接采用 Cache-Control: no-cache 指定缓存的处理方式最为理想。但是要整体掌握所有中间服务器使用的 HTTP 协议版本却是不现实的，所以，发送的请求会同时包含下面两个首部字段： Cache-Control: no-cache Pragma: no-cache 6.4.5 Trailer Trailer: Expires 首部字段 Trailer 会事先说明在报文主体后记录了哪些首部字段。可应用在 HTTP/1.1 版本分块传输编码时。 6.4.6 Transfer-Encoding Transfer-Encoding: chunked 规定了传输报文主体时采用的编码方式。 HTTP/1.1 的传输编码方式仅对分块传输编码有效。 6.4.7 Upgrade Upgrade: TSL/1.0 用于检测 HTTP 协议及其他协议是否可使用更高的版本进行通信，其参数值可以用来指定一个完全不同的通信协议。 6.4.8 Via Via: 1.1 a1.sample.com(Squid/2.7) 为了追踪客户端和服务器端之间的请求和响应报文的传输路径。 报文经过代理或网关时，会现在首部字段 Via 中附加该服务器的信息，然后再进行转发。 首部字段 Via 不仅用于追踪报文的转发，还可避免请求回环的发生。 6.4.9 Warning 该首部字段通常会告知用户一些与缓存相关的问题的警告。 Warning 首部字段的格式如下： Warning：[警告码][警告的主机:端口号] \"[警告内容]\"([日期时间]) 最后的日期时间可省略。 HTTP/1.1 中定义了7种警告，警告码对应的警告内容仅推荐参考，另外，警告码具备扩展性，今后有可能追加新的警告码。 警告码 警告内容 说明 110 Response is stale(响应已过期) 代理返回已过期的资源 111 Revalidation failed(再验证失败) 代理再验证资源有效性时失败（服务器无法到达等原因） 112 Disconnection operation(断开连接操作) 代理与互联网连接被故意切断 113 Heuristic expiration(试探性过期) 响应的试用期超过24小时(有效缓存的设定时间大于24小时的情况下) 199 Miscellaneous warning(杂项警告) 任意的警告内容 214 Transformation applied(使用了转换) 代理对内容编码或媒体类型等执行了某些处理时 299 Miscellaneous persistent warning(持久杂项警告) 任意的警告内容 6.5 请求首部字段（HTTP/1.1） 首部字段名 说明 Accept 用户代理可处理的媒体类型 Accept-Charset 优先的字符集 Accept-Encoding 优先的内容编码 Accept-Language 优先的语言（自然语言） Authorization Web认证信息 Expect 期待服务器的特定行为 From 用户的电子邮箱地址 Host 请求资源所在服务器 If-Match 比较实体标记（ETag） If-Modified-Since 比较资源的更新时间 If-None-Match 比较实体标记（与 If-Macth 相反） If-Range 资源未更新时发送实体 Byte 的范围请求 If-Unmodified-Since 比较资源的更新时间(与 If-Modified-Since 相反) Max-Forwards 最大传输逐跳数 Proxy-Authorization 代理服务器要求客户端的认证信息 Range 实体的字节范围请求 Referer 对请求中 URI 的原始获取方 TE 传输编码的优先级 User-Agent HTTP 客户端程序的信息 6.5.1 Accept Accept: text/html, application/xhtml+xml, application/xml; q=0.5 Accept 首部字段可通知服务器，用户代理能够处理的媒体类型及媒体类型的相对优先级。可使用 type/subtype 这种形式，一次指定多种媒体类型。 若想要给显示的媒体类型增加优先级，则使用 q=[数值] 来表示权重值，用分号（;）进行分隔。权重值的范围 0~1（可精确到小数点后三位），且 1 为最大值。不指定权重值时，默认为 1。 6.5.2 Accept-Charset Accept-Charset: iso-8859-5, unicode-1-1; q=0.8 Accept-Charset 首部字段可用来通知服务器用户代理支持的字符集及字符集的相对优先顺序。另外，可一次性指定多种字符集。同样使用 q=[数值] 来表示相对优先级。 6.5.3 Accept-Encoding Accept-Encoding: gzip, deflate Accept-Encoding 首部字段用来告知服务器用户代理支持的内容编码及内容编码的优先顺序，并可一次性指定多种内容编码。同样使用 q=[数值] 来表示相对优先级。也可使用星号（*）作为通配符，指定任意的编码格式。 6.5.4 Accept-Language Accept-Lanuage: zh-cn,zh;q=0.7,en=us,en;q=0.3 告知服务器用户代理能够处理的自然语言集（指中文或英文等），以及自然语言集的相对优先级，可一次性指定多种自然语言集。同样使用 q=[数值] 来表示相对优先级。 6.5.5 Authorization Authorization: Basic ldfKDHKfkDdasSAEdasd== 告知服务器用户代理的认证信息（证书值）。通常，想要通过服务器认证的用户代理会在接收到返回的 401 状态码响应后，把首部字段 Authorization 加入请求中。共用缓存在接收到含有 Authorization 首部字段的请求时的操作处理会略有差异。 6.5.6 Expect Expect: 100-continue 告知服务器客户端期望出现的某种特定行为。 6.5.7 From From: Deeson_Woo@163.com 告知服务器使用用户代理的电子邮件地址。 6.5.8 Host Host: www.jianshu.com 告知服务器，请求的资源所处的互联网主机和端口号。 Host 首部字段是 HTTP/1.1 规范内唯一一个必须被包含在请求内的首部字段。 若服务器未设定主机名，那直接发送一个空值即可 Host: 。 6.5.9 If-Match 形如 If-xxx 这种样式的请求首部字段，都可称为条件请求。服务器接收到附带条件的请求后，只有判断指定条件为真时，才会执行请求。 If-Match: \"123456\" 首部字段 If-Match，属附带条件之一，它会告知服务器匹配资源所用的实体标记（ETag）值。这时的服务器无法使用弱 ETag 值。 服务器会比对 If-Match 的字段值和资源的 ETag 值，仅当两者一致时，才会执行请求。反之，则返回状态码 412 Precondition Failed 的响应。 还可以使用星号（*）指定 If-Match 的字段值。针对这种情况，服务器将会忽略 ETag 的值，只要资源存在就处理请求。 6.5.10 If-Modified-Since If-Modified-Since: Mon, 10 Jul 2017 15:50:06 GMT 首部字段 If-Modified-Since，属附带条件之一，用于确认代理或客户端拥有的本地资源的有效性。 它会告知服务器若 If-Modified-Since 字段值早于资源的更新时间，则希望能处理该请求。而在指定 If-Modified-Since 字段值的日期时间之后，如果请求的资源都没有过更新，则返回状态码 304 Not Modified 的响应。 6.5.11 If-None-Match If-None-Match: \"123456\" 首部字段 If-None-Match 属于附带条件之一。它和首部字段 If-Match 作用相反。用于指定 If-None-Match 字段值的实体标记（ETag）值与请求资源的 ETag 不一致时，它就告知服务器处理该请求。 6.5.12 If-Range If-Range: \"123456\" 首部字段 If-Range 属于附带条件之一。它告知服务器若指定的 If-Range 字段值（ETag 值或者时间）和请求资源的 ETag 值或时间相一致时，则作为范围请求处理。反之，则返回全体资源。 下面我们思考一下不使用首部字段 If-Range 发送请求的情况。服务器端的资源如果更新，那客户端持有资源中的一部分也会随之无效，当然，范围请求作为前提是无效的。这时，服务器会暂且以状态码 412 Precondition Failed 作为响应返回，其目的是催促客户端再次发送请求。这样一来，与使用首部字段 If-Range 比起来，就需要花费两倍的功夫。 6.5.13 If-Unmodified-Since If-Unmodified-Since: Mon, 10 Jul 2017 15:50:06 GMT 首部字段 If-Unmodified-Since 和首部字段 If-Modified-Since 的作用相反。它的作用的是告知服务器，指定的请求资源只有在字段值内指定的日期时间之后，未发生更新的情况下，才能处理请求。如果在指定日期时间后发生了更新，则以状态码 412 Precondition Failed 作为响应返回。 6.5.14 Max-Forwards Max-Forwards: 10 通过 TRACE 方法或 OPTIONS 方法，发送包含首部字段 Max-Forwards 的请求时，该字段以十进制整数形式指定可经过的服务器最大数目。服务器在往下一个服务器转发请求之前，Max-Forwards 的值减 1 后重新赋值。当服务器接收到 Max-Forwards 值为 0 的请求时，则不再进行转发，而是直接返回响应。 6.5.15 Proxy-Authorization Proxy-Authorization: Basic dGlwOjkpNLAGfFY5 接收到从代理服务器发来的认证质询时，客户端会发送包含首部字段 Proxy-Authorization 的请求，以告知服务器认证所需要的信息。 这个行为是与客户端和服务器之间的 HTTP 访问认证相类似的，不同之处在于，认证行为发生在客户端与代理之间。 6.5.16 Range Range: bytes=5001-10000 对于只需获取部分资源的范围请求，包含首部字段 Range 即可告知服务器资源的指定范围。 接收到附带 Range 首部字段请求的服务器，会在处理请求之后返回状态码为 206 Partial Content 的响应。无法处理该范围请求时，则会返回状态码 200 OK 的响应及全部资源。 6.5.17 Referer Referer: http://www.sample.com/index.html 首部字段 Referer 会告知服务器请求的原始资源的 URI。 6.5.18 TE TE: gzip, deflate; q=0.5 首部字段 TE 会告知服务器客户端能够处理响应的传输编码方式及相对优先级。它和首部字段 Accept-Encoding 的功能很相像，但是用于传输编码。 首部字段 TE 除指定传输编码之外，还可以指定伴随 trailer 字段的分块传输编码的方式。应用后者时，只需把 trailers 赋值给该字段值。TE: trailers 6.5.19 User-Agent User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:13.0) Gecko/20100101 首部字段 User-Agent 会将创建请求的浏览器和用户代理名称等信息传达给服务器。 由网络爬虫发起请求时，有可能会在字段内添加爬虫作者的电子邮件地址。此外，如果请求经过代理，那么中间也很可能被添加上代理服务器的名称。 6.6 响应首部字段（HTTP/1.1） 首部字段名 说明 Accept-Ranges 是否接受字节范围请求 Age 推算资源创建经过时间 ETag 资源的匹配信息 Location 令客户端重定向至指定 URI Proxy-Authenticate 代理服务器对客户端的认证信息 Retry-After 对再次发起请求的时机要求 Server HTTP 服务器的安装信息 Vary 代理服务器缓存的管理信息 WWW-Authenticate 服务器对客户端的认证信息 6.6.1 Accept-Ranges Accept-Ranges: bytes 首部字段 Accept-Ranges 是用来告知客户端服务器是否能处理范围请求，以指定获取服务器端某个部分的资源。 可指定的字段值有两种，可处理范围请求时指定其为 bytes，反之则指定其为 none。 6.6.2 Age Age: 1200 首部字段 Age 能告知客户端，源服务器在多久前创建了响应。字段值的单位为秒。 若创建该响应的服务器是缓存服务器，Age 值是指缓存后的响应再次发起认证到认证完成的时间值。代理创建响应时必须加上首部字段 Age。 6.6.3 ETag ETag: \"usagi-1234\" 首部字段 ETag 能告知客户端实体标识。它是一种可将资源以字符串形式做唯一性标识的方式。服务器会为每份资源分配对应的 ETag 值。 另外，当资源更新时，ETag 值也需要更新。生成 ETag 值时，并没有统一的算法规则，而仅仅是由服务器来分配。 ETag 中有强 ETag 值和弱 ETag 值之分。强 ETag 值，不论实体发生多么细微的变化都会改变其值；弱 ETag 值只用于提示资源是否相同。只有资源发生了根本改变，产生差异时才会改变 ETag 值。这时，会在字段值最开始处附加 W/： ETag: W/\"usagi-1234\"。 6.6.4 Location Location: http://www.sample.com/sample.html 使用首部字段 Location 可以将响应接收方引导至某个与请求 URI 位置不同的资源。 基本上，该字段会配合 3xx ：Redirection 的响应，提供重定向的 URI。 几乎所有的浏览器在接收到包含首部字段 Location 的响应后，都会强制性地尝试对已提示的重定向资源的访问。 6.6.5 Proxy-Authenticate Proxy-Authenticate: Basic realm=\"Usagidesign Auth\" 首部字段 Proxy-Authenticate 会把由代理服务器所要求的认证信息发送给客户端。 它与客户端和服务器之间的 HTTP 访问认证的行为相似，不同之处在于其认证行为是在客户端与代理之间进行的。 6.6.6 Retry-After Retry-After: 180 首部字段 Retry-After 告知客户端应该在多久之后再次发送请求。主要配合状态码 503 Service Unavailable 响应，或 3xx Redirect 响应一起使用。 字段值可以指定为具体的日期时间（Mon, 10 Jul 2017 15:50:06 GMT 等格式），也可以是创建响应后的秒数。 6.6.7 Server Server: Apache/2.2.6 (Unix) PHP/5.2.5 首部字段 Server 告知客户端当前服务器上安装的 HTTP 服务器应用程序的信息。不单单会标出服务器上的软件应用名称，还有可能包括版本号和安装时启用的可选项。 6.6.8 Vary Vary: Accept-Language 首部字段 Vary 可对缓存进行控制。源服务器会向代理服务器传达关于本地缓存使用方法的命令。 从代理服务器接收到源服务器返回包含 Vary 指定项的响应之后，若再要进行缓存，仅对请求中含有相同 Vary 指定首部字段的请求返回缓存。即使对相同资源发起请求，但由于 Vary 指定的首部字段不相同，因此必须要从源服务器重新获取资源。 6.6.9 WWW-Authenticate WWW-Authenticate: Basic realm=\"Usagidesign Auth\" 首部字段 WWW-Authenticate 用于 HTTP 访问认证。它会告知客户端适用于访问请求 URI 所指定资源的认证方案（Basic 或是 Digest）和带参数提示的质询（challenge）。 6.7 实体首部字段（HTTP/1.1） 首部字段名 说明 Allow 资源可支持的 HTTP 方法 Content-Encoding 实体主体适用的编码方式 Content-Language 实体主体的自然语言 Content-Length 实体主体的大小（单位：字节） Content-Location 替代对应资源的 URI Content-MD5 实体主体的报文摘要 Content-Range 实体主体的位置范围 Content-Type 实体主体的媒体类型 Expires 实体主体过期的日期时间 Last-Modified 资源的最后修改日期时间 6.7.1 Allow Allow: GET, HEAD 首部字段 Allow 用于通知客户端能够支持 Request-URI 指定资源的所有 HTTP 方法。 当服务器接收到不支持的 HTTP 方法时，会以状态码 405 Method Not Allowed 作为响应返回。与此同时，还会把所有能支持的 HTTP 方法写入首部字段 Allow 后返回。 6.7.2 Content-Encoding Content-Encoding: gzip 首部字段 Content-Encoding 会告知客户端服务器对实体的主体部分选用的内容编码方式。内容编码是指在不丢失实体信息的前提下所进行的压缩。 主要采用这 4 种内容编码的方式（gzip、compress、deflate、identity）。 6.7.3 Content-Language Content-Language: zh-CN 首部字段 Content-Language 会告知客户端，实体主体使用的自然语言（指中文或英文等语言）。 6.7.4 Content-Length Content-Length: 15000 首部字段 Content-Length 表明了实体主体部分的大小（单位是字节）。对实体主体进行内容编码传输时，不能再使用 Content-Length首部字段。 6.7.5 Content-Location Content-Location: http://www.sample.com/index.html 首部字段 Content-Location 给出与报文主体部分相对应的 URI。和首部字段 Location 不同，Content-Location 表示的是报文主体返回资源对应的 URI。 6.7.6 Content-MD5 Content-MD5: OGFkZDUwNGVhNGY3N2MxMDIwZmQ4NTBmY2IyTY== 首部字段 Content-MD5 是一串由 MD5 算法生成的值，其目的在于检查报文主体在传输过程中是否保持完整，以及确认传输到达。 6.7.7 Content-Range Content-Range: bytes 5001-10000/10000 针对范围请求，返回响应时使用的首部字段 Content-Range，能告知客户端作为响应返回的实体的哪个部分符合范围请求。字段值以字节为单位，表示当前发送部分及整个实体大小。 6.7.8 Content-Type Content-Type: text/html; charset=UTF-8 首部字段 Content-Type 说明了实体主体内对象的媒体类型。和首部字段 Accept 一样，字段值用 type/subtype 形式赋值。参数 charset 使用 iso-8859-1 或 euc-jp 等字符集进行赋值。 6.7.9 Expires Expires: Mon, 10 Jul 2017 15:50:06 GMT 首部字段 Expires 会将资源失效的日期告知客户端。 缓存服务器在接收到含有首部字段 Expires 的响应后，会以缓存来应答请求，在 Expires 字段值指定的时间之前，响应的副本会一直被保存。当超过指定的时间后，缓存服务器在请求发送过来时，会转向源服务器请求资源。 源服务器不希望缓存服务器对资源缓存时，最好在 Expires 字段内写入与首部字段 Date 相同的时间值。 6.7.10 Last-Modified Last-Modified: Mon, 10 Jul 2017 15:50:06 GMT 首部字段 Last-Modified 指明资源最终修改的时间。一般来说，这个值就是 Request-URI 指定资源被修改的时间。但类似使用 CGI 脚本进行动态数据处理时，该值有可能会变成数据最终修改时的时间。 6.8 为 Cookie 服务的首部字段 首部字段名 说明 首部类型 Set-Cookie 开始状态管理所使用的 Cookie 信息 响应首部字段 Cookie 服务器接收到的 Cookie 信息 请求首部字段 6.8.1 Set-Cookie Set-Cookie: status=enable; expires=Mon, 10 Jul 2017 15:50:06 GMT; path=/; 下面的表格列举了 Set-Cookie 的字段值。 属性 说明 NAME=VALUE 赋予 Cookie 的名称和其值（必需项） expires=DATE Cookie 的有效期（若不明确指定则默认为浏览器关闭前为止） path=PATH 将服务器上的文件目录作为Cookie的适用对象（若不指定则默认为文档所在的文件目录） domain=域名 作为 Cookie 适用对象的域名 （若不指定则默认为创建 Cookie的服务器的域名） Secure 仅在 HTTPS 安全通信时才会发送 Cookie HttpOnly 加以限制，使 Cookie 不能被 JavaScript 脚本访问 6.8.1.1 expires 属性 Cookie 的 expires 属性指定浏览器可发送 Cookie 的有效期。 当省略 expires 属性时，其有效期仅限于维持浏览器会话（Session）时间段内。这通常限于浏览器应用程序被关闭之前。 另外，一旦 Cookie 从服务器端发送至客户端，服务器端就不存在可以显式删除 Cookie 的方法。但可通过覆盖已过期的 Cookie，实现对客户端 Cookie 的实质性删除操作。 6.8.1.2 path 属性 Cookie 的 path 属性可用于限制指定 Cookie 的发送范围的文件目录。 6.8.1.3 domain 属性 通过 Cookie 的 domain 属性指定的域名可做到与结尾匹配一致。比如，当指定 example.com 后，除example.com 以外，www.example.com 或 www2.example.com 等都可以发送 Cookie。 因此，除了针对具体指定的多个域名发送 Cookie 之 外，不指定 domain 属性显得更安全。 6.8.1.4 secure 属性 Cookie 的 secure 属性用于限制 Web 页面仅在 HTTPS 安全连接时，才可以发送 Cookie。 6.8.1.5 HttpOnly 属性 Cookie 的 HttpOnly 属性是 Cookie 的扩展功能，它使 JavaScript 脚本无法获得 Cookie。其主要目的为防止跨站脚本攻击（Cross-site scripting，XSS）对 Cookie 的信息窃取。 通过上述设置，通常从 Web 页面内还可以对 Cookie 进行读取操作。但使用 JavaScript 的 document.cookie 就无法读取附加 HttpOnly 属性后的 Cookie 的内容了。因此，也就无法在 XSS 中利用 JavaScript 劫持 Cookie 了。 6.8.2 Cookie Cookie: status=enable 首部字段 Cookie 会告知服务器，当客户端想获得 HTTP 状态管理支持时，就会在请求中包含从服务器接收到的 Cookie。接收到多个 Cookie 时，同样可以以多个 Cookie 形式发送。 6.9 其他首部字段 HTTP 首部字段是可以自行扩展的。所以在 Web 服务器和浏览器的应用上，会出现各种非标准的首部字段。 以下是最为常用的首部字段。 6.9.1 X-Frame-Options X-Frame-Options: DENY 首部字段 X-Frame-Options 属于 HTTP 响应首部，用于控制网站内容在其他 Web 网站的 Frame 标签内的显示问题。其主要目的是为了防止点击劫持（clickjacking）攻击。首部字段 X-Frame-Options 有以下两个可指定的字段值： DENY：拒绝 SAMEORIGIN：仅同源域名下的页面（Top-level-browsing-context）匹配时许可。（比如，当指定 http://sample.com/sample.html 页面为 SAMEORIGIN 时，那么 sample.com 上所有页面的 frame 都被允许可加载该页面，而 example.com 等其他域名的页面就不行了） 6.9.2 X-XSS-Protection X-XSS-Protection: 1 首部字段 X-XSS-Protection 属于 HTTP 响应首部，它是针对跨站脚本攻击（XSS）的一种对策，用于控制浏览器 XSS 防护机制的开关。首部字段 X-XSS-Protection 可指定的字段值如下: 0 ：将 XSS 过滤设置成无效状态 1 ：将 XSS 过滤设置成有效状态 6.9.3 DNT DNT: 1 首部字段 DNT 属于 HTTP 请求首部，其中 DNT 是 Do Not Track 的简称，意为拒绝个人信息被收集，是表示拒绝被精准广告追踪的一种方法。首部字段 DNT 可指定的字段值如下： 0 ：同意被追踪 1 ：拒绝被追踪 由于首部字段 DNT 的功能具备有效性，所以 Web 服务器需要对 DNT做对应的支持。 6.9.4 P3P P3P: CP=\"CAO DSP LAW CURa ADMa DEVa TAIa PSAa PSDa IVAa IVDa OUR BUS IND 首部字段 P3P 属于 HTTP 响应首部，通过利用 P3P（The Platform for Privacy Preferences，在线隐私偏好平台）技术，可以让 Web 网站上的个人隐私变成一种仅供程序可理解的形式，以达到保护用户隐私的目的。 要进行 P3P 的设定，需按以下操作步骤进行： 步骤 1：创建 P3P 隐私 步骤 2：创建 P3P 隐私对照文件后，保存命名在 /w3c/p3p.xml 步骤 3：从 P3P 隐私中新建 Compact policies 后，输出到 HTTP 响应中 七、HTTP 响应状态码（重点分析） 7.1 状态码概述 HTTP 状态码负责表示客户端 HTTP 请求的返回结果、标记服务器端的处理是否正常、通知出现的错误等工作。 HTTP 状态码如 200 OK ，以 3 位数字和原因短语组成。数字中的第一位指定了响应类别，后两位无分类。 不少返回的响应状态码都是错误的，但是用户可能察觉不到这点。比如 Web 应用程序内部发生错误，状态码依然返回 200 OK。 7.2 状态码类别 类别 原因短语 1xx Informational(信息性状态码) 接收的请求正在处理 2xx Success(成功状态码) 请求正常处理完毕 3xx Redirection(重定向状态码) 需要进行附加操作以完成请求 4xx Client Error(客户端错误状态码) 服务器无法处理请求 5xx Server Error(服务器错误状态码) 服务器处理请求出错 我们可以自行改变 RFC2616 中定义的状态码或者服务器端自行创建状态码，只要遵守状态码的类别定义就可以了。 7.3 常用状态码解析 HTTP 状态码种类繁多，数量达几十种。其中最常用的有以下 14 种，一起来看看。 7.3.1 200 OK 表示从客户端发来的请求在服务器端被正常处理了。 7.3.2 204 No Content 代表服务器接收的请求已成功处理，但在返回的响应报文中不含实体的主体部分。另外，也不允许返回任何实体的主体。 一般在只需要从客户端向服务器端发送消息，而服务器端不需要向客户端发送新消息内容的情况下使用。 7.3.3 206 Partial Content 表示客户端进行了范围请求，而服务器成功执行了这部分的 GET 请求。响应报文中包含由 Content-Range 首部字段指定范围的实体内容。 7.3.4 301 Moved Permanently 永久性重定向。表示请求的资源已被分配了新的 URI。以后应使用资源现在所指的 URI。也就是说，如果已经把资源对应的 URI 保存为书签了，这时应该按 Location 首部字段提示的 URI 重新保存。 7.3.5 302 Found 临时性重定向。表示请求的资源已被分配了新的 URI，希望用户（本次）能使用新的 URI 访问。 和 301 Moved Permanently 状态码相似，但 302 Found 状态码代表资源不是被永久移动，只是临时性质的。换句话说，已移动的资源对应的 URI 将来还有可能发生改变。 7.3.6 303 See Other 表示由于请求的资源存在着另一个 URI，应使用 GET 方法定向获取请求的资源。 303 See Other 和 302 Found 状态码有着相同的功能，但 303 See Other 状态码明确表示客户端应采用 GET 方法获取资源，这点与 302 Found 状态码有区别。 7.3.7 304 Not Modified 表示客户端发送附带条件的请求时，服务器端允许请求访问的资源，但未满足条件的情况。 304 Not Modified 状态码返回时，不包含任何响应的主体部分。 304 Not Modified 虽然被划分到 3xx 类别中，但和重定向没有关系。 7.3.8 307 Temporary Redirect 临时重定向。该状态码与 302 Found 有着相同的含义。 7.3.9 400 Bad Request 表示请求报文中存在语法错误。当错误发生时，需修改请求的内容后再次发送请求。 另外，浏览器会像 200 OK 一样对待该状态码。 7.3.10 401 Unauthorized 表示发送的请求需要有通过 HTTP 认证（BASIC 认证、DIGEST 认证）的认证信息。 另外，若之前已进行过 1 次请求，则表示用户认证失败。 返回含有 401 Unauthorized 的响应必须包含一个适用于被请求资源的 WWW-Authenticate 首部用以质询（challenge）用户信息。 7.3.11 403 Forbidden 表明对请求资源的访问被服务器拒绝了。服务器端没有必要给出详细的拒绝理由，当然也可以在响应报文的实体主体部分对原因进行描述。 7.3.12 404 Not Found 表明服务器上无法找到请求的资源。除此之外，也可以在服务器端拒绝请求且不想说明理由的时候使用。 7.3.13 500 Internal Server Error 表明服务器端在执行请求时发生了错误。也可能是 Web 应用存在的 bug 或某些临时的故障。 7.3.14 503 Service Unavailable 表明服务器暂时处于超负载或正在进行停机维护，现在无法处理请求。如果事先得知解除以上状况需要的时间，最好写入 Retry-After 首部字段再返回给客户端。 八、HTTP 报文实体 8.1 HTTP 报文实体概述 HTTP 报文结构 大家请仔细看看上面示例中，各个组成部分对应的内容。 接着，我们来看看报文和实体的概念。如果把 HTTP 报文想象成因特网货运系统中的箱子，那么 HTTP 实体就是报文中实际的货物。 报文：是网络中交换和传输的数据单元，即站点一次性要发送的数据块。报文包含了将要发送的完整的数据信息，其长短很不一致，长度不限且可变。 实体：作为请求或响应的有效载荷数据（补充项）被传输，其内容由实体首部和实体主体组成。（实体首部相关内容在上面第六点中已有阐述。） 我们可以看到，上面示例右图中深红色框的内容就是报文的实体部分，而蓝色框的两部分内容分别就是实体首部和实体主体。而左图中粉红框内容就是报文主体。 通常，报文主体等于实体主体。只有当传输中进行编码操作时，实体主体的内容发生变化，才导致它和报文主体产生差异。 8.2 内容编码 HTTP 应用程序有时在发送之前需要对内容进行编码。例如，在把很大的 HTML 文档发送给通过慢速连接上来的客户端之前，服务器可能会对其进行压缩，这样有助于减少传输实体的时间。服务器还可以把内容搅乱或加密，以此来防止未授权的第三方看到文档的内容。 这种类型的编码是在发送方应用到内容之上的。当内容经过内容编码后，编好码的数据就放在实体主体中，像往常一样发送给接收方。 内容编码类型： 编码方式 描述 gzip 表明实体采用 GNU zip 编码 compress 表明实体采用 Unix 的文件压缩程序 deflate 表明实体采用 zlib 的格式压缩的 identity 表明没有对实体进行编码，当没有 Content-Encoding 首部字段时，默认采用此编码方式 8.3 传输编码 内容编码是对报文的主体进行的可逆变换，是和内容的具体格式细节紧密相关的。 传输编码也是作用在实体主体上的可逆变换，但使用它们是由于架构方面的原因，同内容的格式无关。使用传输编码是为了改变报文中的数据在网络上传输的方式。 内容编码和传输编码的对比 8.4 分块编码 分块编码把报文分割成若干已知大小的块。块之间是紧挨着发送的，这样就不需要在发送之前知道整个报文的大小了。分块编码是一种传输编码，是报文的属性。 分块编码与持久连接 若客户端与服务器端之间不是持久连接，客户端就不需要知道它在读取的主体的长度，而只需要读取到服务器关闭主体连接为止。 当使用持久连接时，在服务器写主体之前，必须知道它的大小并在 Content-Length 首部中发送。如果服务器动态创建内容，就可能在发送之前无法知道主体的长度。 分块编码为这种困难提供了解决方案，只要允许服务器把主体分块发送，说明每块的大小就可以了。因为主体是动态创建的，服务器可以缓冲它的一部分，发送其大小和相应的块，然后在主体发送完之前重复这个过程。服务器可以用大小为 0 的块作为主体结束的信号，这样就可以继续保持连接，为下一个响应做准备。 来看看一个分块编码的报文示例： 分块编码的报文 8.5 多部分媒体类型 MIME 中的 multipart（多部分）电子邮件报文中包含多个报文，它们合在一起作为单一的复杂报文发送。每一部分都是独立的，有各自的描述其内容的集，不同部分之间用分界字符串连接在一起。 相应得，HTTP 协议中也采纳了多部分对象集合，发送的一份报文主体内可包含多种类型实体。 多部分对象集合包含的对象如下： multipart/form-data：在 Web 表单文件上传时使用。 multipart/byteranges：状态码 206 Partial Content 响应报文包含了多个范围的内容时使用。 8.6 范围请求 假设你正在下载一个很大的文件，已经下了四分之三，忽然网络中断了，那下载就必须重头再来一遍。为了解决这个问题，需要一种可恢复的机制，即能从之前下载中断处恢复下载。要实现该功能，这就要用到范围请求。 有了范围请求， HTTP 客户端可以通过请求曾获取失败的实体的一个范围（或者说一部分），来恢复下载该实体。当然这有一个前提，那就是从客户端上一次请求该实体到这一次发出范围请求的时间段内，该对象没有改变过。例如： GET /bigfile.html HTTP/1.1 Host: www.sample.com Range: bytes=20224- ··· 实体范围请求示例 上面示例中，客户端请求的是文档开头20224字节之后的部分。 九、与 HTTP 协作的 Web 服务器 HTTP 通信时，除客户端和服务器外，还有一些用于协助通信的应用程序。如下列出比较重要的几个：代理、缓存、网关、隧道、Agent 代理。 9.1 代理 HTTP 代理服务器是 Web 安全、应用集成以及性能优化的重要组成模块。代理位于客户端和服务器端之间，接收客户端所有的 HTTP 请求，并将这些请求转发给服务器（可能会对请求进行修改之后再进行转发）。对用户来说，这些应用程序就是一个代理，代表用户访问服务器。 出于安全考虑，通常会将代理作为转发所有 Web 流量的可信任中间节点使用。代理还可以对请求和响应进行过滤，安全上网或绿色上网。 9.2 缓存 浏览器第一次请求： 浏览器再次请求： Web 缓存或代理缓存是一种特殊的 HTTP 代理服务器，可以将经过代理传输的常用文档复制保存起来。下一个请求同一文档的客户端就可以享受缓存的私有副本所提供的服务了。客户端从附近的缓存下载文档会比从远程 Web 服务器下载快得多。 9.3 网关 HTTP / FTP 网关 网关是一种特殊的服务器，作为其他服务器的中间实体使用。通常用于将 HTTP 流量转换成其他的协议。网关接收请求时就好像自己是资源的源服务器一样。客户端可能并不知道自己正在跟一个网关进行通信。 9.4 隧道 HTTP/SSL 隧道 隧道是会在建立起来之后，就会在两条连接之间对原始数据进行盲转发的 HTTP 应用程序。HTTP 隧道通常用来在一条或多条 HTTP 连接上转发非 HTTP 数据，转发时不会窥探数据。 HTTP 隧道的一种常见用途就是通过 HTTP 连接承载加密的安全套接字层（SSL）流量，这样 SSL 流量就可以穿过只允许 Web 流量通过的防火墙了。 9.5 Agent 代理 自动搜索引擎“网络蜘蛛” Agent 代理是代表用户发起 HTTP 请求的客户端应用程序。所有发布 Web 请求的应用程序都是 HTTP Agent 代理。 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/网络协议/http/HTTPS协议.html":{"url":"linux/网络协议/http/HTTPS协议.html","title":"HTTPS协议","keywords":"","body":"[toc] HTTPS协议 本文严重抄袭至互联网 本篇将讨论 HTTPS 的加解密原理，很多人都知道 RSA，以为 HTTPS=RSA，使用 RSA 加解密数据，实际上这是不对的。 HTTPS 是使用 RSA 进行身份验证和交换密钥，然后再使用交换的密钥进行加解密数据。 身份验证是使用 RSA 的非对称加密，而数据传输是双方使用相同的密钥进行的对称加密。那么，什么是对称加密和非对称加密？ 对称加密和非对称加密 假设隔壁小王想要约小红出来，但是他不想让小明知道，于是他想用对称加密给小红传了个小纸条。 如下图所示： 他想发送的数据是\"Meet at 5:00 PM\"（5 点见面，如果是中文的话可以使用 UTF-8 编码），加密方式是直接在 ASCII 表进行左移或右移。 他的密钥是 3，表示在 ASCII 表往后移 3 位，就会变成\"Phhw#dw#8=33#SP\"，这样一般人如果截获了不知道是什么意思的。 但是我们可以想一下，如果既然他可以截获你的数据，自然也可以截获你的密钥，进而进行解密。 如下图所示： 所以小王打算用非对称加密，非对称加密的特点是双方都有自己的公钥和私钥对，其中公钥发给对方，密钥不交换自己保管不泄漏。 如下图所示： 其中小红的公钥为： public_key = (N, e) = (3233, 17) 她把公钥发给了小明，她自己的私钥为： private_key = (N, e) = (3233, 2753) 这里注意公钥和私钥都是两个数，N 通常是一个大整数，e 表示一个幂指数。现在小王想给小红发消息，于是他用小红的公钥进行加密，怎么加密呢？ 他要发送的第一个字母为 t=“M”，“M”的 ASCII 编码为 77，77 的加密过程如下计算： T = 77 ^ e % N = 77 ^ 17 % 3233 = 3123 把 77 做 e 次幂然后模以 N，便得到了 T=3123，然后把这个数发给小红（其他字母按同样方式处理）。 小红收到 T 之后便用她的私钥进行解密，计算如下： t = T ^ e % N = 3123 ^ 2753 % 3233 = 77 计算方法是一样的，这样便把 T 还原成了 t，只要公私钥配对，便可通过一些数学公式证明上面的推算是成立的。这个就是 RSA 的加解密原理，如果无法知道私钥便无法进行正确解密。 反过来，使用私钥进行加密，公钥进行解密也是可行的。那么 HTTPS 是怎么利用 RSA 进行加解密的呢，我们从 HTTPS 连接建立过程说起。 HTTPS 连接建立过程 HTTPS 主要有以下作用： 验证服务方身份，如我访问 google.com 的时候连的确实就是谷歌服务器 防止数据被劫持，例如有些运营商会给 http 的页面插入广告 防止敏感数据被窃取篡改等 正如 openssl 的注释所说，这是防止中间人攻击的唯一方法： 我们以 MDN（https://developer.mozilla.org）的网站为例，然后用 wireshark 抓包，观察 HTTPS 连接建立的过程。 如下图所示： 首先是 TCP 三次握手，然后客户端（浏览器）发起一个 HTTPS 连接建立请求，客户端先发一个 Client Hello 的包，然后服务端响应一个 Server Hello。 接着再给客户端发送它的证书，然后双方经过密钥交换，最后使用交换的密钥加行加解密数据。 在 Client Hello 里面客户端会告知服务端自己当前的一些信息，如下图所示： 包括客户端要使用的 TLS 版本，支持的加密套装，要访问的域名，给服务端生成的一个随机数（Nonce）等。 需要提前告知服务器想要访问的域名以便服务器发送相应的域名的证书过来，因为此时还没有发生 HTTP 请求。 服务端在 Server Hello 里面会做一些响应： 服务端选中的加密套装叫 TLSECDHERSAWITHAES128GCM_SHA256，这一串的意思是： 密钥交换使用 ECDHE 证书签名算法 RSA 数据加密使用 AES 128 GCM 签名校验使用 SHA256 接着服务给客户端发来了 4 个证书： 第一个证书的公用名（common name）就是我们当前访问的域名 developer.mozilla.org。 如果公用名是 *.mozilla.org 的话那么这个证书便能给 mozilla.org 的所有二级子域名使用。 第二个证书是第一个证书的签发机构（CA）的证书，它是 Amazon，也就是说 Amazon 会用它的私钥给 developer.mozilla.org 进行签名。 依此类推，第三个证书会给第二个证书签名，第四个证书会给第三个证书签名，并且我们可以看到第四个证书是一个根（Root）证书。 一个证书里面会有什么东西呢，我们可以展开第一个证书看一下，如下图所示： 证书包含三部分内容： tbsCertificate（to be signed certificate）待签名证书内容 证书签名算法 CA 给的签名 也就是说 CA 会用它的私钥对 tbsCertificate 进行签名，并放在签名部分。为什么证书要签名呢？签名是为了验证身份。 身份验证 我们先来看一下 tbsCertificate 里面有什么内容，如下图所示： 它里面包括了证书的公钥、证书的适用公用名、证书的有效期还有它的签发者等信息。 Amazon 的证书也具备上述结构，我们可以把 Amazon 证书的公钥拷出来，如下图所示： 中间有一些填充的数字，用灰色字表示。可以看到N通常是一个很大的整数（二进制 2048 位），而 e 通常为 65537。 然后我们用这个 CA 的公钥对 mozilla.org 的证书签名进行解密，方法和上面的类似： 取解密后的数字 decrypted 的十六进制的末 64 位，即为二进制 256 位的 SHA 哈希签名。 接下来我们手动计算一下 tbsCertificate 的 SHA256 哈希值，方法是在 wireshark 里面把 tbsCertificate 导出一个原始二进制文件： 然后再使用 openssl 计算它的哈希值，如下所示： liyinchengs-MBP:https liyincheng$ openssl dgst -sha256 ~/tbsCertificate.binSHA256(/Users/liyincheng/tbsCertificate.bin)= 5e300091593a10b944051512d39114d56909dc9a504e55cfa2e2984a883a827d 我们发现手动计算的哈希值和加密后的证书里的哈希值一致！说明只有知道了 Amazon 私钥的人才能正确地对 mozilla.org 的证书签名，因为公私钥是唯一匹配的。 因此我们验证了第一个证书 mozilla.org 确实是由第二个证书 Amazon 签发的，使用同样的方式，我们可以验证 Amazon 是由第三个签发的，第三个是由第四个根证书签发。 并且第四个证书是根证书，它是内置于操作系统的（通过 Mac 的 keychain 工具可以查看）： 假如 Hacker 通过 DNS 欺骗之类的方式把你访问的域名指向了他的机器，然后他再伪造一个证书。 但是由于根证书都是内置于操作系统的，所以它改不了签名的公钥，并且它没有正确的私钥，只能用自己的私钥，由于公私钥不配对，很难保证加解密后的信息一致。 或者直接把浏览器拿到的证书搬到他自己的服务器？这样再给浏览器发的证书便是一模一样，但是由于他不知道证书的私钥，所以无法进行后续的操作，因此这样是没有意义的。 这个就是 HTTPS 能够验证身份的原理。另外一个例子是 SSH，需要手动验证签名是否正确。 例如通过打电话或者发邮件等方式告知服务器的签名，与自己算的证书的签名是否一致，如果一致说明证书没有被篡改过（如证书的公钥没有被改为 Hacker 的公钥）： 上面展示的便是自己手动计算的值，拿这个值和之前的值进行比较是否相等便可知发过来的证书是否被修改过。 那么，为什么不直接使用 RSA 的密钥对进行加密数据？因为 RSA 的密钥对数值太大，不太合适频繁地加解密数据，所以需要更小的密钥。 另一个原因是服务端没有浏览器或者客户端的密钥，无法向浏览器发送加密的数据（不能用自己的私钥加密，因为公钥是公开的）。所以需要进行密钥交换。 密钥交换 密钥交换的方式有两种：RSA 和 ECDHE，RSA 的方式比较简单，浏览器生成一把密钥，然后使用证书 RSA 的公钥进行加密发给服务端，服务再使用它的密钥进行解密得到密钥，这样就能够共享密钥了。 它的缺点是攻击者虽然在发送的过程中无法破解，但是如果它保存了所有加密的数据，等到证书到期没有被维护之类的原因导致私钥泄露，那么它就可以使用这把私钥去解密之前传送过的所有数据。 而使用 ECDHE 是一种更安全的密钥交换算法。如下图所示，双方通过 ECDHE 进行密钥交换： ECDHE 的全称是 Elliptic Curve Diffie–Hellman key Exchange 椭圆曲线迪非-赫尔曼密钥交换，它是对迪非-赫尔曼密钥交换算法的改进。 这个算法的思想如下图所示： 为了得到共享秘钥 K，甲用它的私钥计算一个数 g^a，发送给乙，乙的私钥为 b，乙便得到 K= g^a^b，同时发送 g^b 给甲，甲也得到了 K=g^b^a。 这个应该比较好理解，而引入椭圆曲线加密能够提高破解难度。 椭圆曲线加密 现在的证书的签名算法有两种：RSA 和新起的 EC。如下图所示，google.com 便是使用的 ECC 证书： 我们上面讨论的便是 RSA，破解 RSA 的难点在于无法对公钥的 N 进行质数分解。 如果你能对证书的 N 拆成两个质数相乘，便可推算出证书的私钥，但是在当前的计算能力下是不可能的。而 ECC 的破解难点在于找到指定点的系数。 如下图所示，有一条椭圆曲线方程： y ^ 3 = x ^ 2 + ax + b: 给定一个起点 G（x，y），现在要计算点 P=2G 的坐标，其过程是在 G 点上做一条线与曲线相切于 -2G，做 -2G 相对于 x 轴的反射便得到 2G 点。 为了计算 3G 的坐标，如下图所示： 连接 2G 与 G 与曲线相郊于 -3G，再做反射得到 3G，同理计算 4G 便是连接 G 与 3G 再做反射。如果最后一个点和起点的连线垂直于 x 轴，说明所有的点已用完。 EC 的难点在于给定起点 G 和点 K： K = kG 想要得到 K（K 足够大）是一件很困难的事情。这个 K 便是私钥，而 K=kG 便是公钥。ECC 是怎么加解密数据的呢？ 假设要加密的数据为 m，把这个点当作x坐标得到在曲线上的一个点 M，取定一个随机数 r，计算点 C1=rG，C2=M+rK。 把这两个点便是加密后的数据，发给对方，对方收到后使用私钥 K 进行解密，过程如下： M = C2 - rK = C2 - rkG = C2 - rkG = C2 - kC1 通过上面的计算便能还原得到 M，而不知道私钥 K 的人是无法解密的。更多细节可见 Medium 的这篇文章《ECC elliptic curve encryption》。这样我们便理解了 ECC 的原理，那么怎么利用 ECC 进行密钥交换呢？ ECC 密钥交换 原理很简单，如下图所示： 之前交换的是两个幂次方的数，现在变成交换两个曲线上的点。 而曲线方程是规定好的，例如 Curve X25519 使用的曲线方程为： y^2 = x^3 + 486662x^2 + x 在密钥交换里面会指定所使用的曲线方程，如下图所示： mozilla.org 所使用的曲线方程为 secp256r1，这个也是比较流行的一个，它的参数比 Curve X25519 大很多。 密钥交换也使用了证书的私钥进行签名，保证交换的密钥不会被人篡改，只是这里的私钥是 mozilla 自己的私钥。 也就是说从连接建立到现在都是明文传输的。接下来双方发送 Change Cipher Spec 的包通知，接下来的包都按照之前约定好的方式进行加密。至此整个安全连接建立完毕。 HTTPS 证书的应用 那么是谁在做 HTTPS 加密呢？服务端通常是 Nginx、Apache 这些反向代理服务器做的，而具体的业务服务器不需要处理，客户端通常是浏览器等做的加解密，Chrome 是使用 boringSSL 这个库，fork 自 openssl。 我们通过 let’s encrypt 可以申请免费的 TLS 证书，每 3 个月需要手动续。 证书分为 3 种：DV、OV、EV，DV 适用于个人，OV 和 EV 需要身份审核，EV 最高端。 EV 证书会在浏览器的地址栏显示证书的企业名称： 但是新版的 Chrome 似乎把这个去掉了，所以我们打开 medium 的控制台可以看到一个提示： As part of an experiment, Chrome temporarily shows only the lock icon in the address bar. Your SSL certificate with Extended Validation is still valid. 另外我们可以用 openssl 生成一个自签名证书，执行以下命令： openssl req -x509 -nodes -sha256 -days 365 -newkey rsa:2048 -keyout test.com.key -out test.com.crt 便会得到两个文件，test.com.crt 是证书，test.com.key 是证书的私钥，如下图所示： 然后把这两个文件给 Nginx 使用便能使用 HTTPS 访问，如下代码所示： server { listen 443; server_name test.com; ssl on; ssl_certificate test.com.crt; ssl_certificate_key test.com.key; } 可以把这个证书添加到系统证书里面，这样浏览器等便能信任，或者直接使用 mkcert 工具一步到位。 客户端证书 还有一种证书叫客户端证书，同样需要向 CA 机构申请一个客户端证书，和服务端 TLS 证书不一样的地方是，服务端证书通常是和域名绑定的，而客户端证书可以给本地的任意可执行文件进行签名。 签名验证算法和上文讨论的 TLS 证书一致。为什么可执行文件需要签名呢，因为如果不签名的话，系统会拦截安装或者运行，如 Mac 双击一个未签名的 dmg 包的提示： 直接不让你运行了，而 Windows 也有类似的提示，Windows 是会给一个警告： 而当我们运行一个已签名的 exe 文件将会是正常的提示，如 Chrome 的提示： 综上本文主要讨论了对称加密和非对称加密的原理，并介绍了如何利用 RSA 对证书签名的检验以验证连接服务器的身份，怎么利用 ECC 进行数据加密和密钥交换，介绍了下怎么生成和使用 HTTPS 证书，并介绍了下客户端证书。 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/网络协议/tcp:ip/TCP三次握手和四次挥手.html":{"url":"linux/网络协议/tcp:ip/TCP三次握手和四次挥手.html","title":"TCP三次握手和四次挥手","keywords":"","body":"[toc] TCP三次握手和四次挥手 三次握手（重点） TCP 提供面向有连接的通信传输。面向有连接是指在数据通信开始之前先做好两端之间的准备工作。 所谓三次握手是指建立一个 TCP 连接时需要客户端和服务器端总共发送三个包以确认连接的建立。在socket编程中，这一过程由客户端执行connect来触发。 下面来看看三次握手的流程图： 第一次握手：客户端将标志位SYN置为1，随机产生一个值seq=J，并将该数据包发送给服务器端，客户端进入SYN_SENT状态，等待服务器端确认。 第二次握手：服务器端收到数据包后由标志位SYN=1知道客户端请求建立连接，服务器端将标志位SYN和ACK都置为1，ack=J+1，随机产生一个值seq=K，并将该数据包发送给客户端以确认连接请求，服务器端进入SYN_RCVD状态。 第三次握手：客户端收到确认后，检查ack是否为J+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=K+1，并将该数据包发送给服务器端，服务器端检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，客户端和服务器端进入ESTABLISHED状态，完成三次握手，随后客户端与服务器端之间可以开始传输数据了。 这里小写的ack为确认编号(上一次对方主机传输过来的seq+1)，大写的ACK是确认值，确认值为1表示确认连接 四次挥手（重点） 四次挥手即终止TCP连接，就是指断开一个TCP连接时，需要客户端和服务端总共发送4个包以确认连接的断开。在socket编程中，这一过程由客户端或服务端任一方执行close来触发。 由于TCP连接是全双工的，因此，每个方向都必须要单独进行关闭，这一原则是当一方完成数据发送任务后，发送一个FIN来终止这一方向的连接，收到一个FIN只是意味着这一方向上没有数据流动了，即不会再收到数据了，但是在这个TCP连接上仍然能够发送数据，直到这一方向也发送了FIN。首先进行关闭的一方将执行主动关闭，而另一方则执行被动关闭。 下面来看看四次挥手的流程图： 中断连接端可以是客户端，也可以是服务器端。 第一次挥手：客户端发送一个FIN=M，用来关闭客户端到服务器端的数据传送，客户端进入FIN_WAIT_1状态。意思是说\"我客户端没有数据要发给你了\"，但是如果你服务器端还有数据没有发送完成，则不必急着关闭连接，可以继续发送数据。 第二次挥手：服务器端收到FIN后，先发送ack=M+1，告诉客户端，你的请求我收到了，但是我还没准备好，请继续你等我的消息。这个时候客户端就进入FIN_WAIT_2 状态，继续等待服务器端的FIN报文。 第三次挥手：当服务器端确定数据已发送完成，则向客户端发送FIN=N报文，告诉客户端，好了，我这边数据发完了，准备好关闭连接了。服务器端进入LAST_ACK状态。 第四次挥手：客户端收到FIN=N报文后，就知道可以关闭连接了，但是他还是不相信网络，怕服务器端不知道要关闭，所以发送ack=N+1后进入TIME_WAIT状态，如果Server端没有收到ACK则可以重传。服务器端收到ACK后，就知道可以断开连接了。客户端等待了2MSL后依然没有收到回复，则证明服务器端已正常关闭，那好，我客户端也可以关闭连接了。最终完成了四次握手。 上面是一方主动关闭，另一方被动关闭的情况，实际中还会出现同时发起主动关闭的情况， 具体流程如下图： 关于TCP协议中三次握手中的大写ACK和小写ack number的区别 在三次握手发送的数据包中有两个ACK值（Acknowledgement），人们习惯一个大写，一个小写来加以区分。大小写只是为了便于区分 一个是确认值(Acknowledgement)，为1便是确认连接。 另一个是确认编号(Acknowledgement Number)，即接收到的上一次远端主机传来的seq然后+1，再发送给远端主机。提示远端主机已经成功接收上一次所有数据。 三次握手的数据包 第一次握手 红框内为第一次握手时IP为192.168.56.1的请求端（请求连接端）发送的seq，值为0（实际中此值不一定为0） 第二次握手 红框内为第二次握手时IP为192.168.56.130的服务端（被请求连接端）发送的seq，因为是服务端发给请求端的一个新的seq，所以值为0（实际中此值不一定为0） 蓝框内为Ack（Acknowledgement Number确认编号）即我理解的小写的ack，值为第一次握手时，请求端发送来的seq+1即0+1=1 第三次握手 红框内为第三次握手时IP为192.168.56.1的请求端（请求连接端）发送的seq，因为第一次握手时它发送给服务端的seq为0（黄框内），在上次的基础上+1，值就是1。 蓝框内的Ack（Acknowledgement Number确认编号）还是我理解的小写的ack，值为第二次握手时请求端发来的seq+1，即绿框中的seq+1，值为0+1=1 那么问题来了，那个起确认连接作用的确认值即我理解的那个大写的ACK在哪呢？ 在这里，展开看一下： 对照网上找到的关于第二次握手的标志位的一张图可以看出： 确认位即ACK，为1即为确认进行连接 同步位即SYN，从第一次握手时，此位就为1 下面是网上找到的三次握手的标志图，供参考： 第一次握手的标志位 我们可以看到标志位里面只有个同步位，也就是在做请求(SYN) 第二次握手的标志位 我们可以看到标志位里面有个确认位和同步位，也就是在做应答(SYN + ACK) 第三次握手的标志位 我们可以看到标志位里面只有个确认位，也就是再做再次确认(ACK) 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/linux其他/CentOS7安装字体.html":{"url":"linux/linux其他/CentOS7安装字体.html","title":"CentOS7安装字体","keywords":"","body":"CentOS7安装字体 1.验证目录是否存在 查看 /usr/share/ 下是否有 fonts 和 fontconfig 目录，如果没有执行命令 yum -y install fontconfig 安装 2.上传字体 在 /usr/share/fonts 下新建一个目录，以便和系统区分 cd /usr/share/fonts mkdir chinese 在windows中 C:\\Windows\\Fonts 目录下找到相应的字体，并上传字体到 /usr/share/fonts/chinese 下，然后修改字体权限为600，这里选择微软雅黑 MSYH.TTC chmod 644 MSYH.TTC 3.修改配置文件 安装ttmkfdir yum -y install ttmkfdir ttmkfdir -e /usr/share/X11/fonts/encodings/encodings.dir 修改配置文件 /etc/fonts/fonts.conf，在 Font directory 下增加如下配置 /usr/share/fonts/chinese 4.验证安装是否成功 # 刷新 fc-cache # 验证安装是否成功，这里过滤我们在/usr/share/fonts新建的目录chinese $ fc-list|grep chinese /usr/share/fonts/chinese/MSYH.TTC: Microsoft YaHei:style=Normal /usr/share/fonts/chinese/MSYH.TTC: Microsoft YaHei UI:style=Normal 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql基础/1.mysql基本操作.html":{"url":"db/mysql/mysql基础/1.mysql基本操作.html","title":"1.mysql基本操作","keywords":"","body":"[toc] mysql基础 mysql基本操作 Mysql 关系型数据库，表跟表之间可以建立关系 库-->表：列(字段 faield) ​ 行(记录 record) 1.连接mysql mysql -u用户名 -p密码 [root@mysql ~]# mysql -u root -p Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 32 Server version: 5.6.40 MySQL Community Server (GPL) Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> 2.设置root密码 方法一 命令行设置 mysqladmin -uroot password '密码' 方法二 进入mysql设置 set password = password('密码'); 3.mysql初始安全设置 mysql_secure_installation [root@mysql ~]# mysql_secure_installation Enter current password for root (enter for none): #输入root密码 Change the root password? [Y/n] #是否改变root密码 Remove anonymous users? [Y/n] #是否移除匿名用户 Disallow root login remotely? [Y/n] #是否允许root远程登陆 Remove test database and access to it? [Y/n] #是否移除test库并且不能访问 Reload privilege tables now? [Y/n] #是否重新加载权限表 Enter current password for root (enter for none): #输入root密码 4.mysql文件(yum安装) [root@mysql ~]# cd /var/lib/mysql/ [root@mysql mysql]# ls ibdata1 ib_logfile0 ib_logfile1 mysql mysql.sock ibdata1 #InnoDB存储引擎的系统表空间，存放InnoDB表的数据、回滚段 ib_logfile0、ib_logfile1 #InnoDB日志文件组 mysql #数据库 库名字 mysql.sock #mysql的socket文件，用于本机用户登陆mysql mysql主配置文件 /etc/my.cnf 5.库的管理 5.1创建数据库 create database 数据库名 mysql> create database DB1; Query OK, 1 row affected (0.00 sec) mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | DB1 | | mysql | | performance_schema | | sys | +--------------------+ 5 rows in set (0.05 sec) 5.2删除数据库 drop database 数据库名 mysql> drop database DB1; Query OK, 0 rows affected (0.00 sec) mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | +--------------------+ 4 rows in set (0.09 sec) 5.3查询数据库 show databases mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | +--------------------+ 2 rows in set (0.00 sec) 5.4使用数据库 use 数据库名 mysql> use DB1; Database changed mysql> select database(); #查看当前使用哪个数据库 +------------+ | database() | +------------+ | DB1 | +------------+ 1 row in set (0.00 sec) 6.表的管理 6.1创建表 create table 表名(列名1 数据类型,列名2 数据类型); mysql> create table t1(id int(3),name char(30),sex enum('M','F'),hobby set('a','b','c')); Query OK, 0 rows affected (0.03 sec) #desc 表名 //描述表 mysql> desc t1; +-------+------------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+------------------+------+-----+---------+-------+ | id | int(3) | YES | | NULL | | | name | char(30) | YES | | NULL | | | sex | enum('M','F') | YES | | NULL | | | hobby | set('a','b','c') | YES | | NULL | | +-------+------------------+------+-----+---------+-------+ 4 rows in set (0.13 sec) #insert into 表名 values(......); //向表中插入数据 mysql> insert into t1 values(1,'xiaoming','M','a,b,c'); Query OK, 1 row affected (0.00 sec) mysql> select * from t1; +----+----------+-----+-------+ | id | name | sex | hobby | +----+----------+-----+-------+ | 1 | xiaoming | M | a,b,c | +----+----------+-----+-------+ 1 row in set (0.12 sec) 6.2删除表 删除一个表 drop table 表名 删除多个表 drop table 表1,表2，。。。表n; #删除一个表 mysql> show tables; +---------------+ | Tables_in_db1 | +---------------+ | t1 | +---------------+ 1 row in set (0.13 sec) mysql> drop table t1; Query OK, 0 rows affected (0.12 sec) #删除多个表 mysql> show tables; +---------------+ | Tables_in_db1 | +---------------+ | t1 | | t2 | +---------------+ 2 rows in set (0.16 sec) mysql> drop table t1,t2; Query OK, 0 rows affected (0.20 sec) 6.3修改表 6.3.1增加列 alter table 表名 add 列名 数据类型; #查看t1表，此时表中只有一个列id，现在想增加一个列name mysql> desc t1; +-------+--------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+--------+------+-----+---------+-------+ | id | int(3) | YES | | NULL | | +-------+--------+------+-----+---------+-------+ 1 row in set (0.10 sec) #给t1表增加列name mysql> alter table t1 add name char(10); Query OK, 0 rows affected (0.15 sec) Records: 0 Duplicates: 0 Warnings: 0 #查看t1表，已经增加name列 mysql> desc t1; +-------+----------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+----------+------+-----+---------+-------+ | id | int(3) | YES | | NULL | | | name | char(10) | YES | | NULL | | +-------+----------+------+-----+---------+-------+ 2 rows in set (0.10 sec) add增加列默认是在最后边添加，如果需要指定追到的位置需要做以下操作 #t2表内容如下，现在要追加一个phone列，追加到name列的后边 mysql> desc t2; +---------+----------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +---------+----------+------+-----+---------+-------+ | id | int(11) | YES | | NULL | | | name | char(10) | YES | | NULL | | | address | char(30) | YES | | NULL | | +---------+----------+------+-----+---------+-------+ 3 rows in set (0.01 sec) #需要用到after关键字，此语句表明在name列后追加phone列 mysql> alter table t2 add phone char(11) after name; Query OK, 0 rows affected (0.04 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql> desc t2; +---------+----------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +---------+----------+------+-----+---------+-------+ | id | int(11) | YES | | NULL | | | name | char(10) | YES | | NULL | | | phone | char(11) | YES | | NULL | | | address | char(30) | YES | | NULL | | +---------+----------+------+-----+---------+-------+ 4 rows in set (0.00 sec) #如果要追加到第一列，需要用到first关键字 mysql> alter table t2 add sex enum('F','M') first; Query OK, 0 rows affected (0.05 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql> desc t2; +---------+---------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +---------+---------------+------+-----+---------+-------+ | sex | enum('F','M') | YES | | NULL | | | id | int(11) | YES | | NULL | | | name | char(10) | YES | | NULL | | | phone | char(11) | YES | | NULL | | | address | char(30) | YES | | NULL | | +---------+---------------+------+-----+---------+-------+ 5 rows in set (0.00 sec) ⚠️⚠️⚠️ 修改列中不支持before，只有after和first 6.3.2删除列 alter table 表名 drop 列名; #查看t1表，表中有两个列id和name,现在要删除name列 mysql> desc t1; +-------+----------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+----------+------+-----+---------+-------+ | id | int(3) | YES | | NULL | | | name | char(10) | YES | | NULL | | +-------+----------+------+-----+---------+-------+ 2 rows in set (0.10 sec) #删除name列 mysql> alter table t1 drop name; Query OK, 0 rows affected (0.14 sec) Records: 0 Duplicates: 0 Warnings: 0 #查看t1表name列已经删除 mysql> desc t1; +-------+--------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+--------+------+-----+---------+-------+ | id | int(3) | YES | | NULL | | +-------+--------+------+-----+---------+-------+ 1 row in set (0.12 sec) 6.3.3修改列名 alter table 表名 change 旧列名 新列名 数据类型; change既可以修改列名，又可以修改列类型 #查看t1表，表中有id和name两个列，现在要将name列修改为address列 mysql> desc t1; +-------+----------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+----------+------+-----+---------+-------+ | id | int(3) | YES | | NULL | | | name | char(10) | YES | | NULL | | +-------+----------+------+-----+---------+-------+ 2 rows in set (0.10 sec) #修改name列 mysql> alter table t1 change name address varchar(30); Query OK, 0 rows affected (0.19 sec) Records: 0 Duplicates: 0 Warnings: 0 #查看t1表，原name列已经修改为address，列类型已由char修改为varchar mysql> desc t1; +---------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +---------+-------------+------+-----+---------+-------+ | id | int(3) | YES | | NULL | | | address | varchar(30) | YES | | NULL | | +---------+-------------+------+-----+---------+-------+ 2 rows in set (0.17 sec) 6.3.4修改列的数据类型 alter table 表名 modify 列名 新列数据类型; #t1表中有id列和address列，现在要把address列的数据类型由varchar改为char mysql> desc t1; +---------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +---------+-------------+------+-----+---------+-------+ | id | int(3) | YES | | NULL | | | address | varchar(30) | YES | | NULL | | +---------+-------------+------+-----+---------+-------+ 2 rows in set (0.16 sec) #修改address列数据类型为char mysql> alter table t1 modify address char(20); Query OK, 0 rows affected (0.22 sec) Records: 0 Duplicates: 0 Warnings: 0 #查看t1表，address列的数据类型已经修改为char mysql> desc t1; +---------+----------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +---------+----------+------+-----+---------+-------+ | id | int(3) | YES | | NULL | | | address | char(20) | YES | | NULL | | +---------+----------+------+-----+---------+-------+ 2 rows in set (0.19 sec) 6.3.5修改表名 rename table 旧表名 to 新表名 #查看表，现在要将表t1修改为table1 mysql> show tables; +---------------+ | Tables_in_db1 | +---------------+ | t1 | +---------------+ 1 row in set (0.14 sec) #修改表t1为table1 mysql> rename table t1 to table1; Query OK, 0 rows affected (0.16 sec) mysql> show tables; +---------------+ | Tables_in_db1 | +---------------+ | table1 | +---------------+ 1 row in set (0.18 sec) 6.4查看表 show tables #先进入一个库 mysql> use db1; Database changed #查看库中所有的表 mysql> show tables; +---------------+ | Tables_in_db1 | +---------------+ | t1 | +---------------+ 1 row in set (0.18 sec) 7.数据管理 7.1增加数据 insert into 表名 values(......); 方式一 直接插入值 #查看表t1，现在是一张空表 mysql> desc t1; +---------+----------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +---------+----------+------+-----+---------+-------+ | id | int(3) | YES | | NULL | | | address | char(20) | YES | | NULL | | +---------+----------+------+-----+---------+-------+ 2 rows in set (0.17 sec) mysql> select * from t1; Empty set //向表中插入数据，插入一条 mysql> insert into t1 values(1,'北京'); Query OK, 1 row affected (0.19 sec) //向表中插入数据，插入多条 mysql> insert into t1 values(2,'上海'),(3,'广州'),(4,'深圳'); Query OK, 3 rows affected (0.15 sec) Records: 3 Duplicates: 0 Warnings: 0 //查看t1表中的数据 mysql> select * from t1; +----+---------+ | id | address | +----+---------+ | 1 | 北京 | | 2 | 上海 | | 3 | 广州 | | 4 | 深圳 | +----+---------+ 4 rows in set (0.16 sec) 方式二 从别的表中选择数据插入 #创建t1表 mysql> create table t1(id int primary key auto_increment,address char(10)); Query OK, 0 rows affected (0.02 sec) //向表中插入数据 mysql> insert into t1(address) values('北京'),('杭州'),('深圳'); Query OK, 3 rows affected (0.00 sec) Records: 3 Duplicates: 0 Warnings: 0 mysql> select * from t1; +----+---------+ | id | address | +----+---------+ | 1 | 北京 | | 2 | 杭州 | | 3 | 深圳 | +----+---------+ 3 rows in set (0.00 sec) #创建t2表 mysql> create table t2(id int primary key auto_increment,address char(10)); Query OK, 0 rows affected (0.02 sec) //将t1表中的内容插入到t2表中 mysql> insert into t2(select * from t1); Query OK, 3 rows affected (0.01 sec) Records: 3 Duplicates: 0 Warnings: 0 mysql> select * from t2; +----+---------+ | id | address | +----+---------+ | 1 | 北京 | | 2 | 杭州 | | 3 | 深圳 | +----+---------+ 3 rows in set (0.00 sec) ⚠️如果两个表中的字段不一致，从另一张表中插入数据的时候需要手动指定字段 #创建t3表 mysql> create table t3(id int primary key auto_increment,address char(10),qnum tinyint); Query OK, 0 rows affected (0.02 sec) //此时想插入t1表的数据，需要手动指定一下两张表中的共同字段 mysql> insert into t3(id,address) (select * from t1); Query OK, 3 rows affected (0.01 sec) Records: 3 Duplicates: 0 Warnings: 0 mysql> select * from t3; +----+---------+------+ | id | address | qnum | +----+---------+------+ | 1 | 北京 | NULL | | 2 | 杭州 | NULL | | 3 | 深圳 | NULL | +----+---------+------+ 3 rows in set (0.00 sec) 7.2删除数据 delete from 表名 where 条件; #查看t1表中的数据 mysql> select * from t1; +----+---------+ | id | address | +----+---------+ | 1 | 北京 | | 2 | 上海 | | 3 | 广州 | | 4 | 深圳 | +----+---------+ 4 rows in set (0.16 sec) //删除表中地址为上海的数据 mysql> delete from t1 where id=2; Query OK, 1 row affected (0.16 sec) //查看t1表中内容 mysql> select * from t1; +----+---------+ | id | address | +----+---------+ | 1 | 北京 | | 3 | 广州 | | 4 | 深圳 | +----+---------+ 3 rows in set (0.16 sec) #再次新建t1表 ysql> create table t1(id int primary key auto_increment,address char(10)); Query OK, 0 rows affected (0.02 sec) //向t1表中插入数据 mysql> insert into t1(address) values('北京'),('上海'),('广州'); Query OK, 3 rows affected (0.00 sec) Records: 3 Duplicates: 0 Warnings: 0 mysql> select * from t1; +----+---------+ | id | address | +----+---------+ | 1 | 北京 | | 2 | 上海 | | 3 | 广州 | +----+---------+ //delete方式清空t1表 mysql> delete from t1; Query OK, 3 rows affected (0.00 sec) mysql> select * from t1; Empty set (0.00 sec) //向表中插入数据 mysql> insert into t1(address) values('杭州'),('深圳'); Query OK, 2 rows affected (0.00 sec) Records: 2 Duplicates: 0 Warnings: 0 //发现并没有清空自增字段，如果需要清空自增字段，需要用到truncate语句 mysql> select * from t1; +----+---------+ | id | address | +----+---------+ | 4 | 杭州 | | 5 | 深圳 | +----+---------+ 2 rows in set (0.00 sec) truncate table 表名 清空表并重置自增字段 #创建一个t1表 mysql> create table t1(id int,address char(10)); Query OK, 0 rows affected (0.02 sec) mysql> insert into t1 values(1,'北京'); Query OK, 1 row affected (0.00 sec) mysql> select * from t1; +------+---------+ | id | address | +------+---------+ | 1 | 北京 | +------+---------+ 1 row in set (0.00 sec) #truncate清空表，两种写法，truncate后边的table可以不加 mysql> truncate t1; Query OK, 0 rows affected (0.01 sec) mysql> truncate table t1; Query OK, 0 rows affected (0.03 sec) //查看表t1，已经清空 mysql> select * from t1; Empty set (0.00 sec) #再创建一个t2表，表中有自增字段 mysql> create table t2(id int primary key auto_increment,address char(10)); Query OK, 0 rows affected (0.02 sec) //向表中插入数据 mysql> insert into t2(address) values('杭州'),('深圳'); Query OK, 2 rows affected (0.00 sec) Records: 2 Duplicates: 0 Warnings: 0 mysql> select * from t2; +----+---------+ | id | address | +----+---------+ | 1 | 杭州 | | 2 | 深圳 | +----+---------+ 2 rows in set (0.00 sec) //truncate清空表 mysql> truncate t2; Query OK, 0 rows affected (0.02 sec) //向表中插入数据 mysql> insert into t2(address) values('杭州'),('深圳'); Query OK, 2 rows affected (0.01 sec) Records: 2 Duplicates: 0 Warnings: 0 //truncate清空表的方式会将表中的自增字段同时删除，而delete from 表名的方式不可以删除自增 mysql> select * from t2; +----+---------+ | id | address | +----+---------+ | 1 | 杭州 | | 2 | 深圳 | +----+---------+ 2 rows in set (0.00 sec) 7.3修改数据 update 表名 set 旧值=新值 where 条件; #查看表t1，现在要将表中的上海修改为杭州 mysql> select * from t1; +----+---------+ | id | address | +----+---------+ | 1 | 北京 | | 2 | 上海 | | 3 | 广州 | | 4 | 深圳 | +----+---------+ 4 rows in set (0.12 sec) #修改表t1中地址为上海的列为杭州 mysql> update t1 set address='杭州' where id=2; Query OK, 1 row affected (0.18 sec) Rows matched: 1 Changed: 1 Warnings: 0 #查看表t1，上海已经修改为杭州 mysql> select * from t1; +----+---------+ | id | address | +----+---------+ | 1 | 北京 | | 2 | 杭州 | | 3 | 广州 | | 4 | 深圳 | +----+---------+ 4 rows in set (0.13 sec) 7.4查询数据 select 列名 from 表名 where 条件 #查询表中所有内容 mysql> select * from t1; +----+---------+ | id | address | +----+---------+ | 1 | 北京 | | 2 | 杭州 | | 3 | 广州 | | 4 | 深圳 | +----+---------+ 4 rows in set (0.14 sec) #根据条件查询 查询t1表中id=1的address列 mysql> select address from t1 where id=1; +---------+ | address | +---------+ | 北京 | +---------+ 1 row in set (0.18 sec) 8.mysql常用函数 常用函数 user() #查看当前用户 database() #查看当前所属库 version() #查看MySQL版本 now() #系统时间 sum() #求和 avg() #平均值 max() #最大值 min() #最小值 count() #统计数量 8.1user() 查看当前用户 mysql> select user(); +----------------+ | user() | +----------------+ | root@localhost | +----------------+ 1 row in set (0.00 sec) 当前登陆用户是root，登陆的主机是本机 8.2database() 查看当前所在库 mysql> select database(); +------------+ | database() | +------------+ | db1 | +------------+ 1 row in set (0.12 sec) 当前所在数据库是db1 8.3version() 查看mysql版本 mysql> select version(); +-----------+ | version() | +-----------+ | 5.7.22 | +-----------+ 1 row in set (0.11 sec) 当前mysql版本是5.7.22 8.4now() 查看当前系统时间 mysql> select now(); +---------------------+ | now() | +---------------------+ | 2018-10-25 21:10:18 | +---------------------+ 1 row in set (0.11 sec) 当前系统时间是2018年10月25日21时10分18秒 8.5sum() 求和 #查看student表 mysql> select * from student; +------+--------+-------+ | id | name | score | +------+--------+-------+ | 1 | 小明 | 99 | | 2 | 小红 | 80 | | 3 | 小丽 | 70 | | 4 | 小强 | 59 | | 5 | 小洲 | 66 | +------+--------+-------+ 5 rows in set (0.00 sec) #求学生表中所有人的总成绩 mysql> select sum(score) as total_points from student; +--------------+ | total_points | +--------------+ | 374 | +--------------+ #求学生表中成绩大于70分的总和 mysql> select sum(score) as total_points from student where score>=70; +--------------+ | total_points | +--------------+ | 249 | +--------------+ 1 row in set (0.00 sec) 8.6avg() 平均值 #查看student表 mysql> select * from student; +------+--------+-------+ | id | name | score | +------+--------+-------+ | 1 | 小明 | 99 | | 2 | 小红 | 80 | | 3 | 小丽 | 70 | | 4 | 小强 | 59 | | 5 | 小洲 | 66 | +------+--------+-------+ 5 rows in set (0.00 sec) #求所有人分数的平均值 mysql> select avg(score) as avg_points from student; +------------+ | avg_points | +------------+ | 74.8 | +------------+ 1 row in set (0.00 sec) #求70分以上的人的分数平均值 mysql> select avg(score) as avg_points from student where score>=70; +------------+ | avg_points | +------------+ | 83 | +------------+ 1 row in set (0.00 sec) 8.7max() 最大值 #查看student表 mysql> select * from student; +------+--------+-------+ | id | name | score | +------+--------+-------+ | 1 | 小明 | 99 | | 2 | 小红 | 80 | | 3 | 小丽 | 70 | | 4 | 小强 | 59 | | 5 | 小洲 | 66 | +------+--------+-------+ 5 rows in set (0.00 sec) #查询student表中分数最高的 mysql> select max(score) from student; +------------+ | max(score) | +------------+ | 99 | +------------+ 1 row in set (0.00 sec) 8.8min() 最小值 #查看student表 mysql> select * from student; +------+--------+-------+ | id | name | score | +------+--------+-------+ | 1 | 小明 | 99 | | 2 | 小红 | 80 | | 3 | 小丽 | 70 | | 4 | 小强 | 59 | | 5 | 小洲 | 66 | +------+--------+-------+ 5 rows in set (0.00 sec) #传student表中分数最低的 mysql> select min(score) from student; +------------+ | min(score) | +------------+ | 59 | +------------+ 1 row in set (0.00 sec) 8.9count() 统计数量 #查看student表 mysql> select * from student; +------+--------+-------+ | id | name | score | +------+--------+-------+ | 1 | 小明 | 99 | | 2 | 小红 | 80 | | 3 | 小丽 | 70 | | 4 | 小强 | 59 | | 5 | 小洲 | 66 | +------+--------+-------+ 5 rows in set (0.00 sec) #查询student表中分数大于60的人数 mysql> select count(*) from student where score>60; +----------+ | count(*) | +----------+ | 4 | +----------+ 1 row in set (0.00 sec) 9.mysql数据查询操作 9.1数学运算 + 加法 mysql> select 1+1; +-----+ | 1+1 | +-----+ | 2 | +-----+ 1 row in set (0.00 sec) - 减法 mysql> select 100-1; +-------+ | 100-1 | +-------+ | 99 | +-------+ 1 row in set (0.00 sec) * 乘法 mysql> select 100*3; +-------+ | 100*3 | +-------+ | 300 | +-------+ 1 row in set (0.00 sec) / 除法 mysql> select 100/3; +---------+ | 100/3 | +---------+ | 33.3333 | +---------+ 1 row in set (0.00 sec) % 取余 mysql> select 7%3; +------+ | 7%3 | +------+ | 1 | +------+ 1 row in set (0.00 sec) pow 幂运算 mysql> select pow(3,3); +----------+ | pow(3,3) | +----------+ | 27 | +----------+ 1 row in set (0.00 sec) ⚠️mysql中不支持**方式的幂运算，需要用到pow方法 9.2比较运算 > 大于 #返回值为0表示结果为假 mysql> select 3>6; +-----+ | 3>6 | +-----+ | 0 | +-----+ 1 row in set (0.01 sec) #返回值为1表示结果为真 mysql> select 6>3; +-----+ | 6>3 | +-----+ | 1 | +-----+ 1 row in set (0.00 sec) #返回值为1表示结果为真 mysql> select 6>3; +-----+ | 6>3 | +-----+ | 1 | +-----+ 1 row in set (0.00 sec) #返回值为0表示结果为假 mysql> select 6 >= 大于等于 #返回值为0表示结果为假 mysql> select 6 select 6>=3; +------+ | 6>=3 | +------+ | 1 | +------+ 1 row in set (0.00 sec) #返回值为1表示结果为真 mysql> select 6 select 6 = 等于 #返回值为1表示结果为真 mysql> select 5=5; +-----+ | 5=5 | +-----+ | 1 | +-----+ 1 row in set (0.00 sec) #返回值为0表示结果为假 mysql> select 5=6; +-----+ | 5=6 | +-----+ | 0 | +-----+ 1 row in set (0.00 sec) != 不等于 #返回值为1表示结果为真 mysql> select 3!=2; +------+ | 3!=2 | +------+ | 1 | +------+ 1 row in set (0.01 sec) #返回值为0表示结果为假 mysql> select 3!=3; +------+ | 3!=3 | +------+ | 0 | +------+ 1 row in set (0.00 sec) 9.3逻辑运算 && 与 #返回值为1表示结果为真 mysql> select 3>1 && 5>1; +------------+ | 3>1 && 5>1 | +------------+ | 1 | +------------+ 1 row in set (0.00 sec) #返回值为0表示结果为假 mysql> select 3>1 && 51 && 5 || 或 #返回值为1表示结果为真 或关系中有一个为真结果就为真 mysql> select 3>1 || 51 || 5 select 3 not 非 #此写法不正确 mysql> select 3>6 not 3>5; ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '3>5' at line 1 #返回值为1表示结果为真 mysql> select not 3>5; +---------+ | not 3>5 | +---------+ | 1 | +---------+ 1 row in set (0.00 sec) #返回值为0表示结果为假 mysql> select not 5>3; +---------+ | not 5>3 | +---------+ | 0 | +---------+ 1 row in set (0.00 sec) 9.4排序 order by order by 列名 #默认升序 order by 列名 desc #降序 #student表内容如下 mysql> select * from student; +------+--------+-------+ | id | name | score | +------+--------+-------+ | 1 | 小明 | 99 | | 2 | 小红 | 79 | | 3 | 小丽 | 88 | | 4 | 小强 | 59 | | 5 | 小洲 | 66 | +------+--------+-------+ 5 rows in set (0.00 sec) #按照成绩升序排序 mysql> select * from student order by score; +------+--------+-------+ | id | name | score | +------+--------+-------+ | 4 | 小强 | 59 | | 5 | 小洲 | 66 | | 2 | 小红 | 79 | | 3 | 小丽 | 88 | | 1 | 小明 | 99 | +------+--------+-------+ 5 rows in set (0.00 sec) #按照成绩降序排序 mysql> select * from student order by score desc; +------+--------+-------+ | id | name | score | +------+--------+-------+ | 1 | 小明 | 99 | | 3 | 小丽 | 88 | | 2 | 小红 | 79 | | 5 | 小洲 | 66 | | 4 | 小强 | 59 | +------+--------+-------+ 5 rows in set (0.00 sec) 9.5限制 limit limit用于限制查询结果的条数 #student表内容如下 mysql> select * from student; +------+--------+-------+ | id | name | score | +------+--------+-------+ | 1 | 小明 | 99 | | 2 | 小红 | 79 | | 3 | 小丽 | 88 | | 4 | 小强 | 59 | | 5 | 小洲 | 66 | +------+--------+-------+ 5 rows in set (0.00 sec) #限制查询结果显示3条 mysql> select * from student limit 3; +------+--------+-------+ | id | name | score | +------+--------+-------+ | 1 | 小明 | 99 | | 2 | 小红 | 79 | | 3 | 小丽 | 88 | +------+--------+-------+ 3 rows in set (0.00 sec) 9.6分组 group by 9.6.1group by简单使用示例 #t1表内容如下 mysql> select * from t1; +------+--------+-------+ | id | name | score | +------+--------+-------+ | 1 | 小明 | 99.9 | | 2 | 小明 | 60.6 | | 3 | 小红 | 70 | | 4 | 小洲 | 88.5 | | 5 | 小明 | 77.5 | | 6 | 小洲 | 59 | +------+--------+-------+ 6 rows in set (0.00 sec) #根据姓名分组，group by会将列中值相同的行合并 mysql> select name from t1 group by name; +--------+ | name | +--------+ | 小明 | | 小洲 | | 小红 | +--------+ 3 rows in set (0.00 sec) #来个错误示例 mysql> select * from t1 group by name; ERROR 1055 (42000): Expression #1 of SELECT list is not in GROUP BY clause and contains nonaggregated column 'db1.t1.id' which is not functionally dependent on columns in GROUP BY clause; this is incompatible with sql_mode=only_full_group_by 错误原因： group by语句会将列中值相同的行合并，例如上边的t1表中，有多个名字相同的人小明，group by已经将多个小明的值合并为一个，但是小明是姓名相同的不同的人，成绩也不同，因此查询的时候会报错 9.6.2group by+having 分组后再过滤 #创建score表 mysql> create table score(sname char(10), cname char(10), grade int); Query OK, 0 rows affected (0.03 sec) //向表中插入数据 mysql> insert into score(sname,cname,grade) values ('张三','数学',80), ('张三','语文',90), ('张三','英语',70), ('张三','物理',60), ('李四','数学',66), ('李四','语文',60), ('李四','英语',80), ('李四','物理',90), ('刘五','语文',99), ('刘五','数学',50), ('刘五','英语',50), ('刘五','物理',89), ('罗六','语文',99), ('罗六','数学',80), ('罗六','物理',78), ('罗六','英语',96), ('许七','数学',96), ('许七','语文',96), ('许七','英语',96), ('许七','物理',96); Query OK, 20 rows affected (0.03 sec) Records: 20 Duplicates: 0 Warnings: 0 //查询平均成绩大于90分并且语文课95分以上的学生名和平均成绩 mysql> select sname,avg(grade) from score where sname in (select sname from score where cname='语文' and grade > 95) group by sname; +--------+------------+ | sname | avg(grade) | +--------+------------+ | 刘五 | 72.0000 | | 罗六 | 88.2500 | | 许七 | 96.0000 | +--------+------------+ 3 rows in set (0.00 sec) mysql> select sname,avg(grade) from score where sname in (select sname from score where cname='语文' and grade > 95) group by sname having avg(grade) > 90; +--------+------------+ | sname | avg(grade) | +--------+------------+ | 许七 | 96.0000 | +--------+------------+ 1 row in set (0.00 sec) 9.6.3group by+group_concat函数使用示例 #创建book表 mysql> create table book(书名 char(20) not null, 作者 char(10) not null, 出版社 char(20) not null, 价格 int unsigned, 出版日期 date); Query OK, 0 rows affected (0.02 sec) #向book表中插入数据 mysql> insert into book values( '那个女孩','小明','工业出版社',80,'2016-07-01'), ('阿三传说','小洲','人民出版社',10,'2019-09-09'), ('奔跑的草泥马','小明','盗版出版社',60,'2017-07-12'), ('上课必备三件套','小颖','人民出版社',250,'2018-11-11'), ('童话故事','','工业出版社',50,'2019-09-01'); Query OK, 5 rows affected (0.01 sec) Records: 5 Duplicates: 0 Warnings: 0 mysql> select * from book; +-----------------------+--------+-----------------+--------+--------------+ | 书名 | 作者 | 出版社 | 价格 | 出版日期 | +-----------------------+--------+-----------------+--------+--------------+ | 那个女孩 | 小明 | 工业出版社 | 80 | 2016-07-01 | | 阿三传说 | 小洲 | 人民出版社 | 10 | 2019-09-09 | | 奔跑的草泥马 | 小明 | 盗版出版社 | 60 | 2017-07-12 | | 上课必备三件套 | 小颖 | 人民出版社 | 250 | 2018-11-11 | | 童话故事 | | 工业出版社 | 50 | 2019-09-01 | +-----------------------+--------+-----------------+--------+--------------+ 5 rows in set (0.00 sec) //查询各出版社出版的所有图书，这里需要用到group_concat()函数 mysql> select 出版社,group_concat(书名) from book group by 出版社; +-----------------+------------------------------------+ | 出版社 | group_concat(书名) | +-----------------+------------------------------------+ | 人民出版社 | 阿三传说,上课必备三件套 | | 工业出版社 | 那个女孩,童话故事 | | 盗版出版社 | 奔跑的草泥马 | +-----------------+------------------------------------+ 3 rows in set (0.00 sec) 9.7检索区间 between...and... #student表内容如下 mysql> select * from student; +------+--------+-------+ | id | name | score | +------+--------+-------+ | 1 | 小明 | 99 | | 2 | 小红 | 79 | | 3 | 小丽 | 88 | | 4 | 小强 | 59 | | 5 | 小洲 | 66 | +------+--------+-------+ 5 rows in set (0.00 sec) #查询成绩在60分到90分之间的学生姓名 mysql> select name,score from student where score between 60 and 90; +--------+-------+ | name | score | +--------+-------+ | 小红 | 79 | | 小丽 | 88 | | 小洲 | 66 | +--------+-------+ 3 rows in set (0.00 sec) 9.8判断是否在范围 in not in #in 在一个范围内 //查看t1表 mysql> select * from t1; +------+---------+------+ | id | name | age | +------+---------+------+ | 1 | 小明 | 20 | | 2 | 大B哥 | 30 | | 3 | 小B哥 | 25 | | 4 | 龙哥 | 38 | | 5 | 小颖 | 18 | | 6 | 小洲 | 19 | +------+---------+------+ 6 rows in set (0.00 sec) //查询年龄是18、20、30岁之间的人的信息 mysql> select * from t1 where age in(18,19,30); +------+---------+------+ | id | name | age | +------+---------+------+ | 2 | 大B哥 | 30 | | 5 | 小颖 | 18 | | 6 | 小洲 | 19 | +------+---------+------+ 3 rows in set (0.00 sec) #not in 不在一个范围内 //查询年龄不在18、20、30岁之间的人的信息 mysql> select * from t1 where age not in(18,19,30); +------+---------+------+ | id | name | age | +------+---------+------+ | 1 | 小明 | 20 | | 3 | 小B哥 | 25 | | 4 | 龙哥 | 38 | +------+---------+------+ 3 rows in set (0.00 sec) 10.mysql的API及接口自带命令 API：应用程序接口 10.1mysql API mysql API就是能够不进入mysql而在命令行中执行sql语句，在命令行中使用-e选项 mysqlAPI示例1，在命令行中使用SQL语句 [root@mysql ~]# mysql -uroot -p123 mysql -e \"show databases;\" +--------------------+ | Database | +--------------------+ | information_schema | | DB1 | | mysql | | performance_schema | | test | +--------------------+ mysqlAPI示例2，php连接mysql 10.2mysql接口自带命令 以下命令在mysql中或者命令行执行都可以 help 或？ #查看帮助 \\G #格式化查看数据（key：value） \\T 或 tee #记录日志 \\c（5.7可以ctrl+c） #结束命令 \\s 或 status #查看状态信息 \\. 或 source #导入SQL数据 \\u或 use #使用数据库 \\q 或 exit 或 quit #退出 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql基础/3.CentOS7二进制安装MySQL5.6.html":{"url":"db/mysql/mysql基础/3.CentOS7二进制安装MySQL5.6.html","title":"MySQL5.6","keywords":"","body":"[toc] CentOS7二进制安装MySQL5.6 mysql历史版本官方下载地址 mysql官网 mysql github地址 1.安装依赖包 yum -y install -y gcc gcc-c++ autoconf bison-devel ncurses-devel libaio-devel numactl 2.下载MySQL5.6二进制包 export MYSQL_VERSION=5.6.51 wget https://cdn.mysql.com/archives/mysql-5.6/mysql-${MYSQL_VERSION}-linux-glibc2.12-x86_64.tar.gz 3.解压缩mysql二进制包到 /usr/local tar xf mysql-${MYSQL_VERSION}-linux-glibc2.12-x86_64.tar.gz -C /usr/local 4.修改目录名称、做软连接 mv /usr/local/mysql-${MYSQL_VERSION}-linux-glibc2.12-x86_64 /usr/local/mysql-${MYSQL_VERSION} ln -s /usr/local/mysql-${MYSQL_VERSION} /usr/local/mysql 5.创建mysql用户 # 创建mysql用户 useradd mysql -s /bin/nologin -M 6.编辑主配置文件 mysql5.6二进制包中 support-files/my-default.cnf 有示例配置文件，这里选择手动编辑基础配置文件 ⚠️如果指定了mysql的socket文件位置，则必须添加 [client] 标签并同时指定socket文件位置，否则客户端会从 /tmp 下找socket文件 # 备份/etc/my.cnf mv /etc/my.cnf /etc/my.cnf.old # 以下配置为最精简版，可根据实际情况进行相应设置 cat > /etc/my.cnf 7.创建socket文件目录 mkdir -p /var/lib/mysql 8.相关目录、文件授权 chown -R mysql.mysql /usr/local/mysql* /var/lib/mysql /etc/my.cnf 9.拷贝启动脚本 cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysqld 10.初始化mysql /usr/local/mysql/scripts/mysql_install_db \\ --user=mysql \\ --basedir=/usr/local/mysql \\ --datadir=/usr/local/mysql/data mysql5.6初始化参数说明 参数 说明 --user 指定mysql用户 --basedir 指定mysql安装目录 --datadir 指定mysql数据目录 11.导出mysql命令环境变量 # 导出mysql命令环境变量 echo \"export PATH=/usr/local/mysql/bin:$PATH\" > /etc/profile.d/mysql.sh # 使配置生效 source /etc/profile 12.配置systemd管理mysql cat > /usr/lib/systemd/system/mysqld.service 13.启动mysql、检查启动 # 重新加载systemd系统服务 systemctl daemon-reload # 启动mysql并加入开机自启 systemctl start mysqld && systemctl enable mysqld 查看 # 查看mysql端口 $ netstat -ntpl | grep 3306 tcp6 0 0 :::3306 :::* LISTEN 31349/mysqld 14.进入mysql并设置密码 # 进入mysql mysql # 设置mysql密码 mysql> set password='123'; Query OK, 0 rows affected (0.00 sec) mysql> flush privileges; Query OK, 0 rows affected (0.01 sec) 到此，mysql5.6二进制安装完成！！！ 如果设置mysql密码遇到报错，这是因为mysql要求输入16进制 mysql> set password='123'; ERROR 1372 (HY000): Password hash should be a 41-digit hexadecimal number 先查询16进制 mysql> select password('123'); +-------------------------------------------+ | password('PAPAqike.com!1') | +-------------------------------------------+ | *23AE809DDACAF96AF0FD78ED04B6A265E05AA257 | +-------------------------------------------+ 1 row in set (0.00 sec) 再进行设置即可 mysql> set password='*23AE809DDACAF96AF0FD78ED04B6A265E05AA257'; Query OK, 0 rows affected (0.00 sec) 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql基础/4.CentOS7二进制安装MySQL5.7.html":{"url":"db/mysql/mysql基础/4.CentOS7二进制安装MySQL5.7.html","title":"MySQL5.7","keywords":"","body":"[toc] CentOS7二进制安装MySQL5.7 mysql历史版本官方下载地址 mysql官网 mysql github地址 ⚠️⚠️⚠️ 二进制安装mysql的启动脚本support-files/mysql.server 和安装目录下的 bin/mysqld_safe 这两个文件中都是默认使用 /usr/local/mysql，如果安装目录不在 /usr/local/ 下，需要修改这两个文件中的路径，即把 /usr/local 替换为mysql安装目录 1.安装依赖包 yum -y install -y gcc gcc-c++ autoconf bison-devel ncurses-devel libaio-devel numactl 2.下载MySQL5.7二进制包 export MYSQL_VERSION=5.7.30 wget https://cdn.mysql.com/archives/mysql-5.7/mysql-${MYSQL_VERSION}-linux-glibc2.12-x86_64.tar.gz 3.解压缩mysql二进制包到 /usr/local tar xf mysql-${MYSQL_VERSION}-linux-glibc2.12-x86_64.tar.gz -C /usr/local 4.修改目录名称、做软连接 mv /usr/local/mysql-${MYSQL_VERSION}-linux-glibc2.12-x86_64 /usr/local/mysql-${MYSQL_VERSION} ln -s /usr/local/mysql-${MYSQL_VERSION} /usr/local/mysql 5.创建mysql用户 useradd -M -s /bin/nologin mysql 6.编辑主配置文件，myql5.7二进制包默认没有mysql配置文件 ⚠️如果指定了mysql的socket文件位置，则必须添加[client]标签并同时指定socket文件位置，否则客户端会从/tmp下找socket文件 # 备份/etc/my.cnf mv /etc/my.cnf /etc/my.cnf.old # 以下配置为最精简版，可根据实际情况进行相应设置 cat > /etc/my.cnf 7.创建socket文件目录 mkdir -p /var/lib/mysql 8.相关目录、文件授权 chown -R mysql.mysql /usr/local/mysql* /var/lib/mysql /etc/my.cnf 9.拷贝启动脚本 cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysqld 10.初始化mysql ⚠️mysql5.7初始化没有提示！！！ /usr/local/mysql/bin/mysqld \\ --user=mysql \\ --basedir=/usr/local/mysql \\ --datadir=/usr/local/mysql/data \\ --initialize-insecure mysql5.7初始化参数说明 参数 说明 --user 指定mysql用户 --basedir 指定mysql安装目录 --datadir 指定mysql数据目录 --initialize-insecure 不生成随机密码 11.导出mysql命令环境变量 # 导出mysql命令环境变量 echo \"export PATH=/usr/local/mysql/bin:$PATH\" > /etc/profile.d/mysql.sh # 使配置生效 source /etc/profile 12.配置systemd管理mysql cat > /usr/lib/systemd/system/mysqld.service 13.启动mysql、检查启动 启动mysql # 重新加载systemd系统服务 systemctl daemon-reload # 启动mysql并加入开机自启 systemctl start mysqld && systemctl enable mysqld 查看 # 查看mysql端口 $ netstat -ntpl | grep 3306 tcp6 0 0 :::3306 :::* LISTEN 31349/mysqld 14.进入mysql并设置密码 # 进入mysql mysql # 设置mysql密码 mysql> set password='123'; Query OK, 0 rows affected (0.00 sec) mysql> flush privileges; Query OK, 0 rows affected (0.01 sec) 到此，mysql5.7二进制安装完成！！！ 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql基础/5.CentOS7二进制安装MySQL8.0.html":{"url":"db/mysql/mysql基础/5.CentOS7二进制安装MySQL8.0.html","title":"MySQL8.0","keywords":"","body":"[toc] CentOS7二进制安装MySQL8.0 mysql历史版本官方下载地址 mysql官网 mysql github地址 1.安装依赖包 yum -y install -y gcc gcc-c++ autoconf bison-devel ncurses-devel libaio-devel numactl 2.下载MySQL8.0二进制包 export MYSQL_VERSION=8.0.22 wget https://cdn.mysql.com/archives/mysql-8.0/mysql-${MYSQL_VERSION}-linux-glibc2.12-x86_64.tar.xz 3.解压缩mysql二进制包到 /usr/local tar xf mysql-${MYSQL_VERSION}-linux-glibc2.12-x86_64.tar.xz -C /usr/local/ 4.修改目录名称、做软连接 mv /usr/local/mysql-${MYSQL_VERSION}-linux-glibc2.12-x86_64 /usr/local/mysql-${MYSQL_VERSION} ln -s /usr/local/mysql-${MYSQL_VERSION} /usr/local/mysql 5.创建mysql用户 useradd -M -s /sbin/nologin mysql 6.编辑主配置文件，myql8.0二进制包默认没有mysql配置文件 ⚠️如果指定了mysql的socket文件位置，则必须添加[client]标签并同时指定socket文件位置，否则客户端会从/tmp下找socket文件 # 备份原有/etc/my.cnf mv /etc/my.cnf /etc/my.cnf.old # 以下为精简版主配置文件，后续根据实际情况修改 cat >> /etc/my.cnf 7.创建socket文件目录 mkdir -p /var/lib/mysql 8.相关目录、文件授权 chown -R mysql.mysql /usr/local/mysql* /var/lib/mysql /etc/my.cnf 9.拷贝启动脚本 cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysqld 10.初始化mysql ⚠️mysql8.0初始化没有提示！！！ /usr/local/mysql/bin/mysqld \\ --user=mysql \\ --basedir=/usr/local/mysql \\ --datadir=/usr/local/mysql/data \\ --initialize-insecure mysql8.0初始化参数说明 参数 说明 --user 指定mysql用户 --basedir 指定mysql安装目录 --datadir 指定mysql数据目录 --initialize-insecure 不生成随机密码 11.导出mysql命令环境变量 # 导出mysql命令环境变量 echo \"export PATH=/usr/local/mysql/bin:$PATH\" > /etc/profile.d/mysql.sh # 使配置生效 source /etc/profile 12.配置systemd管理mysql cat > /usr/lib/systemd/system/mysqld.service 13.启动mysql、检查启动 启动mysql # 重新加载systemd系统服务 systemctl daemon-reload # 启动mysql并加入开机自启 systemctl start mysqld && systemctl enable mysqld 查看 # 查看mysql端口 $ netstat -ntpl | grep 3306 tcp6 0 0 :::3306 :::* LISTEN 31349/mysqld 14.连接mysql并设置密码 # 进入mysql mysql # 设置mysql密码 mysql> set password='123'; Query OK, 0 rows affected (0.00 sec) mysql> flush privileges; Query OK, 0 rows affected (0.01 sec) 到此，mysql8.0二进制安装完成！！！ 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql基础/6.CentOS7编译安装MySQL5.6.html":{"url":"db/mysql/mysql基础/6.CentOS7编译安装MySQL5.6.html","title":"MySQL5.6","keywords":"","body":"[toc] CentOS7编译安装MySQL5.6 mysql历史版本官方下载地址 mysql官网 mysql github地址 1.安装依赖包 yum -y install gcc gcc-c++ automake autoconf cmake bison-devel ncurses-devel libaio-devel openssl-devel 2.下载MySQL5.6源码包 export MYSQL_VERSION=5.6.51 wget https://cdn.mysql.com/archives/mysql-5.6/mysql-${MYSQL_VERSION}.tar.gz 3.解压缩源码包 tar xf mysql-${MYSQL_VERSION}.tar.gz 4.进入解压缩目录，进行编译安装 # 进入到解压目录 cd mysql-${MYSQL_VERSION} # 进行cmake cmake . -DCMAKE_INSTALL_PREFIX=/usr/local/mysql-${MYSQL_VERSION} \\ -DDEFAULT_CHARSET=utf8 \\ -DDEFAULT_COLLATION=utf8_general_ci \\ -DWITH_EXTRA_CHARSETS=all \\ -DWITH_INNOBASE_STORAGE_ENGINE=1 \\ -DWITH_FEDERATED_STORAGE_ENGINE=1 \\ -DWITH_BLACKHOLE_STORAGE_ENGINE=1 \\ -DWITHOUT_EXAMPLE_STORAGE_ENGINE=1 \\ -DWITH_ZLIB=bundled \\ -DENABLED_LOCAL_INFILE=1 \\ -DWITH_EMBEDDED_SERVER=1 \\ -DENABLE_DOWNLOADS=1 \\ -DWITH_DEBUG=0 # 编译并安装 make -j`nproc` && make install 5.做目录软连接 ln -s /usr/local/mysql-${MYSQL_VERSION} /usr/local/mysql 6.创建mysql用户 useradd -M -s /bin/nologin mysql 7.初始化数据库 /usr/local/mysql/scripts/mysql_install_db \\ --user=mysql \\ --basedir=/usr/local/mysql \\ --datadir=/usr/local/mysql/data mysql5.6初始化参数说明 参数 说明 --user 指定mysql用户 --basedir 指定mysql安装目录 --datadir 指定mysql数据目录 8.编辑主配置文件 # 以下配置为最精简版，可根据实际情况进行相应设置 cat > /etc/my.cnf 9.创建socket文件目录 mkdir -p /var/lib/mysql 10.相关目录、文件授权 chown -R mysql.mysql /usr/local/mysql* /var/lib/mysql /etc/my.cnf 11.拷贝mysql启动文件 cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysqld 12.导出mysql命令环境变量 # 导出mysql命令环境变量 echo \"export PATH=/usr/local/mysql/bin:$PATH\" > /etc/profile.d/mysql.sh # 使配置生效 source /etc/profile 13.用systemd管理mysql cat > /usr/lib/systemd/system/mysqld.service 14.启动mysql、检查启动 # 重新加载systemd系统服务 systemctl daemon-reload # 启动mysql并加入开机自启 systemctl start mysqld && systemctl enable mysqld 查看 # 查看mysql端口 $ netstat -ntpl | grep 3306 tcp6 0 0 :::3306 :::* LISTEN 31349/mysqld 15.进入mysql并设置密码 # 进入mysql mysql # 设置mysql密码 mysql> set password='123'; Query OK, 0 rows affected (0.00 sec) mysql> flush privileges; Query OK, 0 rows affected (0.01 sec) 到此，mysql5.6二进制安装完成！！！ 扩展：一台主机mysql已经编译好，通过拷贝相关目录和文件的方式快速部署其他mysql实例 步骤： 1.几台机器的环境要一致，操作系统、硬件环境 2.拷贝mysql编译安装路径目录，本文为 /usr/local/mysql-5.6.51 3.拷贝mysql配置文件、启动文件、mysql命令环境变量文件 /etc/profile.d/mysql.sh(建议在另外的mysql主机手动填写，避免覆盖原先的PATH环境变量) 4.拷贝systemd管理文件 /usr/lib/systemd/system/mysqld.service 5.创建mysql用户和组 1.已经编译安装好的mysql主机拷贝相关包 # 打包mysql编译安装目录然后将压缩包拷贝至另一台mysql主机下的/usr/local tar cf mysql.tar /usr/local/mysql-5.6.51 # 拷贝配置文件 /etc/my.cnf # 拷贝启动文件 /etc/init.d/mysqld # 拷贝systemd管理文件 /usr/lib/systemd/system/mysqld.service 2.另外一台mysql主机操作 # 创建mysql用户 useradd -M -s /bin/nologin mysql # 解压缩拷贝过来的mysql包，包位置放到/usr/local tar xf mysql.tar # 修改权限 chown -R mysql.mysql mysql-5.6.40 # 做软连接 ln -s mysql-5.6.40/ mysql # 使mysql命令环境变量配置生效 echo \"export PATH=/usr/local/mysql/bin:$PATH\" > /etc/profile.d/mysql.sh source /etc/profile # 移动拷贝过来的mysql配置文件my.cnf到/etc mv my.cnf /etc # 启动mysql并设置开机自启 systemctl start mysqld && systemctl enable mysqld 或者 /etc/init.d/mysqld start # 验证端口 $ netstat -ntpl|grep 3306 tcp6 0 0 :::3306 :::* LISTEN 1799/mysqld 到此，快速部署mysql完成 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql基础/7.CentOS7编译安装MySQL5.7.html":{"url":"db/mysql/mysql基础/7.CentOS7编译安装MySQL5.7.html","title":"MySQL5.7","keywords":"","body":"[toc] CentOS7编译安装MySQL5.7 mysql历史版本官方下载地址 mysql官网 mysql github地址 1.安装依赖包 yum -y install -y gcc gcc-c++ automake autoconf cmake bison-devel ncurses-devel libaio-devel openssl-devel 2.下载boost 5.7版本源码编译安装需要下载一个Boost C++ 1.59.0（这是一组扩充C++功能的经过同行评审（Peer-reviewed）且开放源代码程序库。大多数的函数为了能够以开放源代码、封闭项目的方式运作，而授权于Boost软件许可协议（Boost Software License）之下。） wget https://sourceforge.net/projects/boost/files/boost/1.59.0/boost_1_59_0.tar.gz 3.解压缩boost至 /usr/local 下 tar xf boost_1_59_0.tar.gz -C /usr/local 4.下载MySQL5.7源码包 export MYSQL_VERSION=5.7.30 wget https://cdn.mysql.com/archives/mysql-5.7/mysql-${MYSQL_VERSION}.tar.gz 5.解压缩源码包 tar xf mysql-${MYSQL_VERSION}.tar.gz 6.进入解压目录，开始编译安装 # 进入到解压目录 cd mysql-${MYSQL_VERSION} # 进行cmake cmake -DCMAKE_INSTALL_PREFIX=/usr/local/mysql-${MYSQL_VERSION} \\ -DMYSQL_USER=mysql \\ -DWITH_MYISAM_STORAGE_ENGINE=1 \\ -DWITH_INNOBASE_STORAGE_ENGINE=1 \\ -DEXTRA_CHARSETS=all \\ -DDEFAULT_CHARSET=utf8 \\ -DDEFAULT_COLLATION=utf8_general_ci \\ -DWITH_DEBUG=0 \\ -DMYSQL_MAINTAINER_MODE=0 \\ -DWITH_ZLIB:STRING=bundled \\ -DWITH_SYSTEMD=1 \\ -DDOWNLOAD_BOOST=0 \\ -DWITH_DEBUG=0 \\ -DWITH_BOOST=/usr/local/boost_1_59_0 # 编译并安装 make -j`nproc` && make install 7.做目录软连接 ln -s /usr/local/mysql-${MYSQL_VERSION} /usr/local/mysql 8.创建mysql用户 useradd -M -s /bin/nologin mysql 9.编辑主配置文件 ⚠️如果指定了mysql的socket文件位置，则必须添加[client]标签并同时指定socket文件位置，否则客户端会从/tmp下找socket文件 # 备份/etc/my.cnf mv /etc/my.cnf /etc/my.cnf.old # 以下配置为最精简版，可根据实际情况进行相应设置 cat > /etc/my.cnf 10.创建socket文件目录 mkdir -p /var/lib/mysql 11.相关目录、文件授权 chown -R mysql.mysql /usr/local/mysql* /var/lib/mysql /etc/my.cnf 12.拷贝启动脚本 cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysqld 13.初始化mysql ⚠️MySQL 5.7.6之前的版本使用如下方式初始化数据库 /usr/local/mysql/bin/mysql_install_db \\ --user=mysql \\ --basedir=/usr/local/mysql \\ --datadir=/usr/local/mysql/data ⚠️MySQL 5.7.6之后的版本使用如下方式初始化数据库 /usr/local/mysql/bin/mysqld \\ --user=mysql \\ --basedir=/usr/local/mysql \\ --datadir=/usr/local/mysql/data \\ --initialize-insecure mysql5.7初始化参数说明 参数 说明 --user 指定mysql用户 --basedir 指定mysql安装目录 --datadir 指定mysql数据目录 --initialize-insecure 不生成随机密码 通过 mysqld --verbose --help|grep initialize 查看说明 14.导出mysql命令环境变量 # 导出mysql命令环境变量 echo \"export PATH=/usr/local/mysql/bin:$PATH\" > /etc/profile.d/mysql.sh # 使配置生效 source /etc/profile 15.systemd管理mysql cat > /usr/lib/systemd/system/mysqld.service 16.启动mysql、检查启动 启动mysql # 重新加载systemd系统服务 systemctl daemon-reload # 启动mysql并加入开机自启 systemctl start mysqld && systemctl enable mysqld 查看 # 查看mysql端口 $ netstat -ntpl | grep 3306 tcp6 0 0 :::3306 :::* LISTEN 31349/mysqld 17.进入mysql并设置密码 # 进入mysql mysql # 设置mysql密码 mysql> set password='123'; Query OK, 0 rows affected (0.00 sec) mysql> flush privileges; Query OK, 0 rows affected (0.01 sec) 到此，mysql5.7源码安装完成！！！ 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql基础/8.CentOS7编译安装MySQL8.0.html":{"url":"db/mysql/mysql基础/8.CentOS7编译安装MySQL8.0.html","title":"MySQL8.0","keywords":"","body":"[toc] CentOS7编译安装MySQL8.0 mysql历史版本官方下载地址 mysql官网 mysql github地址 1.安装依赖包 yum -y install -y gcc gcc-c++ automake autoconf bison-devel ncurses-devel libaio-devel openssl-devel gmp-devel mpfr-devel flex 2.安装cmake ⚠️mysql8.0.22需要cmake 3.5+ # 下载源码包 wget https://github.com/Kitware/CMake/releases/download/v3.21.1/cmake-3.21.1.tar.gz # 解压缩 tar xf cmake-3.21.1.tar.gz # 编译安装 cd cmake-3.21.1 ./configure --prefix=/usr/local/cmake make -j`nproc` && make install # 导出cmake命令环境变量 echo 'export PATH=/usr/local/cmake/bin:$PATH' > /etc/profile.d/cmake.sh source /etc/profile.d/cmake.sh # 验证版本 $ cmake --version cmake version 3.21.1 3.安装mpc configure: error: Building GCC requires GMP 4.2+, MPFR 2.4.0+ and MPC 0.8.0+. wget http://www.multiprecision.org/downloads/mpc-0.9.tar.gz tar xf mpc-0.9.tar.gz cd mpc-0.9 ./configure --prefix=/usr/local/mpc make -j`nproc` && make install 3.安装gcc ⚠️mysql8.0.22需要gcc 5.3+ # 下载源码包 wget https://github.com/gcc-mirror/gcc/archive/refs/tags/releases/gcc-5.3.0.tar.gz # 解压缩 tar xf gcc-releases-gcc-5.3.0.tar.gz # 编译安装 cd gcc-releases-gcc-5.3.0 ./configure --prefix=/usr/local/gcc --with-mpc=/usr/local/mpc --disable-multilib make -j`nproc` && make install export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/gcc-4.6.3/mpc-0.9/mpc_install/lib:/opt/gcc-4.6.3/gmp-5.0.4/gmp_install/lib:/opt/gcc-4.6.3/mpfr-3.1.0/mpfr_install/lib export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/mpc/lib 2.下载boost 5.7版本源码编译安装需要下载一个Boost C++ 1.59.0（这是一组扩充C++功能的经过同行评审（Peer-reviewed）且开放源代码程序库。大多数的函数为了能够以开放源代码、封闭项目的方式运作，而授权于Boost软件许可协议（Boost Software License）之下。） wget https://sourceforge.net/projects/boost/files/boost/1.59.0/boost_1_59_0.tar.gz 3.解压缩boost至 /usr/local 下 tar xf boost_1_59_0.tar.gz -C /usr/local 4.下载MySQL8.0源码包 export MYSQL_VERSION=8.0.22 wget https://cdn.mysql.com/archives/mysql-8.0/mysql-${MYSQL_VERSION}.tar.gz 5.解压缩源码包 tar xf mysql-${MYSQL_VERSION}.tar.gz 6.进入解压目录，开始编译安装 # 进入到解压目录 cd mysql-${MYSQL_VERSION} # 进行cmake cmake -DCMAKE_INSTALL_PREFIX=/usr/local/mysql-${MYSQL_VERSION} \\ -DMYSQL_USER=mysql \\ -DWITH_MYISAM_STORAGE_ENGINE=1 \\ -DWITH_INNOBASE_STORAGE_ENGINE=1 \\ -DEXTRA_CHARSETS=all \\ -DDEFAULT_CHARSET=utf8 \\ -DDEFAULT_COLLATION=utf8_general_ci \\ -DWITH_DEBUG=0 \\ -DMYSQL_MAINTAINER_MODE=0 \\ -DWITH_ZLIB:STRING=bundled \\ -DWITH_SYSTEMD=1 \\ -DDOWNLOAD_BOOST=0 \\ -DWITH_DEBUG=0 \\ -DFORCE_INSOURCE_BUILD=1 \\ -DWITH_BOOST=/usr/local/boost_1_59_0 # 编译并安装 make -j`nproc` && make install 7.做目录软连接 ln -s /usr/local/mysql-${MYSQL_VERSION} /usr/local/mysql 8.创建mysql用户 useradd -M -s /bin/nologin mysql 9.编辑主配置文件 ⚠️如果指定了mysql的socket文件位置，则必须添加[client]标签并同时指定socket文件位置，否则客户端会从/tmp下找socket文件 # 备份/etc/my.cnfmv /etc/my.cnf /etc/my.cnf.old# 以下配置为最精简版，可根据实际情况进行相应设置 cat > /etc/my.cnf 10.创建socket文件目录 mkdir -p /var/lib/mysql 11.相关目录、文件授权 chown -R mysql.mysql /usr/local/mysql* /var/lib/mysql /etc/my.cnf 12.拷贝启动脚本 cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysqld 13.初始化mysql ⚠️MySQL 5.7.6之前的版本使用如下方式初始化数据库 /usr/local/mysql/bin/mysql_install_db \\--user=mysql \\--basedir=/usr/local/mysql \\ --datadir=/usr/local/mysql/data ⚠️MySQL 5.7.6之后的版本使用如下方式初始化数据库 /usr/local/mysql/bin/mysqld \\--user=mysql \\--basedir=/usr/local/mysql \\--datadir=/usr/local/mysql/data \\--initialize-insecure mysql5.7初始化参数说明 参数 说明 --user 指定mysql用户 --basedir 指定mysql安装目录 --datadir 指定mysql数据目录 --initialize-insecure 不生成随机密码 通过 mysqld --verbose --help|grep initialize 查看说明 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql基础/9.mysql连接与启动过程.html":{"url":"db/mysql/mysql基础/9.mysql连接与启动过程.html","title":"4.mysql连接与启动过程","keywords":"","body":"[toc] mysql连接与启动过程 1.mysql连接管理 1.1连接工具 1)MySQL自带的连接工具 mysql 常见的特定于客户机的连接选项： -u：指定用户 -p：指定密码 -h：指定主机 -P：指定端口 -S：指定sock文件 -e：指定SQL(命令行执行sql命令) 2)第三方的连接工具 sqlyog、navicat 应用程序连接MySQL 注意：需要加载对应语言程序的API 1.2连接方式 1.2.1socket连接 第一种方式 //mysql -uroot -p [root@db01 ~]# mysql -uroot -p mysql> status; Connection: Localhost via UNIX socket 第二种方式 [root@db01 ~]# mysql -uroot -p -S /tmp/mysql.sock mysql> status; Connection: Localhost via UNIX socket 1.2.2TCP/IP连接 [root@db01 ~]# mysql -uroot -p -h10.0.0.11 -P3306 mysql> status; Connection: 10.0.0.11 via TCP/IP 2.mysql启动流程 2.1示意图 /etc/init.d/mysqld start ------> mysqld_safe ------> mysqld 2.2mysql启动优先顺序 1.命令行选项 2.初始化配置文件 3.预编译选项，预编译选项优先级最低，因此，即使在编译的时候没有指定某些选项，也可以在配置文件中修改 2.3配置文件读取顺序 ⚠️⚠️⚠️此顺序为使用/etc/init.d/mysql.service启动脚本方法生效，使用systemd不生效 1./etc/my.cnf 2./etc/mysql/my.cnf 3.安装目录/my.cnf（前提是在环境变量中定义了 安装目录 变量） 4.defaults-extra-file （类似include） 5.~/.my.cnf 命令行中加上--defaults-file=xxx，以上文件都不读取 ⚠️⚠️⚠️配置文件顺序优先，但是配置优先级最低 配置文件优先级最终结论 1、命令行 2、defaults-file 3、配置文件 4、预编译 查看mysql默认读取my.cnf的顺序 $ mysql --help --verbose | grep 'my.cnf' order of preference, my.cnf, $MYSQL_TCP_PORT, /etc/my.cnf /etc/mysql/my.cnf /usr/local/mysql/etc/my.cnf ~/.my.cnf 查看mysqld默认读取my.cnf的顺序 $ mysqld --verbose --help | grep 'my.cnf' /etc/my.cnf /etc/mysql/my.cnf /usr/local/mysql/etc/my.cnf ~/.my.cnf my.cnf, $MYSQL_TCP_PORT, /etc/services, built-in default 2.4命令行与defaults-file优先级比较示例 在/tmp/a.txt中指定socket文件位置，然后用mysqld_safe启动，socket文件最终会在/tmp/b.sock //编辑一个文件 [root@db01 ~]# cat /tmp/a.txt [mysqld] socket=/tmp/a.sock //用mysqld_safe启动并指定socket文件位置 [root@db01 ~]# mysqld_safe --defaults-file=/etc/my.cnf --socket=/tmp/b.sock 最终socket文件在/tmp/b.sock，因此说明了在命令行中的优先级最大 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql基础/10.mysql数据类型和SQL语句分类.html":{"url":"db/mysql/mysql基础/10.mysql数据类型和SQL语句分类.html","title":"5.mysql数据类型和SQL语句分类","keywords":"","body":"[toc] mysql数据类型和SQL语句分类 1.SQL语句分类 DDL(Data Definition Language)数据定义语言 create、alter、drop DML(Data Manipulation Language)数据操纵语言 insert、update、delete DCL(Data Control Language)数据控制语言 grant、revoke DQL(Data Query Language)数据查询语言 select、show 2.mysql数据类型 2.1 数值类型 2.1.1 整型 类型 大小 范围（有符号） 范围（无符号）unsigned约束 用途 TINYINT 1 字节 (-128，127) (0，255) 小整数值 SMALLINT 2 字节 (-32 768，32 767) (0，65 535) 大整数值 MEDIUMINT 3 字节 (-8 388 608，8 388 607) (0，16 777 215) 大整数值 INT或INTEGER 4 字节 (-2 147 483 648，2 147 483 647) (0，4 294 967 295) 大整数值 BIGINT 8 字节 (-9 233 372 036 854 775 808，9 223 372 036 854 775 807) (0，18 446 744 073 709 551 615) 极大整数值 2.1.1.1 int和tinyint使用示例 常用的数据类型是tinyint和int #创建表t1 mysql> create table t1(t tinyint,i int); Query OK, 0 rows affected (0.02 sec) //向表中插入数据，有符号的tinyint下限是-128，因此插入-129报错 mysql> insert into t1 values(-129,-129); ERROR 1264 (22003): Out of range value for column 't' at row 1 mysql> insert into t1 values(-128,-129); Query OK, 1 row affected (0.01 sec) #创建表t2 mysql> create table t2(t tinyint(3),i int(3)); Query OK, 0 rows affected (0.08 sec) //向表t2中插入数据 mysql> insert into t2 values(1000,1000); ERROR 1264 (22003): Out of range value for column 't' at row 1 mysql> insert into t2 values(999,1000); ERROR 1264 (22003): Out of range value for column 't' at row 1 mysql> insert into t2 values(128,999); ERROR 1264 (22003): Out of range value for column 't' at row 1 mysql> insert into t2 values(127,999); Query OK, 1 row affected (0.01 sec) 2.1.2 浮点型 类型 大小 范围(有符号) 范围(无符号) 用途 FLOAT 4 字节float(255,30) (-3.402 823 466 E+38，-1.175 494 351 E-38)，0，(1.175 494 351 E-38，3.402 823 466 351 E+38) 0，(1.175 494 351 E-38，3.402 823 466 E+38) 单精度 浮点数值 DOUBLE 8 字节double(255,30) (-1.797 693 134 862 315 7 E+308，-2.225 073 858 507 201 4 E-308)，0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 双精度 浮点数值 DECIMAL 对DECIMAL(M,D) ，如果M>D，为M+2否则为D+2double(65,30) 依赖于M和D的值 依赖于M和D的值 小数值 float、double、decimal的格式相同 float(m,n) double(m,n) decimal(m,n) 其中m表示一共有m位，有n位小数 #创建t1表，三个字段分别为float，double和decimal参数表示一共显示5位，小数部分占2位 mysql> create table t1(id1 float(5,2),id2 double(5,2),id3 decimal(5,2)); Query OK, 0 rows affected (0.01 sec) mysql> desc t1; +-------+--------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+--------------+------+-----+---------+-------+ | id1 | float(5,2) | YES | | NULL | | | id2 | double(5,2) | YES | | NULL | | | id3 | decimal(5,2) | YES | | NULL | | +-------+--------------+------+-----+---------+-------+ 3 rows in set (0.00 sec) //向表中插入1.11，结果正常 mysql> insert into t1 values(1.11,1.11,1.11); Query OK, 1 row affected (0.00 sec) mysql> select * from t1; +------+------+------+ | id1 | id2 | id3 | +------+------+------+ | 1.11 | 1.11 | 1.11 | +------+------+------+ 1 row in set (0.00 sec) //向表中插入1.234发现只能保留2位小数 mysql> insert into t1 values(1.234,1.234,1.234); Query OK, 1 row affected, 1 warning (0.01 sec) mysql> select * from t1; +------+------+------+ | id1 | id2 | id3 | +------+------+------+ | 1.11 | 1.11 | 1.11 | | 1.23 | 1.23 | 1.23 | +------+------+------+ 2 rows in set (0.00 sec) //mysql> insert into t1 values(1.236,1.236,1.236); Query OK, 1 row affected, 1 warning (0.02 sec) mysql> select * from t1; +------+------+------+ | id1 | id2 | id3 | +------+------+------+ | 1.11 | 1.11 | 1.11 | | 1.23 | 1.23 | 1.23 | | 1.24 | 1.24 | 1.24 | +------+------+------+ 3 rows in set (0.00 sec).236，虽然只能保留2位小数，但是有四舍五入的规则 #新建t2表 mysql> create table t2(f float,do double,de decimal); Query OK, 0 rows affected (0.08 sec) mysql> desc t2; +-------+---------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+---------------+------+-----+---------+-------+ | f | float | YES | | NULL | | | do | double | YES | | NULL | | | de | decimal(10,0) | YES | | NULL | | +-------+---------------+------+-----+---------+-------+ 3 rows in set (0.00 sec) //分别插入1.2345，查看表可以看到decimal默认是整型 mysql> insert into t2 values(1.2345,1.2345,1.2345); Query OK, 1 row affected, 1 warning (0.00 sec) mysql> select * from t2; +--------+--------+------+ | f | do | de | +--------+--------+------+ | 1.2345 | 1.2345 | 1 | +--------+--------+------+ 1 row in set (0.00 sec) //分别插入超长的数，查看3者的区别，可以看到double的精度更高 mysql> insert into t2 values(1.234567890123456789,1.234567890123456789,1.234567890123456789); Query OK, 1 row affected, 1 warning (0.02 sec) mysql> select * from t2; +---------+--------------------+------+ | f | do | de | +---------+--------------------+------+ | 1.2345 | 1.2345 | 1 | | 1.23457 | 1.2345678901234567 | 1 | +---------+--------------------+------+ 2 rows in set (0.00 sec) 结论： 在不指定长度的情况下，decimal默认是整型存储 double的精度要比float高 2.2 字符串类型 类型 大小 用途 CHAR 0-255字节 定长字符串 VARCHAR 0-65535 字节 变长字符串 TINYBLOB 0-255字节 不超过 255 个字符的二进制字符串 TINYTEXT 0-255字节 短文本字符串 BLOB 0-65 535字节 二进制形式的长文本数据 TEXT 0-65 535字节 长文本数据 MEDIUMBLOB 0-16 777 215字节 二进制形式的中等长度文本数据 MEDIUMTEXT 0-16 777 215字节 中等长度文本数据 LONGBLOB 0-4 294 967 295字节 二进制形式的极大文本数据 LONGTEXT 0-4 294 967 295字节 极大文本数据 2.2.1 char与varchar使用示例 CHAR 和 VARCHAR 类型类似，但它们保存和检索的方式不同。它们的最大长度和是否尾部空格被保留等方面也不同。在存储或检索过程中不进行大小写转换。 CHAR列的长度固定为创建表是声明的长度,范围(0-255);而VARCHAR的值是可变长字符串范围(0-65535)。 #创建表t1 mysql> create table t1(c char(4),v varchar(4)); Query OK, 0 rows affected (0.09 sec) //向t1表中插入数据 mysql> insert into t1 values('ab ','ab '); Query OK, 1 row affected (0.01 sec) mysql> select * from t1; +------+------+ | c | v | +------+------+ | ab | ab | +------+------+ 1 row in set (0.00 sec) //检索的时候char会去掉空格 mysql> select length(c),length(v) from t1; +-----------+-----------+ | length(c) | length(v) | +-----------+-----------+ | 2 | 4 | +-----------+-----------+ 1 row in set (0.00 sec) //给查询结果加上一个符号会看的更清晰，char在检索的时候会去掉空格 mysql> select concat(c,'+'),concat(v,'+') from t1; +---------------+---------------+ | concat(c,'+') | concat(v,'+') | +---------------+---------------+ | ab+ | ab + | +---------------+---------------+ 1 row in set (0.01 sec) char存储的时候是定长，例如，char(5)，但是只插入了一个字符a，则会在字符a后边补全4个空格 varchar存储的时候是变长，例如，varchar(5)，但是只插入了一个字符a，则存储a后还会在a的位置后边加一个a的位置编号 2.3 日期和时间类型 类型 大小 (字节) 范围 格式 用途 DATE 3 1000-01-01/9999-12-31 YYYY-MM-DD 年月日 TIME 3 '-838:59:59'/'838:59:59' HH:MM:SS 时分秒 YEAR 1 1901/2155 YYYY 年份值 DATETIME 8 1000-01-01 00:00:00/9999-12-31 23:59:59 YYYY-MM-DD HH:MM:SS 年月日时分秒 TIMESTAMP 4 1970-01-01 00:00:00/2038结束时间是第 2147483647 秒，北京时间 2038-1-19 11:14:07，格林尼治时间 2038年1月19日 凌晨 03:14:07 YYYYMMDD HHMMSS 混合日期和时间值，时间戳 ⚠️⚠️⚠️IMESTAM只支持到2038年的时间，因此用的比较少 2.3.1 DATE、TIME、DATETIME使用示例 #创建表t2，3个列数据类型依次为date、time、datetime mysql> create table t2(d date,t time,dt datetime); Query OK, 0 rows affected (0.02 sec) //向t2表中插入数据，这里使用now()函数 mysql> insert into t2 values(now(),now(),now()); Query OK, 1 row affected, 1 warning (0.00 sec) //查看t2表中的数据 列d的数据类型是date 年月日 列t的数据类型是time 时分秒 列dt的数据类型是datetime 年月日时分秒 mysql> select * from t2; +------------+----------+---------------------+ | d | t | dt | +------------+----------+---------------------+ | 2018-10-28 | 17:32:33 | 2018-10-28 17:32:33 | +------------+----------+---------------------+ 1 row in set (0.00 sec) //向t2表中插入数据，这里手动指定，有两种方法 方法一 mysql> insert into t2 values('2018-08-08','08:08:08','2016-06-06 06:06:06'); Query OK, 1 row affected (0.01 sec) 方法二 mysql> insert into t2 values('20180808','080808','20160606060606'); Query OK, 1 row affected (0.00 sec) mysql> select * from t2; +------------+----------+---------------------+ | d | t | dt | +------------+----------+---------------------+ | 2018-08-08 | 08:08:08 | 2016-06-06 06:06:06 | | 2018-08-08 | 08:08:08 | 2016-06-06 06:06:06 | +------------+----------+---------------------+ 2 rows in set (0.00 sec) ⚠️只有TIMESTAMP类型插入数据为空时会自动添加当前时间，其余数据类型插入为空就是空 mysql> insert into t2 values(null,null,null); Query OK, 1 row affected (0.04 sec) mysql> select * from t2; +------------+----------+---------------------+ | d | t | dt | +------------+----------+---------------------+ | 2018-10-28 | 17:32:33 | 2018-10-28 17:32:33 | | NULL | NULL | NULL | +------------+----------+---------------------+ 2 rows in set (0.00 sec) 2.3.2 YEAR使用示例 #创建t3表，列y的类型为year mysql> create table t3(y year); Query OK, 0 rows affected (0.02 sec) //插入数据，这里使用now()方法 mysql> insert into t3 values(now()); Query OK, 1 row affected (0.01 sec) //插入数据，这里手动指定 mysql> insert into t3 values('2020'); Query OK, 1 row affected (0.00 sec) mysql> select * from t3; +------+ | y | +------+ | 2019 | | 2020 | +------+ 2 rows in set (0.00 sec) 2.3.3 TIMESTAMP使用示例 #创建t1表 mysql> create table t1(t timestamp); Query OK, 0 rows affected (0.03 sec) #描述t1表，可以看到timestamp有默认的CURRENT_TIMESTAMP和on update CURRENT_TIMESTAMP，即更新当前的时间 mysql> desc t1; +-------+-----------+------+-----+-------------------+-----------------------------+ | Field | Type | Null | Key | Default | Extra | +-------+-----------+------+-----+-------------------+-----------------------------+ | t | timestamp | NO | | CURRENT_TIMESTAMP | on update CURRENT_TIMESTAMP | +-------+-----------+------+-----+-------------------+-----------------------------+ 1 row in set (0.00 sec) //向t1表中插入数据,使用now()函数 mysql> insert into t1 values(now()); Query OK, 1 row affected (0.01 sec) //向t1表中插入空，timestamp会自动补全当前时间 mysql> insert into t1 values(null); Query OK, 1 row affected (0.00 sec) mysql> select * from t1; +---------------------+ | t | +---------------------+ | 2018-10-28 17:54:32 | | 2018-10-28 17:56:07 | +---------------------+ 2 rows in set (0.00 sec) timestamp时间上限及下限 mysql> desc t1; +-------+-----------+------+-----+-------------------+-----------------------------+ | Field | Type | Null | Key | Default | Extra | +-------+-----------+------+-----+-------------------+-----------------------------+ | t | timestamp | NO | | CURRENT_TIMESTAMP | on update CURRENT_TIMESTAMP | +-------+-----------+------+-----+-------------------+-----------------------------+ 1 row in set (0.00 sec) //timestamp时间上限是2038-01-19 11:14:07 mysql> insert into t1 values('2038-01-19 11:14:08'); ERROR 1292 (22007): Incorrect datetime value: '2038-01-19 11:14:08' for column 't' at row 1 mysql> insert into t1 values('2038-01-19 11:14:07'); Query OK, 1 row affected (0.01 sec) //timestamp时间下限是1970-01-01 08:00:01 mysql> insert into t1 values('1970-01-01 08:00:00'); ERROR 1292 (22007): Incorrect datetime value: '1970-01-01 08:00:00' for column 't' at row 1 mysql> insert into t1 values('1970-01-01 08:00:01'); Query OK, 1 row affected (0.00 sec) 2.4 ENUM和SET类型 类型 大小 用途 ENUM 对1-255个成员的枚举需要1个字节存储;对于255-65535个成员，需要2个字节存储;最多允许65535个成员。 单选：选择性别 SET 1-8个成员的集合，占1个字节9-16个成员的集合，占2个字节17-24个成员的集合，占3个字节25-32个成员的集合，占4个字节33-64个成员的集合，占8个字节 多选：兴趣爱好 2.4.1 ENUM(枚举)使用示例 ENUM中文名称叫枚举类型，它的值范围需要在创建表时通过枚举方式显示。ENUM只允许从值集合中选取单个值，而不能一次取多个值 //创建t1表，性别设置为ENUM类型 mysql> create table t1(id int,name char(10),sex enum('F','M')); Query OK, 0 rows affected (0.02 sec) //向t1表中插入数据，性别是在ENUM中定义的就可插入成功 mysql> insert into t1 values(1,'小明','M'); Query OK, 1 row affected (0.00 sec) mysql> insert into t1 values(1,'小红','F'); Query OK, 1 row affected (0.01 sec) //性别插入ENUM中没有定义的就会报错 mysql> insert into t1 values(1,'小红','A'); ERROR 1265 (01000): Data truncated for column 'sex' at row 1 2.4.2 SET(集合)使用示例 SET和ENUM非常相似，也是一个字符串对象，里面可以包含0-64个成员。根据成员的不同，存储上也有所不同。set类型可以允许值集合中任意选择1或多个元素进行组合。对超出范围的内容将不允许注入，而对重复的值将进行自动去重。 //创建表t1，爱好列设置为SET类型 mysql> create table t1(id int,name char(10),hobby set('篮球','足球','看电影')); Query OK, 0 rows affected (0.02 sec) mysql> insert into t1 values(1,'小明','篮球'); Query OK, 1 row affected (0.01 sec) mysql> insert into t1 values(2,'小洲','篮球,足球'); Query OK, 1 row affected (0.00 sec) mysql> insert into t1 values(3,'小丽','篮球,足球,篮球,看电影,足球'); Query OK, 1 row affected (0.00 sec) //查询结果可以看到SET可以只选择一个爱好，选择多个爱好，并且可以去重 mysql> select * from t1; +------+--------+-------------------------+ | id | name | hobby | +------+--------+-------------------------+ | 1 | 小明 | 篮球 | | 2 | 小洲 | 篮球,足球 | | 3 | 小丽 | 篮球,足球,看电影 | +------+--------+-------------------------+ 3 rows in set (0.00 sec) 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql基础/11.mysql中表与表之间的关系.html":{"url":"db/mysql/mysql基础/11.mysql中表与表之间的关系.html","title":"6.mysql中表与表之间的关系","keywords":"","body":"[toc] mysql中表与表之间的关系 1.如何分析表与表之间的关系 分析步骤： #1、先站在左表的角度去找 是否左表的多条记录可以对应右表的一条记录，如果是，则证明左表的一个字段foreign key 右表一个字段（通常是id） #2、再站在右表的角度去找 是否右表的多条记录可以对应左表的一条记录，如果是，则证明右表的一个字段foreign key 左表一个字段（通常是id） #3、总结： #多对一： 如果只有步骤1成立，则是左表多对一右表 如果只有步骤2成立，则是右表多对一左表 #多对多 如果步骤1和2同时成立，则证明这两张表时一个双向的多对一，即多对多,需要定义一个这两张表的关系表来专门存放二者的关系 #一对一: 如果1和2都不成立，而是左表的一条记录唯一对应右表的一条记录，反之亦然。这种情况很简单，就是在左表foreign key右表的基础上，将左表的外键字段设置成unique即可 2.mysql中表与表之间的关系 2.1 一对多 关联方式 外键 foreign key 表与表之间的关系为一对多 例如，出版社与书之间的关系就是一对多，一个出版社可以出版多个书 例如，班级表和学生表，一个班级可以有多个学生，但是一个学生只能属于一个班级 例如，服务器和机房，一个机房可以有多台服务器，但是一个服务器只能属于一个机房 2.1.1示例1，出版社表与图书表 一个出版社可以出版多个书 #创建出版社表 mysql> create table chubanshe(id int primary key auto_increment,name char(10)); Query OK, 0 rows affected (0.02 sec) #创建图书表 mysql> create table book( id int primary key auto_increment, name char(20), chubanshe_id int not null, foreign key(chubanshe_id) references chubanshe(id) on delete cascade on update cascade); Query OK, 0 rows affected (0.04 sec) //向出版社表中插入数据 mysql> insert into chubanshe(name) values ('人民出版社'), ('邮电出版社'), ('机械出版社'); Query OK, 3 rows affected (0.01 sec) Records: 3 Duplicates: 0 Warnings: 0 mysql> select * from chubanshe; +----+-----------------+ | id | name | +----+-----------------+ | 1 | 人民出版社 | | 2 | 邮电出版社 | | 3 | 机械出版社 | +----+-----------------+ 3 rows in set (0.00 sec) //向图书表中插入数据 mysql> insert into book(name,chubanshe_id) values ('童话故事',1), ('阿童木',2), ('老人与海',1), ('会飞的鸟',3), ('葵花宝典',2); Query OK, 5 rows affected (0.00 sec) Records: 5 Duplicates: 0 Warnings: 0 mysql> select * from book; +----+--------------+--------------+ | id | name | chubanshe_id | +----+--------------+--------------+ | 1 | 童话故事 | 1 | | 2 | 阿童木 | 2 | | 3 | 老人与海 | 1 | | 4 | 会飞的鸟 | 3 | | 5 | 葵花宝典 | 2 | +----+--------------+--------------+ 5 rows in set (0.00 sec) 现在出版社表与图书表就是一对多关系，图书表中的chubanshe_id对应出版社表中的出版社id，一个出版社可以出版多个书 2.1.2示例2，学生表和班级表 一个班级可以有多个学生，但是一个学生只能属于一个班级 #创建学生表 mysql> create table student( sid int primary key auto_increment, sname char(10) not null, class_id int not null, foreign key(class_id) references class(cid)); Query OK, 0 rows affected (0.02 sec) #创建班级表 mysql> create table class(cid int auto_increment primary key); Query OK, 0 rows affected (0.04 sec) //向学生表中插入数据 mysql> insert into student(sname,class_id) values('小明',1),('小红',1),('小洲',2),('小肖',3),('小丽',3); Query OK, 5 rows affected (0.00 sec) Records: 5 Duplicates: 0 Warnings: 0 mysql> select * from student; +-----+--------+----------+ | sid | sname | class_id | +-----+--------+----------+ | 1 | 小明 | 1 | | 2 | 小红 | 1 | | 3 | 小洲 | 2 | | 4 | 小肖 | 3 | | 5 | 小丽 | 3 | +-----+--------+----------+ 5 rows in set (0.00 sec) //向班级表中插入数据，因为只有一个cid，自动增长 mysql> insert into class values(); Query OK, 1 row affected (0.01 sec) mysql> select * from class; +-----+ | cid | +-----+ | 1 | | 2 | | 3 | +-----+ 3 rows in set (0.00 sec) //错误示例，向学生表中插入一条数据，班级id指定一个不存在的，会报错 mysql> insert into student(sname,class_id) values('小黑',4); ERROR 1452 (23000): Cannot add or update a child row: a foreign key constraint fails (`ppp`.`student`, CONSTRAINT `student_ibfk_1` FOREIGN KEY (`class_id`) REFERENCES `class` (`cid`)) //错误示例，尝试删除班级表中的一个记录，报错，不可以删除，因为学生表中有对应的班级，这个无法删除 mysql> delete from class where cid=1; ERROR 1451 (23000): Cannot delete or update a parent row: a foreign key constraint fails (`ppp`.`student`, CONSTRAINT `student_ibfk_1` FOREIGN KEY (`class_id`) REFERENCES `class` (`cid`)) 2.1.3示例3，机房表与服务器表 一个机房可以有多个服务器，但是一个服务器只能归属一个机房 #创建机房表 mysql> create table server_room( rid int primary key auto_increment, rname char(10) not null); Query OK, 0 rows affected (0.03 sec) #创建服务器表 mysql> create table server( sid int primary key auto_increment, sname char(10) not null,room_id int not null, foreign key(room_id) references server_room(rid)); Query OK, 0 rows affected (0.02 sec) //向机房表中插入数据 mysql> insert into server_room(rname) values('房山机房'),('石景山机房'),('丰台 机房'); Query OK, 3 rows affected (0.00 sec) Records: 3 Duplicates: 0 Warnings: 0 mysql> select * from server_room; +-----+-----------------+ | rid | rname | +-----+-----------------+ | 1 | 房山机房 | | 2 | 石景山机房 | | 3 | 丰台机房 | +-----+-----------------+ 3 rows in set (0.00 sec) //向服务器表中插入数据 mysql> insert into server(sname,room_id) values('HP',1),('DELL',1),('联想',2),('IBM',3),('华为',3); Query OK, 5 rows affected (0.00 sec) Records: 5 Duplicates: 0 Warnings: 0 mysql> select * from server; +-----+--------+---------+ | sid | sname | room_id | +-----+--------+---------+ | 1 | HP | 1 | | 2 | DELL | 1 | | 3 | 联想 | 2 | | 4 | IBM | 3 | | 5 | 华为 | 3 | +-----+--------+---------+ 5 rows in set (0.00 sec) //错误示例，尝试修改机房表中的机房编号，结果报错，因为服务器表中有对应编号为3的丰台机房的服务器 mysql> select * from server_room; +-----+-----------------+ | rid | rname | +-----+-----------------+ | 1 | 房山机房 | | 2 | 石景山机房 | | 3 | 丰台机房 | +-----+-----------------+ 3 rows in set (0.00 sec) mysql> update server_room set rid=5 where rid=3; ERROR 1451 (23000): Cannot delete or update a parent row: a foreign key constraint fails (`ppp`.`server`, CONSTRAINT `server_ibfk_1` FOREIGN KEY (`room_id`) REFERENCES `server_room` (`rid`)) 2.2 一对一 关联方式 foreign key+unique 表与表之间的关系是一对一 例如，公司中员工与员工的企业邮箱就是一对一关系 2.2.1示例1，员工表与企业邮箱表 #创建员工表 mysql> create table employee( eid int primary key auto_increment, #员工id，主键，自增 ename char(20) not null, #员工姓名 sex enum('F','M'), #员工性别 enterprise_mail_id int unique); #企业邮箱id，不能重复 Query OK, 0 rows affected (0.03 sec) #创建企业邮箱表 mysql> create table enterprise_mail( mid int primary key, #企业邮箱id，主键 email char(50), #邮箱信息 employee_id int unique not null, #员工id，不能重复 foreign key(employee_id) references employee(eid)); #企业邮箱表中的员工id是外键，关联员工表中的员工id Query OK, 0 rows affected (0.03 sec) //向员工表中插入数据 mysql> insert into employee(ename,sex,enterprise_mail_id) values -> ('小明','M',101), -> ('小洲','M',102), -> ('小颖','F',103); Query OK, 3 rows affected (0.00 sec) Records: 3 Duplicates: 0 Warnings: 0 mysql> select * from employee; +-----+--------+------+--------------------+ | eid | ename | sex | enterprise_mail_id | +-----+--------+------+--------------------+ | 1 | 小明 | M | 101 | | 2 | 小洲 | M | 102 | | 3 | 小颖 | F | 103 | +-----+--------+------+--------------------+ 3 rows in set (0.00 sec) //向企业邮箱表中插入数据 mysql> insert into enterprise_mail values (101,'xiaoming@testin.cn',1), (102,'xiaozhou@testin.cn',2), (103,'xiaoying@testin.cn',3); Query OK, 3 rows affected (0.01 sec) Records: 3 Duplicates: 0 Warnings: 0 mysql> select * from enterprise_mail; +-----+--------------------+-------------+ | mid | email | employee_id | +-----+--------------------+-------------+ | 101 | xiaoming@testin.cn | 1 | | 102 | xiaozhou@testin.cn | 2 | | 103 | xiaoying@testin.cn | 3 | +-----+--------------------+-------------+ 3 rows in set (0.00 sec) //尝试删除员工表中的任意一条数据，因为有外键约束，因此无法删除 mysql> delete from employee where eid=1; ERROR 1451 (23000): Cannot delete or update a parent row: a foreign key constraint fails (`db1`.`enterprise_mail`, CONSTRAINT `enterprise_mail_ibfk_1` FOREIGN KEY (`employee_id`) REFERENCES `employee` (`eid`)) //尝试删除企业邮箱表中的任意一条数据 mysql> delete from enterprise_mail where mid=101; Query OK, 1 row affected (0.01 sec) 2.3 多对多 关联方式 外键foreign key+一张新的表 表与表之间的关系是多对多 例如，一本书可以有多个作者，一个作者可以写多本书 2.3.1示例1，书籍表、作者表、关联书籍表与作者表的第3张表 #创建书籍表 mysql> create table book( bid int primary key auto_increment, bname char(20) not null); Query OK, 0 rows affected (0.02 sec) #创建作者表 mysql> create table author( aid int primary key auto_increment, aname char(20) not null); Query OK, 0 rows affected (0.03 sec) #创建关联书籍与作者表，表中有书籍id和作者id，分别作为书籍表中bid和作者表中aid的外键 mysql> create table book_and_author( book_id int not null, author_id int not null, foreign key(book_id) references book(bid), foreign key(author_id) references author(aid)); Query OK, 0 rows affected (0.03 sec) //向书籍表中插入数据 mysql> insert into book(bname) values('童话故事'),('那个女孩'),('上课必备三件套'); Query OK, 3 rows affected (0.00 sec) Records: 3 Duplicates: 0 Warnings: 0 mysql> select * from book; +-----+-----------------------+ | bid | bname | +-----+-----------------------+ | 1 | 童话故事 | | 2 | 那个女孩 | | 3 | 上课必备三件套 | +-----+-----------------------+ 3 rows in set (0.00 sec) //向作者表中插入数据 mysql> insert into author(aname) values('作者小明'),('作者小丽'),('作者小洲'); Query OK, 3 rows affected (0.01 sec) Records: 3 Duplicates: 0 Warnings: 0 mysql> select * from author; +-----+--------------+ | aid | aname | +-----+--------------+ | 1 | 作者小明 | | 2 | 作者小丽 | | 3 | 作者小洲 | +-----+--------------+ 3 rows in set (0.00 sec) //向关联表中插入数据 mysql> insert into book_and_author values(1,1),(1,2),(2,2),(2,3),(3,1),(3,3); Query OK, 6 rows affected (0.00 sec) Records: 6 Duplicates: 0 Warnings: 0 mysql> select * from book_and_author; +---------+-----------+ | book_id | author_id | +---------+-----------+ | 1 | 1 | #童话故事的作者是小明 | 1 | 2 | #童话故事的作者是小丽 | 2 | 2 | #那个女孩的作者是小丽 | 2 | 3 | #那个女孩的作者是小洲 | 3 | 1 | #上课必备三件套的作者是小明 | 3 | 3 | #上课必备三件套的作者是小洲 +---------+-----------+ 6 rows in set (0.00 sec) 这个关联书籍表与作者表的关联表中就是一个作者写了多本书，一本书有多个作者 //错误示范，尝试删除书籍表与作者表中的任意一条数据 mysql> delete from book where bid=1; ERROR 1451 (23000): Cannot delete or update a parent row: a foreign key constraint fails (`db1`.`book_and_author`, CONSTRAINT `book_and_author_ibfk_1` FOREIGN KEY (`book_id`) REFERENCES `book` (`bid`)) mysql> delete from author where aid=3; ERROR 1451 (23000): Cannot delete or update a parent row: a foreign key constraint fails (`db1`.`book_and_author`, CONSTRAINT `book_and_author_ibfk_2` FOREIGN KEY (`author_id`) REFERENCES `author` (`aid`)) //删除关联表中的数据，是可以删除的，因为这只是一个关联表，删除了关联id，只是把原来两张表的关联关系删除了 这里删除了关联表中的图书id，这样就是原来的图书表中为1的书找不到对应的作者了 mysql> delete from book_and_author where book_id=1; Query OK, 2 rows affected (0.01 sec) mysql> select * from book_and_author; +---------+-----------+ | book_id | author_id | +---------+-----------+ | 2 | 2 | | 2 | 3 | | 3 | 1 | | 3 | 3 | +---------+-----------+ 4 rows in set (0.00 sec) //删除关联id后就可以删除原先不能删除的图书或者作者id了 mysql> delete from book where bid=1; Query OK, 1 row affected (0.00 sec) //关联id为2的没有删除，因此不能删除图书表中bid为2的 mysql> delete from book where bid=2; ERROR 1451 (23000): Cannot delete or update a parent row: a foreign key constraint fails (`db1`.`book_and_author`, CONSTRAINT `book_and_author_ibfk_1` FOREIGN KEY (`book_id`) REFERENCES `book` (`bid`)) 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql基础/12.mysql关键字和sql语句执行顺序.html":{"url":"db/mysql/mysql基础/12.mysql关键字和sql语句执行顺序.html","title":"7.mysql关键字和sql语句执行顺序","keywords":"","body":"[toc] mysql关键字和sql语句执行顺序 1.mysql关键字 关键字 含义 not null 非空 primary key 主键(唯一且非空) foreign key 外键 unique 唯一键 auto_increment 自增(此列必须是主键或者唯一键) default 默认值 unsigned 非负数 comment 注释说明 distinct 去重 limit 限制 having 过滤 group by 分组 order by 排序(默认升序，加desc降序) like where条件中使用，与%配合使用，表示模糊匹配 in where条件中使用，查询范围内的数据 2.sql语句执行顺序 单表查询语句 语句 含义 select dictinct 字段名 去重，可以使用函数，四则运算，重命名 from 表名 as 别名 查询的时候临时修改表名 where 条件 条件可以用比较运算，逻辑运算，like，in group by 根据某个字段一致的项进行分组 having 过滤，可以使用聚合函数，在分组之后对数据 order by 字段 排序，默认升序，desc降序 limit m,n 从m+1开始取n条，m默认为0 执行顺序 1.from 2.where 3.group by 4.having 5.select 6.distinct 7.order by 8.limit 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql基础/13.select高级用法.html":{"url":"db/mysql/mysql基础/13.select高级用法.html","title":"8.select高级用法","keywords":"","body":"[toc] select高级用法 1.多表连接查询 select 表1.列名,表2.列名 from 表1,表2 where 表1.列1=表2.列1 and 表1.列2=值 1.1 创建两张表 mysql> create table t1(id int,name char(20)); Query OK, 0 rows affected (0.02 sec) mysql> create table t2(id int,age tinyint); Query OK, 0 rows affected (0.02 sec) 1.2 向表中插入数据 mysql> insert into t1 values(1,'aaa'),(2,'bbb'),(3,'ccc'); Query OK, 3 rows affected (0.01 sec) Records: 3 Duplicates: 0 Warnings: 0 mysql> insert into t2 values(1,25),(2,26),(3,27); Query OK, 3 rows affected (0.00 sec) Records: 3 Duplicates: 0 Warnings: 0 1.3 查询t1、t2表中id为1的人的年龄 mysql> select t1.name,t2.age from t1,t2 where t1.id=t2.id and t1.id=1; +------+------+ | name | age | +------+------+ | aaa | 25 | +------+------+ 1 row in set (0.00 sec) 1.sql join连接 2.1 sql join连接示意图 下图展示了 LEFT JOIN、RIGHT JOIN、INNER JOIN、OUTER JOIN 相关的 7 种用法 2.2 sql数据准备 2.2.1 创建一个人名表和地址表 1.创建一个数据库 mysql> create database DB1 charset utf8 collate=utf8_general_ci; Query OK, 1 row affected (0.01 sec) 2.创建人名表 mysql> create table person(person_id int,name varchar(20)); Query OK, 0 rows affected (0.02 sec) 3.创建地址表 mysql> create table address(address_id int,person_id int,city varchar(20)); Query OK, 0 rows affected (0.02 sec) 2.2.2 向表中插入数据 1.向人名表中插入数据 mysql> insert into person values(1,'张三'),(2,'李四'),(3,'王五'),(4,'杨六'); Query OK, 4 rows affected (0.01 sec) Records: 4 Duplicates: 0 Warnings: 0 2.向地址表中插入数据 mysql> insert into address values(1,1,'北京'),(2,2,'上海'),(3,3,'广州'),(5,5,'杭州'); Query OK, 4 rows affected (0.01 sec) Records: 4 Duplicates: 0 Warnings: 0 2.2.3 查看表信息 //人名表 mysql> select * from person; +-----------+--------+ | person_id | name | +-----------+--------+ | 1 | 张三 | | 2 | 李四 | | 3 | 王五 | | 4 | 杨六 | +-----------+--------+ 4 rows in set (0.01 sec) //地址表 mysql> select * from address; +------------+-----------+--------+ | address_id | person_id | city | +------------+-----------+--------+ | 1 | 1 | 北京 | | 2 | 2 | 上海 | | 3 | 3 | 广州 | | 5 | 5 | 杭州 | +------------+-----------+--------+ 4 rows in set (0.00 sec) 2.3 sql join连接查询示例 2.3.1 传统连接与JOIN ON //使用DB1库 mysql> use DB1; Database changed //查询张三的地址 传统连接 mysql> select person.name,address.city from person,address where person.name='张三' and person.person_id=address.person_id; +--------+--------+ | name | city | +--------+--------+ | 张三 | 北京 | +--------+--------+ 1 row in set (0.01 sec) //查询张三的地址 JOIN ON连接 mysql> select person.name,address.city from person join address on(person.person_id=address.person_id) where person.name='张三'; +--------+--------+ | name | city | +--------+--------+ | 张三 | 北京 | +--------+--------+ 1 row in set (0.00 sec) ⚠️传统连接与JOIN ON查询的结果是一样的 2.3.2 自连接 NATURAL JOIN 自然连接:根据连接的两个表中的公共列为您创建隐式连接子句。公共列是两个表中名称相同的列。自然连接可以是内连接、左外连接或右外连接。默认情况下是内部连接。 自连接的表要有共同的列名字，person表和address表中都有列person_id //查询张三的地址 mysql> select person.name,address.city from person natural join address where person.name='张三'; +--------+--------+ | name | city | +--------+--------+ | 张三 | 北京 | +--------+--------+ 1 row in set (0.00 sec) 2.3.3 内连接 INNER JOIN INNER JOIN 关键字在表中存在至少一个匹配时返回行。 //person表中和address表中相同的id列 mysql> select * from person inner join address on person.person_id=address.address_id; +-----------+--------+------------+-----------+--------+ | person_id | name | address_id | person_id | city | +-----------+--------+------------+-----------+--------+ | 1 | 张三 | 1 | 1 | 北京 | | 2 | 李四 | 2 | 2 | 上海 | | 3 | 王五 | 3 | 3 | 广州 | +-----------+--------+------------+-----------+--------+ 3 rows in set (0.00 sec) 2.3.4 左外连接 LEFT JOIN ON 左外连接:从左表返回所有的行(表1),与正确的匹配行(表2)。当没有匹配时，右边的结果为NULL。 select 查询内容 from 左表 left join 右表 on 左表.列=右表.列 //人名表person为左表，地址表address为右表 左外连接 mysql> select * from person left join address on person.person_id=address.person_id; +-----------+--------+------------+-----------+--------+ | person_id | name | address_id | person_id | city | +-----------+--------+------------+-----------+--------+ | 1 | 张三 | 1 | 1 | 北京 | | 2 | 李四 | 2 | 2 | 上海 | | 3 | 王五 | 3 | 3 | 广州 | | 4 | 杨六 | NULL | NULL | NULL | +-----------+--------+------------+-----------+--------+ 4 rows in set (0.01 sec) //地址表address为左表，人名表person位右表 右外连接 mysql> select * from address right join person on person.person_id=address.person_id; +------------+-----------+--------+-----------+--------+ | address_id | person_id | city | person_id | name | +------------+-----------+--------+-----------+--------+ | 1 | 1 | 北京 | 1 | 张三 | | 2 | 2 | 上海 | 2 | 李四 | | 3 | 3 | 广州 | 3 | 王五 | | NULL | NULL | NULL | 4 | 杨六 | +------------+-----------+--------+-----------+--------+ 4 rows in set (0.00 sec) 2.3.5 右外连接 RIGHT JOIN ON 右外连接:返回右表(表2)中的所有行，以及左表(表1)中的匹配行。当没有匹配时，左边的结果为NULL。 select 查询内容 from 左表 right join 右表 on 右表.列=左表.列 //人名表person为左表，地址表address为右表 mysql> select * from person right join address on person.person_id=address.person_id; +-----------+--------+------------+-----------+--------+ | person_id | name | address_id | person_id | city | +-----------+--------+------------+-----------+--------+ | 1 | 张三 | 1 | 1 | 北京 | | 2 | 李四 | 2 | 2 | 上海 | | 3 | 王五 | 3 | 3 | 广州 | | NULL | NULL | 5 | 5 | 杭州 | +-----------+--------+------------+-----------+--------+ 4 rows in set (0.00 sec) //address表为左表，person表为右表 mysql> select * from address left join person on person.person_id=address.person_id; +------------+-----------+--------+-----------+--------+ | address_id | person_id | city | person_id | name | +------------+-----------+--------+-----------+--------+ | 1 | 1 | 北京 | 1 | 张三 | | 2 | 2 | 上海 | 2 | 李四 | | 3 | 3 | 广州 | 3 | 王五 | | 5 | 5 | 杭州 | NULL | NULL | +------------+-----------+--------+-----------+--------+ 4 rows in set (0.00 sec) 2.3.6 合并查询 UNION UNION 去重复并合并 UNION ALL 不去重 全交: 返回左表的所有行和右表的所有行，是左交和右交的联合。 注意，由于MySql中没有Full Join命令，所以我们通过把Left Join和Right Join的结果Union起来也是可以的： //查询城市为北京或上海的所有信息 mysql> select * from address where city='北京' or city='上海'; +------------+-----------+--------+ | address_id | person_id | city | +------------+-----------+--------+ | 1 | 1 | 北京 | | 2 | 2 | 上海 | +------------+-----------+--------+ 2 rows in set (0.00 sec) 或 mysql> select * from address where city in ('北京','上海'); +------------+-----------+--------+ | address_id | person_id | city | +------------+-----------+--------+ | 1 | 1 | 北京 | | 2 | 2 | 上海 | +------------+-----------+--------+ 2 rows in set (0.01 sec) //union合并查询效率最高 mysql> select * from address where city='北京' union select * from address where city='上海'; +------------+-----------+--------+ | address_id | person_id | city | +------------+-----------+--------+ | 1 | 1 | 北京 | | 2 | 2 | 上海 | +------------+-----------+--------+ 2 rows in set (0.00 sec) 或 mysql> select * from address where city='北京' union all select * from address where city='上海'; +------------+-----------+--------+ | address_id | person_id | city | +------------+-----------+--------+ | 1 | 1 | 北京 | | 2 | 2 | 上海 | +------------+-----------+--------+ 2 rows in set (0.00 sec) 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql基础/14.mysql字符集.html":{"url":"db/mysql/mysql基础/14.mysql字符集.html","title":"9.mysql字符集","keywords":"","body":"[toc] mysql字符集 1.字符集定义 字符集(charset)：是一个系统支持的所有抽象字符的集合。字符是各种文字和符号的总称， 包括各国家文字、标点符号、图形符号、数字等 2.mysql数据库的字符集 1.字符集 charset 2.校对规则 collation 3.mysql中常见的字符集 ASCII字符集 1.一共128个字符，包括空格、标点符号、数字、大小写字母和一些不可见字符 2.每个字符使用一个字节编码--一个字节是8位，一共有256个编码方式足以承担128个 ISO 8859-1字符集-latin1 1.一共256个字符，足以用一个字节标识。 2.共收录256个字符，是在ASCII字符集的基础上又扩充了128个西欧常用字符 GB2312字符集 1.收录了汉字以及拉丁字母、希腊字母、日文平假名及片假名字母、俄语西里尔字母 2.同时这种字符集又兼容ASCII字符集 3.如果该字符在ASCII字符集中，则采用1字节编码。否则采用2字节编码。 4.类似这种一个字符需要的字节数不同称为边长编码 GBK字符集 GBK字符集只是在收录字符范围上对GB2312字符集作了扩充，编码方式上兼容GB2312 utf8字符集 1.收录地球上能想到的所有字符，而且还在不断扩充。这种字符集兼容ASCII字符集，采用变长编码方式，编码一个字符需要使用1～4个字节 2.兼容ASCII 3.utf8只是Unicode字符集的一种编码方案，Unicode字符集可以采用utf8、utf16、utf32这几种编码方案，utf8使用1～4个字节编码一个字符，utf16使用2个或4个字节编码一个字符，utf32使用4个字节编码一个字符。 4.mysql常见校对规则 1.ci：大小写不敏感 2.cs或bin：大小写敏感 示例 create database DB1 charset utf8 collate=utf8_general_ci 5.查看字符集和校对规则 查看字符集(mysql5.7) mysql> show charset; +----------+---------------------------------+---------------------+--------+ | Charset | Description | Default collation | Maxlen | +----------+---------------------------------+---------------------+--------+ | big5 | Big5 Traditional Chinese | big5_chinese_ci | 2 | | dec8 | DEC West European | dec8_swedish_ci | 1 | | cp850 | DOS West European | cp850_general_ci | 1 | | hp8 | HP West European | hp8_english_ci | 1 | | koi8r | KOI8-R Relcom Russian | koi8r_general_ci | 1 | | latin1 | cp1252 West European | latin1_swedish_ci | 1 | | latin2 | ISO 8859-2 Central European | latin2_general_ci | 1 | | swe7 | 7bit Swedish | swe7_swedish_ci | 1 | | ascii | US ASCII | ascii_general_ci | 1 | | ujis | EUC-JP Japanese | ujis_japanese_ci | 3 | | sjis | Shift-JIS Japanese | sjis_japanese_ci | 2 | | hebrew | ISO 8859-8 Hebrew | hebrew_general_ci | 1 | | tis620 | TIS620 Thai | tis620_thai_ci | 1 | | euckr | EUC-KR Korean | euckr_korean_ci | 2 | | koi8u | KOI8-U Ukrainian | koi8u_general_ci | 1 | | gb2312 | GB2312 Simplified Chinese | gb2312_chinese_ci | 2 | | greek | ISO 8859-7 Greek | greek_general_ci | 1 | | cp1250 | Windows Central European | cp1250_general_ci | 1 | | gbk | GBK Simplified Chinese | gbk_chinese_ci | 2 | | latin5 | ISO 8859-9 Turkish | latin5_turkish_ci | 1 | | armscii8 | ARMSCII-8 Armenian | armscii8_general_ci | 1 | | utf8 | UTF-8 Unicode | utf8_general_ci | 3 | | ucs2 | UCS-2 Unicode | ucs2_general_ci | 2 | | cp866 | DOS Russian | cp866_general_ci | 1 | | keybcs2 | DOS Kamenicky Czech-Slovak | keybcs2_general_ci | 1 | | macce | Mac Central European | macce_general_ci | 1 | | macroman | Mac West European | macroman_general_ci | 1 | | cp852 | DOS Central European | cp852_general_ci | 1 | | latin7 | ISO 8859-13 Baltic | latin7_general_ci | 1 | | utf8mb4 | UTF-8 Unicode | utf8mb4_general_ci | 4 | | cp1251 | Windows Cyrillic | cp1251_general_ci | 1 | | utf16 | UTF-16 Unicode | utf16_general_ci | 4 | | utf16le | UTF-16LE Unicode | utf16le_general_ci | 4 | | cp1256 | Windows Arabic | cp1256_general_ci | 1 | | cp1257 | Windows Baltic | cp1257_general_ci | 1 | | utf32 | UTF-32 Unicode | utf32_general_ci | 4 | | binary | Binary pseudo charset | binary | 1 | | geostd8 | GEOSTD8 Georgian | geostd8_general_ci | 1 | | cp932 | SJIS for Windows Japanese | cp932_japanese_ci | 2 | | eucjpms | UJIS for Windows Japanese | eucjpms_japanese_ci | 3 | | gb18030 | China National Standard GB18030 | gb18030_chinese_ci | 4 | +----------+---------------------------------+---------------------+--------+ 41 rows in set (0.00 sec) 查看校对规则 mysql> show collation; +--------------------------+----------+-----+---------+----------+---------+ | Collation | Charset | Id | Default | Compiled | Sortlen | +--------------------------+----------+-----+---------+----------+---------+ | big5_chinese_ci | big5 | 1 | Yes | Yes | 1 | | big5_bin | big5 | 84 | | Yes | 1 | | dec8_swedish_ci | dec8 | 3 | Yes | Yes | 1 | | dec8_bin | dec8 | 69 | | Yes | 1 | | cp850_general_ci | cp850 | 4 | Yes | Yes | 1 | | cp850_bin | cp850 | 80 | | Yes | 1 | | hp8_english_ci | hp8 | 6 | Yes | Yes | 1 | | hp8_bin | hp8 | 72 | | Yes | 1 | | koi8r_general_ci | koi8r | 7 | Yes | Yes | 1 | | koi8r_bin | koi8r | 74 | | Yes | 1 | | latin1_german1_ci | latin1 | 5 | | Yes | 1 | | latin1_swedish_ci | latin1 | 8 | Yes | Yes | 1 | | latin1_danish_ci | latin1 | 15 | | Yes | 1 | | latin1_german2_ci | latin1 | 31 | | Yes | 2 | | latin1_bin | latin1 | 47 | | Yes | 1 | | latin1_general_ci | latin1 | 48 | | Yes | 1 | | latin1_general_cs | latin1 | 49 | | Yes | 1 | | latin1_spanish_ci | latin1 | 94 | | Yes | 1 | | latin2_czech_cs | latin2 | 2 | | Yes | 4 | | latin2_general_ci | latin2 | 9 | Yes | Yes | 1 | | latin2_hungarian_ci | latin2 | 21 | | Yes | 1 | | latin2_croatian_ci | latin2 | 27 | | Yes | 1 | | latin2_bin | latin2 | 77 | | Yes | 1 | | swe7_swedish_ci | swe7 | 10 | Yes | Yes | 1 | | swe7_bin | swe7 | 82 | | Yes | 1 | | ascii_general_ci | ascii | 11 | Yes | Yes | 1 | | ascii_bin | ascii | 65 | | Yes | 1 | | ujis_japanese_ci | ujis | 12 | Yes | Yes | 1 | | ujis_bin | ujis | 91 | | Yes | 1 | | sjis_japanese_ci | sjis | 13 | Yes | Yes | 1 | | sjis_bin | sjis | 88 | | Yes | 1 | | hebrew_general_ci | hebrew | 16 | Yes | Yes | 1 | | hebrew_bin | hebrew | 71 | | Yes | 1 | | tis620_thai_ci | tis620 | 18 | Yes | Yes | 4 | | tis620_bin | tis620 | 89 | | Yes | 1 | | euckr_korean_ci | euckr | 19 | Yes | Yes | 1 | | euckr_bin | euckr | 85 | | Yes | 1 | | koi8u_general_ci | koi8u | 22 | Yes | Yes | 1 | | koi8u_bin | koi8u | 75 | | Yes | 1 | | gb2312_chinese_ci | gb2312 | 24 | Yes | Yes | 1 | | gb2312_bin | gb2312 | 86 | | Yes | 1 | | greek_general_ci | greek | 25 | Yes | Yes | 1 | | greek_bin | greek | 70 | | Yes | 1 | | cp1250_general_ci | cp1250 | 26 | Yes | Yes | 1 | | cp1250_czech_cs | cp1250 | 34 | | Yes | 2 | | cp1250_croatian_ci | cp1250 | 44 | | Yes | 1 | | cp1250_bin | cp1250 | 66 | | Yes | 1 | | cp1250_polish_ci | cp1250 | 99 | | Yes | 1 | | gbk_chinese_ci | gbk | 28 | Yes | Yes | 1 | | gbk_bin | gbk | 87 | | Yes | 1 | | latin5_turkish_ci | latin5 | 30 | Yes | Yes | 1 | | latin5_bin | latin5 | 78 | | Yes | 1 | | armscii8_general_ci | armscii8 | 32 | Yes | Yes | 1 | | armscii8_bin | armscii8 | 64 | | Yes | 1 | | utf8_general_ci | utf8 | 33 | Yes | Yes | 1 | | utf8_bin | utf8 | 83 | | Yes | 1 | | utf8_unicode_ci | utf8 | 192 | | Yes | 8 | | utf8_icelandic_ci | utf8 | 193 | | Yes | 8 | | utf8_latvian_ci | utf8 | 194 | | Yes | 8 | | utf8_romanian_ci | utf8 | 195 | | Yes | 8 | | utf8_slovenian_ci | utf8 | 196 | | Yes | 8 | | utf8_polish_ci | utf8 | 197 | | Yes | 8 | | utf8_estonian_ci | utf8 | 198 | | Yes | 8 | | utf8_spanish_ci | utf8 | 199 | | Yes | 8 | | utf8_swedish_ci | utf8 | 200 | | Yes | 8 | | utf8_turkish_ci | utf8 | 201 | | Yes | 8 | | utf8_czech_ci | utf8 | 202 | | Yes | 8 | | utf8_danish_ci | utf8 | 203 | | Yes | 8 | | utf8_lithuanian_ci | utf8 | 204 | | Yes | 8 | | utf8_slovak_ci | utf8 | 205 | | Yes | 8 | | utf8_spanish2_ci | utf8 | 206 | | Yes | 8 | | utf8_roman_ci | utf8 | 207 | | Yes | 8 | | utf8_persian_ci | utf8 | 208 | | Yes | 8 | | utf8_esperanto_ci | utf8 | 209 | | Yes | 8 | | utf8_hungarian_ci | utf8 | 210 | | Yes | 8 | | utf8_sinhala_ci | utf8 | 211 | | Yes | 8 | | utf8_german2_ci | utf8 | 212 | | Yes | 8 | | utf8_croatian_ci | utf8 | 213 | | Yes | 8 | | utf8_unicode_520_ci | utf8 | 214 | | Yes | 8 | | utf8_vietnamese_ci | utf8 | 215 | | Yes | 8 | | utf8_general_mysql500_ci | utf8 | 223 | | Yes | 1 | | ucs2_general_ci | ucs2 | 35 | Yes | Yes | 1 | | ucs2_bin | ucs2 | 90 | | Yes | 1 | | ucs2_unicode_ci | ucs2 | 128 | | Yes | 8 | | ucs2_icelandic_ci | ucs2 | 129 | | Yes | 8 | | ucs2_latvian_ci | ucs2 | 130 | | Yes | 8 | | ucs2_romanian_ci | ucs2 | 131 | | Yes | 8 | | ucs2_slovenian_ci | ucs2 | 132 | | Yes | 8 | | ucs2_polish_ci | ucs2 | 133 | | Yes | 8 | | ucs2_estonian_ci | ucs2 | 134 | | Yes | 8 | | ucs2_spanish_ci | ucs2 | 135 | | Yes | 8 | | ucs2_swedish_ci | ucs2 | 136 | | Yes | 8 | | ucs2_turkish_ci | ucs2 | 137 | | Yes | 8 | | ucs2_czech_ci | ucs2 | 138 | | Yes | 8 | | ucs2_danish_ci | ucs2 | 139 | | Yes | 8 | | ucs2_lithuanian_ci | ucs2 | 140 | | Yes | 8 | | ucs2_slovak_ci | ucs2 | 141 | | Yes | 8 | | ucs2_spanish2_ci | ucs2 | 142 | | Yes | 8 | | ucs2_roman_ci | ucs2 | 143 | | Yes | 8 | | ucs2_persian_ci | ucs2 | 144 | | Yes | 8 | | ucs2_esperanto_ci | ucs2 | 145 | | Yes | 8 | | ucs2_hungarian_ci | ucs2 | 146 | | Yes | 8 | | ucs2_sinhala_ci | ucs2 | 147 | | Yes | 8 | | ucs2_german2_ci | ucs2 | 148 | | Yes | 8 | | ucs2_croatian_ci | ucs2 | 149 | | Yes | 8 | | ucs2_unicode_520_ci | ucs2 | 150 | | Yes | 8 | | ucs2_vietnamese_ci | ucs2 | 151 | | Yes | 8 | | ucs2_general_mysql500_ci | ucs2 | 159 | | Yes | 1 | | cp866_general_ci | cp866 | 36 | Yes | Yes | 1 | | cp866_bin | cp866 | 68 | | Yes | 1 | | keybcs2_general_ci | keybcs2 | 37 | Yes | Yes | 1 | | keybcs2_bin | keybcs2 | 73 | | Yes | 1 | | macce_general_ci | macce | 38 | Yes | Yes | 1 | | macce_bin | macce | 43 | | Yes | 1 | | macroman_general_ci | macroman | 39 | Yes | Yes | 1 | | macroman_bin | macroman | 53 | | Yes | 1 | | cp852_general_ci | cp852 | 40 | Yes | Yes | 1 | | cp852_bin | cp852 | 81 | | Yes | 1 | | latin7_estonian_cs | latin7 | 20 | | Yes | 1 | | latin7_general_ci | latin7 | 41 | Yes | Yes | 1 | | latin7_general_cs | latin7 | 42 | | Yes | 1 | | latin7_bin | latin7 | 79 | | Yes | 1 | | utf8mb4_general_ci | utf8mb4 | 45 | Yes | Yes | 1 | | utf8mb4_bin | utf8mb4 | 46 | | Yes | 1 | | utf8mb4_unicode_ci | utf8mb4 | 224 | | Yes | 8 | | utf8mb4_icelandic_ci | utf8mb4 | 225 | | Yes | 8 | | utf8mb4_latvian_ci | utf8mb4 | 226 | | Yes | 8 | | utf8mb4_romanian_ci | utf8mb4 | 227 | | Yes | 8 | | utf8mb4_slovenian_ci | utf8mb4 | 228 | | Yes | 8 | | utf8mb4_polish_ci | utf8mb4 | 229 | | Yes | 8 | | utf8mb4_estonian_ci | utf8mb4 | 230 | | Yes | 8 | | utf8mb4_spanish_ci | utf8mb4 | 231 | | Yes | 8 | | utf8mb4_swedish_ci | utf8mb4 | 232 | | Yes | 8 | | utf8mb4_turkish_ci | utf8mb4 | 233 | | Yes | 8 | | utf8mb4_czech_ci | utf8mb4 | 234 | | Yes | 8 | | utf8mb4_danish_ci | utf8mb4 | 235 | | Yes | 8 | | utf8mb4_lithuanian_ci | utf8mb4 | 236 | | Yes | 8 | | utf8mb4_slovak_ci | utf8mb4 | 237 | | Yes | 8 | | utf8mb4_spanish2_ci | utf8mb4 | 238 | | Yes | 8 | | utf8mb4_roman_ci | utf8mb4 | 239 | | Yes | 8 | | utf8mb4_persian_ci | utf8mb4 | 240 | | Yes | 8 | | utf8mb4_esperanto_ci | utf8mb4 | 241 | | Yes | 8 | | utf8mb4_hungarian_ci | utf8mb4 | 242 | | Yes | 8 | | utf8mb4_sinhala_ci | utf8mb4 | 243 | | Yes | 8 | | utf8mb4_german2_ci | utf8mb4 | 244 | | Yes | 8 | | utf8mb4_croatian_ci | utf8mb4 | 245 | | Yes | 8 | | utf8mb4_unicode_520_ci | utf8mb4 | 246 | | Yes | 8 | | utf8mb4_vietnamese_ci | utf8mb4 | 247 | | Yes | 8 | | cp1251_bulgarian_ci | cp1251 | 14 | | Yes | 1 | | cp1251_ukrainian_ci | cp1251 | 23 | | Yes | 1 | | cp1251_bin | cp1251 | 50 | | Yes | 1 | | cp1251_general_ci | cp1251 | 51 | Yes | Yes | 1 | | cp1251_general_cs | cp1251 | 52 | | Yes | 1 | | utf16_general_ci | utf16 | 54 | Yes | Yes | 1 | | utf16_bin | utf16 | 55 | | Yes | 1 | | utf16_unicode_ci | utf16 | 101 | | Yes | 8 | | utf16_icelandic_ci | utf16 | 102 | | Yes | 8 | | utf16_latvian_ci | utf16 | 103 | | Yes | 8 | | utf16_romanian_ci | utf16 | 104 | | Yes | 8 | | utf16_slovenian_ci | utf16 | 105 | | Yes | 8 | | utf16_polish_ci | utf16 | 106 | | Yes | 8 | | utf16_estonian_ci | utf16 | 107 | | Yes | 8 | | utf16_spanish_ci | utf16 | 108 | | Yes | 8 | | utf16_swedish_ci | utf16 | 109 | | Yes | 8 | | utf16_turkish_ci | utf16 | 110 | | Yes | 8 | | utf16_czech_ci | utf16 | 111 | | Yes | 8 | | utf16_danish_ci | utf16 | 112 | | Yes | 8 | | utf16_lithuanian_ci | utf16 | 113 | | Yes | 8 | | utf16_slovak_ci | utf16 | 114 | | Yes | 8 | | utf16_spanish2_ci | utf16 | 115 | | Yes | 8 | | utf16_roman_ci | utf16 | 116 | | Yes | 8 | | utf16_persian_ci | utf16 | 117 | | Yes | 8 | | utf16_esperanto_ci | utf16 | 118 | | Yes | 8 | | utf16_hungarian_ci | utf16 | 119 | | Yes | 8 | | utf16_sinhala_ci | utf16 | 120 | | Yes | 8 | | utf16_german2_ci | utf16 | 121 | | Yes | 8 | | utf16_croatian_ci | utf16 | 122 | | Yes | 8 | | utf16_unicode_520_ci | utf16 | 123 | | Yes | 8 | | utf16_vietnamese_ci | utf16 | 124 | | Yes | 8 | | utf16le_general_ci | utf16le | 56 | Yes | Yes | 1 | | utf16le_bin | utf16le | 62 | | Yes | 1 | | cp1256_general_ci | cp1256 | 57 | Yes | Yes | 1 | | cp1256_bin | cp1256 | 67 | | Yes | 1 | | cp1257_lithuanian_ci | cp1257 | 29 | | Yes | 1 | | cp1257_bin | cp1257 | 58 | | Yes | 1 | | cp1257_general_ci | cp1257 | 59 | Yes | Yes | 1 | | utf32_general_ci | utf32 | 60 | Yes | Yes | 1 | | utf32_bin | utf32 | 61 | | Yes | 1 | | utf32_unicode_ci | utf32 | 160 | | Yes | 8 | | utf32_icelandic_ci | utf32 | 161 | | Yes | 8 | | utf32_latvian_ci | utf32 | 162 | | Yes | 8 | | utf32_romanian_ci | utf32 | 163 | | Yes | 8 | | utf32_slovenian_ci | utf32 | 164 | | Yes | 8 | | utf32_polish_ci | utf32 | 165 | | Yes | 8 | | utf32_estonian_ci | utf32 | 166 | | Yes | 8 | | utf32_spanish_ci | utf32 | 167 | | Yes | 8 | | utf32_swedish_ci | utf32 | 168 | | Yes | 8 | | utf32_turkish_ci | utf32 | 169 | | Yes | 8 | | utf32_czech_ci | utf32 | 170 | | Yes | 8 | | utf32_danish_ci | utf32 | 171 | | Yes | 8 | | utf32_lithuanian_ci | utf32 | 172 | | Yes | 8 | | utf32_slovak_ci | utf32 | 173 | | Yes | 8 | | utf32_spanish2_ci | utf32 | 174 | | Yes | 8 | | utf32_roman_ci | utf32 | 175 | | Yes | 8 | | utf32_persian_ci | utf32 | 176 | | Yes | 8 | | utf32_esperanto_ci | utf32 | 177 | | Yes | 8 | | utf32_hungarian_ci | utf32 | 178 | | Yes | 8 | | utf32_sinhala_ci | utf32 | 179 | | Yes | 8 | | utf32_german2_ci | utf32 | 180 | | Yes | 8 | | utf32_croatian_ci | utf32 | 181 | | Yes | 8 | | utf32_unicode_520_ci | utf32 | 182 | | Yes | 8 | | utf32_vietnamese_ci | utf32 | 183 | | Yes | 8 | | binary | binary | 63 | Yes | Yes | 1 | | geostd8_general_ci | geostd8 | 92 | Yes | Yes | 1 | | geostd8_bin | geostd8 | 93 | | Yes | 1 | | cp932_japanese_ci | cp932 | 95 | Yes | Yes | 1 | | cp932_bin | cp932 | 96 | | Yes | 1 | | eucjpms_japanese_ci | eucjpms | 97 | Yes | Yes | 1 | | eucjpms_bin | eucjpms | 98 | | Yes | 1 | | gb18030_chinese_ci | gb18030 | 248 | Yes | Yes | 2 | | gb18030_bin | gb18030 | 249 | | Yes | 1 | | gb18030_unicode_520_ci | gb18030 | 250 | | Yes | 8 | +--------------------------+----------+-----+---------+----------+---------+ 222 rows in set (0.01 sec) 6.字符集设置 mysql有4个级别的字符集 1.服务器级别(操作系统级别) 2.数据库级别 3.表级别 4.列级别 6.1 服务器级别 1.操作系统级别 #centOS7 [root@hwyun ~]# cat /etc/locale.conf LANG=en_US.UTF-8 #centOS6 [root@test1 ~]# cat /etc/sysconfig/i18n LANG=\"en_US.UTF-8\" SYSFONT=\"latarcyrheb-sun16\" [root@test1 ~]# echo $LANG en_US.UTF-8 2.操作系统客户端级别 ssh连接工具设置 3.mysql实例级别 方法1：在编译安装时候就指定如下服务器端字符集。 cmake . -DDEFAULT_CHARSET=utf8 \\ -DDEFAULT_COLLATION=utf8_general_ci \\ -DWITH_EXTRA_CHARSETS=all \\ 方法2：在配置文件中设置字符集 [mysqld] character-set-server=utf8 6.2 数据库级别 mysql> create database 数据库名 charset utf8 default collate = utf8_general_ci; 6.3 表级别 mysql> CREATE TABLE `test` ( `id` int(4) NOT NULL AUTO_INCREMENT, `name` char(20) NOT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=13 DEFAULT CHARSET=utf8; 6.4 列级别 类型为CHAR、VARCHAR、TEXT的列，可以指定字符集/校验规则 create table t1(id int,address char(30) character set utf8); 7.生产环境更改数据库(含数据)字符集的方法 mysql> alter database 数据库名 CHARACTER SET utf8 collate utf8_general_ci; mysql> alter table t1 CHARACTER SET utf8; 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql基础/15.mysql日志.html":{"url":"db/mysql/mysql基础/15.mysql日志.html","title":"10.mysql日志","keywords":"","body":"[toc] mysql日志 1.错误日志 1.1 yum安装默认路径 /var/log/mysqld.log mysql> show variables like \"%log_error%\"; +---------------+---------------------+ | Variable_name | Value | +---------------+---------------------+ | log_error | /var/log/mysqld.log | +---------------+---------------------+ 1 row in set (0.01 sec) 1.2 二进制安装、编译安装路径 $MYSQL_HOME/data/主机名.err //编辑配置文件 [root@db01 ~]# vim /etc/my.cnf [mysqld] log_error=/usr/local/mysql/data/error.log //查看方式 mysql> show variables like 'log_error'; +---------------+---------------------------------+ | Variable_name | Value | +---------------+---------------------------------+ | log_error | /usr/local/mysql/data/error.log | +---------------+---------------------------------+ 1 row in set (0.00 sec) 2.二进制日志 2.1 含义 binlog日志，记录对mysql的所有更新的操作，插入的操作(增删改操作) 2.2 作用 恢复数据，MySQL AB复制，记录所有对数据库发生修改的操作 2.3 二进制日志模式 statement：语句模式，上图中将update语句进行记录（默认模式） 优点 简单明了，容易被看懂，就是sql语句，记录时不需要太多的磁盘空间 缺点 记录不够严谨 row：行模式，即数据行的变化过程，上图中Age=19修改成Age=20的过程事件。 mixed：以上两者的混合模式。 企业推荐使用row模式 优点 记录更加严谨 缺点 有可能会需要更多的磁盘空间，不太容易被读懂 2.4 开启方式 //开启二进制日志 [root@db01 data]# vim /etc/my.cnf [mysqld] log-bin=mysql-bin binlog_format=row //注意:在mysql5.7中开启binlog必须要加上server-id。 [root@db01 data]# vim /etc/my.cnf [mysqld] log-bin=mysql-bin binlog_format=row server_id=1 2.5 二进制日志的操作 //物理查看 [root@db01 data]# ll /usr/local/mysql/data/ -rw-rw---- 1 mysql mysql 285 Mar 6 2017 mysql-bin.000001 //命令行查看 mysql> show binary logs; +------------------+-----------+ | Log_name | File_size | +------------------+-----------+ | mysql-bin.000001 | 120 | +------------------+-----------+ 1 row in set (0.00 sec) mysql> show master status; +------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000001 | 120 | | | | +------------------+----------+--------------+------------------+-------------------+ 1 row in set (0.00 sec) //查看binlog事件 mysql> show binlog events in 'mysql-bin.000001'; +------------------+-----+-------------+-----------+-------------+---------------------------------------+ | Log_name | Pos | Event_type | Server_id | End_log_pos | Info | +------------------+-----+-------------+-----------+-------------+---------------------------------------+ | mysql-bin.000001 | 4 | Format_desc | 1 | 120 | Server ver: 5.6.40-log, Binlog ver: 4 | +------------------+-----+-------------+-----------+-------------+---------------------------------------+ 1 row in set (0.01 sec) 2.6 事件 2.6.1 事件含义 1.在binlog中最小的记录单元为event 2.一个事务会被拆分成多个事件（event） 2.6.2 事件特性 1.每个event都有一个开始位置（start position）和结束位置（stop position） 2.所谓的位置就是event对整个二进制的文件的相对位置 3.对于一个二进制日志中，前120个position是文件格式信息预留空间 4.MySQL第一个记录的事件，都是从120开始的 2.6.3 利用二进制日志恢复数据示例 1.修改mysql二进制日志为row模式 [root@db01 data]# vim /etc/my.cnf [mysqld] log-bin=mysql-bin binlog_format=row 2.查看二进制日志格式 mysql> show variables like 'binlog_format'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | binlog_format | ROW | +---------------+-------+ 1 row in set (0.01 sec) 3.查看binlog信息 mysql> show master status; +------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000001 | 120 | | | | +------------------+----------+--------------+------------------+-------------------+ 1 row in set (0.05 sec) 4.先刷新，然后再查看binlog信息，此时位置为120 mysql> flush logs; Query OK, 0 rows affected (0.01 sec) mysql> show master status; +------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000002 | 120 | | | | +------------------+----------+--------------+------------------+-------------------+ 1 row in set (0.00 sec) 5.创建一个binlog库 mysql> create database binlog; Query OK, 1 row affected (0.01 sec) 6.创建一个表binlog_table mysql> use binlog; Database changed mysql> create table binlog_table(id int); Query OK, 0 rows affected (0.03 sec) 7.创建库和表后查看binlog信息，此时binlog位置为331 mysql> show master status; +------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000003 | 331 | | | | +------------------+----------+--------------+------------------+-------------------+ 1 row in set (0.00 sec) 8.向表中插入一条数据并查看binlog信息，此时binlog位置为533 mysql> insert into binlog_table values(1); Query OK, 1 row affected (0.01 sec) mysql> show master status; +------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000003 | 533 | | | | +------------------+----------+--------------+------------------+-------------------+ 1 row in set (0.00 sec) 9.向表中插入2条数据并查看binlog信息，此时binlog位置为937 mysql> insert into binlog_table values(2); Query OK, 1 row affected (0.00 sec) mysql> insert into binlog_table values(3); Query OK, 1 row affected (0.01 sec) mysql> show master status; +------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000002 | 937 | | | | +------------------+----------+--------------+------------------+-------------------+ 1 row in set (0.00 sec) 10.删除一条数据、然后删除表、库 mysql> select * from binlog_table; +------+ | id | +------+ | 1 | | 2 | | 3 | +------+ 3 rows in set (0.00 sec) mysql> delete from binlog_table where id=1; Query OK, 1 row affected (0.02 sec) mysql> drop table binlog_table; Query OK, 0 rows affected (0.01 sec) mysql> drop database binlog; Query OK, 0 rows affected (0.01 sec) 11.开始查找要截取的段，这里要恢复删除的数据库 mysqlbinlog --base64-output=decode-rows -vvv /usr/local/mysql/data/mysql-bin.000002 [root@db02 data]# mysqlbinlog --base64-output=decode-rows -vvv /usr/local/mysql/data/mysql-bin.000002 12.开始恢复，恢复前一定要临时关闭二进制日志，否则二进制日志会多记录 #临时关闭二进制日志 mysql> set sql_log_bin=0; Query OK, 0 rows affected (0.01 sec) #截取并写入到一个文件中 [root@db02 data]# mysqlbinlog --start-position=120 --stop-position=1011 /usr/local/mysql/data/mysql-bin.000002 >/tmp/binlog.sql #开始恢复 [root@db02 data]# mysql -uroot -p show databases; +--------------------+ | Database | +--------------------+ | information_schema | | binlog | | mysql | | performance_schema | | test | +--------------------+ 5 rows in set (0.00 sec) mysql> use binlog; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed mysql> show tables; +------------------+ | Tables_in_binlog | +------------------+ | binlog_table | +------------------+ 1 row in set (0.00 sec) mysql> select * from binlog_table; +------+ | id | +------+ | 1 | | 2 | | 3 | +------+ 3 rows in set (0.00 sec) 3.通用查询日志 /var/run/mysqld/mysqld.log #对MySQL所有增删改查都会记录，默认关闭 mysql> show variables like \"%general%\"; +------------------+----------------------------+ | Variable_name | Value | +------------------+----------------------------+ | general_log | OFF | | general_log_file | /var/run/mysqld/mysqld.log | +------------------+----------------------------+ 2 rows in set (0.00 sec) #开启通用查询日志后才会有文件/var/run/mysqld/mysqld.log mysql> set global general_log=on; Query OK, 0 rows affected (0.00 sec) 4.慢查询日志 /var/run/mysqld/mysqld-slow.log #记录查询时间长的语句，用来优化，默认10s为慢查询 mysql> show variables like \"%slow%\"; +---------------------+---------------------------------+ | Variable_name | Value | +---------------------+---------------------------------+ | log_slow_queries | OFF | | slow_launch_time | 2 | | slow_query_log | OFF | | slow_query_log_file | /var/run/mysqld/mysqld-slow.log | +---------------------+---------------------------------+ 4 rows in set (0.00 sec) #设置慢查询日志存放路径 mysql>set global slow_query_log_file=路径 #开启慢查询 mysql> set global slow_query_log=on; Query OK, 0 rows affected (0.00 sec) #设置慢查询超时时间 mysql> set long_query_time=2; Query OK, 0 rows affected (0.00 sec) #模拟慢查询时间为5s mysql> select sleep(5); +----------+ | sleep(5) | +----------+ | 0 | +----------+ 1 row in set (5.00 sec) #此时查看MySQL慢查询日志就可以看到超时的语句 [root@mysql ~]# less /var/run/mysqld/mysqld-slow.log /usr/libexec/mysqld, Version: 5.1.73 (Source distribution). started with: Tcp port: 3306 Unix socket: /var/lib/mysql/mysql.sock Time Id Command Argument # Time: 171123 9:58:55 # User@Host: root[root] @ mysql [] # Query_time: 5.001586 Lock_time: 0.000000 Rows_sent: 1 Rows_examined: 0 use DB1; SET timestamp=1511402335; select sleep(5); 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql基础/16.mysql设置密码及忘记密码如何解决.html":{"url":"db/mysql/mysql基础/16.mysql设置密码及忘记密码如何解决.html","title":"11.mysql修改密码及忘记密码如何解决","keywords":"","body":"[toc] mysql设置密码及忘记密码如何解决 mysql5.6 方法一 mysqladmin 修改指定用户密码 mysqladmin -u用户名 -p旧密码 password 新密码 示例：使用mysqladmin命令给root设置密码 mysql5.7和mysql8中在命令行使用命令mysqladmin会有警告信息，mysql5.6没有 $ mysqladmin -uroot -p password Enter password: New password: Confirm new password: Warning: Since password will be sent to server in plain text, use ssl connection to ensure password safety. 方法二 set password 修改指定用户密码 set password for 用户名@localhost = password('新密码'); 默认修改root密码 set password='新密码' 或 set password=password('新密码'); 方法三 update更新user表 ⚠️mysql5.6中user表中密码的字段是password ⚠️mysql5.7和8中user表中密码的字段是authentication_string ⚠️mysql8中user表中密码的字段authentication_string后边直接跟字符串即可，不能写password mysql5.6 update mysql.user set password=password('新密码') where user='root' and host='localhost'; mysql5.7 update mysql.user set authentication_string=password('新密码') where user='root' and host='localhost'; mysql8 update mysql.user set authentication_string='新密码' where user='root' and host='localhost'; mysql5.7 方法一 mysqladmin 修改指定用户密码 mysqladmin -u用户名 -p旧密码 password 新密码 示例：使用mysqladmin命令给root设置密码 mysql5.7和mysql8中在命令行使用命令mysqladmin会有警告信息，mysql5.6没有 $ mysqladmin -uroot -p password Enter password: New password: Confirm new password: Warning: Since password will be sent to server in plain text, use ssl connection to ensure password safety. 方法二 set password 修改指定用户密码 set password for 用户名@localhost = password('新密码'); 默认修改root密码 set password='新密码' 或 set password=password('新密码'); 方法三 update更新user表 ⚠️mysql5.6中user表中密码的字段是password ⚠️mysql5.7和8中user表中密码的字段是authentication_string ⚠️mysql8中user表中密码的字段authentication_string后边直接跟字符串即可，不能写password mysql5.6 update mysql.user set password=password('新密码') where user='root' and host='localhost'; mysql5.7 update mysql.user set authentication_string=password('新密码') where user='root' and host='localhost'; mysql8 update mysql.user set authentication_string='新密码' where user='root' and host='localhost'; mysql8.0 方法一 mysqladmin 修改指定用户密码 mysqladmin -u用户名 -p旧密码 password 新密码 示例：使用mysqladmin命令给root设置密码 mysql5.7和mysql8中在命令行使用命令mysqladmin会有警告信息，mysql5.6没有 $ mysqladmin -uroot -p password Enter password: New password: Confirm new password: Warning: Since password will be sent to server in plain text, use ssl connection to ensure password safety. 方法二 set password ⚠️mysql8中以下两种方式不可用 set password=password('新密码'); set password for root@localhost = password('新密码'); 只能用如下方法 set password='新密码' 方法三 update更新user表 ⚠️mysql5.6中user表中密码的字段是password ⚠️mysql5.7和8中user表中密码的字段是authentication_string ⚠️mysql8中user表中密码的字段authentication_string后边直接跟字符串即可，不能写password mysql5.6 update mysql.user set password=password('新密码') where user='root' and host='localhost'; mysql5.7 update mysql.user set authentication_string=password('新密码') where user='root' and host='localhost'; mysql8 update mysql.user set authentication_string='新密码' where user='root' and host='localhost'; mysql忘记密码 第一步、停止mysql数据库 第二步、编辑mysql配置文件my.cnf，在[mysqld]下加参数skip-grant-tables 第三步、启动mysql数据库，进入数据库修改密码，修改完成之后把配置文件中的参数skip-grant-tables注释或者删除 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql基础/17.mysql逻辑备份 mysqldump.html":{"url":"db/mysql/mysql基础/17.mysql逻辑备份 mysqldump.html","title":"12.mysql逻辑备份 mysqldump","keywords":"","body":"[toc] mysql逻辑备份 mysqldump mysql5.7官方手册 mysql5.7备份和恢复官方文档 1.mysql备份 1.1 为什么要备份 ①防止数据丢失 ②恢复数据，误操作最多 1.2 数据的备份类型 ①完全备份-->备份整个数据库 ②部分备份-->备份部分数据库，例如只备份一个库、一张表 ③增量备份-->备份自上一次备份以来(增量或完全)以来变化的数据 特点: 节约空间、还原麻烦 ④差异备份-->备份自上一次完全备份以来变化的数据 特点: 浪费空间、还原比增量备份简单 1.3 备份方式 热备份 热备份指的是当数据库进行备份时, 数据库的读写操作均不是受影响 温备份 温备份指的是当数据库进行备份时, 数据库的读操作可以执行, 但是不能执行写操作 冷备份 冷备份指的是当数据库进行备份时, 数据库不能进行读写操作, 即数据库要下线 MySQL中进行不同方式的备份还要考虑存储引擎是否支持 MyISAM 热备 × 温备 √ 冷备 √ InnoDB 热备 √ 温备 √ 冷备 √ 1.4 备份命令 mysqldump（逻辑） mysql原生自带很好用的逻辑备份工具 mysqlbinlog（逻辑） 实现binlog备份的原生态命令 xtrabackup（物理） precona公司开发的性能很高的物理备份工具 1.5 mysql备份方式总结 备份方法 备份速度 恢复速度 便捷性 功能 一般用于 cp 快 快 一般、灵活性低 很弱 少量数据备份 mysqldump 慢 慢 一般、可无视存储引擎的差异 一般 中小型数据量的备份 lvm2快照 快 快 一般、支持几乎热备、速度快 一般 中小型数据量的备份 xtrabackup 较快 较快 实现innodb热备、对存储引擎有要求 强大 较大规模的备份 2.mysqldump备份 2.0 利用存储过程生成大量数据 #1.创建数据库 mysql> create database db1; Query OK, 1 row affected (0.00 sec) #2.创建表 mysql> use db1; Database changed mysql> create table db1_t1( id int, name varchar(20), gender char(6), email varchar(50), first_name char(10), last_name char(10) ); Query OK, 0 rows affected (0.01 sec) #3.创建存储过程 mysql> delimiter $$ #声明存储过程的结束符号为$$ mysql> create procedure auto_insert1() BEGIN declare i int default 1; while(i delimiter ; #重新声明分号为结束符号，注意有空格 #4.查看存储过程 show create procedure auto_insert1\\G #5.调用存储过程 call auto_insert1(); #6.查看数据 mysql> select count(*) from s1; +----------+ | count(*) | +----------+ | 100000 | +----------+ 1 row in set (0.04 sec) mysql> select * from s1 limit 10; +------+--------+--------+-------------+------------+-----------+ | id | name | gender | email | first_name | last_name | +------+--------+--------+-------------+------------+-----------+ | 1 | xboyww | man | xboyww1@qq | a1 | b1 | | 2 | xboyww | man | xboyww2@qq | a2 | b2 | | 3 | xboyww | man | xboyww3@qq | a3 | b3 | | 4 | xboyww | man | xboyww4@qq | a4 | b4 | | 5 | xboyww | man | xboyww5@qq | a5 | b5 | | 6 | xboyww | man | xboyww6@qq | a6 | b6 | | 7 | xboyww | man | xboyww7@qq | a7 | b7 | | 8 | xboyww | man | xboyww8@qq | a8 | b8 | | 9 | xboyww | man | xboyww9@qq | a9 | b9 | | 10 | xboyww | man | xboyww10@qq | a10 | b10 | +------+--------+--------+-------------+------------+-----------+ 10 rows in set (0.00 sec) #删除存储过程 DROP PROCEDURE auto_insert1; 生成的数据库和表 db1 db1_t1 10万条数据 db1_t2 10万条数据 db2 db2_t1 10万条数据 db2_t2 10万条数据 mysqldump连接服务端参数 -u 指定用户 -p 指定密码 -S 指定套接字文件 -h 指定主机 -P 指定端口 mysqldump的三种语法 shell> mysqldump [options] db_name [tbl_name ...] shell> mysqldump [options] --databases db_name ... shell> mysqldump [options] --all-databases 2.1 全库备份 -A,--all-databases mysqldump -uroot -p -A >all.sql 2.2 单库、多库备份 -B,--databases 单库备份，可以不加选项-B mysqldump -uroot -p -B db1 > db1.sql 或者 mysqldump -uroot -p db1 > db1.sql 多库备份，必须加参数-B mysqldump -uroot -p -B db1 db2 > db1_db2.sql 2.3 单表、多表备份 不需要参数 单表备份 mysqldump -uroot -p db1 db1_t1 > db1.db1_t1.sql 多表备份 mysqldump -uroot -p db1 db1_t1 db1_t2 > db1.db1_t1_t2.sql 2.4 备份的一些选项 2.4.1 --master-data 备份时加入change master语句，需要开启binlog日志 有3个参数 0 没有 1 不注释 2 注释 当参数=0时，备份的文件中是没有change master to语句的 mysqldump -uroot -p -B db1 --master-data=0 > db1.sql 当参数=1时，备份的文件中就会有change master to语句，并且没有注释 mysqldump -uroot -p -B db1 --master-data=1 > db1.sql #备份的sql文件中会有change master to语句，并且没有注释 $ grep 'CHANGE MASTER' db1.sql CHANGE MASTER TO MASTER_LOG_FILE='binlog.000004', MASTER_LOG_POS=154; 当参数=2时，备份的文件中就会有change master to语句，并且是注释的 mysqldump -uroot -p -B db1 --master-data=2 > db1.sql #备份的sql文件中会有change master to语句，并且是注释的 $ grep 'CHANGE MASTER' db1.sql -- CHANGE MASTER TO MASTER_LOG_FILE='binlog.000004', MASTER_LOG_POS=154; 2.4.2 -R, --routines 备份存储过程和函数数据 mysqldump -uroot -A -R --master-data=2 > all.sql 2.4.3 --triggers 备份触发器数据 默认启用，使用选项--skip-triggers禁用 mysqldump -uroot -A --triggers --master-data=2 > all.sql 2.4.4 --single-transaction 快照备份，仅对InnoDB引擎生效 此选项将事务隔离模式设置为， REPEATABLE READ并START TRANSACTION mysqldump -uroot -A --single-transaction --master-data=2 > all.sql 2.4.5 --lock-all-tables，-x 锁表备份 锁定所有数据库中的所有表，此选项将自动关闭 --single-transaction和 --lock-tables mysqldump -uroot -A -x --master-data=2 > all.sql 3.mysqldump恢复 #先不记录二进制日志 mysql> set sql_log_bin=0; #库内恢复操作 mysql> source /backup/all.sql #库外恢复操作 mysql -uroot -p mysqldump恢复特点 mysqldump在备份和恢复时都需要MySQL实例启动为前提 一般数据量级100G以内，大约15-30分钟可以恢复（PB、EB就需要考虑别的方式） mysqldump是以覆盖的形式恢复数据的 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql基础/18.mysql物理备份 xtrabackup.html":{"url":"db/mysql/mysql基础/18.mysql物理备份 xtrabackup.html","title":"13.mysql物理备份 xtrabackup","keywords":"","body":"[toc] mysql物理备份 xtrabackup xtrabackup github地址 xtrabackup2.4官方文档 xtrabackup8.0官方文档 xtrabackup2.4官方下载地址 xtrabackup8.0官方下载地址 xtrabackup备份方式（物理备份） 对于非innodb表（比如myisam）是直接锁表cp数据文件，属于一种温备 对于innodb的表（支持事务），不锁表，cp数据页最终以数据文件方式保存下来，并且把redo和undo一并备走，属于热备方式 备份时读取配置文件/etc/my.cnf 1.安装xtrabackup 1.1 下载软件包并安装 wget https://www.percona.com/downloads/Percona-XtraBackup-2.4/Percona-XtraBackup-2.4.20/binary/redhat/7/x86_64/percona-xtrabackup-24-2.4.20-1.el7.x86_64.rpm yum -y localinstall percona-xtrabackup-24-2.4.20-1.el7.x86_64.rpm 1.2 查看版本 $ xtrabackup -v xtrabackup: recognized server arguments: --datadir=/usr/local/mysql/data --server-id=1 --log_bin=binlog xtrabackup version 2.4.20 based on MySQL server 5.7.26 Linux (x86_64) (revision id: c8b4056) 2.xtrabackup全备 xtrabackup xtrabackup可以在不加锁的情况下备份innodb数据表，不过此工具不能操作myisam。 innobackupex innobackupex是一个封装了xtrabackup的脚本，能同时处理innodb和myisam，但在处理myisam时需要加一个读锁。 利用存储过程生成大量数据 db1、db2每个库中有两张表，每张表10万条数据 #1.创建数据库 mysql> create database db1; Query OK, 1 row affected (0.00 sec) #2.创建表 mysql> use db1; Database changed mysql> create table db1_t1( id int, name varchar(20), gender char(6), email varchar(50), first_name char(10), last_name char(10) ); Query OK, 0 rows affected (0.01 sec) #3.创建存储过程 mysql> delimiter $$ #声明存储过程的结束符号为$$ mysql> create procedure auto_insert1() BEGIN declare i int default 1; while(i delimiter ; #重新声明分号为结束符号，注意有空格 #4.查看存储过程 show create procedure auto_insert1\\G #5.调用存储过程 call auto_insert1(); #6.查看数据 mysql> select count(*) from s1; +----------+ | count(*) | +----------+ | 100000 | +----------+ 1 row in set (0.04 sec) mysql> select * from s1 limit 10; +------+--------+--------+-------------+------------+-----------+ | id | name | gender | email | first_name | last_name | +------+--------+--------+-------------+------------+-----------+ | 1 | xboyww | man | xboyww1@qq | a1 | b1 | | 2 | xboyww | man | xboyww2@qq | a2 | b2 | | 3 | xboyww | man | xboyww3@qq | a3 | b3 | | 4 | xboyww | man | xboyww4@qq | a4 | b4 | | 5 | xboyww | man | xboyww5@qq | a5 | b5 | | 6 | xboyww | man | xboyww6@qq | a6 | b6 | | 7 | xboyww | man | xboyww7@qq | a7 | b7 | | 8 | xboyww | man | xboyww8@qq | a8 | b8 | | 9 | xboyww | man | xboyww9@qq | a9 | b9 | | 10 | xboyww | man | xboyww10@qq | a10 | b10 | +------+--------+--------+-------------+------------+-----------+ 10 rows in set (0.00 sec) #删除存储过程 DROP PROCEDURE auto_insert1; db3，一张表，两条数据 mysql> create database db3; Query OK, 1 row affected (0.00 sec) mysql> create table t3(id int,name char(10)) engine=myisam; Query OK, 0 rows affected (0.01 sec) mysql> insert into t3 values(1,'xiaoming'),(2,'xiaohong'); Query OK, 2 rows affected (0.00 sec) Records: 2 Duplicates: 0 Warnings: 0 mysql> select * from t3; +------+----------+ | id | name | +------+----------+ | 1 | xiaoming | | 2 | xiaohong | +------+----------+ 2 rows in set (0.00 sec) 2.1 全备 2.1.1 备份方法一 xtrabackup --backup xtrabackup2.4 xtrabackup选项 执行备份命令 xtrabackup -uroot -p --backup --target-dir=/data/backups/ /data/backups备份目录下的内容 $ ll 总用量 12340 -rw-r----- 1 root root 487 7月 2 22:43 backup-my.cnf drwxr-x--- 2 root root 92 7月 2 22:43 db1 drwxr-x--- 2 root root 92 7月 2 22:43 db2 drwxr-x--- 2 root root 62 7月 2 22:43 db3 -rw-r----- 1 root root 646 7月 2 22:43 ib_buffer_pool -rw-r----- 1 root root 12582912 7月 2 22:43 ibdata1 drwxr-x--- 2 root root 4096 7月 2 22:43 mysql drwxr-x--- 2 root root 8192 7月 2 22:43 performance_schema drwxr-x--- 2 root root 8192 7月 2 22:43 sys -rw-r----- 1 root root 19 7月 2 22:43 xtrabackup_binlog_info -rw-r----- 1 root root 141 7月 2 22:43 xtrabackup_checkpoints -rw-r----- 1 root root 474 7月 2 22:43 xtrabackup_info -rw-r----- 1 root root 2560 7月 2 22:43 xtrabackup_logfile 2.1.2 备份方法二 innobackupex xtrabackup2.4 innobackupex选项 执行备份命令 -S /var/lib/mysql/mysql.sock -S选项可以不加，会从mysql配置文件/etc/my.cnf中读取sock文件位置 innobackupex -uroot -p1 /backup 备份完成后会在/backup目录下生成一个以时间命令并精确到秒的目录 $ ls 2020-07-02_22-57-09 $ cd 2020-07-02_22-57-09/ $ ll 总用量 12340 -rw-r----- 1 root root 487 7月 2 22:57 backup-my.cnf drwxr-x--- 2 root root 92 7月 2 22:57 db1 drwxr-x--- 2 root root 92 7月 2 22:57 db2 drwxr-x--- 2 root root 62 7月 2 22:57 db3 -rw-r----- 1 root root 646 7月 2 22:57 ib_buffer_pool -rw-r----- 1 root root 12582912 7月 2 22:57 ibdata1 drwxr-x--- 2 root root 4096 7月 2 22:57 mysql drwxr-x--- 2 root root 8192 7月 2 22:57 performance_schema drwxr-x--- 2 root root 8192 7月 2 22:57 sys -rw-r----- 1 root root 19 7月 2 22:57 xtrabackup_binlog_info -rw-r----- 1 root root 141 7月 2 22:57 xtrabackup_checkpoints -rw-r----- 1 root root 494 7月 2 22:57 xtrabackup_info -rw-r----- 1 root root 2560 7月 2 22:57 xtrabackup_logfile 如果想要自主命名备份目录的话，需要加参数--no-timestamp innobackupex -uroot -p1 --no-timestamp /backup/bak 在备份目录下会有一个文件xtrabackup_binlog_info，这个文件记录了binlog文件名和binlog的位置点 $ cat xtrabackup_binlog_info binlog.000009 1061 在备份目录会有一个文件xtrabackup_info，这个文件记录了备份信息汇总 $ cat xtrabackup_info uuid = c47c158f-bc74-11ea-82fa-001c42f33c78 name = tool_name = innobackupex tool_command = --user=root --password=... --no-timestamp /backup/bak tool_version = 2.4.20 ibbackup_version = 2.4.20 server_version = 5.7.28-log start_time = 2020-07-02 23:00:27 end_time = 2020-07-02 23:00:28 lock_time = 0 binlog_pos = filename 'binlog.000009', position '1061' innodb_from_lsn = 0 innodb_to_lsn = 332706000 partial = N incremental = N format = file compact = N compressed = N encrypted = N 在备份目录下会有一个文件xtrabackup_logfile，这个文件是备份的redo_log文件 $ ll xtrabackup_logfile -rw-r----- 1 root root 2560 7月 2 23:00 xtrabackup_logfile 2.2 全备恢复 2.2.1 查看原有数据库 有3个库(db1，db2，db3)， mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | db1 | | db2 | | db3 | | mysql | | performance_schema | | sys | +--------------------+ 7 rows in set (0.00 sec) 2.2.2 原有数据库中的表 #db1，有两张表，每张表中有10万条数据 mysql> use db1; mysql> show tables; +---------------+ | Tables_in_db1 | +---------------+ | db1_t1 | | db1_t2 | +---------------+ 2 rows in set (0.00 sec) mysql> select count(*) from db1_t1; +----------+ | count(*) | +----------+ | 100000 | +----------+ 1 row in set (0.04 sec) mysql> select count(*) from db1_t2; +----------+ | count(*) | +----------+ | 100000 | +----------+ 1 row in set (0.04 sec) #db2，有两张表，每张表中有10万条数据 mysql> use db1; mysql> show tables; +---------------+ | Tables_in_db2 | +---------------+ | db2_t1 | | db2_t2 | +---------------+ 2 rows in set (0.00 sec) mysql> select count(*) from db2_t1; +----------+ | count(*) | +----------+ | 100000 | +----------+ 1 row in set (0.04 sec) mysql> select count(*) from db2_t2; +----------+ | count(*) | +----------+ | 100000 | +----------+ 1 row in set (0.03 sec) #db3，有1张表，表中2行数据 mysql> use db3; mysql> show tables; +---------------+ | Tables_in_db3 | +---------------+ | t3 | +---------------+ 1 row in set (0.00 sec) mysql> select count(*) from t3; +----------+ | count(*) | +----------+ | 2 | +----------+ 1 row in set (0.00 sec) 2.2.3 进行全库备份 innobackupex -uroot -p1 --no-timestamp /backup/bak 2.2.4 删除db1-3数据库 mysql> drop database db1; Query OK, 2 rows affected (0.00 sec) mysql> drop database db2; Query OK, 2 rows affected (0.01 sec) mysql> drop database db3; Query OK, 1 row affected (0.00 sec) mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | +--------------------+ 4 rows in set (0.00 sec) 2.2.5 将redo进行重做，已提交的写到数据文件，未提交的使用undo回滚，模拟CSR的过程 --apply-log参数的作用是应用 BACKUP-DIR 中的 xtrabackup_logfile 事务日志文件。一般情况下，在备份完成后，数据尚且不能用于恢复操作，因为备份的数据中可能会包含尚未提交的事务或已经提交但尚未同步至数据文件中的事务。因此，此时数据文件仍处于不一致状态。\"准备\"的主要作用正是通过回滚未提交的事务及同步已经提交的事务至数据文件使得数据文件处于一致性状态。 innobackupex -uroot -p1 --apply-log /backup/bak 2.2.6 停止mysql，删除原先数据目录 停止mysql $ /etc/init.d/mysqld stop Shutting down MySQL.... SUCCESS! 2.2.7 删除原先数据目录或者修改名称 ⚠️恢复数据之前需要保证数据目录是空的状态 mysql安装的数据目录为/usr/local/mysql/data #如果做删除操作，最好先备份然后再删除，虽然数据已经丢失一部分，这里选择修改目录名称 mv /usr/local/mysql/data{,-bak} 2.2.8 进行数据恢复 这里要注意，恢复的时候mysql配置文件[mysqld]下一定要指定mysql data目录 innobackupex -uroot -p1 --copy-back /backup/bak 或 xtrabackup -uroot p1 --copy-back --target-dir=/backup/bak 2.2.9 给恢复的目录重新授权所有者为mysql chown -R mysql.mysql /usr/local/mysql/data 2.2.10 启动mysql $ /etc/init.d/mysqld start Starting MySQL.Logging to '/usr/local/mysql/data/error.log'. . SUCCESS! 2.2.11 验证数据恢复 验证数据库是否还原 mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | db1 | | db2 | | db3 | | mysql | | performance_schema | | sys | +--------------------+ 7 rows in set (0.00 sec) 验证表中的数据是否还原 #db1，有两张表，每张表中有10万条数据 mysql> use db1; mysql> show tables; +---------------+ | Tables_in_db1 | +---------------+ | db1_t1 | | db1_t2 | +---------------+ 2 rows in set (0.00 sec) mysql> select count(*) from db1_t1; +----------+ | count(*) | +----------+ | 100000 | +----------+ 1 row in set (0.04 sec) mysql> select count(*) from db1_t2; +----------+ | count(*) | +----------+ | 100000 | +----------+ 1 row in set (0.04 sec) #db2，有两张表，每张表中有10万条数据 mysql> use db1; mysql> show tables; +---------------+ | Tables_in_db2 | +---------------+ | db2_t1 | | db2_t2 | +---------------+ 2 rows in set (0.00 sec) mysql> select count(*) from db2_t1; +----------+ | count(*) | +----------+ | 100000 | +----------+ 1 row in set (0.04 sec) mysql> select count(*) from db2_t2; +----------+ | count(*) | +----------+ | 100000 | +----------+ 1 row in set (0.03 sec) #db3，有1张表，表中2行数据 mysql> use db3; mysql> show tables; +---------------+ | Tables_in_db3 | +---------------+ | t3 | +---------------+ 1 row in set (0.00 sec) mysql> select count(*) from t3; +----------+ | count(*) | +----------+ | 2 | +----------+ 1 row in set (0.00 sec) 三、xtrabackup增量备份 xtrabackup2.4 增量备份官方文档 3.1 做全备 db1、db2、db3数据库及表数据说明 $ tree db1 db2 db3 db1 #两张表，每张表中10万条数据 ├── db1_t1.frm ├── db1_t1.ibd ├── db1_t2.frm ├── db1_t2.ibd └── db.opt db2 #两张表，每张表中10万条数据 ├── db2_t1.frm ├── db2_t1.ibd ├── db2_t2.frm ├── db2_t2.ibd └── db.opt db3 #一张表，2条数据 ├── db.opt ├── t3.frm ├── t3.MYD └── t3.MYI 删除之前的全备，重新做全备 innobackupex -uroot -p1 --no-timestamp /backup/bak 或 xtrabackup -uroot -p --backup --target-dir=/backup/bak 在备份目录/backup/bak中有一个文件xtrabackup_checkpoints，这个文件记录了备份类型、lsn(日志序列号) $ cat xtrabackup_checkpoints backup_type = full-backuped from_lsn = 0 to_lsn = 332706875 last_lsn = 332706884 compact = 0 recover_binlog_info = 0 flushed_lsn = 332706884 3.2 开始增量备份 3.2.1 基于全备的的增备 开始第一次增备，只要全备和多个增备的LSN号连续，那么就可以逐个进行恢复。可以在备份目录xtrabackup_checkpoints文件中看到，其中全备的from_lsn=0,增备的from_lsn应该等于上一个增备或者全备的to_lsn $ cat xtrabackup_checkpoints backup_type = full-backuped from_lsn = 0 to_lsn = 332706875 last_lsn = 332706884 compact = 0 recover_binlog_info = 0 flushed_lsn = 332706884 3.2.1.1 插入数据 db1、db2、db3每个数据库中的表都插入一条数据 #db1 mysql> insert into db1_t1 values(100001,'xboyww','man',concat( 'xboyww',100001,'@qq'),concat('a',100001),concat('b',100001)); Query OK, 1 row affected (0.00 sec) mysql> insert into db1_t2 values(100001,'xboyww','man',concat( 'xboyww',100001,'@qq'),concat('a',100001),concat('b',100001)); Query OK, 1 row affected (0.00 sec) #db2 mysql> insert into db2_t1 values(100001,'xboyww','man',concat( 'xboyww',100001,'@qq'),concat('a',100001),concat('b',100001)); Query OK, 1 row affected (0.01 sec) mysql> insert into db2_t2 values(100001,'xboyww','man',concat( 'xboyww',100001,'@qq'),concat('a',100001),concat('b',100001)); Query OK, 1 row affected (0.00 sec) #db3 mysql> insert into t3 values(3,'dabai'); Query OK, 1 row affected (0.00 sec) 查看每张表的数据 每张表都在原先的基础上增加了一行数据 #db1的两张表 mysql> select count(*) from db1_t1; +----------+ | count(*) | +----------+ | 100001 | +----------+ 1 row in set (0.03 sec) mysql> select count(*) from db1_t2; +----------+ | count(*) | +----------+ | 100001 | +----------+ 1 row in set (0.04 sec) #db2的两张表 mysql> select count(*) from db2_t1; +----------+ | count(*) | +----------+ | 100001 | +----------+ 1 row in set (0.04 sec) mysql> select count(*) from db2_t2; +----------+ | count(*) | +----------+ | 100001 | +----------+ 1 row in set (0.04 sec) #db3的一张表 mysql> select count(*) from t3; +----------+ | count(*) | +----------+ | 3 | +----------+ 1 row in set (0.00 sec) 3.2.1.2 执行增备命令 增备就是在全备的命令基础上加一个参数--incremental-basedir=指定增备的目录 innobackupex -uroot -p1 --no-timestamp --incremental /backup/bak_incr1 --incremental-basedir=/backup/bak 或 xtrabackup -uroot -p1 --backup --target-dir=/backup/bak_incr1 --incremental-basedir=/backup/bak 说明： --incremental-basedir是上次全备或者增备出来的文件夹。当第一次增备的时候，一般填上次全备，第二次增备的时候，如果--incremental-basedir填上次全备，那么本次增备就会包含上次全备到现在变化的内容，相当于oracle依次做1级，2级，3级。。。增备如果--incremental-basedir填第一次增备的目录，那么该次增备只包含第一次增备到现在的变化，文件会更小，相当于oracle rman里面每次都做1级增备。 3.2.1.3 增备完成查看备份的数据目录大小 $ pwd /backup $ du -sh * 90M bak 3.2M bak_incr1 #全备的数据大小 $ du -sh * 4.0K backup-my.cnf 33M db1 33M db2 24K db3 4.0K ib_buffer_pool 12M ibdata1 12M mysql 1.1M performance_schema 680K sys 4.0K xtrabackup_binlog_info 4.0K xtrabackup_checkpoints 4.0K xtrabackup_info 4.0K xtrabackup_logfile #增备的数据大小 $ du -sh * 4.0K backup-my.cnf 100K db1 100K db2 24K db3 4.0K ib_buffer_pool 176K ibdata1.delta 4.0K ibdata1.meta 1.2M mysql 1.1M performance_schema 604K sys 4.0K xtrabackup_binlog_info 4.0K xtrabackup_checkpoints 4.0K xtrabackup_info 4.0K xtrabackup_logfile 3.2.1.4 全备的xtrabackup_checkpoints与增备的xtrabackup_checkpoints文件对比 可以看到增备中的from_lsn = 332706875与全备中的to_lsn = 332706875是相同的 #全备 $ cat xtrabackup_checkpoints backup_type = full-backuped from_lsn = 0 to_lsn = 332706875 last_lsn = 332706884 compact = 0 recover_binlog_info = 0 flushed_lsn = 332706884 #增备 $ cat xtrabackup_checkpoints backup_type = incremental from_lsn = 332706875 to_lsn = 332708551 last_lsn = 332708560 compact = 0 recover_binlog_info = 0 flushed_lsn = 332708560 3.2.2 基于增备的增备 3.2.2.1 插入数据 db1、db2、db3每个数据库中的表都插入一条数据 #db1 mysql> insert into db1_t1 values(100002,'xboyww','man',concat( 'xboyww',100002,'@qq'),concat('a',100002),concat('b',100002)); Query OK, 1 row affected (0.00 sec) mysql> insert into db1_t2 values(100002,'xboyww','man',concat( 'xboyww',100002,'@qq'),concat('a',100002),concat('b',100002)); Query OK, 1 row affected (0.00 sec) #db2 mysql> insert into db2_t1 values(100002,'xboyww','man',concat( 'xboyww',100002,'@qq'),concat('a',100002),concat('b',100002)); Query OK, 1 row affected (0.01 sec) mysql> insert into db2_t2 values(100002,'xboyww','man',concat( 'xboyww',100002,'@qq'),concat('a',100002),concat('b',100002)); Query OK, 1 row affected (0.00 sec) #db3 mysql> insert into t3 values(4,'xixixi'); Query OK, 1 row affected (0.00 sec) 查看每张表的数据 每张表都在原先的基础上增加了一行数据 #db1的两张表 mysql> select count(*) from db1_t1; +----------+ | count(*) | +----------+ | 100002 | +----------+ 1 row in set (0.03 sec) mysql> select count(*) from db1_t2; +----------+ | count(*) | +----------+ | 100002 | +----------+ 1 row in set (0.04 sec) #db2的两张表 mysql> select count(*) from db2_t1; +----------+ | count(*) | +----------+ | 100002 | +----------+ 1 row in set (0.04 sec) mysql> select count(*) from db2_t2; +----------+ | count(*) | +----------+ | 100002 | +----------+ 1 row in set (0.04 sec) #db3的一张表 mysql> select count(*) from t3; +----------+ | count(*) | +----------+ | 4 | +----------+ 1 row in set (0.00 sec) 3.2.2.2 执行增备命令 innobackupex -uroot -p1 --no-timestamp --incremental /backup/bak_incr2 --incremental-basedir=/backup/bak_incr1 或 xtrabackup -uroot -p1 --backup --target-dir=/backup/bak_incr2 --incremental-basedir=/backup/bak_incr1 3.2.2.3 增备完成查看备份的数据目录大小 $ pwd /backup $ du -sh * 90M bak 3.2M bak_incr1 3.2M bak_incr2 #第一次增备的数据大小 $ du -sh * 4.0K backup-my.cnf 100K db1 100K db2 24K db3 4.0K ib_buffer_pool 176K ibdata1.delta 4.0K ibdata1.meta 1.2M mysql 1.1M performance_schema 604K sys 4.0K xtrabackup_binlog_info 4.0K xtrabackup_checkpoints 4.0K xtrabackup_info 4.0K xtrabackup_logfile #基于增备的数据大小 du -sh * 4.0K backup-my.cnf 100K db1 100K db2 24K db3 4.0K ib_buffer_pool 144K ibdata1.delta 4.0K ibdata1.meta 1.2M mysql 1.1M performance_schema 604K sys 4.0K xtrabackup_binlog_info 4.0K xtrabackup_checkpoints 4.0K xtrabackup_info 4.0K xtrabackup_logfile 3.2.2.4 基于增备的xtrabackup_checkpoints与增备的xtrabackup_checkpoints文件对比 之前增备中的from_lsn = 332706875与全备中的to_lsn = 332706875是相同的 基于增备的增备from_lsn = 332708551与基于全备的增备中的to_lsn = 332708551是相同的 #基于全备的增备 $ cat xtrabackup_checkpoints backup_type = incremental from_lsn = 332706875 to_lsn = 332708551 last_lsn = 332708560 compact = 0 recover_binlog_info = 0 flushed_lsn = 332708560 #基于增备的增备份 $ cat xtrabackup_checkpoints backup_type = incremental from_lsn = 332708551 to_lsn = 332710179 last_lsn = 332710188 compact = 0 recover_binlog_info = 0 flushed_lsn = 332710188 3.3 增备恢复 3.3.1 先准备一个全备 innobackupex -uroot -p1 --apply-log --redo-only /backup/bak 或 xtrabackup -uroot -p --backup --target-dir=/backup/bak 3.3.2 将增备1应用到全备份 增备1就是基于全备的增备，也就是第一次增备⚠️这里要加--redo-only参数 这次要加入--redo-only参数，因为在每个备份过程中，都会碰到一些事务进来执行，而备份结束时可能有些事务并没有执行完毕，所以在默认prepare中这些事务就会被回滚（rollback），而加入了--redo-only就不会回滚这些事务，而是等待prepare下次增备。 innobackupex -uroot -p1 --apply-log --redo-only /backup/bak --incremental-dir=/backup/bak_incr1 或 xtrabackup -uroot -p1 --prepare --apply-log-only --target-dir=/backup/bak --incremental-dir=/backup/bak_incr1 3.3.3 将增备2应用到全备 增备2就是基于增备的增备，⚠️这里不要加--redo-only参数 innobackupex -uroot -p1 --apply-log /backup/bak --incremental-dir=/backup/bak_incr2 或 xtrabackup -uroot -p1 --prepare --target-dir=/backup/bak --incremental-dir=/backup/bak_incr2 3.3.4 把所有合在一起的完全备份整体进行一次apply操作，回滚未提交的数据 innobackupex -uroot -p1 --apply-log /backup/bak 或 xtrabackup --prepare --apply-log --target-dir=/backup/bak 3.3.5 删除db1-3数据库 mysql> drop database db1; Query OK, 2 rows affected (0.00 sec) mysql> drop database db2; Query OK, 2 rows affected (0.01 sec) mysql> drop database db3; Query OK, 1 row affected (0.00 sec) mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | +--------------------+ 4 rows in set (0.00 sec) 3.3.6 停止mysql，删除原先数据目录 停止mysql $ /etc/init.d/mysqld stop Shutting down MySQL.... SUCCESS! 3.3.7 删除原先数据目录或者修改名称 ⚠️恢复数据之前需要保证数据目录是空的状态 mysql安装的数据目录为/usr/local/mysql/data #如果做删除操作，最好先备份然后再删除，虽然数据已经丢失一部分，这里选择修改目录名称 mv /usr/local/mysql/data{,-bak} 3.3.8 进行数据恢复 这里要注意，恢复的时候mysql配置文件[mysqld]下一定要指定mysql data目录 innobackupex -uroot -p1 --copy-back /backup/bak 或 xtrabackup -uroot p1 --copy-back --target-dir=/backup/bak 3.3.9 给恢复的目录重新授权所有者为mysql chown -R mysql.mysql /usr/local/mysql/data 3.3.10 启动mysql $ /etc/init.d/mysqld start Starting MySQL.Logging to '/usr/local/mysql/data/error.log'. . SUCCESS! 3.3.11 验证数据恢复 验证数据库是否还原 mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | db1 | | db2 | | db3 | | mysql | | performance_schema | | sys | +--------------------+ 7 rows in set (0.00 sec) 验证表中的数据是否还原 #db1，有两张表，每张表中有10万零2条数据 mysql> use db1; mysql> show tables; +---------------+ | Tables_in_db1 | +---------------+ | db1_t1 | | db1_t2 | +---------------+ 2 rows in set (0.00 sec) mysql> select count(*) from db1_t1; +----------+ | count(*) | +----------+ | 100002 | +----------+ 1 row in set (0.04 sec) mysql> select count(*) from db1_t2; +----------+ | count(*) | +----------+ | 100002 | +----------+ 1 row in set (0.04 sec) #db2，有两张表，每张表中有10万零2条数据 mysql> use db1; mysql> show tables; +---------------+ | Tables_in_db2 | +---------------+ | db2_t1 | | db2_t2 | +---------------+ 2 rows in set (0.00 sec) mysql> select count(*) from db2_t1; +----------+ | count(*) | +----------+ | 100002 | +----------+ 1 row in set (0.04 sec) mysql> select count(*) from db2_t2; +----------+ | count(*) | +----------+ | 100002 | +----------+ 1 row in set (0.03 sec) #db3，有1张表，表中4行数据 mysql> use db3; mysql> show tables; +---------------+ | Tables_in_db3 | +---------------+ | t3 | +---------------+ 1 row in set (0.00 sec) mysql> select count(*) from t3; +----------+ | count(*) | +----------+ | 4 | +----------+ 1 row in set (0.00 sec) innobackupex命令与xtrabackup命令的区别 #全备 innobackupex --user=root --password=1 backup xtrabackup -uroot -p --backup --target-dir=/data/backups/ #增备 innobackupex -uroot -p1 --no-timestamp --incremental /backup/bak_incr2 --incremental-basedir=/backup/bak_incr1 xtrabackup -uroot -p1 --backup --target-dir=/backup/bak_incr2 --incremental-basedir=/backup/bak_incr1 需要注意的点 xtrabackup --apply-log-only合并除最后一个以外的所有增量时使用 增量备份的步骤与完全备份的步骤不同。在完全备份中，执行两种类型的操作以使数据库保持一致：已提交的事务相对于数据文件从日志文件中重放，未提交的事务被回滚。准备增量备份时，必须跳过未提交事务的回滚，因为在备份时未提交的事务可能正在进行中，并且很有可能将在下一次增量备份中提交。您应该使用该 选项来防止回滚阶段。xtrabackup --apply-log-only ⚠️如果不使用该选项xtrabackup --apply-log-only阻止回滚阶段，则增量备份将无用。事务回滚后，不能再应用增量备份 4.备份脚本 授权 CREATE USER 'bkpuser'@'localhost' IDENTIFIED BY 'bkpuser@1234'; GRANT RELOAD, LOCK TABLES, PROCESS, REPLICATION CLIENT,super ON *.* TO 'bkpuser'@'localhost'; FLUSH PRIVILEGES; 全备脚本 #!/bin/bash local_ip=\"$(/sbin/ifconfig ens160|grep 'inet'|grep -v '::'| awk '{print $2}')\" user='bkpuser' passwd='bkpuser@1234' config='/etc/my.cnf' backup_path='/tmsdata/xtrabackup_full' backup_date=`date +\"%Y%m%d_%H%M%S\"` backup_dir=\"$backup_path/$backup_date/\" backup_log=\"$backup_path/log\" #邮件设置 title1='percona xtrabackup information(success)' title2='percona xtrabackup information(failed)' content1='Server_name:'$(hostname)' \\n Server_ip:'$local_ip' \\n '$(date +\"%y-%m-%d %H:%M:%S\")' \\n mysql full backup Success!' content2='Server_name:'$(hostname)' \\n Server_ip:'$local_ip' \\n '$(date +\"%y-%m-%d %H:%M:%S\")' \\n mysql full backup Faild!' #判断备份目录是否存在 if [ ! -d \"$backup_dir\" ] && [ ! -d \"$backup_log\" ];then mkdir -p $backup_dir && mkdir -p $backup_log fi #[[ -d $backup_dir ]] || mkdir -p $backup_dir #[[ -d $backup_log ]] || mkdir -p $backup_log echo \"=========================================Start to backup at $(date +%Y%m%d-%H:%M:%S)=========================================\" >> $backup_log/$backup_date.log #开始备份 xtrabackup --defaults-file=$config --user=$user --password=\"$passwd\" --backup --target-dir=$backup_dir &>> $backup_log/$backup_date.log if [ $? -eq 0 ];then echo \"Backup is success! at $(date +%Y%m%d%H%M)\" echo \"Server_name:$(hostname) Server_ip:$local_ip $(date +\"%y-%m-%d %H:%M:%S\") mysql full backup Success!\" >> $backup_log/$backup_date.log echo \"$content1\" | mail -s \"$title1\" huangwb@fslgz.com else echo \"Backup is Fail! at $(date +%Y%m%d%H%M)\" echo \"Server_name:$(hostname) Server_ip:$local_ip $(date +\"%y-%m-%d %H:%M:%S\") mysql full backup Fail!\" >> $backup_log/$backup_date.log echo \"$content2\" | mail -s \"$title2\" huangwb@fslgz.com fi sleep 3 #xtrabackup从该文件读取最近一次的全备路径 egrep \".* Backup created in directory .*\" $backup_log/$backup_date.log >> $backup_path/complete.info #清除超过14天的备份 find $backup_path -mtime +14 -type d -exec rm -rf {} \\; echo \"Backup Process Done\" 增备脚本 #!/bin/bash local_ip=\"$(/sbin/ifconfig ens160|grep 'inet'|grep -v '::'| awk '{print $2}')\" user='bkpuser' passwd='bkpuser@1234' config='/etc/my.cnf' backup_full='/tmsdata/xtrabackup_full' backup_path='/tmsdata/xtrabackup_incr' backup_date=`date +\"%Y%m%d_%H%M%S\"` backup_dir=\"$backup_path/$backup_date/\" backup_log=\"$backup_path/log\" FULL_BASE_DIR=$(tail -1 $backup_full/complete.info | cut -d\\' -f2) #判断备份目录是否存在 if [ ! -d \"$backup_dir\" ] && [ ! -d \"$backup_log\" ];then mkdir -p $backup_dir && mkdir -p $backup_log fi #[[ -d $backup_dir ]] || mkdir -p $backup_dir #[[ -d $backup_log ]] || mkdir -p $backup_log echo \"========================================xtrabackup根据$FULL_BASE_DIR目录开始增备=========================================\" >> $backup_log/$backup_date.log #邮件设置 title1='percona xtrabackup information(success)' title2='percona xtrabackup information(failed)' content1='Server_name:'$(hostname)' Server_ip:'$local_ip' '$(date +\"%y-%m-%d %H:%M:%S\")' mysql increment backup Success!' content2='Server_name:'$(hostname)' Server_ip:'$local_ip' '$(date +\"%y-%m-%d %H:%M:%S\")' mysql increment backup Faild!' echo \"=========================================Start to backup at $(date +%Y%m%d-%H:%M:%S)=========================================\" >> $backup_log/$backup_date.log #开始备份 xtrabackup --defaults-file=$config --user=$user --password=\"$passwd\" --backup --target-dir=$backup_dir --incremental-basedir=$FULL_BASE_DIR &>> $backup_log/$backup_date.log if [ $? -eq 0 ];then echo \"Backup is success! at $(date +%Y%m%d%H%M)\" echo \"Server_name:$(hostname) Server_ip:$local_ip $(date +\"%y-%m-%d %H:%M:%S\") mysql full backup Success!\" >> $backup_log/$backup_date.log echo \"$content1\" | mail -s \"$title1\" huangwb@fslgz.com else echo \"Backup is Fail! at $(date +%Y%m%d%H%M)\" echo \"Server_name:$(hostname) Server_ip:$local_ip $(date +\"%y-%m-%d %H:%M:%S\") mysql full backup Fail!\" >> $backup_log/$backup_date.log echo \"$content2\" | mail -s \"$title2\" huangwb@fslgz.com fi #清除超过14天的备份 find $backup_path -mtime +14 -type d -exec rm -rf {} \\; echo \"Backup Process Done\" 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql进阶/1.mysql多实例-5.6.40.html":{"url":"db/mysql/mysql进阶/1.mysql多实例-5.6.40.html","title":"5.6.40","keywords":"","body":"[toc] mysql多实例-5.6.40 1.mysql多实例介绍 1.1 什么是MySQL多实例 MySQL多实例就是在一台机器上开启多个不同的服务端口（如：3306,3307），运行多个MySQL服务进程，通过不同的socket监听不同的服务端口来提供各自的服务 1.2 MySQL多实例的特点有以下几点 1：有效利用服务器资源，当单个服务器资源有剩余时，可以充分利用剩余的资源提供更多的服务 2：节约服务器资源 3：资源互相抢占问题，当某个服务实例服务并发很高时或者开启慢查询时，会消耗更多的内存、CPU、磁盘IO资源，导致服务器上的其他实例提供服务的质量下降 1.3 部署mysql多实例的两种方式 第一种是使用多个配置文件启动不同的进程来实现多实例，这种方式的优势逻辑简单，配置简单，缺点是管理起来不太方便 第二种是通过官方自带的mysqld_multi使用单独的配置文件来实现多实例，这种方式定制每个实例的配置不太方面，优点是管理起来很方便，集中管理 1.4 同一开发环境下安装两个数据库，必须处理以下问题 配置文件安装路径不能相同 数据库目录不能相同 启动脚本不能同名 端口不能相同 socket文件的生成路径不能相同 2.mysql多实例安装路径说明 mysql安装路径 第一个实例：/data/mysql3306 第二个实例：/data/mysql3307 第三个实例：/data/mysql3308 3.安装部署过程 3.1 安装依赖包 yum -y install gcc gcc-c++ autoconf bison-devel ncurses-devel libaio-devel 3.2 创建mysql用户和组 groupadd mysql && useradd -g mysql -s /sbin/nologin mysql 3.3 下载mysql-5.6.40二进制包 wget https://downloads.mysql.com/archives/get/file/mysql-5.6.40-linux-glibc2.12-x86_64.tar.gz 3.4 解压缩并修改目录名称 tar xf mysql-5.6.40-linux-glibc2.12-x86_64.tar.gz && \\ mv mysql-5.6.40-linux-glibc2.12-x86_64/ mysql-5.6.40 3.5 修改目录所有者为mysql chown -R mysql.mysql mysql-5.6.40 3.6 创建3个mysql安装目录 mkdir -p /data/mysql330{6..8} 3.7 将mysql包分别拷贝到3个安装目录 for i in {6..8};do cp -rp mysql-5.6.40 /data/mysql330$i ;done 3.8 做软连接 for i in {6..8};do ln -s /data/mysql330$i/mysql-5.6.40 /data/mysql330$i/mysql;done 3.9 编辑配置文件 basedir、datadir、log-error、port、socket文件位置不同，如果要做主从，serverid要不同 //备份原有/etc/my.cnf mv /etc/my.cnf /etc/my.cnf.old //编辑第一个实例配置文件 cat >/etc/my-3306.cnf/etc/my-3307.cnf/etc/my-3308.cnf 3.10 拷贝启动脚本 //分别拷贝3个实例启动脚本 for i in {6..8};do cp mysql-5.6.40/support-files/mysql.server /etc/init.d/mysqld330$i ;done //修改文件 sed -i.bak 's#/usr/local#/data/mysql3306#g' /etc/init.d/mysqld3306 /data/mysql3306/mysql/bin/mysqld_safe sed -i.bak 's#/usr/local#/data/mysql3307#g' /etc/init.d/mysqld3307 /data/mysql3307/mysql/bin/mysqld_safe sed -i.bak 's#/usr/local#/data/mysql3308#g' /etc/init.d/mysqld3308 /data/mysql3308/mysql/bin/mysqld_safe 3.11 初始化mysql //初始化第一个实例 /data/mysql3306/mysql/scripts/mysql_install_db --user=mysql --basedir=/data/mysql3306/mysql --datadir=/data/mysql3306/mysql/data //初始化第二个实例 /data/mysql3307/mysql/scripts/mysql_install_db --user=mysql --basedir=/data/mysql3307/mysql --datadir=/data/mysql3307/mysql/data //初始化第三个实例 /data/mysql3308/mysql/scripts/mysql_install_db --user=mysql --basedir=/data/mysql3308/mysql --datadir=/data/mysql3308/mysql/data 3.12 添加mysql命令环境变量 //这里只需要导出一个即可 echo \"export PATH=/data/mysql3306/mysql/bin:$PATH\" > /etc/profile.d/mysql.sh source /etc/profile 3.13 配置systemd管理mysql //配置第一个实例 cat >/etc/systemd/system/mysqld3306.service/etc/systemd/system/mysqld3307.service/etc/systemd/system/mysqld3308.service 3.14 启动mysql、检查启动 //启动mysql systemctl start mysqld3306 ; systemctl enable mysqld3306 systemctl start mysqld3307 ; systemctl enable mysqld3307 systemctl start mysqld3308 ; systemctl enable mysqld3308 //检查启动 netstat -ntpl|grep 330* tcp6 0 0 :::3306 :::* LISTEN 16413/mysqld tcp6 0 0 :::3307 :::* LISTEN 16422/mysqld tcp6 0 0 :::3308 :::* LISTEN 16463/mysqld 3.15 进入mysql，设置密码 //设置第一个实例密码 mysql -S /data/mysql3306/mysql/mysql.sock mysql> set password=password('3306'); mysql> flush privileges; //设置第二个实例密码 mysql -S /data/mysql3307/mysql/mysql.sock mysql> set password=password('3307'); mysql> flush privileges; //设置第三个实例密码 mysql -S /data/mysql3308/mysql/mysql.sock mysql> set password=password('3308'); mysql> flush privileges; 3.16 设置快捷登陆 //原有登陆方式，需要指定mysql用户名密码和套接字文件 mysql -uroot -p3306 -S /data/mysql3306/mysql/mysql.sock 设置快捷登陆 //设置第一个实例 cat >/usr/bin/mysql3306/usr/bin/mysql3307/usr/bin/mysql3308 到此，mysql多实例配置完成！！！ 扩展：基于以上多实例实现mysql主从复制 3306为主 3307、3308为从 1.编辑主库3306（master）配置文件/etc/my-3306.cnf vim /etc/my-3306.cnf #[mysqld]下方增加以下3行 server_id=3306 log_bin=binlog log_bin_index=binlog.index //重启mysql systemctl restart mysqld3306 2.创建专用复制用户 mysql3306 mysql> grant replication slave on *.* to 'backup'@'10.0.0.%' identified by '3306'; Query OK, 0 rows affected (0.01 sec) 3.查看master状态 mysql> show master status; +---------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +---------------+----------+--------------+------------------+-------------------+ | binlog.000001 | 327 | | | | +---------------+----------+--------------+------------------+-------------------+ 1 row in set (0.00 sec) 4.编辑从库配置文件 //编辑3307配置文件 [root@mysql ~]# vim /etc/my-3307.cnf #[mysqld]下方增加以下3行 server_id=3307 relay_log=/data/mysql3307/mysql/relay_log relay_log_index=/data/mysql3307/mysql/relay_log.index //编辑3308配置文件 [root@mysql ~]# vim /etc/my-3308.cnf #[mysqld]下方增加以下3行 server_id=3308 relay_log=/data/mysql3308/mysql/relay_log relay_log_index=/data/mysql3308/mysql/relay_log.index //重启mysql [root@mysql ~]# systemctl restart mysqld3307 && systemctl restart mysqld3308 5.设置slave从master拉取binlog及拉取的位置 //3307 mysql> change master to master_host='10.0.0.55',master_port=3306,master_user='backup',master_password='123',master_log_file='binlog.000001',master_log_pos=327; Query OK, 0 rows affected, 2 warnings (0.06 sec) //启动slave mysql> start slave; Query OK, 0 rows affected (0.01 sec) //查看slave状态 mysql> show slave status\\G #IO线程和SQL线程都必须为YES Slave_IO_Running: Yes Slave_SQL_Running: Yes ------------------------------------------------------------------------------ //3308 mysql> change master to master_host='10.0.0.55',master_port=3306,master_user='backup',master_password='123',master_log_file='binlog.000001',master_log_pos=327; Query OK, 0 rows affected, 2 warnings (0.02 sec) //启动slave mysql> start slave; Query OK, 0 rows affected (0.00 sec) //查看slave状态 mysql> show slave status\\G #IO线程和SQL线程都必须为YES Slave_IO_Running: Yes Slave_SQL_Running: Yes 6.验证，在3306中创建一个数据库，看3307和3308是否会同步 //3306中创建一个数据库 mysql> create database bxb; Query OK, 1 row affected (0.00 sec) mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | DB3306 | | bxb | | mysql | | performance_schema | | test | +--------------------+ 6 rows in set (0.00 sec) //3307中查看数据库 mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | DB3307 | | bxb | | mysql | | performance_schema | | test | +--------------------+ 6 rows in set (0.00 sec) //3308中查看数据库 mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | DB3308 | | bxb | | mysql | | performance_schema | | test | +--------------------+ 6 rows in set (0.01 sec) 到此，mysql基于多实例实现主从复制完成！！！ 实验过程中遇到的错误 在3308（第二个实例）上启动slave报错 //启动slave报错，从存储库初始化中继日志信息结构失败 mysql> start slave; ERROR 1872 (HY000): Slave failed to initialize relay log info structure from the repository 解决方法：先重置slave,然后停止slvae再重新change master mysql> reset slave all; Query OK, 0 rows affected (0.02 sec) mysql> stop slave; 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql进阶/2.mysql多实例-5.7.23.html":{"url":"db/mysql/mysql进阶/2.mysql多实例-5.7.23.html","title":"5.7.23","keywords":"","body":"[toc] mysql多实例-5.7.23 1.mysql多实例介绍 1.1 什么是MySQL多实例 MySQL多实例就是在一台机器上开启多个不同的服务端口（如：3306,3307），运行多个MySQL服务进程，通过不同的socket监听不同的服务端口来提供各自的服务 1.2 MySQL多实例的特点有以下几点 1：有效利用服务器资源，当单个服务器资源有剩余时，可以充分利用剩余的资源提供更多的服务 2：节约服务器资源 3：资源互相抢占问题，当某个服务实例服务并发很高时或者开启慢查询时，会消耗更多的内存、CPU、磁盘IO资源，导致服务器上的其他实例提供服务的质量下降 1.3 部署mysql多实例的两种方式 第一种是使用多个配置文件启动不同的进程来实现多实例，这种方式的优势逻辑简单，配置简单，缺点是管理起来不太方便 第二种是通过官方自带的mysqld_multi使用单独的配置文件来实现多实例，这种方式定制每个实例的配置不太方面，优点是管理起来很方便，集中管理 1.4 同一开发环境下安装两个数据库，必须处理以下问题 配置文件安装路径不能相同 数据库目录不能相同 启动脚本不能同名 端口不能相同 socket文件的生成路径不能相同 2.mysql多实例安装路径说明 mysql安装路径 第一个实例：/data/mysql3306 第二个实例：/data/mysql3307 第三个实例：/data/mysql3308 3.安装部署过程 3.1 安装依赖包 yum -y install gcc gcc-c++ autoconf bison-devel ncurses-devel libaio-devel 3.2 创建mysql用户和组 groupadd mysql && useradd -g mysql -s /sbin/nologin mysql 3.3 下载mysql-5.7.23二进制包 wget https://downloads.mysql.com/archives/get/file/mysql-5.7.23-linux-glibc2.12-x86_64.tar.gz 3.4 解压缩并修改目录名称 tar xf mysql-5.7.23-linux-glibc2.12-x86_64.tar.gz && \\ mv mysql-5.7.23-linux-glibc2.12-x86_64/ mysql-5.7.23 3.5 修改目录所有者为mysql chown -R mysql.mysql mysql-5.7.23 3.6 创建3个mysql安装目录 mkdir -p /data/mysql330{6..8} 3.7 将mysql包分别拷贝到3个安装目录 for i in {6..8};do cp -rp mysql-5.7.23 /data/mysql330$i ;done 3.8 做软连接 for i in {6..8};do ln -s /data/mysql330$i/mysql-5.7.23 /data/mysql330$i/mysql;done 3.9 编辑配置文件 basedir、datadir、log-error、port、socket文件位置不同，如果要做主从，serverid要不同 //备份原有/etc/my.cnf mv /etc/my.cnf /etc/my.cnf.old //编辑第一个实例配置文件 cat >/etc/my-3306.cnf/etc/my-3307.cnf/etc/my-3308.cnf 3.10 拷贝启动脚本 //分别拷贝3个实例启动脚本 for i in {6..8};do cp mysql-5.7.23/support-files/mysql.server /etc/init.d/mysqld330$i ;done //修改文件 sed -i.bak 's#/usr/local#/data/mysql3306#g' /etc/init.d/mysqld3306 /data/mysql3306/mysql/bin/mysqld_safe sed -i.bak 's#/usr/local#/data/mysql3307#g' /etc/init.d/mysqld3307 /data/mysql3307/mysql/bin/mysqld_safe sed -i.bak 's#/usr/local#/data/mysql3308#g' /etc/init.d/mysqld3308 /data/mysql3308/mysql/bin/mysqld_safe 3.11 初始化mysql //初始化第一个实例 /data/mysql3306/bin/mysqld --initialize-insecure --user=mysql --basedir=/data/mysql3306 --datadir=/data/mysql3306/data //初始化第二个实例 /data/mysql3306/bin/mysqld --initialize-insecure --user=mysql --basedir=/data/mysql3306 --datadir=/data/mysql3306/data //初始化第三个实例 /data/mysql3306/bin/mysqld --initialize-insecure --user=mysql --basedir=/data/mysql3306 --datadir=/data/mysql3306/data 3.12 添加mysql命令环境变量 //这里只需要导出一个即可 echo \"export PATH=/data/mysql3306/mysql/bin:$PATH\" > /etc/profile.d/mysql.sh source /etc/profile 3.13 配置systemd管理mysql //配置第一个实例 cat >/etc/systemd/system/mysqld3306.service/etc/systemd/system/mysqld3307.service/etc/systemd/system/mysqld3308.service 3.14 启动mysql、检查启动 //启动mysql systemctl start mysqld3306 ; systemctl enable mysqld3306 systemctl start mysqld3307 ; systemctl enable mysqld3307 systemctl start mysqld3308 ; systemctl enable mysqld3308 //检查启动 netstat -ntpl|grep 330* tcp6 0 0 :::3306 :::* LISTEN 16413/mysqld tcp6 0 0 :::3307 :::* LISTEN 16422/mysqld tcp6 0 0 :::3308 :::* LISTEN 16463/mysqld 3.15 进入mysql，设置密码 //设置第一个实例密码 mysql -S /data/mysql3306/mysql/mysql.sock mysql> set password=password('3306'); mysql> flush privileges; //设置第二个实例密码 mysql -S /data/mysql3307/mysql/mysql.sock mysql> set password=password('3307'); mysql> flush privileges; //设置第三个实例密码 mysql -S /data/mysql3308/mysql/mysql.sock mysql> set password=password('3308'); mysql> flush privileges; 3.16 设置快捷登陆 //原有登陆方式，需要指定mysql用户名密码和套接字文件 mysql -uroot -p3306 -S /data/mysql3306/mysql/mysql.sock 设置快捷登陆 //设置第一个实例 cat >/usr/bin/mysql3306/usr/bin/mysql3307/usr/bin/mysql3308 到此，mysql多实例配置完成！！！ 扩展：基于以上多实例实现mysql主从复制 3306为主 3307、3308为从 1.编辑主库3306（master）配置文件/etc/my-3306.cnf vim /etc/my-3306.cnf #[mysqld]下方增加以下3行 server_id=3306 log_bin=binlog log_bin_index=binlog.index //重启mysql systemctl restart mysqld3306 2.创建专用复制用户 mysql3306 mysql> grant replication slave on *.* to 'backup'@'10.0.0.%' identified by '3306'; Query OK, 0 rows affected (0.01 sec) 3.查看master状态 mysql> show master status; +---------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +---------------+----------+--------------+------------------+-------------------+ | binlog.000001 | 327 | | | | +---------------+----------+--------------+------------------+-------------------+ 1 row in set (0.00 sec) 4.编辑从库配置文件 //编辑3307配置文件 [root@mysql ~]# vim /etc/my-3307.cnf #[mysqld]下方增加以下3行 server_id=3307 relay_log=/data/mysql3307/mysql/relay_log relay_log_index=/data/mysql3307/mysql/relay_log.index //编辑3308配置文件 [root@mysql ~]# vim /etc/my-3308.cnf #[mysqld]下方增加以下3行 server_id=3308 relay_log=/data/mysql3308/mysql/relay_log relay_log_index=/data/mysql3308/mysql/relay_log.index //重启mysql [root@mysql ~]# systemctl restart mysqld3307 && systemctl restart mysqld3308 5.设置slave从master拉取binlog及拉取的位置 //3307 mysql> change master to master_host='10.0.0.55',master_port=3306,master_user='backup',master_password='123',master_log_file='binlog.000001',master_log_pos=327; Query OK, 0 rows affected, 2 warnings (0.06 sec) //启动slave mysql> start slave; Query OK, 0 rows affected (0.01 sec) //查看slave状态 mysql> show slave status\\G #IO线程和SQL线程都必须为YES Slave_IO_Running: Yes Slave_SQL_Running: Yes ------------------------------------------------------------------------------ //3308 mysql> change master to master_host='10.0.0.55',master_port=3306,master_user='backup',master_password='123',master_log_file='binlog.000001',master_log_pos=327; Query OK, 0 rows affected, 2 warnings (0.02 sec) //启动slave mysql> start slave; Query OK, 0 rows affected (0.00 sec) //查看slave状态 mysql> show slave status\\G #IO线程和SQL线程都必须为YES Slave_IO_Running: Yes Slave_SQL_Running: Yes 6.验证，在3306中创建一个数据库，看3307和3308是否会同步 //3306中创建一个数据库 mysql> create database bxb; Query OK, 1 row affected (0.00 sec) mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | DB3306 | | bxb | | mysql | | performance_schema | | test | +--------------------+ 6 rows in set (0.00 sec) //3307中查看数据库 mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | DB3307 | | bxb | | mysql | | performance_schema | | test | +--------------------+ 6 rows in set (0.00 sec) //3308中查看数据库 mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | DB3308 | | bxb | | mysql | | performance_schema | | test | +--------------------+ 6 rows in set (0.01 sec) 到此，mysql基于多实例实现主从复制完成！！！ 实验过程中遇到的错误 在3308（第二个实例）上启动slave报错 //启动slave报错，从存储库初始化中继日志信息结构失败 mysql> start slave; ERROR 1872 (HY000): Slave failed to initialize relay log info structure from the repository 解决方法：先重置slave,然后停止slvae再重新change master mysql> reset slave all; Query OK, 0 rows affected (0.02 sec) mysql> stop slave; 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql进阶/3.mysql普通主从复制.html":{"url":"db/mysql/mysql进阶/3.mysql普通主从复制.html","title":"普通主从复制","keywords":"","body":"[toc] mysql普通主从复制 1.mysql主从复制过程 1.master开启binlog（二进制）日志、授权slave复制用户，slave开启relay-log（中继）日志 2.slave IO线程会通过在master上已经授权的复制用户权限请求连接master服务器，带着change master to信息（user、host、password、binlog、binlog_pos、port）去问master dump线程有没有slave指定的binlog、binlog_pos,有则拉取binlog 3.IO线程拉取binlog后会先写入到TCP/IP缓存，TCP/IP缓存完成后会给IO线程返回ACK，告知IO线程缓存完成，然后再写入到relay-log中，relay-log写入完成后缓存就会清空，同时会把这一次拉取的binlog文件、binlog_pos记录到master.info中，以便于下一次去拉取的时候知道上一次拉取的位置 4.SQL线程会从relay-log中读取binlog解析成sql语句执行，同时会把上一次读取的relay-log位置记录到relay-log.info，以便于下一次读取的时候知道从什么位置读取，因为SQL线程从relay-log中读取binlog并不是一次全部读完的 官方示意图 2.实验环境 角色 IP 主机名 mysql版本 master 10.0.0.100 db01 5.7.22 slave 10.0.0.101 db02 5.7.22 3.实验过程 master10.0.0.100操作 1.master编辑/etc/my.cnf，指定serverid，并开启binlog和binlog索引 [root@db01 ~]# vim /etc/my.cnf #在[mysqld]下方写入以下3行 server_id=1 #指定serverid，越小优先级越大 log_bin=binlog #开启binlog日志 log_bin_index=binlog.index #开启binlog日志索引 #重启mysql [root@db01 ~]# systemctl restart mysqld 2.创建专用复制用户，允许从slave上连接过来的复制用户 mysql> grant replication slave on *.* to backup@'10.0.0.%' identified by '123'; 3.查看master当前的binlog日志及位置信息 mysql> show master status; +---------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +---------------+----------+--------------+------------------+-------------------+ | binlog.000001 | 154 | | | | +---------------+----------+--------------+------------------+-------------------+ 1 row in set (0.00 sec) slave10.0.0.101操作 1.slave编辑配置文件/etc/my.cnf，指定serverid，并开启中继日志 [root@db02 ~]# vim /etc/my.cnf #在[mysqld]下放写入以下3行 server_id=2 #指定serverid，要比mysql-master大 relay_log=/var/lib/mysql/relay_log #开启中继日志 relay_log_index=/var/lib/mysql/relay_log.index #开启中继日志索引 #重启mysql [root@db02 ~]# systemctl restart mysqld 2.设置slave从master拉取binlog，及拉取的位置 mysql> change master to master_host='10.0.0.100', \\ master_port=3306, \\ master_user='backup', \\ master_password='123', \\ master_log_file='binlog.000001', \\ master_log_pos=155; Query OK, 0 rows affected, 2 warnings (0.02 sec) mysql> show warnings; #会有警告，但不影响 | Note | 1759 | Sending passwords in plain text without SSL/TLS is extremely insecure. | Note | 1760 | Storing MySQL user name or password information in the master info repository is not secure and is therefore not recommended. Please consider using the USER and PASSWORD connection options for START SLAVE; see the 'START SLAVE Syntax' in the MySQL Manual for more information. | 没有SSL/TLS的纯文本发送密码是非常不安全的 在主信息存储库中存储MySQL用户名或密码信息是不安全的，因此不建议这样做。请考虑使用用户和密码连接选项启动从;有关更多信息，请参阅MySQL手册中的“开始从属语法”。 语句说明 change master to master_host='10.0.0.100', #mysql-master主机IP地址 master_port=3306, #mysql-master端口 master_user='backup', #slave拉取的用户 master_password='123456', #slave拉去的用户的密码 master_log_file='binlog.000001', #mysql-master的binlog文件，在master中show master status查看 master_log_pos=155; 3.启动slave并查看slave状态 //启动slave mysql> start slave; //查看slave状态，IO线程、SQL线程都为yes才算正确 mysql> show slave status\\G Slave_IO_Running: Yes Slave_SQL_Running: Yes 4.验证，在master上创建数据库和表，然后在slave上看是否可以同步 //master操作 mysql> show databases; #此时mysql-master上有4个库 +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | +--------------------+ 4 rows in set (0.01 sec) mysql> create database DB1; #创建一个数据库DB1 Query OK, 1 row affected (0.01 sec) //slave验证 mysql> show databases; #可以看到，在master上创建的数据库DB1已经同步至slave +--------------------+ | Database | +--------------------+ | information_schema | | DB1 | | mysql | | performance_schema | | sys | +--------------------+ 5 rows in set (0.01 sec) 4.延时从库 4.1 延时从库优点 1.误删除时，能更快恢复数据。 有时候手抖了，把线上数据给误删除了，或者误删除库、表、其他对象，或不加WHERE条件的更新、删除，都可以让延迟从库在误操作前的时间点停下，然后进行恢复。 2.把延迟从库作为专用的备份节点。虽然有一定的延迟，但并不影响利用该节点作为备份角色，也不影响生产节点数据库库。 3.还可以把延迟从库当做一些问题、案例研究的对象。个别时候，可能有些binlog event在普通从库上会有问题（例如早期版本中无主键会导致从库更新非常慢的经典问题），这时就有时间在延迟从库上慢慢琢磨研究了。 普通主从最大的缺点：主库误删除数据后从库上的数据也会被同步删除 4.2 配置延时从库 从库10.0.0.101操作 //停止主从 mysql>stop slave; //设置延时为60秒 mysql>change master to master_delay = 60; //开启主从 mysql>start slave; //查看状态 mysql> show slave status \\G SQL_Delay: 60 #延时从库停止方法 //停止主从 mysql> stop slave; //设置延时为0 mysql> CHANGE MASTER TO MASTER_DELAY = 0; //开启主从 mysql> start slave; 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql进阶/4.mysql-5.6.40 GTID主从复制.html":{"url":"db/mysql/mysql进阶/4.mysql-5.6.40 GTID主从复制.html","title":"GTID主从复制","keywords":"","body":"[toc] mysql-5.6.40 GTID主从复制 1.GTID主从复制简介 1.1 GTID主从复制概念 基于GTID的复制是从Mysql5.6开始支持的一种新的复制方式，此方式与传统基于日志的方式存在很大的差异，在原来的基于日志的复制中，从服务器连接到主服务器并告诉主服务器要从哪个二进制日志的偏移量开始执行增量同步，这时我们如果指定的日志偏移量不对，这可能造成主从数据的不一致，而基于GTID的复制会避免这种情况 1.2 GTID工作过程 ①首先从服务器会告诉主服务器已经在从服务器执行完了哪些事务的GTID值 ②主库会把所有没有在从库上执行的事务，发送到从库上进行执行，并且使用GTID的复制可以保证同一个事务只在指定的从库上执行一次，这样可以避免由于偏移量的问题造成数据不一致 1.3 GTID含义 全局事务ID，其保证为每一个在主服务器上提交的事务在复制集群中可以生成一个唯一的ID 1.4 GTID组成 UUID+TID UUID：是在mysql服务首次启动生成的，保存在数据库的数据目录中，在数据目录中有一个auto.conf文件，这个文件保存了UUDI值 TID：事务ID则是从1开始自增的序列，表示这个事务是在主库上执行的第几个事务，Mysql会保证这个事务和GTID是一比一的关系 2.实验环境 服务器角色 ip 主机名 master 10.0.0.10 db01 slave1 10.0.0.11 db02 slave2 10.0.0.12 db03 3.基于GTID主从前提条件 先决条件 1.主库和从库都要开启binlog 2.主库和从库server-id不同 3.要有主从复制用户 4.实验过程 主库10.0.0.10操作 1.修改配置文件 //编辑主库配置文件/etc/my.cnf，在[mysqld]下添加以下几行 [root@db01 ~]# vim /etc/my.cnf server_id=1 log_bin=mysql-bin gtid_mode=ON enforce_gtid_consistency log-slave-updates skip_name_resolve //重启mysql [root@db01 ~]# systemctl restart mysqld //主库查看git开启情况，enforce_gtid_consistency与gtid_mode都为on即为正确 mysql> show global variables like '%gtid%'; +---------------------------------+----------------------------------------+ | Variable_name | Value | +---------------------------------+----------------------------------------+ | binlog_gtid_simple_recovery | OFF | | enforce_gtid_consistency | ON | | gtid_executed | e257d15f-ed9f-11e8-ab08-000c29b62ed9:1 | | gtid_mode | ON | | gtid_owned | | | gtid_purged | | | simplified_binlog_gtid_recovery | OFF | +---------------------------------+----------------------------------------+ 7 rows in set (0.01 sec) #参数说明 server_id=1 #server id主库和从库要不一样 log_bin=mysql-bin #开启binlog gtid_mode=ON #开启gtid enforce_gtid_consistency #执行GTID一致 log-slave-updates #通常情况，从服务器从主服务器接收到的更新不记入它的二进制日志。 该选项告诉从服务器将其SQL线程执行的更新记入到从服务器自己的二进制日志，mysql-5.6必须加这个参数， 5.7可以不加 skip_name_resolve #跳过mysql域名解析 2.主库创建主从复制用户 mysql> grant replication slave on *.* to backup@'10.0.0.%' identified by '123'; Query OK, 0 rows affected (0.02 sec) 从库10.0.0.11、10.0.0.12操作 1.修改配置文件 //从库10.0.0.11修改配置文件/etc/my.cnf，server id要与主库不同 [mysqld] server_id=2 log-bin=mysql-bin gtid_mode=ON enforce_gtid_consistency log-slave-updates skip_name_resolve //重启mysql [root@db02 ~]# systemctl restart mysqld //查看gtid开启状态 mysql> show variables like '%gtid%'; +---------------------------------+-----------+ | Variable_name | Value | +---------------------------------+-----------+ | binlog_gtid_simple_recovery | OFF | | enforce_gtid_consistency | ON | | gtid_executed | | | gtid_mode | ON | | gtid_next | AUTOMATIC | | gtid_owned | | | gtid_purged | | | simplified_binlog_gtid_recovery | OFF | +---------------------------------+-----------+ 8 rows in set (0.00 sec) ------------------------------------------------------- //从库10.0.0.12修改配置文件/etc/my.cnf，server id要与主库不同 [mysqld] server_id=3 log-bin=mysql-bin gtid_mode=ON enforce_gtid_consistency log-slave-updates skip_name_resolve //重启mysql [root@db03 ~]# systemctl restart mysqld //查看gtid开启状态 mysql> show variables like '%gtid%'; +---------------------------------+-----------+ | Variable_name | Value | +---------------------------------+-----------+ | binlog_gtid_simple_recovery | OFF | | enforce_gtid_consistency | ON | | gtid_executed | | | gtid_mode | ON | | gtid_next | AUTOMATIC | | gtid_owned | | | gtid_purged | | | simplified_binlog_gtid_recovery | OFF | +---------------------------------+-----------+ 8 rows in set (0.00 sec) 2.从库拉取 //从库10.0.0.11执行拉取语句 mysql> change master to master_host='10.0.0.10', master_user='backup', master_password='123', master_auto_position=1; Query OK, 0 rows affected, 2 warnings (0.02 sec) //启动IO、SQL线程 mysql> start slave; //查看slave状态，IO、SQL线程都为yes，auto_position为yes即为成功 mysql> show slave status\\G Slave_IO_Running: Yes Slave_SQL_Running: Yes #以下为事务ID，1-3表明从库从主库同步了3次 Retrieved_Gtid_Set: e257d15f-ed9f-11e8-ab08-000c29b62ed9:1-3 Executed_Gtid_Set: e257d15f-ed9f-11e8-ab08-000c29b62ed9:1-3 ------------------------------------------------------------ //从库10.0.0.12执行拉取语句 mysql> change master to master_host='10.0.0.10', master_user='backup', master_password='123', master_auto_position=1; Query OK, 0 rows affected, 2 warnings (0.02 sec) //启动IO、SQL线程 mysql> start slave; //查看slave状态，IO、SQL线程都为yes，auto_position为yes即为成功 mysql> show slave status\\G Slave_IO_Running: YesSlave_SQL_Running: Yes Auto_Position: 1 #以下为事务ID，1-3表明从库从主库同步了3次 Retrieved_Gtid_Set: e257d15f-ed9f-11e8-ab08-000c29b62ed9:1-3 Executed_Gtid_Set: e257d15f-ed9f-11e8-ab08-000c29b62ed9:1-3 3.验证主从同步 //主库创建数据库 mysql> create database GTID; Query OK, 1 row affected (0.00 sec) //从库查看是否同步，可以看到GTID库已经同步过来 mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | GTID | | mysql | | performance_schema | | test | +--------------------+ 5 rows in set (0.00 sec) 以上过程同样适用于mysql-5.7 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql进阶/5.mysql5.7半同步复制.html":{"url":"db/mysql/mysql进阶/5.mysql5.7半同步复制.html","title":"半同步复制","keywords":"","body":"[toc] mysql5.7半同步复制 1.复制架构衍生史 在谈这个特性之前，我们先来看看MySQL的复制架构衍生史。 在2000年，MySQL 3.23.15版本引入了Replication。Replication作为一种准实时同步方式，得到广泛应用。这个时候的Replicaton的实现涉及到两个线程，一个在Master，一个在Slave。Slave的I/O和SQL功能是作为一个线程，从Master获取到event后直接apply，没有relay log。这种方式使得读取event的速度会被Slave replay速度拖慢，当主备存在较大延迟时候，会导致大量binary log没有备份到Slave端。 在2002年，MySQL 4.0.2版本将Slave端event读取和执行独立成两个线程（IO线程和SQL线程），同时引入了relay log。IO线程读取event后写入relay log，SQL线程从relay log中读取event然后执行。这样即使SQL线程执行慢，Master的binary log也会尽可能的同步到Slave。当Master宕机，切换到Slave，不会出现大量数据丢失。 在2010年MySQL 5.5版本之前，一直采用的是这种异步复制的方式。主库的事务执行不会管备库的同步进度，如果备库落后，主库不幸crash，那么就会导致数据丢失。于是在MySQL在5.5中就顺其自然地引入了半同步复制，主库在应答客户端提交的事务前需要保证至少一个从库接收并写到relay log中。那么半同步复制是否可以做到不丢失数据呢？下面分析。 在2016年，MySQL在5.7.17中引入了一个全新的技术，称之为InnoDB Group Replication。目前官方MySQL 5.7.17基于Group replication的全同步技术已经问世，全同步技术带来了更多的数据一致性保障。相信是未来同步技术一个重要方向，值得期待。MySQL 5.7 Group Replication 根据上面提到的这几种复制协议，分别对应MySQL几种复制类型，分别是异步、半同步、全同步。 对于异步复制，主库将事务Binlog事件写入到Binlog文件中，此时主库只会通知一下Dump线程发送这些新的Binlog，然后主库就会继续处理提交操作，而此时不会保证这些Binlog传到任何一个从库节点上。 对于全同步复制，当主库提交事务之后，所有的从库节点必须收到，APPLY并且提交这些事务，然后主库线程才能继续做后续操作。这里面有一个很明显的缺点就是，主库完成一个事务的时间被拉长，性能降低。 对于半同步复制，是介于全同步复制和异步复制之间的一种，主库只需要等待至少一个从库节点收到并且Flush Binlog到Relay Log文件即可，主库不需要等待所有从库给主库反馈。同时，这里只是一个收到的反馈，而不是已经完全执行并且提交的反馈，这样就节省了很多时间。 2.半同步复制技术 我们今天谈论第二种架构。我们知道，普通的replication，即MySQL的异步复制，依靠MySQL二进制日志也即binary log进行数据复制。比如两台机器，一台主机（master），另外一台是从机（slave）。 1）正常的复制为：事务一（t1）写入binlog buffer；dumper线程通知slave有新的事务t1；binlog buffer进行checkpoint；slave的io线程接收到t1并写入到自己的的relay log；slave的sql线程写入到本地数据库。 这时，master和slave都能看到这条新的事务，即使master挂了，slave可以提升为新的master。 2）异常的复制为：事务一（t1）写入binlog buffer；dumper线程通知slave有新的事务t1；binlog buffer进行checkpoint；slave因为网络不稳定，一直没有收到t1；master挂掉，slave提升为新的master，t1丢失。 3）很大的问题是：主机和从机事务更新的不同步，就算是没有网络或者其他系统的异常，当业务并发上来时，slave因为要顺序执行master批量事务，导致很大的延迟。 为了弥补以上几种场景的不足，MySQL从5.5开始推出了半同步复制。相比异步复制，半同步复制提高了数据完整性，因为很明确知道，在一个事务提交成功之后，这个事务就至少会存在于两个地方。即在master的dumper线程通知slave后，增加了一个ack（消息确认），即是否成功收到t1的标志码，也就是dumper线程除了发送t1到slave，还承担了接收slave的ack工作。如果出现异常，没有收到ack，那么将自动降级为普通的复制，直到异常修复后又会自动变为半同步复制。 半同步复制具体特性： 从库会在连接到主库时告诉主库，它是不是配置了半同步。 如果半同步复制在主库端是开启了的，并且至少有一个半同步复制的从库节点，那么此时主库的事务线程在提交时会被阻塞并等待，结果有两种可能，要么至少一个从库节点通知它已经收到了所有这个事务的Binlog事件，要么一直等待直到超过配置的某一个时间点为止，而此时，半同步复制将自动关闭，转换为异步复制。 从库节点只有在接收到某一个事务的所有Binlog，将其写入并Flush到Relay Log文件之后，才会通知对应主库上面的等待线程。 如果在等待过程中，等待时间已经超过了配置的超时时间，没有任何一个从节点通知当前事务，那么此时主库会自动转换为异步复制，当至少一个半同步从节点赶上来时，主库便会自动转换为半同步方式的复制。 半同步复制必须是在主库和从库两端都开启时才行，如果在主库上没打开，或者在主库上开启了而在从库上没有开启，主库都会使用异步方式复制。 半同步复制潜在问题： 先看一下半同步复制原理图，如下： master将每个事务写入binlog（sync_binlog=1），传递到slave刷新到磁盘(sync_relay=1)，同时主库提交事务（commit）。master等待slave反馈收到relay log，只有收到ACK后master才将commit OK结果反馈给客户端。 在MySQL 5.5~5.6使用after_commit的模式下，客户端事务在存储引擎层提交后，在得到从库确认的过程中，主库宕机了。此时，即主库在等待Slave ACK的时候，虽然没有返回当前客户端，但事务已经提交，其他客户端会读取到已提交事务。如果Slave端还没有读到该事务的events，同时主库发生了crash，然后切换到备库。那么之前读到的事务就不见了，出现了幻读。如下图所示，图片引自Loss-less Semi-Synchronous Replication on MySQL 5.7.2。 如果主库永远启动不了，那么实际上在主库已经成功提交的事务，在从库上是找不到的，也就是数据丢失了，这是MySQL不愿意看到的。所以在MySQL 5.7版本中增加了after_sync（无损复制）参数，并将其设置为默认半同步方式，解决了数据丢失的问题 3.MySQL 5.6半同步复制配置 半同步复制的前提是已经做好了普通的主从复制 Master配置 3.1 master安装半同步模块并启动 此模块就在/usr/local/mysql/lib/plugin/semisync_master.so 1.安装半同步模块 mysql> install plugin rpl_semi_sync_master soname 'semisync_master.so'; Query OK, 0 rows affected (0.00 sec) 2.启动插件 mysql> set global rpl_semi_sync_master_enabled = 1; Query OK, 0 rows affected (0.00 sec) 3.设置超时时间，单位是毫秒，默认为10000毫秒即10秒 mysql> set global rpl_semi_sync_master_timeout = 2000; Query OK, 0 rows affected (0.00 sec) 4.查看配置 mysql> show global variables like '%semi%'; +------------------------------------+-------+ | Variable_name | Value | +------------------------------------+-------+ | rpl_semi_sync_master_enabled | ON | #半同步开启 | rpl_semi_sync_master_timeout | 2000 | #超时时间为2秒 | rpl_semi_sync_master_trace_level | 32 | | rpl_semi_sync_master_wait_no_slave | ON | +------------------------------------+-------+ 4 rows in set (0.00 sec) 5.写入mysql配置文件，永久生效，在[mysqld]下写入以下两行内容 rpl_semi_sync_master_enabled = 1; rpl_semi_sync_master_timeout = 2000; 安装后启动和定制主从连接错误的超时时间默认是10s，可改为2s，一旦有一次超时自动降级为异步 slave配置 3.2 slave安装半同步模块并启动 1.安装半同步模块 mysql> install plugin rpl_semi_sync_slave soname 'semisync_slave.so'; Query OK, 0 rows affected (0.01 sec) 2.启动插件 mysql> set global rpl_semi_sync_slave_enabled = 1; Query OK, 0 rows affected (0.00 sec) 3.重启IO线程 mysql> stop slave io_thread; Query OK, 0 rows affected (0.00 sec) mysql> start slave io_thread; Query OK, 0 rows affected (0.00 sec) 4.在master上查看是否启用了半同步 mysql> show global status like 'rpl%'; +--------------------------------------------+-------+ | Variable_name | Value | +--------------------------------------------+-------+ | Rpl_semi_sync_master_clients | 1 | #为1表明开启 | Rpl_semi_sync_master_net_avg_wait_time | 0 | | Rpl_semi_sync_master_net_wait_time | 0 | | Rpl_semi_sync_master_net_waits | 0 | | Rpl_semi_sync_master_no_times | 0 | | Rpl_semi_sync_master_no_tx | 0 | | Rpl_semi_sync_master_status | ON | #为ON表明开启 | Rpl_semi_sync_master_timefunc_failures | 0 | | Rpl_semi_sync_master_tx_avg_wait_time | 0 | | Rpl_semi_sync_master_tx_wait_time | 0 | | Rpl_semi_sync_master_tx_waits | 0 | | Rpl_semi_sync_master_wait_pos_backtraverse | 0 | | Rpl_semi_sync_master_wait_sessions | 0 | | Rpl_semi_sync_master_yes_tx | 0 | +--------------------------------------------+-------+ 14 rows in set (0.00 sec) 现在半同步已经正常工作了，主要看Rpl_semi_sync_master_clients是否不为0，Rpl_semi_sync_master_status是否为ON。如果Rpl_semi_sync_master_status为OFF，说明出现了网络延迟或Slave IO线程延迟。 3.3 验证半同步超时 slave上关闭半同步并重启IO线程 mysql> set global rpl_semi_sync_slave_enabled = 0 ; Query OK, 0 rows affected (0.00 sec) mysql> stop slave io_thread; Query OK, 0 rows affected (0.00 sec) mysql> start slave io_thread; Query OK, 0 rows affected (0.00 sec) master上创建数据库验证 //master创建数据库 mysql> create database dbtest; Query OK, 1 row affected (2.00 sec) mysql> create database dbtest01; Query OK, 1 row affected (0.00 sec) 创建第一个数据库花了2.00秒，而我们前面设置的超时时间是2秒，而创建第二个数据库花了0.00秒，由此得出结论是超时转换为异步传送。 可以在Master上查看半同步相关的参数值Rpl_semi_sync_master_clients和Rpl_semi_sync_master_status是否正常。 mysql> show global status like '%semi%'; +--------------------------------------------+-------+ | Variable_name | Value | +--------------------------------------------+-------+ | Rpl_semi_sync_master_clients | 0 | #为0表示关闭半同步 | Rpl_semi_sync_master_net_avg_wait_time | 0 | | Rpl_semi_sync_master_net_wait_time | 0 | | Rpl_semi_sync_master_net_waits | 0 | | Rpl_semi_sync_master_no_times | 1 | | Rpl_semi_sync_master_no_tx | 2 | | Rpl_semi_sync_master_status | OFF | #OFF表示关闭半同步 | Rpl_semi_sync_master_timefunc_failures | 0 | | Rpl_semi_sync_master_tx_avg_wait_time | 0 | | Rpl_semi_sync_master_tx_wait_time | 0 | | Rpl_semi_sync_master_tx_waits | 0 | | Rpl_semi_sync_master_wait_pos_backtraverse | 0 | | Rpl_semi_sync_master_wait_sessions | 0 | | Rpl_semi_sync_master_yes_tx | 0 | +--------------------------------------------+-------+ 14 rows in set (0.00 sec) 可以看到都自动关闭了，需要注意一点的是，当Slave开启半同步后，或者当主从之间网络延迟恢复正常的时候，半同步复制会自动从异步复制又转为半同步复制，还是相当智能的。 另外个人在实际使用中还碰到一种情况从库IO线程有延迟时，主库会自动把半同步复制降为异步复制；当从库IO延迟没有时，主库又会把异步复制升级为半同步复制。可以进行压测模拟，但是此时查看Master的状态跟上面直接关闭Slave半同步有些不同，会发现Rpl_semi_sync_master_clients仍然等于1，而Rpl_semi_sync_master_status等于OFF。 随着MySQL 5.7版本的发布，半同步复制技术升级为全新的Loss-less Semi-Synchronous Replication架构，其成熟度、数据一致性与执行效率得到显著的提升。 4.MySQL 5.7半同步复制的改进 现在我们已经知道，在半同步环境下，主库是在事务提交之后等待Slave ACK，所以才会有数据不一致问题。所以这个Slave ACK在什么时间去等待，也是一个很关键的问题了。因此MySQL针对半同步复制的问题，在5.7.2引入了Loss-less Semi-Synchronous，在调用binlog sync之后，engine层commit之前等待Slave ACK。这样只有在确认Slave收到事务events后，事务才会提交。在commit之前等待Slave ACK，同时可以堆积事务，利于group commit，有利于提升性能。 4.1 master安装半同步模块并启动 master配置 1.安装插件 mysql> install plugin rpl_semi_sync_master soname 'semisync_master.so'; Query OK, 0 rows affected (0.00 sec) 2.查看半同步信息，可以看到默认为10000毫秒即10秒 mysql> show global variables like '%semi%'; +-------------------------------------------+------------+ | Variable_name | Value | +-------------------------------------------+------------+ | rpl_semi_sync_master_enabled | OFF | | rpl_semi_sync_master_timeout | 10000 | | rpl_semi_sync_master_trace_level | 32 | | rpl_semi_sync_master_wait_for_slave_count | 1 | | rpl_semi_sync_master_wait_no_slave | ON | | rpl_semi_sync_master_wait_point | AFTER_SYNC | +-------------------------------------------+------------+ 6 rows in set (0.00 sec) 3.启动插件 mysql> set global rpl_semi_sync_master_enabled = 1; Query OK, 0 rows affected (0.00 sec) 4.设置超时时间为1秒 mysql> set global rpl_semi_sync_master_timeout = 1000; Query OK, 0 rows affected (0.00 sec) 5.查看配置 mysql> show global variables like '%semi%'; +-------------------------------------------+------------+ | Variable_name | Value | +-------------------------------------------+------------+ | rpl_semi_sync_master_enabled | ON | | rpl_semi_sync_master_timeout | 1000 | | rpl_semi_sync_master_trace_level | 32 | | rpl_semi_sync_master_wait_for_slave_count | 1 | | rpl_semi_sync_master_wait_no_slave | ON | | rpl_semi_sync_master_wait_point | AFTER_SYNC | +-------------------------------------------+------------+ 6 rows in set (0.00 sec) 6.写入mysql配置文件，永久生效，在[mysqld]下写入以下两行内容 rpl_semi_sync_master_enabled = 1; rpl_semi_sync_master_timeout = 1000; 4.2 slave安装半同步模块并启动 1.安装半同步模块 mysql> install plugin rpl_semi_sync_slave soname 'semisync_slave.so'; Query OK, 0 rows affected (0.01 sec) 2.启动插件 mysql> set global rpl_semi_sync_slave_enabled = 1; Query OK, 0 rows affected (0.00 sec) 3.重启IO线程 mysql> stop slave io_thread; Query OK, 0 rows affected (0.00 sec) mysql> start slave io_thread; Query OK, 0 rows affected (0.00 sec) 4.在master上查看是否启用了半同步 mysql> show global status like 'rpl%'; +----------------------------+-------+ | Variable_name | Value | +----------------------------+-------+ | Rpl_semi_sync_slave_status | ON | +----------------------------+-------+ 1 row in set (0.00 sec) 4.3 验证半同步超时 slave上关闭半同步并重启IO线程 mysql> set global rpl_semi_sync_slave_enabled = 0 ; Query OK, 0 rows affected (0.00 sec) mysql> stop slave io_thread; Query OK, 0 rows affected (0.00 sec) mysql> start slave io_thread; Query OK, 0 rows affected (0.00 sec) master上创建数据库验证 //master创建数据库 mysql> create database dbtest; Query OK, 1 row affected (1.00 sec) mysql> create database dbtest01; Query OK, 1 row affected (0.00 sec) 创建第一个数据库花了1.00秒，而我们前面设置的超时时间是1秒，而创建第二个数据库花了0.00秒，由此得出结论是超时转换为异步传送。 可以在Master上查看半同步相关的参数值Rpl_semi_sync_master_clients和Rpl_semi_sync_master_status是否正常。 mysql> show global status like '%semi%'; +--------------------------------------------+-------+ | Variable_name | Value | +--------------------------------------------+-------+ | Rpl_semi_sync_master_clients | 0 | #为0表示关闭半同步 | Rpl_semi_sync_master_net_avg_wait_time | 0 | | Rpl_semi_sync_master_net_wait_time | 0 | | Rpl_semi_sync_master_net_waits | 0 | | Rpl_semi_sync_master_no_times | 1 | | Rpl_semi_sync_master_no_tx | 2 | | Rpl_semi_sync_master_status | OFF | #OFF表示关闭半同步 | Rpl_semi_sync_master_timefunc_failures | 0 | | Rpl_semi_sync_master_tx_avg_wait_time | 0 | | Rpl_semi_sync_master_tx_wait_time | 0 | | Rpl_semi_sync_master_tx_waits | 0 | | Rpl_semi_sync_master_wait_pos_backtraverse | 0 | | Rpl_semi_sync_master_wait_sessions | 0 | | Rpl_semi_sync_master_yes_tx | 0 | +--------------------------------------------+-------+ 14 rows in set (0.00 sec) 可以看到都自动关闭了 4.4 支持无损复制(Loss-less Semi-Synchronous) 在Loss-less Semi-Synchronous模式下，master在调用binlog sync之后，engine层commit之前等待Slave ACK（需要收到至少一个Slave节点回复的ACK后）。这样只有在确认Slave收到事务events后，master事务才会提交，然后把结果返回给客户端。此时此事务才对其他事务可见。在这种模式下解决了after_commit模式带来的幻读和数据丢失问题，因为主库没有提交事务。但也会有个问题，假设主库在存储引擎提交之前挂了，那么很明显这个事务是不成功的，但由于对应的Binlog已经做了Sync操作，从库已经收到了这些Binlog，并且执行成功，相当于在从库上多了数据，也算是有问题的，但多了数据，问题一般不算严重。这个问题可以这样理解，作为MySQL，在没办法解决分布式数据一致性问题的情况下，它能保证的是不丢数据，多了数据总比丢数据要好。 无损复制其实就是对semi sync增加了rpl_semi_sync_master_wait_point参数，来控制半同步模式下主库在返回给会话事务成功之前提交事务的方式。rpl_semi_sync_master_wait_point该参数有两个值：AFTER_COMMIT和AFTER_SYNC 4.4.1 第一个值：AFTER_COMMIT（5.6默认值） master将每个事务写入binlog（sync_binlog=1），传递到slave刷新到磁盘(sync_relay=1)，同时主库提交事务。master等待slave反馈收到relay log，只有收到ACK后master才将commit OK结果反馈给客户端。 4.4.2 第二个值：AFTER_SYNC（5.7默认值，但5.6中无此模式） master将每个事务写入binlog , 传递到slave刷新到磁盘(relay log)。master等待slave反馈接收到relay log的ack之后，再提交事务并且返回commit OK结果给客户端。 即使主库crash，所有在主库上已经提交的事务都能保证已经同步到slave的relay log中。 4.5 半同步复制与无损复制的对比 4.5.1 ACK的时间点不同 半同步复制在InnoDB层的Commit Log后等待ACK，主从切换会有数据丢失风险。 无损复制在MySQL Server层的Write binlog后等待ACK，主从切换会有数据变多风险。 4.5.2 主从数据一致性 半同步复制意味着在Master节点上，这个刚刚提交的事物对数据库的修改，对其他事物是可见的。因此，如果在等待Slave ACK的时候crash了，那么会对其他事务出现幻读，数据丢失。 无损复制在write binlog完成后，就传输binlog，但还没有去写commit log，意味着当前这个事物对数据库的修改，其他事物也是不可见的。因此，不会出现幻读，数据丢失风险。 因此5.7引入了无损复制（after_sync）模式，带来的主要收益是解决after_commit导致的master crash后数据丢失问题，因此在引入after_sync模式后，所有提交的数据已经都被复制，故障切换时数据一致性将得到提升。 性能提升，支持发送binlog和接受ack的异步化 旧版本的semi sync受限于dump thread ，原因是dump thread承担了两份不同且又十分频繁的任务：传送binlog给slave ，还需要等待slave反馈信息，而且这两个任务是串行的，dump thread必须等待slave返回之后才会传送下一个events事务。dump thread已然成为整个半同步提高性能的瓶颈。在高并发业务场景下，这样的机制会影响数据库整体的TPS 。 为了解决上述问题，在5.7版本的semi sync框架中，独立出一个Ack Receiver线程 ，专门用于接收slave返回的ack请求，这将之前dump线程的发送和接受工作分为了两个线程来处理。这样master上有两个线程独立工作，可以同时发送binlog到slave，和接收slave的ack信息。因此半同步复制得到了极大的性能提升。这也是MySQL 5.7发布时号称的Faster semi-sync replication。 但是在MySQL 5.7.17之前，这个Ack Receiver线程采用了select机制来监听slave返回的结果，然而select机制监控的文件句柄只能是0-1024，当超过1024时，用户在MySQL的错误日志中或许会收到类似如下的报错，更有甚者会导致MySQL发生宕机。 semi-sync master failed on net_flush() before waiting for slave reply. MySQL 5.7.17版本开始，官方修复了这个bug，开始使用poll机制来替换原来的select机制，从而可以避免上面的问题。其实poll调用本质上和select没有区别，只是在I/O句柄数理论上没有上限了，原因是它是基于链表来存储的。但是同样有缺点：比如大量的fd的数组被整体复制于用户态和内核地址空间之间，而不管这样的复制是不是有意义。poll还有一个特点是“水平触发”，如果报告了fd后，没有被处理，那么下次poll时会再次报告该fd。 其实在高性能软件中都是用另外一种调用机制，名为epoll，高性能的代表，比如Nginx，haproxy等都是使用epoll。可能poll的复杂性比epoll低，另外对于ack receiver线程来说可能poll足矣。 性能提升，控制主库接收slave写事务成功反馈数量 MySQL 5.7新增了rpl_semi_sync_master_wait_slave_count参数，可以用来控制主库接受多少个slave写事务成功反馈，给高可用架构切换提供了灵活性。如图所示，当count值为2时，master需等待两个slave的ack。 性能提升，Binlog互斥锁改进 旧版本半同步复制在主提交binlog的写会话和dump thread读binlog的操作都会对binlog添加互斥锁，导致binlog文件的读写是串行化的，存在并发度的问题。 MySQL 5.7对binlog lock进行了以下两方面优化: 1.移除了dump thread对binlog的互斥锁。 2.加入了安全边际保证binlog的读安全。 可以看到从replication功能引入后，官方MySQL一直在不停的完善，前进。同时我们可以发现当前原生的MySQL主备复制实现实际上很难在满足数据一致性的前提下做到高可用、高性能。 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql进阶/6.MHA基于普通主从复制高可用.html":{"url":"db/mysql/mysql进阶/6.MHA基于普通主从复制高可用.html","title":"MHA基于普通主从复制高可用","keywords":"","body":"[toc] MHA基于普通主从复制高可用 MHA gIthub地址 MHA github wiki地址 1.MHA简介 MHA通常在10到30秒内以最少的停机时间执行自动的主故障转移和从升级。MHA可以防止复制一致性问题，并节省了必须购买其他服务器的费用。所有这些都具有零性能下降，无复杂性（易于安装）并且无需更改现有部署的情况。 MHA还提供了计划的在线主设备切换，可以在停机（仅阻止写入）的几秒钟（0.5-2秒）内将当前正在运行的主设备安全地更改为新的主设备。 MHA提供以下功能，在需要高可用性，数据完整性和近乎不间断的主维护的许多部署中很有用。 自动化的主站监视和故障转移 MHA可以监视现有复制环境中的MySQL主服务器，并在检测到主服务器故障时执行自动主服务器故障转移。MHA通过识别来自最新从站的差分中继日志事件并将其应用于所有其他从站，包括那些尚未收到最新中继日志事件的从站，来保证所有从站的一致性。MHA通常可以在几秒钟内执行故障转移：9到12秒钟用于检测主设备故障，可选地7到10秒钟用于关闭主计算机电源，以避免脑部分裂；几秒钟的时间将差分中继日志应用于新的主设备。总停机时间通常为10-30秒。可以在配置文件中将特定从站指定为候选主站（设置优先级）。由于MHA维护从站之间的一致性，任何奴隶都可以晋升为新主人。通常不会导致突然的复制失败的一致性问题将不会发生。 交互式（手动启动）主故障转移 可以将MHA配置为手动启动（非自动），交互式故障转移，而无需监视主服务器。 非交互式主服务器故障转移 还支持不监视主服务器的非交互式自动主服务器故障转移。当已经使用MySQL主软件监视时，此功能特别有用。例如，您可以使用Pacemaker（Heartbeat）检测主服务器故障和虚拟IP地址接管，而使用MHA进行主服务器故障转移和从属升级。 在线将主服务器切换到其他主机 通常有必要将现有的主服务器迁移到另一台计算机上，例如当前主服务器存在硬件RAID控制器或RAM问题，或者要用速度更快的计算机替换它等时，这不是主服务器崩溃，但需要定期进行主维护。计划的主机维护应尽快完成，因为这会导致部分停机（禁用主机写入）。另一方面，您应该非常仔细地阻止/杀死当前正在运行的会话，因为不同的master之间可能会发生一致性问题（即“更新master1，更新master 2，提交master1，在提交master 2时出错”会导致数据不一致）。快速主开关和平稳阻止写入都是必需的。 MHA在写入器阻塞后的0.5-2秒内提供正常的主设备切换。通常可以接受0.5-2秒的写入器停机时间，因此即使不分配计划的维护时段，您也可以切换主机。升级到更高版本，更快的计算机等操作变得更加容易。 2.MHA架构 MHA架构官方文档 正常工作时架构 主库down机时架构 该软件由两部分组成：MHA Manager（管理节点）和MHA Node（数据节点）。 MHA Manager可以单独部署在一台独立的机器上管理多个master-slave集群，也可以部署在一台slave节点上。 MHA Node运行在每台MySQL服务器上，MHA Manager会定时探测集群中的master节点，当master出现故障时，它可以自动将最新数据的slave提升为新的master，然后将所有其他的slave重新指向新的master。整个故障转移过程对应用程序完全透明。 在MHA自动故障切换过程中，MHA试图从宕机的主服务器上保存二进制日志，最大程度的保证数据的不丢失(配合mysql半同步复制效果更佳)，但这并不总是可行的。例如，如果主服务器硬件故障或无法通过ssh访问，MHA没法保存二进制日志，只进行故障转移而丢失了最新的数据。使用MySQL 5.5的半同步复制，可以大大降低数据丢失的风险。MHA可以与半同步复制结合起来。如果只有一个slave已经收到了最新的二进制日志，MHA可以将最新的二进制日志应用于其他所有的slave服务器上，因此可以保证所有节点的数据一致性。 注意：目前MHA主要支持一主多从的架构，要搭建MHA,要求一个复制集群中必须最少有三台数据库服务器，一主二从，即一台充当master，一台充当备用master，另外一台充当从库，因为至少需要三台服务器。 工作过程：manager节点定时检测复制集群中的每一个node,如果master宕机会选出数据与master最接近的一台slave来作为新master，去到旧master中拷贝binlog到新master中并应用binlog以保证新master与旧master数据一致，然后将剩余的slave重新changer master to,将slave的主切换为新master 3.部署过程 实验环境 角色 IP 主机名 系统 机器配置 mysql版本 manager 10.0.0.130 mha centos7.8 2c4g 5.7.28 mysql-master 10.0.0.133 mysql01 centos7.8 2c4g 5.7.28 mysql-slave1 10.0.0.134 mysql02 centos7.8 2c4g 5.7.28 mysql-slave2 10.0.0.135 mysql03 centos7.8 2c4g 5.7.28 3.1 下载安装包 MHA manager0.58下载地址 MHA node0.58下载地址 MHA 0.56 下载地址 Manager工具包主要包括以下几个工具： 名称 含义 masterha_check_ssh 检查MHA的SSH配置状况 masterha_check_repl 检查MySQL复制状况 masterha_manger 启动MHA masterha_check_status 检测当前MHA运行状态 masterha_master_monitor 检测master是否宕机 masterha_master_switch 控制故障转移（自动或者手动） masterha_conf_host 添加或删除配置的server信息 Node工具包（这些工具通常由MHA Manager的脚本触发，无需人为操作）主要包括以下几个工具： 名称 含义 save_binary_logs 保存和复制master的二进制日志 apply_diff_relay_logs 识别差异的中继日志事件并将其差异的事件应用于其他的slave filter_mysqlbinlog 去除不必要的ROLLBACK事件（MHA已不再使用这个工具） purge_relay_logs 清除中继日志（不会阻塞SQL线程） 3.2 所有机器做相互免密钥配置 ⚠️复制集群中的每个节点都有可能成为master，都得开启binlog和ssh密钥认证，因为当旧master宕机后，mha要拷贝binlog到所有node节点上，而且所有节点都有可能成为master，故每个节点都要彼此密钥认证 lowB脚本运行一下 #!/bin/bash file_path=/root file_name=host.txt file=$file_path/$file_name user=root pwd=1 ssh_file=~/.ssh/id_rsa yum -y install expect &> /dev/null # 生成密钥 [ -f \"$ssh_file\" ] || ssh-keygen -q -N \"\" -f ~/.ssh/id_rsa read -p \"请输入开始数值: \" start_num read -p \"请输入结束数值: \" end_num read -p \"请输入网段(类似格式：10.0.0.)：\" sub_net seq $start_num $end_num >$file # 在文件开头加上网段，在文件末尾加上用户名和密码 sed -i 's/^/'$sub_net'/g' $file sed -i 's/$/ '$user' '$pwd'/g' $file echo -e \"执行成功，文件内容如下:\\n`cat $file`\" while read line;do ip=`echo $line | awk '{print $1}'` username=`echo $line | awk '{print $2}'` password=`echo $line | awk '{print $3}'` expect master操作(mysql01 10.0.0.133) 3.3 mysql主从配置 这里是把10.0.0.133(mysql01)、10.0.0.134(mysql02)、10.0.0.135(mysql03)搭建为ABB，即一主两从，其中mysql01是主库，其余两个节点是从库 3.3.1 编辑/etc/my.cnf 指定serverid，并开启binlog # 指定serverid，越小优先级越大 server_id=1 # 开启binlog日志，位置在mysql数据目录data下 log_bin=binlog # 开启binlog日志索引，位置在mysql数据目录data下 log_bin_index=binlog.index # 开启半同步复制 log_slave_updates=1 重启mysql systemctl restart mysqld 3.3.2 创建专用复制用户和设置监控用户 允许从slave上连接过来的复制用户，3台mysql服务器都要创建复制用户和监控用户！！！ ⚠️复制用户名称为repl，否在在后续检测mysql集群连接情况会报错，也可以在配置文件中修改复制用户 创建专用复制用户 mysql> grant replication slave on *.* to 'repl'@'10.0.0.%' identified by 'Bxb123.com'; Query OK, 0 rows affected, 1 warning (0.00 sec) 设置监控用户 ⚠️所有节点进行授权 mysql> grant all privileges on *.* to 'mha'@'10.0.0.%' identified by 'Bxb123.com'; Query OK, 0 rows affected, 1 warning (0.00 sec) mysql5.7会默认加载validate_password 模块，是来控制密码长度和规则的，可以在配置文件里面关闭该模块加上validate_password = off，或者在mysql命令行执行set global validate_password_policy=0;来临时取消密码规则 禁用自动删除relay log功能 set global relay_log_purge=0; 查看master当前的binlog日志及位置信息 mysql> show master status; slave操作(mysql02、03 10.0.0.134、135) 3.3.3 配置文件 /etc/my.cnf 指定serverid，并开启binlog日志 # 指定serverid，要比mysql-master大，slave1设置为2，slave2设置为3 server_id=2 # 开启binlog日志，位置在mysql数据目录data下 log_bin=binlog # 开启binlog日志索引，位置在mysql数据目录data下 log_bin_index=binlog.index # 开启半同步复制 否则自动切换主从的时候会报主键错误 log_slave_updates = 1 重启mysql systemctl restart mysqld 3.3.4 设置slave从master拉取binlog，及拉取的位置 # 设置只读，不要在配置文件里写 mysql> set global read_only=1; Query OK, 0 rows affected (0.00 sec) # 禁用自动删除relay log set global relay_log_purge=0; # 拉取binlog change master to master_host='10.0.0.133', \\ master_port=3306, \\ master_user='repl', \\ master_password='Bxb123.com', \\ master_log_file='binlog.000001', \\ master_log_pos=155; Query OK, 0 rows affected, 2 warnings (0.01 sec) 启动slave并查看slave状态 # 启动slave mysql> start slave; Query OK, 0 rows affected (0.00 sec) # 查看slave状态，SQL和IO线程都为Yes即为正确 Slave_IO_Running: Yes Slave_SQL_Running: Yes # 这个值是主从延迟，即slave落后master的秒数，也是一个比较重要的查看主从是否正常的标准值 Seconds_Behind_Master: 0 3.3.5 验证主从同步 master操作 # 在主库上创建一个数据库db1 mysql> create database db1; Query OK, 1 row affected (0.00 sec) slave操作 查看主库上创建的数据库是否已经同步过来，同步完成即为成功 mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | db1 | | mysql | | performance_schema | | sys | +--------------------+ 5 rows in set (0.00 sec) 3.4 部署MHA node 所有节点执行 # 下载包 wget https://github.com/yoshinorim/mha4mysql-node/releases/download/v0.58/mha4mysql-node-0.58.tar.gz # 编译安装 $ cd mha4mysql-node-0.58 $ ls AUTHORS bin COPYING debian inc lib Makefile.PL MANIFEST META.yml README rpm t $ perl Makefile.PL $ make && make install 安装完成后拷贝mha4mysql-node-0.58/bin下的所有可执行文件到/usr/local/bin apply_diff_relay_logs #识别差异日志并应用于其他slave save_binary_logs #保存和复制二进制日志 filter_mysqlbinlog #去除不必要的ROLLBACK事件（MHA已不再使用这个工具） purge_relay_logs #清除中继日志 manager操作(mha 10.0.0.130) 3.5 部署MHA Manager 3.5.1 增加epel源 wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo 3.5.2 安装依赖包 yum -y install perl-DBD-MySQL perl-Config-Tiny perl-Log-Dispatch perl-Parallel-ForkManager perl-ExtUtils-CBuilder perl-ExtUtils-MakeMaker perl-ExtUtils-Embed perl-CPAN 3.5.3 安装manager # 下载包 wget https://github.com/yoshinorim/mha4mysql-manager/releases/download/v0.58/mha4mysql-manager-0.58.tar.gz # 编译安装 $ cd cd mha4mysql-manager-0.58 $ ls AUTHORS bin COPYING debian inc lib Makefile.PL MANIFEST META.yml README rpm samples t tests $ perl Makefile.PL $ make && make install 安装完成后会拷贝mha4mysql-manager-0.58/bin下的所有可执行文件到/usr/local/bin 可执行文件 含义 masterha_manager 在主服务器关闭的情况下，主服务器自动监视和运行故障转移 masterha_master_switch 手动或非交互式主故障转移或在线主库 masterha_master_monitor MHA Manager监控程序 masterha_stop 停止MHA Manager masterha_check_repl 检查MySQL复制运行状况 masterha_check_status 检查Manager是否正确监视MySQL master masterha_check_ssh 检查ssh配置 masterha_conf_host 一个帮助程序脚本，用于从配置文件添加/删除主机条目 masterha_secondary_check MHA Manager二次检查 3.5.4 mysql主库节点配置eth0:0网卡，用作VIP，这里设置为10.0.0.200 这一步在mysql主库上(10.0.0.133)执行 $ ifconfig eth0:0 10.0.0.200 $ ip a s eth0 eth0: mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:1c:42:62:a8:c1 brd ff:ff:ff:ff:ff:ff inet 10.0.0.133/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever inet 10.0.0.200/8 brd 10.255.255.255 scope global eth0:0 valid_lft forever preferred_lft forever inet6 fe80::21c:42ff:fe62:a8c1/64 scope link valid_lft forever preferred_lft forever 3.5.5 使用脚本管理vip ⚠️需要修改的是VIP的地址和网卡的名称 VIP是绑定在主库上的，这样当主库宕机后VIP才会漂移到新主库上 cat > /usr/local/bin/master_ip_failover 'all'; use Getopt::Long; my ( $command, $ssh_user, $orig_master_host, $orig_master_ip, $orig_master_port, $new_master_host, $new_master_ip, $new_master_port ); # 这里需要注意一下 eth1:$key的意思是 vip必须绑定在 eth0:0上 my $vip = '10.0.0.200/24'; #此处为你要设置的虚拟ip my $key = '0'; my $ssh_start_vip = \"/usr//sbin/ifconfig eth0:$key $vip\"; #此处改为你的网卡名称 my $ssh_stop_vip = \"/usr/sbin/ifconfig eth0:$key down\"; GetOptions( 'command=s' => \\$command, 'ssh_user=s' => \\$ssh_user, 'orig_master_host=s' => \\$orig_master_host, 'orig_master_ip=s' => \\$orig_master_ip, 'orig_master_port=i' => \\$orig_master_port, 'new_master_host=s' => \\$new_master_host, 'new_master_ip=s' => \\$new_master_ip, 'new_master_port=i' => \\$new_master_port, ); exit &main(); sub main { print \"\\n\\nIN SCRIPT TEST====$ssh_stop_vip==$ssh_start_vip===\\n\\n\"; if ( $command eq \"stop\" || $command eq \"stopssh\" ) { my $exit_code = 1; eval { print \"Disabling the VIP on old master: $orig_master_host \\n\"; &stop_vip(); $exit_code = 0; }; if ($@) { warn \"Got Error: $@\\n\"; exit $exit_code; } exit $exit_code; } elsif ( $command eq \"start\" ) { my $exit_code = 10; eval { print \"Enabling the VIP - $vip on the new master - $new_master_host \\n\"; &start_vip(); $exit_code = 0; }; if ($@) { warn $@; exit $exit_code; } exit $exit_code; } elsif ( $command eq \"status\" ) { print \"Checking the Status of the script.. OK \\n\"; exit 0; } else { &usage(); exit 1; } } sub start_vip() { `ssh $ssh_user\\@$new_master_host \\\" $ssh_start_vip \\\"`; } sub stop_vip() { return 0 unless ($ssh_user); `ssh $ssh_user\\@$orig_master_host \\\" $ssh_stop_vip \\\"`; } sub usage { print \"Usage: master_ip_failover --command=start|stop|stopssh|status --orig_master_host=host --orig_master_ip=ip --orig_master_port=port --new_master_host=host --new_master_ip=ip --new_master_port=port\\n\"; } EOF 给脚本赋予执行权限 chmod +x /usr/local/bin/master_ip_failover 3.5.6 配置mha配置文件 创建mha配置文件目录及日志目录 mkdir /etc/masterha mkdir -p /var/log/masterha/app1 编辑mha配置文件/etc/masterha/app1.conf mha4mysql-manager-0.58/samples/conf/app1.conf是mha的默认配置文件 cat > /etc/masterha/app1.cnf 编辑发生切换后发送报警的脚本，需要自行配置邮箱设置 cat > /etc/mha/send_report 'all'; use Mail::Sender; use Getopt::Long; #new_master_host and new_slave_hosts are set only when recovering master succeeded my ( $dead_master_host, $new_master_host, $new_slave_hosts, $subject, $body ); my $smtp='smtp服务器地址'; my $mail_from='发件人邮箱'; my $mail_user='邮箱登陆用户名'; my $mail_pass='邮箱登陆密码'; my $mail_to=['收件人地址']; GetOptions( 'orig_master_host=s' => \\$dead_master_host, 'new_master_host=s' => \\$new_master_host, 'new_slave_hosts=s' => \\$new_slave_hosts, 'subject=s' => \\$subject, 'body=s' => \\$body, ); mailToContacts($smtp,$mail_from,$mail_user,$mail_pass,$mail_to,$subject,$body); sub mailToContacts { my ( $smtp, $mail_from, $user, $passwd, $mail_to, $subject, $msg ) = @_; open my $DEBUG, \"> /tmp/monitormail.log\" or die \"Can't open the debug file:$!\\n\"; my $sender = new Mail::Sender { ctype => 'text/plain; charset=utf-8', encoding => 'utf-8', smtp => $smtp, from => $mail_from, auth => 'LOGIN', TLS_allowed => '0', authid => $user, authpwd => $passwd, to => $mail_to, subject => $subject, debug => $DEBUG }; $sender->MailMsg( { msg => $msg, debug => $DEBUG } ) or print $Mail::Sender::Error; return 1; } # Do whatever you want here exit 0; EOF 3.5.7 测试MHA Manager 3.5.7.1 测试ssh连接 $ masterha_check_ssh --conf=/etc/masterha/app1.cnf Sat Jun 6 10:27:32 2020 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping. Sat Jun 6 10:27:32 2020 - [info] Reading application default configuration from /etc/mha/mha.conf.. Sat Jun 6 10:27:32 2020 - [info] Reading server configuration from /etc/mha/mha.conf.. Sat Jun 6 10:27:32 2020 - [info] Starting SSH connection tests.. Sat Jun 6 10:27:33 2020 - [debug] Sat Jun 6 10:27:32 2020 - [debug] Connecting via SSH from root@10.0.0.133(10.0.0.133:22) to root@10.0.0.134(10.0.0.134:22).. Sat Jun 6 10:27:32 2020 - [debug] ok. Sat Jun 6 10:27:32 2020 - [debug] Connecting via SSH from root@10.0.0.133(10.0.0.133:22) to root@10.0.0.135(10.0.0.135:22).. Sat Jun 6 10:27:33 2020 - [debug] ok. Sat Jun 6 10:27:33 2020 - [debug] Sat Jun 6 10:27:32 2020 - [debug] Connecting via SSH from root@10.0.0.134(10.0.0.134:22) to root@10.0.0.133(10.0.0.133:22).. Sat Jun 6 10:27:33 2020 - [debug] ok. Sat Jun 6 10:27:33 2020 - [debug] Connecting via SSH from root@10.0.0.134(10.0.0.134:22) to root@10.0.0.135(10.0.0.135:22).. Sat Jun 6 10:27:33 2020 - [debug] ok. Sat Jun 6 10:27:34 2020 - [debug] Sat Jun 6 10:27:33 2020 - [debug] Connecting via SSH from root@10.0.0.135(10.0.0.135:22) to root@10.0.0.133(10.0.0.133:22).. Sat Jun 6 10:27:33 2020 - [debug] ok. Sat Jun 6 10:27:33 2020 - [debug] Connecting via SSH from root@10.0.0.135(10.0.0.135:22) to root@10.0.0.134(10.0.0.134:22).. Sat Jun 6 10:27:33 2020 - [debug] ok. Sat Jun 6 10:27:34 2020 - [info] All SSH connection tests passed successfully. 3.5.7.2 测试mysql集群连接情况 $ masterha_check_repl --conf=/etc/masterha/app1.cnf Sat Jun 6 12:45:42 2020 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping. Sat Jun 6 12:45:42 2020 - [info] Reading application default configuration from /etc/masterha/app1.cnf.. Sat Jun 6 12:45:42 2020 - [info] Reading server configuration from /etc/masterha/app1.cnf.. Sat Jun 6 12:45:42 2020 - [info] MHA::MasterMonitor version 0.58. Sat Jun 6 12:45:43 2020 - [info] GTID failover mode = 0 Sat Jun 6 12:45:43 2020 - [info] Dead Servers: Sat Jun 6 12:45:43 2020 - [info] Alive Servers: Sat Jun 6 12:45:43 2020 - [info] 10.0.0.133(10.0.0.133:3306) Sat Jun 6 12:45:43 2020 - [info] 10.0.0.134(10.0.0.134:3306) Sat Jun 6 12:45:43 2020 - [info] 10.0.0.135(10.0.0.135:3306) Sat Jun 6 12:45:43 2020 - [info] Alive Slaves: Sat Jun 6 12:45:43 2020 - [info] 10.0.0.134(10.0.0.134:3306) Version=5.7.28-log (oldest major version between slaves) log-bin:enabled Sat Jun 6 12:45:43 2020 - [info] Replicating from 10.0.0.133(10.0.0.133:3306) Sat Jun 6 12:45:43 2020 - [info] Primary candidate for the new Master (candidate_master is set) Sat Jun 6 12:45:43 2020 - [info] 10.0.0.135(10.0.0.135:3306) Version=5.7.28-log (oldest major version between slaves) log-bin:enabled Sat Jun 6 12:45:43 2020 - [info] Replicating from 10.0.0.133(10.0.0.133:3306) Sat Jun 6 12:45:43 2020 - [info] Current Alive Master: 10.0.0.133(10.0.0.133:3306) Sat Jun 6 12:45:43 2020 - [info] Checking slave configurations.. Sat Jun 6 12:45:43 2020 - [info] read_only=1 is not set on slave 10.0.0.134(10.0.0.134:3306). Sat Jun 6 12:45:43 2020 - [warning] relay_log_purge=0 is not set on slave 10.0.0.134(10.0.0.134:3306). Sat Jun 6 12:45:43 2020 - [info] read_only=1 is not set on slave 10.0.0.135(10.0.0.135:3306). Sat Jun 6 12:45:43 2020 - [warning] relay_log_purge=0 is not set on slave 10.0.0.135(10.0.0.135:3306). Sat Jun 6 12:45:43 2020 - [info] Checking replication filtering settings.. Sat Jun 6 12:45:43 2020 - [info] binlog_do_db= , binlog_ignore_db= Sat Jun 6 12:45:43 2020 - [info] Replication filtering check ok. Sat Jun 6 12:45:43 2020 - [info] GTID (with auto-pos) is not supported Sat Jun 6 12:45:43 2020 - [info] Starting SSH connection tests.. Sat Jun 6 12:45:50 2020 - [info] All SSH connection tests passed successfully. Sat Jun 6 12:45:50 2020 - [info] Checking MHA Node version.. Sat Jun 6 12:45:51 2020 - [info] Version check ok. Sat Jun 6 12:45:51 2020 - [info] Checking SSH publickey authentication settings on the current master.. Sat Jun 6 12:45:51 2020 - [info] HealthCheck: SSH to 10.0.0.133 is reachable. Sat Jun 6 12:45:51 2020 - [info] Master MHA Node version is 0.58. Sat Jun 6 12:45:51 2020 - [info] Checking recovery script configurations on 10.0.0.133(10.0.0.133:3306).. Sat Jun 6 12:45:51 2020 - [info] Executing command: save_binary_logs --command=test --start_pos=4 --binlog_dir=/usr/local/mysql/data --output_file=/tmp/save_binary_logs_test --manager_version=0.58 --start_file=binlog.000001 Sat Jun 6 12:45:51 2020 - [info] Connecting to root@10.0.0.133(10.0.0.133:22).. Creating /tmp if not exists.. ok. Checking output directory is accessible or not.. ok. Binlog found at /usr/local/mysql/data, up to binlog.000001 Sat Jun 6 12:45:51 2020 - [info] Binlog setting check done. Sat Jun 6 12:45:51 2020 - [info] Checking SSH publickey authentication and checking recovery script configurations on all alive slave servers.. Sat Jun 6 12:45:51 2020 - [info] Executing command : apply_diff_relay_logs --command=test --slave_user='root' --slave_host=10.0.0.134 --slave_ip=10.0.0.134 --slave_port=3306 --workdir=/tmp --target_version=5.7.28-log --manager_version=0.58 --relay_log_info=/usr/local/mysql/data/relay-log.info --relay_dir=/usr/local/mysql/data/ --slave_pass=xxx Sat Jun 6 12:45:51 2020 - [info] Connecting to root@10.0.0.134(10.0.0.134:22).. Checking slave recovery environment settings.. Opening /usr/local/mysql/data/relay-log.info ... ok. Relay log found at /usr/local/mysql/data, up to mysql02-relay-bin.000002 Temporary relay log file is /usr/local/mysql/data/mysql02-relay-bin.000002 Checking if super_read_only is defined and turned on.. not present or turned off, ignoring. Testing mysql connection and privileges.. mysql: [Warning] Using a password on the command line interface can be insecure. done. Testing mysqlbinlog output.. done. Cleaning up test file(s).. done. Sat Jun 6 12:45:51 2020 - [info] Executing command : apply_diff_relay_logs --command=test --slave_user='root' --slave_host=10.0.0.135 --slave_ip=10.0.0.135 --slave_port=3306 --workdir=/tmp --target_version=5.7.28-log --manager_version=0.58 --relay_log_info=/usr/local/mysql/data/relay-log.info --relay_dir=/usr/local/mysql/data/ --slave_pass=xxx Sat Jun 6 12:45:51 2020 - [info] Connecting to root@10.0.0.135(10.0.0.135:22).. Checking slave recovery environment settings.. Opening /usr/local/mysql/data/relay-log.info ... ok. Relay log found at /usr/local/mysql/data, up to mysql03-relay-bin.000002 Temporary relay log file is /usr/local/mysql/data/mysql03-relay-bin.000002 Checking if super_read_only is defined and turned on.. not present or turned off, ignoring. Testing mysql connection and privileges.. mysql: [Warning] Using a password on the command line interface can be insecure. done. Testing mysqlbinlog output.. done. Cleaning up test file(s).. done. Sat Jun 6 12:45:52 2020 - [info] Slaves settings check done. Sat Jun 6 12:45:52 2020 - [info] 10.0.0.133(10.0.0.133:3306) (current master) +--10.0.0.134(10.0.0.134:3306) +--10.0.0.135(10.0.0.135:3306) Sat Jun 6 12:45:52 2020 - [info] Checking replication health on 10.0.0.134.. Sat Jun 6 12:45:52 2020 - [info] ok. Sat Jun 6 12:45:52 2020 - [info] Checking replication health on 10.0.0.135.. Sat Jun 6 12:45:52 2020 - [info] ok. Sat Jun 6 12:45:52 2020 - [info] Checking master_ip_failover_script status: Sat Jun 6 12:45:52 2020 - [info] /usr/local/bin/master_ip_failover --command=status --ssh_user=root --orig_master_host=10.0.0.133 --orig_master_ip=10.0.0.133 --orig_master_port=3306 IN SCRIPT TEST====/usr/sbin/ifconfig eth1:1 down==/usr//sbin/ifconfig eth1:1 10.0.0.200/24=== Checking the Status of the script.. OK Sat Jun 6 12:45:52 2020 - [info] OK. Sat Jun 6 12:45:52 2020 - [warning] shutdown_script is not defined. Sat Jun 6 12:45:52 2020 - [info] Got exit code 0 (Not master dead). MySQL Replication Health is OK. 3.5.8 启动mha 启动mha nohup masterha_manager --conf=/etc/masterha/app1.cnf --remove_dead_master_conf --ignore_last_failover /var/log/masterha/app1/manager.log 2>&1 & 启动参数说明 --remove_dead_master_conf 该参数代表当发生主从切换后，老的主库的ip将会从配置文件中移除。 --manger_log 日志存放位置 --ignore_last_failover 在缺省情况下，如果MHA检测到连续发生宕机，且两次宕机间隔不足8小时的话，则不会进行Failover，之所以这样限制是为了避免ping-pong效应。该参数代表忽略上次MHA触发切换产生的文件，默认情况下，MHA发生切换后会在日志目录，也就是上面我设置的/data产生app1.failover.complete文件，下次再次切换的时候如果发现该目录下存在该文件将不允许触发切换，除非在第一次切换后收到删除该文件，为了方便，这里设置为--ignore_last_failover 测试mha状态 $ masterha_check_status --conf=/etc/masterha/app1.cnf app1 (pid:2262) is running(0:PING_OK), master:10.0.0.133 停止mha $ masterha_stop --conf=/etc/masterha/app1.cnf Stopped app1 successfully. [1]+ Exit 1 nohup masterha_manager --conf=/etc/masterha/app1.cnf --remove_dead_master_conf --ignore_last_failover /var/log/masterha/app1/manager.log 2>&1 重启mha masterha_stop --conf=/etc/masterha/app1.cnf && nohup masterha_manager --conf=/etc/masterha/app1.cnf --remove_dead_master_conf --ignore_last_failover /var/log/masterha/app1/manager.log 2>&1 & 到此，MHA部署完成！ 3.6 测试MHA 3.6.1 手动关闭主库，模拟宕机 10.0.0.133 mysql01操作 systemctl stop mysqld 3.6.2 验证slave1，即10.0.0.134 因为在mha配置文件/etc/masterha/app1.conf中定义了参数candidate_master=1，即设置为候选master，如果设置该参数以后，发生主从切换以后将会将此从库提升为主库，即使这个主库不是集群中事件最新的slave，因此，当主库挂掉时，slave1 10.0.0.134会成为新的主库 10.0.0.135 mysql02操作 可以看到，当主库10.0.0.133宕机后，10.0.0.134成为了新的主库 $ show slave status\\G *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 10.0.0.134 Master_User: repl Master_Port: 3306 Connect_Retry: 60 Master_Log_File: binlog.000002 Read_Master_Log_Pos: 1331 Relay_Log_File: mysql03-relay-bin.000002 Relay_Log_Pos: 317 Relay_Master_Log_File: binlog.000002 Slave_IO_Running: Yes Slave_SQL_Running: Yes 。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。 验证VIP是否已经漂移 在新主库上能够看到VIP即为成功 $ ip a s eth0 2: eth0: mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:1c:42:09:c2:b1 brd ff:ff:ff:ff:ff:ff inet 10.0.0.134/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever inet 10.0.0.200/24 brd 10.0.0.255 scope global secondary eth0:0 valid_lft forever preferred_lft forever inet6 fe80::21c:42ff:fe09:c2b1/64 scope link valid_lft forever preferred_lft forever 4.MHA常用命令总结 1、检查mha的ssh免密登录状态 masterha_check_ssh --conf=/etc/masterha/app1.cnf 2、检查mha的运行状态 masterha_check_status --conf=/etc/masterha/app1.cnf 3、检查主备库的复制情况 masterha_check_repl --conf=/etc/masterha/app1.cnf 4、停止mha masterha_stop --conf=/etc/masterha/app1.cnf 5、启动mha nohup masterha_manager --conf=/etc/masterha/app1.cnf --remove_dead_master_conf --ignore_last_failover /var/log/masterha/app1/manager.log 2>&1 & 6、mha手动切换主库 masterha_master_switch --conf=/etc/masterha/app1.cnf --master_state=alive --new_master_host=10.0.0.135 --new_master_port=3106 --orig_master_is_new_slave 7、mha重新绑定数据库实例 master_ip_failover --command=status --ssh_user=root --orig_master_host=10.0.0.133 --orig_master_ip=10.0.0.200 --orig_master_port=3306 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql进阶/7.MHA基于GTID主从复制高可用.html":{"url":"db/mysql/mysql进阶/7.MHA基于GTID主从复制高可用.html","title":"MHA基于GTID主从复制高可用","keywords":"","body":"[toc] MHA基于GTID主从复制高可用 MHA gIthub地址 MHA github wiki地址 1.MHA简介 MHA通常在10到30秒内以最少的停机时间执行自动的主故障转移和从升级。MHA可以防止复制一致性问题，并节省了必须购买其他服务器的费用。所有这些都具有零性能下降，无复杂性（易于安装）并且无需更改现有部署的情况。 MHA还提供了计划的在线主设备切换，可以在停机（仅阻止写入）的几秒钟（0.5-2秒）内将当前正在运行的主设备安全地更改为新的主设备。 MHA提供以下功能，在需要高可用性，数据完整性和近乎不间断的主维护的许多部署中很有用。 自动化的主站监视和故障转移 MHA可以监视现有复制环境中的MySQL主服务器，并在检测到主服务器故障时执行自动主服务器故障转移。MHA通过识别来自最新从站的差分中继日志事件并将其应用于所有其他从站，包括那些尚未收到最新中继日志事件的从站，来保证所有从站的一致性。MHA通常可以在几秒钟内执行故障转移：9到12秒钟用于检测主设备故障，可选地7到10秒钟用于关闭主计算机电源，以避免脑部分裂；几秒钟的时间将差分中继日志应用于新的主设备。总停机时间通常为10-30秒。可以在配置文件中将特定从站指定为候选主站（设置优先级）。由于MHA维护从站之间的一致性，任何奴隶都可以晋升为新主人。通常不会导致突然的复制失败的一致性问题将不会发生。 交互式（手动启动）主故障转移 可以将MHA配置为手动启动（非自动），交互式故障转移，而无需监视主服务器。 非交互式主服务器故障转移 还支持不监视主服务器的非交互式自动主服务器故障转移。当已经使用MySQL主软件监视时，此功能特别有用。例如，您可以使用Pacemaker（Heartbeat）检测主服务器故障和虚拟IP地址接管，而使用MHA进行主服务器故障转移和从属升级。 在线将主服务器切换到其他主机 通常有必要将现有的主服务器迁移到另一台计算机上，例如当前主服务器存在硬件RAID控制器或RAM问题，或者要用速度更快的计算机替换它等时，这不是主服务器崩溃，但需要定期进行主维护。计划的主机维护应尽快完成，因为这会导致部分停机（禁用主机写入）。另一方面，您应该非常仔细地阻止/杀死当前正在运行的会话，因为不同的master之间可能会发生一致性问题（即“更新master1，更新master 2，提交master1，在提交master 2时出错”会导致数据不一致）。快速主开关和平稳阻止写入都是必需的。 MHA在写入器阻塞后的0.5-2秒内提供正常的主设备切换。通常可以接受0.5-2秒的写入器停机时间，因此即使不分配计划的维护时段，您也可以切换主机。升级到更高版本，更快的计算机等操作变得更加容易。 2.MHA架构 MHA架构官方文档 正常工作时架构 主库down机时架构 该软件由两部分组成：MHA Manager（管理节点）和MHA Node（数据节点）。 MHA Manager可以单独部署在一台独立的机器上管理多个master-slave集群，也可以部署在一台slave节点上。 MHA Node运行在每台MySQL服务器上，MHA Manager会定时探测集群中的master节点，当master出现故障时，它可以自动将最新数据的slave提升为新的master，然后将所有其他的slave重新指向新的master。整个故障转移过程对应用程序完全透明。 在MHA自动故障切换过程中，MHA试图从宕机的主服务器上保存二进制日志，最大程度的保证数据的不丢失(配合mysql半同步复制效果更佳)，但这并不总是可行的。例如，如果主服务器硬件故障或无法通过ssh访问，MHA没法保存二进制日志，只进行故障转移而丢失了最新的数据。使用MySQL 5.5的半同步复制，可以大大降低数据丢失的风险。MHA可以与半同步复制结合起来。如果只有一个slave已经收到了最新的二进制日志，MHA可以将最新的二进制日志应用于其他所有的slave服务器上，因此可以保证所有节点的数据一致性。 注意：目前MHA主要支持一主多从的架构，要搭建MHA,要求一个复制集群中必须最少有三台数据库服务器，一主二从，即一台充当master，一台充当备用master，另外一台充当从库，因为至少需要三台服务器。 工作过程：manager节点定时检测复制集群中的每一个node,如果master宕机会选出数据与master最接近的一台slave来作为新master，去到旧master中拷贝binlog到新master中并应用binlog以保证新master与旧master数据一致，然后将剩余的slave重新changer master to,将slave的主切换为新master 3.部署过程 实验环境 角色 IP 主机名 系统 机器配置 mysql版本 manager 10.0.0.130 mha centos7.8 2c4g 5.7.28 mysql-master 10.0.0.133 mysql01 centos7.8 2c4g 5.7.28 mysql-slave1 10.0.0.134 mysql02 centos7.8 2c4g 5.7.28 mysql-slave2 10.0.0.135 mysql03 centos7.8 2c4g 5.7.28 3.1 下载安装包 MHA manager0.58下载地址 MHA node0.58下载地址 MHA 0.56 下载地址 Manager工具包主要包括以下几个工具： 名称 含义 masterha_check_ssh 检查MHA的SSH配置状况 masterha_check_repl 检查MySQL复制状况 masterha_manger 启动MHA masterha_check_status 检测当前MHA运行状态 masterha_master_monitor 检测master是否宕机 masterha_master_switch 控制故障转移（自动或者手动） masterha_conf_host 添加或删除配置的server信息 Node工具包（这些工具通常由MHA Manager的脚本触发，无需人为操作）主要包括以下几个工具： 名称 含义 save_binary_logs 保存和复制master的二进制日志 apply_diff_relay_logs 识别差异的中继日志事件并将其差异的事件应用于其他的slave filter_mysqlbinlog 去除不必要的ROLLBACK事件（MHA已不再使用这个工具） purge_relay_logs 清除中继日志（不会阻塞SQL线程） 3.2 所有机器做相互免密钥配置 ⚠️复制集群中的每个节点都有可能成为master，都得开启binlog和ssh密钥认证，因为当旧master宕机后，mha要拷贝binlog到所有node节点上，而且所有节点都有可能成为master，故每个节点都要彼此密钥认证 lowB脚本运行一下 #!/bin/bash file_path=/root file_name=host.txt file=$file_path/$file_name user=root pwd=1 ssh_file=~/.ssh/id_rsa yum -y install expect &> /dev/null # 生成密钥 [ -f \"$ssh_file\" ] || ssh-keygen -q -N \"\" -f ~/.ssh/id_rsa read -p \"请输入开始数值: \" start_num read -p \"请输入结束数值: \" end_num read -p \"请输入网段(类似格式：10.0.0.)：\" sub_net seq $start_num $end_num >$file # 在文件开头加上网段，在文件末尾加上用户名和密码 sed -i 's/^/'$sub_net'/g' $file sed -i 's/$/ '$user' '$pwd'/g' $file echo -e \"执行成功，文件内容如下:\\n`cat $file`\" while read line;do ip=`echo $line | awk '{print $1}'` username=`echo $line | awk '{print $2}'` password=`echo $line | awk '{print $3}'` expect master操作(mysql01 10.0.0.133) 3.3 mysql主从配置-GTID 这里是把10.0.0.133(mysql01)、10.0.0.134(mysql02)、10.0.0.135(mysql03)搭建为ABB，即一主两从，其中mysql01是主库，其余两个节点是从库 3.3.1 编辑 /etc/my.cnf 指定serverid，并开启binlog # 指定server id server_id=1 # 开启binlog日志，位置在mysql数据目录data下 log_bin=binlog # 开启binlog日志索引，位置在mysql数据目录data下 log_bin_index=binlog.index # 开启GTID gtid_mode=ON # 开启GTID的一些安全限制 执行GTID一致 enforce_gtid_consistency # 通常情况，从服务器从主服务器接收到的更新不记入它的二进制日志。该选项告诉从服务器将其SQL线程执行的更新记入到从服务器自己的二进制日志，mysql-5.6必须加这个参数，5.7可以不加 log-slave-updates # 禁止mysql域名解析 skip_name_resolve 重启mysql systemctl restart mysqld 查看git开启情况，enforce_gtid_consistency与gtid_mode都为on即为正确 mysql> show global variables like '%gtid%'; +----------------------------------+-------+ | Variable_name | Value | +----------------------------------+-------+ | binlog_gtid_simple_recovery | ON | | enforce_gtid_consistency | ON | | gtid_executed | | | gtid_executed_compression_period | 1000 | | gtid_mode | ON | | gtid_owned | | | gtid_purged | | | session_track_gtids | OFF | +----------------------------------+-------+ 8 rows in set (0.00 sec) 3.3.2 创建专用复制用户和设置监控用户 允许从slave上连接过来的复制用户，3台mysql服务器都要创建复制用户和监控用户！！！ ⚠️复制用户名称为repl，也可以在MHA配置文件中修改复制用户 创建专用复制用户 mysql> grant replication slave on *.* to 'repl'@'10.0.0.%' identified by 'Bxb123.com'; Query OK, 0 rows affected, 1 warning (0.00 sec) 设置监控用户 ⚠️所有节点进行授权 mysql> grant all privileges on *.* to 'mha'@'10.0.0.%' identified by 'Bxb123.com'; Query OK, 0 rows affected, 1 warning (0.00 sec) mysql5.7会默认加载validate_password 模块，是来控制密码长度和规则的，可以在配置文件里面关闭该模块加上validate_password = off，或者在mysql命令行执行set global validate_password_policy=0;来临时取消密码规则 禁用自动删除relay log功能 set global relay_log_purge=0; slave操作(mysql02、03 10.0.0.134、135) 3.3.3 配置文件 /etc/my.cnf 指定serverid，并开启binlog日志 # 指定server id mysql02为2 mysql03为3 server_id=2 # 开启binlog日志，位置在mysql数据目录data下 log_bin=binlog # 开启binlog日志索引，位置在mysql数据目录data下 log_bin_index=binlog.index # 开启GTID gtid_mode=ON # 开启GTID的一些安全限制 执行GTID一致 enforce_gtid_consistency # 通常情况，从服务器从主服务器接收到的更新不记入它的二进制日志。该选项告诉从服务器将其SQL线程执行的更新记入到从服务器自己的二进制日志，mysql-5.6必须加这个参数，5.7可以不加 log-slave-updates # 禁止mysql域名解析 skip_name_resolve 重启mysql systemctl restart mysqld 查看git开启情况，enforce_gtid_consistency与gtid_mode都为on即为正确 mysql> show variables like '%gtid%'; +----------------------------------+-----------+ | Variable_name | Value | +----------------------------------+-----------+ | binlog_gtid_simple_recovery | ON | | enforce_gtid_consistency | ON | | gtid_executed_compression_period | 1000 | | gtid_mode | ON | | gtid_next | AUTOMATIC | | gtid_owned | | | gtid_purged | | | session_track_gtids | OFF | +----------------------------------+-----------+ 8 rows in set (0.00 sec) 3.3.4 从库拉取 # 设置只读，不要在配置文件里写 mysql> set global read_only=1; Query OK, 0 rows affected (0.00 sec) # 禁用自动删除relay log set global relay_log_purge=0; # 拉取 change master to \\ master_host='10.0.0.133', \\ master_user='repl', \\ master_password='Bxb123.com', \\ master_auto_position=1; Query OK, 0 rows affected, 2 warnings (0.00 sec) 启动slave并查看slave状态 # 启动slave mysql> start slave; Query OK, 0 rows affected (0.00 sec) # 查看slave状态，SQL和IO线程都为Yes并且Auto_Position为1即为正确 Slave_IO_Running: Yes Slave_SQL_Running: Yes Auto_Position: 1 # 这个值是主从延迟，即slave落后master的秒数，也是一个比较重要的查看主从是否正常的标准值 Seconds_Behind_Master: 0 3.3.5 验证主从同步 master操作 # 在主库上创建一个数据库db1 mysql> create database db1; Query OK, 1 row affected (0.00 sec) slave操作 查看主库上创建的数据库是否已经同步过来，同步完成即为成功 mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | db1 | | mysql | | performance_schema | | sys | +--------------------+ 5 rows in set (0.00 sec) 3.4 部署MHA node 所有节点执行 # 下载包 wget https://github.com/yoshinorim/mha4mysql-node/releases/download/v0.58/mha4mysql-node-0.58.tar.gz # 编译安装 $ cd mha4mysql-node-0.58 $ ls AUTHORS bin COPYING debian inc lib Makefile.PL MANIFEST META.yml README rpm t $ perl Makefile.PL $ make && make install 安装完成后拷贝mha4mysql-node-0.58/bin下的所有可执行文件到/usr/local/bin apply_diff_relay_logs #识别差异日志并应用于其他slave save_binary_logs #保存和复制二进制日志 filter_mysqlbinlog #去除不必要的ROLLBACK事件（MHA已不再使用这个工具） purge_relay_logs #清除中继日志 manager操作(mha 10.0.0.130) 3.5 部署MHA Manager 3.5.1 增加epel源 wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo 3.5.2 安装依赖包 yum -y install perl-DBD-MySQL perl-Config-Tiny perl-Log-Dispatch perl-Parallel-ForkManager perl-ExtUtils-CBuilder perl-ExtUtils-MakeMaker perl-ExtUtils-Embed perl-CPAN 3.5.3 安装manager # 下载包 wget https://github.com/yoshinorim/mha4mysql-manager/releases/download/v0.58/mha4mysql-manager-0.58.tar.gz # 编译安装 $ cd cd mha4mysql-manager-0.58 $ ls AUTHORS bin COPYING debian inc lib Makefile.PL MANIFEST META.yml README rpm samples t tests $ perl Makefile.PL $ make && make install 安装完成后会拷贝mha4mysql-manager-0.58/bin下的所有可执行文件到/usr/local/bin 可执行文件 含义 masterha_manager 在主服务器关闭的情况下，主服务器自动监视和运行故障转移 masterha_master_switch 手动或非交互式主故障转移或在线主库 masterha_master_monitor MHA Manager监控程序 masterha_stop 停止MHA Manager masterha_check_repl 检查MySQL复制运行状况 masterha_check_status 检查Manager是否正确监视MySQL master masterha_check_ssh 检查ssh配置 masterha_conf_host 一个帮助程序脚本，用于从配置文件添加/删除主机条目 masterha_secondary_check MHA Manager二次检查 3.5.4 mysql主库节点配置eth0:0网卡，用作VIP，这里设置为10.0.0.200 这一步在mysql主库上(10.0.0.133)执行 $ ifconfig eth0:0 10.0.0.200 $ ip a s eth0 eth0: mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:1c:42:62:a8:c1 brd ff:ff:ff:ff:ff:ff inet 10.0.0.133/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever inet 10.0.0.200/8 brd 10.255.255.255 scope global eth0:0 valid_lft forever preferred_lft forever inet6 fe80::21c:42ff:fe62:a8c1/64 scope link valid_lft forever preferred_lft forever 3.5.5 使用脚本管理vip ⚠️需要修改的是VIP的地址和网卡的名称 VIP是绑定在主库上的，这样当主库宕机后VIP才会漂移到新主库上 cat > /usr/local/bin/master_ip_failover 'all'; use Getopt::Long; my ( $command, $ssh_user, $orig_master_host, $orig_master_ip, $orig_master_port, $new_master_host, $new_master_ip, $new_master_port ); # 这里需要注意一下 eth1:$key的意思是 vip必须绑定在 eth0:0上 my $vip = '10.0.0.200/24'; #此处为你要设置的虚拟ip my $key = '0'; my $ssh_start_vip = \"/usr//sbin/ifconfig eth0:$key $vip\"; #此处改为你的网卡名称 my $ssh_stop_vip = \"/usr/sbin/ifconfig eth0:$key down\"; GetOptions( 'command=s' => \\$command, 'ssh_user=s' => \\$ssh_user, 'orig_master_host=s' => \\$orig_master_host, 'orig_master_ip=s' => \\$orig_master_ip, 'orig_master_port=i' => \\$orig_master_port, 'new_master_host=s' => \\$new_master_host, 'new_master_ip=s' => \\$new_master_ip, 'new_master_port=i' => \\$new_master_port, ); exit &main(); sub main { print \"\\n\\nIN SCRIPT TEST====$ssh_stop_vip==$ssh_start_vip===\\n\\n\"; if ( $command eq \"stop\" || $command eq \"stopssh\" ) { my $exit_code = 1; eval { print \"Disabling the VIP on old master: $orig_master_host \\n\"; &stop_vip(); $exit_code = 0; }; if ($@) { warn \"Got Error: $@\\n\"; exit $exit_code; } exit $exit_code; } elsif ( $command eq \"start\" ) { my $exit_code = 10; eval { print \"Enabling the VIP - $vip on the new master - $new_master_host \\n\"; &start_vip(); $exit_code = 0; }; if ($@) { warn $@; exit $exit_code; } exit $exit_code; } elsif ( $command eq \"status\" ) { print \"Checking the Status of the script.. OK \\n\"; exit 0; } else { &usage(); exit 1; } } sub start_vip() { `ssh $ssh_user\\@$new_master_host \\\" $ssh_start_vip \\\"`; } sub stop_vip() { return 0 unless ($ssh_user); `ssh $ssh_user\\@$orig_master_host \\\" $ssh_stop_vip \\\"`; } sub usage { print \"Usage: master_ip_failover --command=start|stop|stopssh|status --orig_master_host=host --orig_master_ip=ip --orig_master_port=port --new_master_host=host --new_master_ip=ip --new_master_port=port\\n\"; } EOF 给脚本赋予执行权限 chmod +x /usr/local/bin/master_ip_failover 3.5.6 配置mha配置文件 创建mha配置文件目录及日志目录 mkdir /etc/masterha mkdir -p /var/log/masterha/app1 编辑mha配置文件/etc/masterha/app1.conf mha4mysql-manager-0.58/samples/conf/app1.conf是mha的默认配置文件 cat > /etc/masterha/app1.cnf 编辑发生切换后发送报警的脚本，需要自行配置邮箱设置 cat > /etc/masterha/send_report 'all'; use Mail::Sender; use Getopt::Long; #new_master_host and new_slave_hosts are set only when recovering master succeeded my ( $dead_master_host, $new_master_host, $new_slave_hosts, $subject, $body ); my $smtp='smtp服务器地址'; my $mail_from='发件人邮箱'; my $mail_user='邮箱登陆用户名'; my $mail_pass='邮箱登陆密码'; my $mail_to=['收件人地址']; GetOptions( 'orig_master_host=s' => \\$dead_master_host, 'new_master_host=s' => \\$new_master_host, 'new_slave_hosts=s' => \\$new_slave_hosts, 'subject=s' => \\$subject, 'body=s' => \\$body, ); mailToContacts($smtp,$mail_from,$mail_user,$mail_pass,$mail_to,$subject,$body); sub mailToContacts { my ( $smtp, $mail_from, $user, $passwd, $mail_to, $subject, $msg ) = @_; open my $DEBUG, \"> /tmp/monitormail.log\" or die \"Can't open the debug file:$!\\n\"; my $sender = new Mail::Sender { ctype => 'text/plain; charset=utf-8', encoding => 'utf-8', smtp => $smtp, from => $mail_from, auth => 'LOGIN', TLS_allowed => '0', authid => $user, authpwd => $passwd, to => $mail_to, subject => $subject, debug => $DEBUG }; $sender->MailMsg( { msg => $msg, debug => $DEBUG } ) or print $Mail::Sender::Error; return 1; } # Do whatever you want here exit 0; EOF 3.5.7 测试MHA Manager 3.5.7.1 测试ssh连接 $ masterha_check_ssh --conf=/etc/masterha/app1.cnf Sat Jun 6 10:27:32 2020 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping. Sat Jun 6 10:27:32 2020 - [info] Reading application default configuration from /etc/mha/mha.conf.. Sat Jun 6 10:27:32 2020 - [info] Reading server configuration from /etc/mha/mha.conf.. Sat Jun 6 10:27:32 2020 - [info] Starting SSH connection tests.. Sat Jun 6 10:27:33 2020 - [debug] Sat Jun 6 10:27:32 2020 - [debug] Connecting via SSH from root@10.0.0.133(10.0.0.133:22) to root@10.0.0.134(10.0.0.134:22).. Sat Jun 6 10:27:32 2020 - [debug] ok. Sat Jun 6 10:27:32 2020 - [debug] Connecting via SSH from root@10.0.0.133(10.0.0.133:22) to root@10.0.0.135(10.0.0.135:22).. Sat Jun 6 10:27:33 2020 - [debug] ok. Sat Jun 6 10:27:33 2020 - [debug] Sat Jun 6 10:27:32 2020 - [debug] Connecting via SSH from root@10.0.0.134(10.0.0.134:22) to root@10.0.0.133(10.0.0.133:22).. Sat Jun 6 10:27:33 2020 - [debug] ok. Sat Jun 6 10:27:33 2020 - [debug] Connecting via SSH from root@10.0.0.134(10.0.0.134:22) to root@10.0.0.135(10.0.0.135:22).. Sat Jun 6 10:27:33 2020 - [debug] ok. Sat Jun 6 10:27:34 2020 - [debug] Sat Jun 6 10:27:33 2020 - [debug] Connecting via SSH from root@10.0.0.135(10.0.0.135:22) to root@10.0.0.133(10.0.0.133:22).. Sat Jun 6 10:27:33 2020 - [debug] ok. Sat Jun 6 10:27:33 2020 - [debug] Connecting via SSH from root@10.0.0.135(10.0.0.135:22) to root@10.0.0.134(10.0.0.134:22).. Sat Jun 6 10:27:33 2020 - [debug] ok. Sat Jun 6 10:27:34 2020 - [info] All SSH connection tests passed successfully. 3.5.7.2 测试mysql集群连接情况 $ masterha_check_repl --conf=/etc/masterha/app1.cnf Sat Jun 6 12:45:42 2020 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping. Sat Jun 6 12:45:42 2020 - [info] Reading application default configuration from /etc/masterha/app1.cnf.. Sat Jun 6 12:45:42 2020 - [info] Reading server configuration from /etc/masterha/app1.cnf.. Sat Jun 6 12:45:42 2020 - [info] MHA::MasterMonitor version 0.58. Sat Jun 6 12:45:43 2020 - [info] GTID failover mode = 0 Sat Jun 6 12:45:43 2020 - [info] Dead Servers: Sat Jun 6 12:45:43 2020 - [info] Alive Servers: Sat Jun 6 12:45:43 2020 - [info] 10.0.0.133(10.0.0.133:3306) Sat Jun 6 12:45:43 2020 - [info] 10.0.0.134(10.0.0.134:3306) Sat Jun 6 12:45:43 2020 - [info] 10.0.0.135(10.0.0.135:3306) Sat Jun 6 12:45:43 2020 - [info] Alive Slaves: Sat Jun 6 12:45:43 2020 - [info] 10.0.0.134(10.0.0.134:3306) Version=5.7.28-log (oldest major version between slaves) log-bin:enabled Sat Jun 6 12:45:43 2020 - [info] Replicating from 10.0.0.133(10.0.0.133:3306) Sat Jun 6 12:45:43 2020 - [info] Primary candidate for the new Master (candidate_master is set) Sat Jun 6 12:45:43 2020 - [info] 10.0.0.135(10.0.0.135:3306) Version=5.7.28-log (oldest major version between slaves) log-bin:enabled Sat Jun 6 12:45:43 2020 - [info] Replicating from 10.0.0.133(10.0.0.133:3306) Sat Jun 6 12:45:43 2020 - [info] Current Alive Master: 10.0.0.133(10.0.0.133:3306) Sat Jun 6 12:45:43 2020 - [info] Checking slave configurations.. Sat Jun 6 12:45:43 2020 - [info] read_only=1 is not set on slave 10.0.0.134(10.0.0.134:3306). Sat Jun 6 12:45:43 2020 - [warning] relay_log_purge=0 is not set on slave 10.0.0.134(10.0.0.134:3306). Sat Jun 6 12:45:43 2020 - [info] read_only=1 is not set on slave 10.0.0.135(10.0.0.135:3306). Sat Jun 6 12:45:43 2020 - [warning] relay_log_purge=0 is not set on slave 10.0.0.135(10.0.0.135:3306). Sat Jun 6 12:45:43 2020 - [info] Checking replication filtering settings.. Sat Jun 6 12:45:43 2020 - [info] binlog_do_db= , binlog_ignore_db= Sat Jun 6 12:45:43 2020 - [info] Replication filtering check ok. Sat Jun 6 12:45:43 2020 - [info] GTID (with auto-pos) is not supported Sat Jun 6 12:45:43 2020 - [info] Starting SSH connection tests.. Sat Jun 6 12:45:50 2020 - [info] All SSH connection tests passed successfully. Sat Jun 6 12:45:50 2020 - [info] Checking MHA Node version.. Sat Jun 6 12:45:51 2020 - [info] Version check ok. Sat Jun 6 12:45:51 2020 - [info] Checking SSH publickey authentication settings on the current master.. Sat Jun 6 12:45:51 2020 - [info] HealthCheck: SSH to 10.0.0.133 is reachable. Sat Jun 6 12:45:51 2020 - [info] Master MHA Node version is 0.58. Sat Jun 6 12:45:51 2020 - [info] Checking recovery script configurations on 10.0.0.133(10.0.0.133:3306).. Sat Jun 6 12:45:51 2020 - [info] Executing command: save_binary_logs --command=test --start_pos=4 --binlog_dir=/usr/local/mysql/data --output_file=/tmp/save_binary_logs_test --manager_version=0.58 --start_file=binlog.000001 Sat Jun 6 12:45:51 2020 - [info] Connecting to root@10.0.0.133(10.0.0.133:22).. Creating /tmp if not exists.. ok. Checking output directory is accessible or not.. ok. Binlog found at /usr/local/mysql/data, up to binlog.000001 Sat Jun 6 12:45:51 2020 - [info] Binlog setting check done. Sat Jun 6 12:45:51 2020 - [info] Checking SSH publickey authentication and checking recovery script configurations on all alive slave servers.. Sat Jun 6 12:45:51 2020 - [info] Executing command : apply_diff_relay_logs --command=test --slave_user='root' --slave_host=10.0.0.134 --slave_ip=10.0.0.134 --slave_port=3306 --workdir=/tmp --target_version=5.7.28-log --manager_version=0.58 --relay_log_info=/usr/local/mysql/data/relay-log.info --relay_dir=/usr/local/mysql/data/ --slave_pass=xxx Sat Jun 6 12:45:51 2020 - [info] Connecting to root@10.0.0.134(10.0.0.134:22).. Checking slave recovery environment settings.. Opening /usr/local/mysql/data/relay-log.info ... ok. Relay log found at /usr/local/mysql/data, up to mysql02-relay-bin.000002 Temporary relay log file is /usr/local/mysql/data/mysql02-relay-bin.000002 Checking if super_read_only is defined and turned on.. not present or turned off, ignoring. Testing mysql connection and privileges.. mysql: [Warning] Using a password on the command line interface can be insecure. done. Testing mysqlbinlog output.. done. Cleaning up test file(s).. done. Sat Jun 6 12:45:51 2020 - [info] Executing command : apply_diff_relay_logs --command=test --slave_user='root' --slave_host=10.0.0.135 --slave_ip=10.0.0.135 --slave_port=3306 --workdir=/tmp --target_version=5.7.28-log --manager_version=0.58 --relay_log_info=/usr/local/mysql/data/relay-log.info --relay_dir=/usr/local/mysql/data/ --slave_pass=xxx Sat Jun 6 12:45:51 2020 - [info] Connecting to root@10.0.0.135(10.0.0.135:22).. Checking slave recovery environment settings.. Opening /usr/local/mysql/data/relay-log.info ... ok. Relay log found at /usr/local/mysql/data, up to mysql03-relay-bin.000002 Temporary relay log file is /usr/local/mysql/data/mysql03-relay-bin.000002 Checking if super_read_only is defined and turned on.. not present or turned off, ignoring. Testing mysql connection and privileges.. mysql: [Warning] Using a password on the command line interface can be insecure. done. Testing mysqlbinlog output.. done. Cleaning up test file(s).. done. Sat Jun 6 12:45:52 2020 - [info] Slaves settings check done. Sat Jun 6 12:45:52 2020 - [info] 10.0.0.133(10.0.0.133:3306) (current master) +--10.0.0.134(10.0.0.134:3306) +--10.0.0.135(10.0.0.135:3306) Sat Jun 6 12:45:52 2020 - [info] Checking replication health on 10.0.0.134.. Sat Jun 6 12:45:52 2020 - [info] ok. Sat Jun 6 12:45:52 2020 - [info] Checking replication health on 10.0.0.135.. Sat Jun 6 12:45:52 2020 - [info] ok. Sat Jun 6 12:45:52 2020 - [info] Checking master_ip_failover_script status: Sat Jun 6 12:45:52 2020 - [info] /usr/local/bin/master_ip_failover --command=status --ssh_user=root --orig_master_host=10.0.0.133 --orig_master_ip=10.0.0.133 --orig_master_port=3306 IN SCRIPT TEST====/usr/sbin/ifconfig eth1:1 down==/usr//sbin/ifconfig eth1:1 10.0.0.200/24=== Checking the Status of the script.. OK Sat Jun 6 12:45:52 2020 - [info] OK. Sat Jun 6 12:45:52 2020 - [warning] shutdown_script is not defined. Sat Jun 6 12:45:52 2020 - [info] Got exit code 0 (Not master dead). MySQL Replication Health is OK. 3.5.8 启动mha 启动mha nohup masterha_manager --conf=/etc/masterha/app1.cnf --remove_dead_master_conf --ignore_last_failover /var/log/masterha/app1/manager.log 2>&1 & 启动参数说明 --remove_dead_master_conf 该参数代表当发生主从切换后，老的主库的ip将会从配置文件中移除。 /var/log/masterha/app1/manager.log 2>&1 & 打印日志 --ignore_last_failover 在缺省情况下，如果MHA检测到连续发生宕机，且两次宕机间隔不足8小时的话，则不会进行Failover，之所以这样限制是为了避免ping-pong效应。该参数代表忽略上次MHA触发切换产生的文件，默认情况下，MHA发生切换后会在日志目录，也就是上面我设置的/data产生app1.failover.complete文件，下次再次切换的时候如果发现该目录下存在该文件将不允许触发切换，除非在第一次切换后收到删除该文件，为了方便，这里设置为--ignore_last_failover MHA切换机制 1、切换后会生成一个临时文件，在MHA的工作目录下 2、下一次切换之前先监测是否存在这个临时文件 3、如果存在，则不做切换 4、如果不存在，则切换 5、该临时文件存在时间8小时 测试mha状态 $ masterha_check_status --conf=/etc/masterha/app1.cnf app1 (pid:2262) is running(0:PING_OK), master:10.0.0.133 停止mha $ masterha_stop --conf=/etc/masterha/app1.cnf Stopped app1 successfully. [1]+ Exit 1 nohup masterha_manager --conf=/etc/masterha/app1.cnf --remove_dead_master_conf --ignore_last_failover /var/log/masterha/app1/manager.log 2>&1 重启mha masterha_stop --conf=/etc/masterha/app1.cnf && nohup masterha_manager --conf=/etc/masterha/app1.cnf --remove_dead_master_conf --ignore_last_failover /var/log/masterha/app1/manager.log 2>&1 & 到此，MHA部署完成！ 3.6 测试MHA 3.6.1 手动关闭主库，模拟宕机 10.0.0.133 mysql01操作 systemctl stop mysqld 3.6.2 验证slave1，即10.0.0.134 因为在mha配置文件/etc/masterha/app1.conf中定义了参数candidate_master=1，即设置为候选master，如果设置该参数以后，发生主从切换以后将会将此从库提升为主库，即使这个主库不是集群中事件最新的slave，因此，当主库挂掉时，slave1 10.0.0.134会成为新的主库 10.0.0.135 mysql02操作 可以看到，当主库10.0.0.133宕机后，10.0.0.134成为了新的主库 $ show slave status\\G *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 10.0.0.134 Master_User: repl Master_Port: 3306 Connect_Retry: 60 Master_Log_File: binlog.000002 Read_Master_Log_Pos: 1331 Relay_Log_File: mysql03-relay-bin.000002 Relay_Log_Pos: 317 Relay_Master_Log_File: binlog.000002 Slave_IO_Running: Yes Slave_SQL_Running: Yes 。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。 验证VIP是否已经漂移 在新主库上能够看到VIP即为成功 $ ip a s eth0 2: eth0: mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:1c:42:09:c2:b1 brd ff:ff:ff:ff:ff:ff inet 10.0.0.134/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever inet 10.0.0.200/24 brd 10.0.0.255 scope global secondary eth0:0 valid_lft forever preferred_lft forever inet6 fe80::21c:42ff:fe09:c2b1/64 scope link valid_lft forever preferred_lft forever 3.7 修复宕机主库步骤 第一步、修复宕机主库 用各种方法把宕掉的旧主库成功启动 第二步、从MHA配置文件/etc/masterha/app1.conf中的[server default]标签下定义的manager_log=/var/log/masterha/app1/manager.log日志文件中找到change master to语句，修改从库复制用户的密码，并且指定主库为新主库 MHA节点操作 $ grep -i 'change master to' /var/log/masterha/app1/manager.log Sun Jun 7 13:17:49 2020 - [info] All other slaves should start replication from here. Statement should be: CHANGE MASTER TO MASTER_HOST='10.0.0.134', MASTER_PORT=3306, MASTER_AUTO_POSITION=1, MASTER_USER='repl', MASTER_PASSWORD='xxx'; 第三步、连接旧主库，执行上一步找到的change master to语句并打开IO、SQL线程 10.0.0.133 mysql01(旧主库)操作 # 执行change master to mysql> CHANGE MASTER TO \\ MASTER_HOST='10.0.0.134', \\ MASTER_PORT=3306, \\ MASTER_AUTO_POSITION=1, \\ MASTER_USER='repl', \\ MASTER_PASSWORD='Bxb123.com'; Query OK, 0 rows affected, 2 warnings (0.01 sec) # 启动IO、SQL线程 mysql> start slave; Query OK, 0 rows affected (0.01 sec) 第四步、把旧主库的server标签在MHA配置文件中添加回来，MHA执行切换后，会把down机的主机server标签删除 编辑MHA配置文件/etc/masterha/app1.cnf添加server1标签 [server1] hostname=10.0.0.133 port=3306 第五步、启动MHA nohup masterha_manager --conf=/etc/masterha/app1.cnf --remove_dead_master_conf --ignore_last_failover /var/log/masterha/app1/manager.log 2>&1 & 4.配置binlog-server mysql5.7官方binlog-server文档 MHA官方对于binlog-server的简单说明 从MHA版本0.56开始，MHA支持new部分[binlogN]。在binlog部分，您可以定义mysqlbinlog流服务器。当MHA进行基于GTID的故障转移时，MHA会检查binlog服务器，并且如果binlog服务器在其他从服务器之前，则MHA会在恢复之前将差分binlog事件应用于新的主服务器。当MHA进行基于非GTID的（传统）故障转移时，MHA将忽略二进制日志服务器。 作用: 防止master断电或其他原因导致binlog缺失 前提: mysql主从必须为GTID模式 配置好GTID主从后执行以下步骤 4.1 MHA配置文件/etc/masterha/app1.cnf加入binlog-server配置 这里把mysql03 10.0.0.135当作binlog-server [binlog1] # 不让这个机器当主库 no_master=1 hostname=10.0.0.135 master_binlog_dir=/data/mysql/binlog 创建binlog备份目录 mkdir -p /data/mysql/binlog && chown mysql.mysql /data/mysql/binlog 4.2 手动备份binlog ⚠️备份binlog，--host后的IP一定要填写VIP地址 ⚠️一定要注意写对mysql binlog文件的名称 # 进入binlog备份目录 cd /data/mysql/binlog # 手动备份 mysqlbinlog -R --host=10.0.0.200 --user=mha --password=Bxb123.com --raw/branch --stop-never binlog.000001 & 4.3 启动MHA nohup masterha_manager --conf=/etc/masterha/app1.cnf --remove_dead_master_conf --ignore_last_failover /var/log/masterha/app1/manager.log 2>&1 & 4.4 验证 mysql master节点执行命令flush logs刷新binlog master刷新binlog，多执行几次 mysql> flush logs; Query OK, 0 rows affected (0.01 sec) mysql03 10.0.0.135节点/data/mysql/binlog查看binlog，会同步master上的binlog $ pwd /data/mysql/binlog $ ls binlog.000001 binlog.000002 binlog.000003 5.MHA常用命令总结 1、检查mha的ssh免密登录状态 masterha_check_ssh --conf=/etc/masterha/app1.cnf 2、检查mha的运行状态 masterha_check_status --conf=/etc/masterha/app1.cnf 3、检查主备库的复制情况 masterha_check_repl --conf=/etc/masterha/app1.cnf 4、停止mha masterha_stop --conf=/etc/masterha/app1.cnf 5、启动mha nohup masterha_manager --conf=/etc/masterha/app1.cnf --remove_dead_master_conf --ignore_last_failover /var/log/masterha/app1/manager.log 2>&1 & 6、mha手动切换主库 masterha_master_switch --conf=/etc/masterha/app1.cnf --master_state=alive --new_master_host=10.0.0.135 --new_master_port=3106 --orig_master_is_new_slave 7、mha重新绑定数据库实例 master_ip_failover --command=status --ssh_user=root --orig_master_host=10.0.0.133 --orig_master_ip=10.0.0.200 --orig_master_port=3306 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mysql/mysql知识点/mysql事务隔离级别.html":{"url":"db/mysql/mysql知识点/mysql事务隔离级别.html","title":"mysql事务隔离级别","keywords":"","body":"[toc] mysql事务隔离级别 mysql事务 一般来说，事务是必须满足4个条件（ACID）：：原子性（Atomicity，或称不可分割性）、一致性（Consistency）、隔离性（Isolation，又称独立性）、持久性（Durability）。 原子性：一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。 一致性：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设规则，这包含资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。 隔离性：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。 持久性：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 mysql 4种事务隔离级别 级别 symbol 对应值 含义 存在问题 读未提交 READ-UNCOMMITTED 0 一个事务可以读到另一个事务未提交的数据 存在脏读、不可重复读、幻读的问题 读已提交 READ-COMMITTED 1 一个事务能读到另一个事务已提交的数据 解决脏读的问题，存在不可重复读、幻读的问题 可重复读 REPEATABLE-READ 2 同一事务的多个实例在并发读取数据时，会看到同样的数据行 mysql默认级别，解决脏读、不可重复读的问题，存在幻读的问题。使用 MMVC机制 实现可重复读 串行化 SERIALIZABLE 3 强制事务排序，使之不可能相互冲突 解决脏读、不可重复读、幻读，可保证事务安全，但完全串行执行，性能最低 关于mysql事务的一些知识点 最开始的时候，5.1.5之前的版本binlog format只支持stament，并没有row模式，在RC一些场景下会造成主从数据不一致，所以选择RR才能最大限度保证主从数据一致性 之后的mysql直接使用RC+row是完全没有问题的 mysql5.7表的默认存储引擎是InnoDB mysql> show create table t1; +-------+---------------------------------------------------------------------------------------------------------------------------------------+ | Table | Create Table | +-------+---------------------------------------------------------------------------------------------------------------------------------------+ | t1 | CREATE TABLE `t1` ( `id` int(3) NOT NULL, `name` char(30) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 | +-------+---------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.05 sec) mysql5.7 InnoDB存储引擎默认的事务隔离级别，全局和当前会话都是REPEATABLE-READ(可重复读) RR的并发性没有RC好 #mysql5.7.22 mysql> SELECT @@global.tx_isolation, @@tx_isolation; +-----------------------+-----------------+ | @@global.tx_isolation | @@tx_isolation | +-----------------------+-----------------+ | REPEATABLE-READ | REPEATABLE-READ | +-----------------------+-----------------+ 1 row in set (0.02 sec) 设置事务隔离级别 #配置文件修改，可选参数 READ-UNCOMMITTED、READ-COMMITTED、REPEATABLE-READ、 SERIALIZABLE [mysqld] transaction-isolation = REPEATABLE-READ #设置全局 SET @@global.tx_isolation = 0; SET @@global.tx_isolation = 'READ-UNCOMMITTED'; SET @@global.tx_isolation = 1; SET @@global.tx_isolation = 'READ-COMMITTED'; SET @@global.tx_isolation = 2; SET @@global.tx_isolation = 'REPEATABLE-READ'; SET @@global.tx_isolation = 3; SET @@global.tx_isolation = 'SERIALIZABLE'; #设置会话 SET @@session.tx_isolation = 0; SET @@session.tx_isolation = 'READ-UNCOMMITTED'; SET @@session.tx_isolation = 1; SET @@session.tx_isolation = 'READ-COMMITTED'; SET @@session.tx_isolation = 2; SET @@session.tx_isolation = 'REPEATABLE-READ'; SET @@session.tx_isolation = 3; SET @@session.tx_isolation = 'SERIALIZABLE'; mysql查看/设置自动提交 //查看自动提交是否开启，默认开启 #方法一 mysql> select @@autocommit; +--------------+ | @@autocommit | +--------------+ | 1 | +--------------+ 1 row in set (0.07 sec) #方法二 mysql> show variables like 'autocommit'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | autocommit | ON | +---------------+-------+ 1 row in set (0.09 sec) //关闭自动提交 mysql> set autocommit = 0; Query OK, 0 rows affected (0.11 sec) 以下所有示例都基于mysql5.7.22 先创建一张示例表t1 mysql> select version(); +-----------+ | version() | +-----------+ | 5.7.22 | +-----------+ mysql> create table t1(id int(3) primary key,name char(30)); mysql> insert into t1 values(1,'xiaoming'); mysql> select * from t1; +----+----------+ | id | name | +----+----------+ | 1 | xiaoming | +----+----------+ 1 读未提交 Read Uncommitted 含义：一个事务可以读到另一个事务未提交的数据！ 示例： 1.mysql默认的事务隔离级别是RR 可用值2来设置 mysql> SELECT @@global.tx_isolation, @@tx_isolation; +-----------------------+-----------------+ | @@global.tx_isolation | @@tx_isolation | +-----------------------+-----------------+ | REPEATABLE-READ | REPEATABLE-READ | +-----------------------+-----------------+ 1 row in set, 2 warnings (0.00 sec) 2.手动修改当前会话为RU mysql> SET @@session.tx_isolation = 0; Query OK, 0 rows affected, 1 warning (0.12 sec) mysql> SELECT @@tx_isolation; +------------------+ | @@tx_isolation | +------------------+ | READ-UNCOMMITTED | +------------------+ 1 row in set, 1 warning (0.04 sec) 3.关闭自动提交 mysql> set autocommit = 0; Query OK, 0 rows affected (0.08 sec) 如图所示，一个事务检索的数据被另一个未提交的事务给修改了。 官网对脏读定义的地址 其内容为 dirty read An operation that retrieves unreliable data, data that was updated by another transaction but not yet committed. 翻译过来就是 检索操作出来的数据是不可靠的，是可以被另一个未提交的事务修改的！ 你会发现，我们的演示结果和官网对脏读的定义一致。根据我们最开始的推理，如果存在脏读，那么不可重复读和幻读一定是存在的。 2 读提交 Read Committed 含义：一个事务能读到另一个事务已提交的数据 示例： 1.mysql默认的事务隔离级别是RR 可用值2来设置 mysql> SELECT @@global.tx_isolation, @@tx_isolation; +-----------------------+-----------------+ | @@global.tx_isolation | @@tx_isolation | +-----------------------+-----------------+ | REPEATABLE-READ | REPEATABLE-READ | +-----------------------+-----------------+ 1 row in set, 2 warnings (0.00 sec) 2.手动修改当前会话为RC mysql> SET @@session.tx_isolation = 1; Query OK, 0 rows affected, 1 warning (0.12 sec) mysql> select @@tx_isolation; +----------------+ | @@tx_isolation | +----------------+ | READ-COMMITTED | +----------------+ 1 row in set, 1 warning (0.04 sec) 3.关闭自动提交 mysql> set autocommit = 0; Query OK, 0 rows affected (0.08 sec) 如图所示，一个事务检索的数据只能被另一个已提交的事务修改。 官网对不可重复读定义的地址 其内容为 non-repeatable read The situation when a query retrieves data, and a later query within the same transaction retrieves what should be the same data, but the queries return different results (changed by another transaction committing in the meantime). 翻译过来就是 一个查询语句检索数据，随后又有一个查询语句在同一个事务中检索数据，两个数据应该是一样的，但是实际情况返回了不同的结果。！ ps:作者注，这里的不同结果，指的是在行不变的情况下(专业点说，主键索引没变)，主键索引指向的磁盘上的数据内容变了。如果主键索引变了，比如新增一条数据或者删除一条数据，就不是不可重复读。 显然，我们这个现象符合不可重复读的定义。下面，大家做一个思考: 这个不可重复读的定义，放到脏读的现象里是不是也可以说的通。显然脏读的现象，也就是读未提交(READ_UNCOMMITTED)的那个例子，是不是也符合在同一个事务中返回了不同结果！ 但是反过来就不一定通了，一个事务A中查询两次的结果在被另一个事务B改变的情况下，如果事务B未提交就改变了事务A的结果，就属于脏读，也属于不可重复读。如果该事务B提交了才改变事务A的结果，就不属于脏读，但属于不可重复读。 3 可重复读 Repeatable Read 官网对幻读定义的地址 phantom A row that appears in the result set of a query, but not in the result set of an earlier query. For example, if a query is run twice within a transaction, and in the meantime, another transaction commits after inserting a new row or updating a row so that it matches the WHERE clause of the query. 翻译过来就是 在一次查询的结果集里出现了某一行数据，但是该数据并未出现在更早的查询结果集里。例如，在一次事务里进行了两次查询，同时另一个事务插入某一行或更新某一行数据后(该数据符合查询语句里where后的条件)，并提交了！ 幻读(Phantom Read)说明 原因：事务A根据相同条件第二次查询，虽然查询不到事务B提交的新增数据，但是会影响事务A之后的一些操作，比如：事务A进行了一次select * from t1表查询，查询出id为1的数据，同时事务B进行了一次insert into t1 values(2,'xx')，也就是此时表中有了id为2的数据，但是在事务A中再次进行查询的时候，根本就查不到id为2的数据，但是当事务A进行insert into t1 values(2,'xx')，也想插入id为2的数据的时候，发现报错了，但是事务A怎么查也查不到有id为2的数据，这就让事务A的使用者出现了幻觉，what happend！。如果不想出现幻读问题，那么自己在查询语句中手动加锁 for update，如果查询的是id为2的数据，即便是现在没有id为2的数据，其他事务也无法对id为2的索引位置进行数据的处理。 含义：同一事务的多个实例在并发读取数据时，会看到同样的数据行 示例： 1.mysql默认的事务隔离级别是RR 可用值2来设置 mysql> SELECT @@global.tx_isolation, @@tx_isolation; +-----------------------+-----------------+ | @@global.tx_isolation | @@tx_isolation | +-----------------------+-----------------+ | REPEATABLE-READ | REPEATABLE-READ | +-----------------------+-----------------+ 1 row in set, 2 warnings (0.00 sec) 显然，该现象是符合幻读的定义的。即同一事务的两次相同查询出现不同行。 官网对解决幻读方法的地址 原文内容如下 By default, InnoDB operates in REPEATABLE READ transaction isolation level. In this case, InnoDB uses next-key locks for searches and index scans, which prevents phantom rows (see Section 14.7.4, “Phantom Rows”). 翻译过来就是 InnoDB默认用了REPEATABLE READ。在这种情况下，使用next-key locks解决幻读问题！ 以下两步均在session2中执行，正确结果是id为1的数据name已经改为abc 不加next-key locks是快照读，根本不能解决幻读问题 mysql> select * from t1; +----+----------+ | id | name | +----+----------+ | 1 | xiaoming | | 2 | hehe | +----+----------+ 2 rows in set (0.09 sec) 加上next-key locks就能解决幻读问题 mysql> select * from t1 lock in share mode; +----+------+ | id | name | +----+------+ | 1 | abc | | 2 | hehe | +----+------+ 2 rows in set (0.05 sec) 4 串行化 Serializable Read 在该隔离级别下，所有的select语句后都自动加上lock in share mode。因此，在该隔离级别下，无论你如何进行查询，都会使用next-key locks。所有的select操作均为当前读! 含义：强制事务排序，使之不可能相互冲突 示例： 1.mysql默认的事务隔离级别是RR 可用值2来设置 mysql> SELECT @@global.tx_isolation, @@tx_isolation; +-----------------------+-----------------+ | @@global.tx_isolation | @@tx_isolation | +-----------------------+-----------------+ | REPEATABLE-READ | REPEATABLE-READ | +-----------------------+-----------------+ 1 row in set, 2 warnings (0.00 sec) 2.手动修改当前会话为串行读 mysql> SET @@session.tx_isolation = 3; Query OK, 0 rows affected, 1 warning (0.12 sec) mysql> select @@tx_isolation; +----------------+ | @@tx_isolation | +----------------+ | READ-COMMITTED | +----------------+ 1 row in set, 1 warning (0.04 sec) 3.关闭自动提交 mysql> set autocommit = 0; Query OK, 0 rows affected (0.08 sec) OK,注意看上表红色部分！就是因为使用了next-key locks,innodb将PiD=1这条索引记录，和(1,++∞)这个间隙锁住了。其他事务要在这个间隙上插数据，就会阻塞，从而防止幻读发生! 有的人会说，你这第二次查询的结果，也变了啊，明显和第一次查询结果不一样啊？但是这是被自己的事务改的，不是被其他事物修改的。这不算是幻读，也不是不可重复读。 脏读、不可重复度、幻读 根据事务的隔离级别不同，会有三种情况发生，即脏读、不可重复度、幻读，这三种情况有如下包含关系 对上图解释 如果发生了脏读，那么不可重复读和幻读是一定发生的。因为拿脏读的现象，用不可重复读，幻读的定义也能解释的通。但是反过来，拿不可重复读的现象，用脏读的定义就不一定解释的通了！ mysql事务隔离级别对应的现象 隔离级别 对应值 脏读 不可重复读 幻读 READ-UNCOMMITTED（读未提交） 0 √ √ √ READ-COMMITTED（读提交） 1 × √ √ REPEATABLE-READ（可重复读） 2 × × √ SERIALIZABLE （可串行化） 3 × × × 脏读 定义 官网对脏读定义的地址 其内容为 dirty read An operation that retrieves unreliable data, data that was updated by another transaction but not yet committed. 翻译过来就是 检索操作出来的数据是不可靠的，是可以被另一个未提交的事务修改的！ 根据我们最开始的推理，如果存在脏读，那么不可重复读和幻读一定是存在的。 示例 新建一张表，并且隔离级别设置为读未提交 READ-UNCOMMITTED 1.新建表t1 mysql> create table t1(id int primary key,name char(30),money double); Query OK, 0 rows affected (0.01 sec) 2.插入数据 mysql> insert into t1 values(1,'xiaoming',100); Query OK, 1 row affected (0.00 sec) 3.设置当前会话事务隔离级别为读未提交 READ-UNCOMMITTED mysql> SET @@session.tx_isolation = 'READ-UNCOMMITTED'; Query OK, 0 rows affected, 1 warning (0.00 sec) 4.查看事务隔离级别 global是全局 session是会话 mysql> SELECT @@global.tx_isolation, @@tx_isolation; +-----------------------+------------------+ | @@global.tx_isolation | @@tx_isolation | +-----------------------+------------------+ | REPEATABLE-READ | READ-UNCOMMITTED | +-----------------------+------------------+ 1 row in set, 2 warnings (0.00 sec) 5.关闭自动提交 mysql> set @@autocommit = 0; Query OK, 0 rows affected (0.00 sec) 6.查看自动提交是否关闭，1为开启，0为关闭 mysql> select @@autocommit; +--------------+ | @@autocommit | +--------------+ | 0 | +--------------+ 1 row in set (0.00 sec) 脏读定义 检索操作出来的数据是不可靠的，是可以被另一个未提交的事务修改的！ 上图中的示例session2中查询的数据是不可靠的，被未提交的事物session1修改了，这就是脏读 举个例子描述一下这个过程，财务(session1)给小明(session2)发了100元工资，然后小明查询自己的账户，果然是多了100元，变成了200元，但是财务发现操作有误，选择了事务回滚，这个时候当小明再查询自己账户的时候，发现账户变成了100元 不可重复读 定义 官网对不可重复读定义的地址 其内容为 non-repeatable read The situation when a query retrieves data, and a later query within the same transaction retrieves what should be the same data, but the queries return different results (changed by another transaction committing in the meantime). 翻译过来就是 一个查询语句检索数据，随后又有一个查询语句在同一个事务中检索数据，两个数据应该是一样的，但是实际情况返回了不同的结果。！ ps:作者注，这里的不同结果，指的是在行不变的情况下(专业点说，主键索引没变)，主键索引指向的磁盘上的数据内容变了。如果主键索引变了，比如新增一条数据或者删除一条数据，就不是不可重复读。 显然，我们这个现象符合不可重复读的定义。下面，大家做一个思考: 这个不可重复读的定义，放到脏读的现象里是不是也可以说的通。显然脏读的现象，也就是读未提交(READ_UNCOMMITTED)的那个例子，是不是也符合在同一个事务中返回了不同结果！ 但是反过来就不一定通了，一个事务A中查询两次的结果在被另一个事务B改变的情况下，如果事务B未提交就改变了事务A的结果，就属于脏读，也属于不可重复读。如果该事务B提交了才改变事务A的结果，就不属于脏读，但属于不可重复读。 示例-读提交 新建一张表，并且隔离级别设置为读提交 READ-COMMITTED 1.新建表t1 mysql> create table t1(id int primary key,name char(30),money double); Query OK, 0 rows affected (0.01 sec) 2.插入数据 mysql> insert into t1 values(1,'xiaoming',100); Query OK, 1 row affected (0.00 sec) 3.设置当前会话事务隔离级别为读提交 READ-COMMITTED mysql> SET @@session.tx_isolation = 'READ-COMMITTED'; Query OK, 0 rows affected, 1 warning (0.00 sec) 4.查看事务隔离级别 global是全局 session是会话 mysql> SELECT @@global.tx_isolation, @@tx_isolation; +-----------------------+----------------+ | @@global.tx_isolation | @@tx_isolation | +-----------------------+----------------+ | REPEATABLE-READ | READ-COMMITTED | +-----------------------+----------------+ 1 row in set, 2 warnings (0.00 sec) 5.关闭自动提交 mysql> set @@autocommit = 0; Query OK, 0 rows affected (0.00 sec) 6.查看自动提交是否关闭，1为开启，0为关闭 mysql> select @@autocommit; +--------------+ | @@autocommit | +--------------+ | 0 | +--------------+ 1 row in set (0.00 sec) 不可重复读是指在事务1内，读取了一个数据，事务1还没有结束时，事务2也访问了这个数据，修改了这个数据，并提交。紧接着，事务1又读这个数据。由于事务2的修改，那么事务1两次读到的的数据可能是不一样的，因此称为是不可重复读。 示例-可重复读 新建一张表，并且隔离级别设置为可重复读 REPEATABLE-READ 1.新建表t1 mysql> create table t1(id int primary key,name char(30),money double); Query OK, 0 rows affected (0.01 sec) 2.插入数据 mysql> insert into t1 values(1,'xiaoming',100); Query OK, 1 row affected (0.00 sec) 3.设置当前会话事务隔离级别为可重复读 REPEATABLE-READ mysql> SET @@session.tx_isolation = 'REPEATABLE-READ'; Query OK, 0 rows affected, 1 warning (0.00 sec) 4.查看事务隔离级别 global是全局 session是会话 mysql> SELECT @@global.tx_isolation, @@tx_isolation; +-----------------------+-----------------+ | @@global.tx_isolation | @@tx_isolation | +-----------------------+-----------------+ | REPEATABLE-READ | REPEATABLE-READ | +-----------------------+-----------------+ 1 row in set, 2 warnings (0.00 sec) 5.关闭自动提交 mysql> set @@autocommit = 0; Query OK, 0 rows affected (0.00 sec) 6.查看自动提交是否关闭，1为开启，0为关闭 mysql> select @@autocommit; +--------------+ | @@autocommit | +--------------+ | 0 | +--------------+ 1 row in set (0.00 sec) 上述示例说明当我们将当前会话的隔离级别设置为可重复读的时候，当前会话可以重复读，就是每次读取的结果集都相同，而不管其他事务有没有提交。 当session1 commit的时候，才能看到最终的正确结果，这会产生幻读 幻读 定义 官网对幻读定义的地址 phantom A row that appears in the result set of a query, but not in the result set of an earlier query. For example, if a query is run twice within a transaction, and in the meantime, another transaction commits after inserting a new row or updating a row so that it matches the WHERE clause of the query. 翻译过来就是 在一次查询的结果集里出现了某一行数据，但是该数据并未出现在更早的查询结果集里。例如，在一次事务里进行了两次查询，同时另一个事务插入某一行或更新某一行数据后(该数据符合查询语句里where后的条件)，并提交了！ 大白话说明一下 所谓幻读，指的是当某个事务在读取某个范围内的记录时，另外一个事务又在该范围内插入了新的记录，当之前的事务再次读取该范围的记录时，会产生幻读 就是事务1查询id 幻读(Phantom Read)原因 原因：事务A根据相同条件第二次查询，虽然查询不到事务B提交的新增数据，但是会影响事务A之后的一些操作，比如：事务A进行了一次select * from t1表查询，查询出id为1的数据，同时事务B进行了一次insert into t1 values(2,'xx')，也就是此时表中有了id为2的数据，但是在事务A中再次进行查询的时候，根本就查不到id为2的数据，但是当事务A进行insert into t1 values(2,'xx')，也想插入id为2的数据的时候，发现报错了，但是事务A怎么查也查不到有id为2的数据，这就让事务A的使用者出现了幻觉，what happend！。如果不想出现幻读问题，那么自己在查询语句中手动加锁 for update，如果查询的是id为2的数据，即便是现在没有id为2的数据，其他事务也无法对id为2的索引位置进行数据的处理。 官网对解决幻读方法的地址 原文内容如下 By default, InnoDB operates in REPEATABLE READ transaction isolation level. In this case, InnoDB uses next-key locks for searches and index scans, which prevents phantom rows (see Section 14.7.4, “Phantom Rows”). 翻译过来就是 InnoDB默认用了REPEATABLE READ。在这种情况下，使用next-key locks解决幻读问题！ 示例 新建一张表，并且隔离级别设置为可重复读 REPEATABLE-READ 1.新建表t1 mysql> create table t1(id int primary key,name char(30),money double); Query OK, 0 rows affected (0.01 sec) 2.插入数据 mysql> insert into t1 values(1,'xiaoming',100); Query OK, 1 row affected (0.00 sec) 3.设置当前会话事务隔离级别为可重复读 REPEATABLE-READ mysql> SET @@session.tx_isolation = 'REPEATABLE-READ'; Query OK, 0 rows affected, 1 warning (0.00 sec) 4.查看事务隔离级别 global是全局 session是会话 mysql> SELECT @@global.tx_isolation, @@tx_isolation; +-----------------------+-----------------+ | @@global.tx_isolation | @@tx_isolation | +-----------------------+-----------------+ | REPEATABLE-READ | REPEATABLE-READ | +-----------------------+-----------------+ 1 row in set, 2 warnings (0.00 sec) 5.关闭自动提交 mysql> set @@autocommit = 0; Query OK, 0 rows affected (0.00 sec) 6.查看自动提交是否关闭，1为开启，0为关闭 mysql> select @@autocommit; +--------------+ | @@autocommit | +--------------+ | 0 | +--------------+ 1 row in set (0.00 sec) 不加next-key locks是快照读，根本不能解决幻读问题 mysql> select * from t1; +----+----------+-------+ | id | name | money | +----+----------+-------+ | 1 | xiaoming | 100 | +----+----------+-------+ 1 row in set (0.00 sec) 加上next-key locks就能解决幻读问题 mysql> select * from t1 lock in share mode; +----+----------+-------+ | id | name | money | +----+----------+-------+ | 1 | xiaoming | 100 | | 2 | dahong | 500 | +----+----------+-------+ 2 rows in set (0.00 sec) mysql> select * from t1 for update; +----+----------+-------+ | id | name | money | +----+----------+-------+ | 1 | xiaoming | 100 | | 2 | dahong | 500 | +----+----------+-------+ 2 rows in set (0.00 sec) 很多人容易搞混不可重复读和幻读，确实这两者有些相似。但不可重复读重点在于update和delete，而幻读的重点在于insert。 总的来说幻读就是事务A对数据进行操作，事务B还是可以用insert插入数据的，因为使用的是行锁，这样导致的各种奇葩问题就是幻读 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/redis/1.CentOS7.5安装redis-5.0.7.html":{"url":"db/redis/1.CentOS7.5安装redis-5.0.7.html","title":"redis安装","keywords":"","body":"[toc] CentOS7.5安装redis-5.0.7 redis官网 官网下载地址 1.下载redis wget http://download.redis.io/releases/redis-5.0.7.tar.gz 2.编译安装redis //将下载的redis包解压到/usr/local tar xf redis-5.0.7.tar.gz -C /usr/local //进入解压后的redis目录进行编译安装 cd /usr/local/redis-5.0.7 make //添加环境变量 cat >/etc/profile.d/redis.sh 3.配置redis //创建redis工作目录 mkdir -p /etc/redis/6379 mkdir -p /var/log/redis/6379 mkdir -p /var/run/redis/6379 //创建redis配置文件 cat >/etc/redis/6379/redis.conf 4.使用systemd管理redis ⚠️#使用redis二进制命令启动redis /usr/local/redis-5.0.7/src/redis-server & //编辑文件 cat >/usr/lib/systemd/system/redis.service 这边并没有使用 ExecStop=/bin/kill -s QUIT $MAINPID 这样的命令来停止redis, 因为使用这个语句在运行systemctl stop redis后, redis并未执行关闭动作, 而是直接退出. 这时候用 systemctl status redis 查看状态是failed. 只有用ExecStop=/install_path/bin/redis-cli -p 16379 shutdown 才能正确停止redis 5.使用systemd管理redis遇到的问题 问题一：重载系统服务后启动redis卡住不动 解决方法 注释/usr/lib/systemd/system/redis.serviceType=forking一项 网上解释原因 If set to forking, it is expected that the process configured with ExecStart= will call fork() as part of its start-up. The parent process is expected to exit when start-up is complete and all communication channels are set up. The child continues to run as the main daemon process. This is the behavior of traditional UNIX daemons. If this setting is used, it is recommended to also use the PIDFile= option, so that systemd can identify the main process of the daemon. systemd will proceed with starting follow-up units as soon as the parent process exits. 如果设置为fork，则使用ExecStart=配置的进程将调用fork()作为启动的一部分。启动完成并设置好所有通信通道后，父进程将退出。子进程继续作为主守护进程运行。这是传统UNIX守护进程的行为。如果使用了该设置，建议还使用PIDFile=选项，以便systemd能够识别守护进程的主进程。一旦父进程退出，systemd将开始启动后续单元。 问题二：pid原先路径为/var/run/redis_6379.pid，报错Failed at step EXEC spawning /usr/local/redis-5.0.7/src: Permission denied 解决方法： 将pid路径改为/var/run/redis/redis_6379.pid就可以了，原因未知 6.redis安全配置 protected-mode 保护模式，是否只允许本地访问，默认是yes protected-mode no bind 指定IP进行监听 bind 127.0.0.1 requirepass 增加密码 requirepass 1 #连接方式1 $ redis-cli 127.0.0.1:6379> set name xiaoming (error) NOAUTH Authentication required. 127.0.0.1:6379> AUTH 1 OK 127.0.0.1:6379> set name xiaoming OK #连接方式2 $ redis-cli -a 1 Warning: Using a password with '-a' or '-u' option on the command line interface may not be safe. 127.0.0.1:6379> set name xiaoliang OK 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/redis/redis持久化.html":{"url":"db/redis/redis持久化.html","title":"redis持久化","keywords":"","body":"[toc] redis持久化 1.什么是持久化？ 持久化：就是将内存中的数据，写入到磁盘上，并且永久存在 2.redis持久化类型 RDB AOF RDB和AOF对比 定义对比 优缺点对比 工作方式对比 2.1 RDB持久化 2.1.1 RDB持久化介绍 可以在指定的时间间隔内生成数据集的时间点快照（point-in-time snapshot） 2.1.2 RDB持久化优点 RDB是一种表示某个即时点的Redis数据的紧凑文件。RDB文件适合用于备份。例如，你可能想要每小时归档最近24小时的RDB文件，每天保存近30天的RDB快照。这允许你很容易的恢复不同版本的数据集以容灾 RDB非常适合于灾难恢复，作为一个紧凑的单一文件，可以被传输到远程的数据中心 RDB最大化了Redis的性能，因为Redis父进程持久化时唯一需要做的是启动(fork)一个子进程，由子进程完成所有剩余工作。父进程实例不需要执行像磁盘IO这样的操作 RDB在重启保存了大数据集的实例时比AOF要快 2.1.3 RDB持久化缺点 当你需要在Redis停止工作(例如停电)时最小化数据丢失，RDB可能不太好。你可以配置不同的保存点(save point)来保存RDB文件(例如，至少5分钟和对数据集100次写之后，但是你可以有多个保存点)。然而，你通常每隔5分钟或更久创建一个RDB快照，所以一旦Redis因为任何原因没有正确关闭而停止工作，你就得做好最近几分钟数据丢失的准备了 RDB需要经常调用fork()子进程来持久化到磁盘。如果数据集很大的话，fork()比较耗时，结果就是，当数据集非常大并且CPU性能不够强大的话，Redis会停止服务客户端几毫秒甚至一秒。AOF也需要fork()，但是你可以调整多久频率重写日志而不会有损(trade-off)持久性(durability) 2.1.4 RDB持久化优缺点总结 优点：速度快，适合于用作备份，主从复制也是基于RDB持久化功能实现的 缺点：会有数据丢失、导致服务停止几秒 2.1.5 RDB持久化配置参数 RDB持久化核心配置参数 #持久化数据文件存储位置 dir /etc/redis/6379 #rdb持久化数据文件名 dbfilename dump.rdb #900秒（15分钟）内有1个更改 save 900 1 #300秒（5分钟）内有10个更改 save 300 10 #60秒（1分钟）内有10000个更改 save 60 10000 配置RDB持久化 ⚠️save这里这是做演示，还是要根据实际使用情况进行修改，例如我们线上主流的配置是 7200 10 1.编辑配置文件 cat > /etc/redis/6379/redis.conf SHUTDOWN not connected> #指定配置文件启动 redis-server /etc/redis/6379/redis.conf 3.写入数据 #连接redis，如果设置了密码，使用-a参数 redis-cli 127.0.0.1:6379> set a 1 OK 127.0.0.1:6379> set b 2 OK 4.手动保存RDB持久化 127.0.0.1:6379> BGSAVE Background saving started 5.查看RDB持久化文件 dump.rdb就是RDB持久化文件 $ ll total 16 -rw-r--r-- 1 root root 92 Dec 10 19:31 dump.rdb -rw-r--r-- 1 root root 410 Dec 10 19:22 redis.conf -rw-r--r-- 1 root root 4314 Dec 10 19:31 redis.log 2.2 AOF持久化 2.2.1 AOF持久化介绍 AOF（append only file）只追加文件，记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。 AOF 文件中的命令全部以 Redis 协议的格式来保存，新命令会被追加到文件的末尾 AOF持久化原理 2.2.2 AOF持久化优点 使用AOF Redis会更具有可持久性(durable)：你可以有很多不同的fsync策略：没有fsync，每秒fsync，每次请求时fsync。使用默认的每秒fsync策略，写性能也仍然很不错(fsync是由后台线程完成的，主线程继续努力地执行写请求)，即便你也就仅仅只损失一秒钟的写数据 AOF日志是一个追加文件，所以不需要定位，在断电时也没有损坏问题。即使由于某种原因文件末尾是一个写到一半的命令(磁盘满或者其他原因),redis-check-aof工具也可以很轻易的修复 当AOF文件变得很大时，Redis会自动在后台进行重写。重写是绝对安全的，因为Redis继续往旧的文件中追加，使用创建当前数据集所需的最小操作集合来创建一个全新的文件，一旦第二个文件创建完毕，Redis就会切换这两个文件，并开始往新文件追加 AOF文件里面包含一个接一个的操作，以易于理解和解析的格式存储。你也可以轻易的导出一个AOF文件。例如，即使你不小心错误地使用FLUSHALL命令清空一切，如果此时并没有执行重写，你仍然可以保存你的数据集，你只要停止服务器，删除最后一条命令，然后重启Redis就可以 2.2.3 AOF持久化缺点 对同样的数据集，AOF文件通常要大于等价的RDB文件 AOF可能比RDB慢，这取决于准确的fsync策略。通常fsync设置为每秒一次的话性能仍然很高，如果关闭fsync，即使在很高的负载下也和RDB一样的快。不过，即使在很大的写负载情况下，RDB还是能提供能好的最大延迟保证 在过去，我们经历了一些针对特殊命令(例如，像BRPOPLPUSH这样的阻塞命令)的罕见bug，导致在数据加载时无法恢复到保存时的样子。这些bug很罕见，我们也在测试套件中进行了测试，自动随机创造复杂的数据集，然后加载它们以检查一切是否正常，但是，这类bug几乎不可能出现在RDB持久化中。为了说得更清楚一点：Redis AOF是通过递增地更新一个已经存在的状态，像MySQL或者MongoDB一样，而RDB快照是一次又一次地从头开始创造一切，概念上更健壮。但是，要注意Redis每次重写AOF时都是以当前数据集中的真实数据从头开始，相对于一直追加的AOF文件(或者一次重写读取老的AOF文件而不是读内存中的数据)对bug的免疫力更强。我们还没有收到一份用户在真实世界中检测到崩溃的报告 2.2.4 AOF持久化优缺点总结 优点：可以最大程度保证数据不丢失 缺点：日志记录量级比较大 2.2.5 AOF持久化配置参数 AOF持久化核心配置参数 #是否打开AOF日志功能 appendonly yes/no 以下三种方法只能同时打开一种 #每一条命令都立即同步到AOF appendfsync always #每秒写一次 appendfsync everysec #写入工作交给操作系统,由操作系统判断缓冲区大小,统一写入到AOF appendfsync no 配置AOF持久化 1.编辑配置文件 cat >/etc/redis/6379/redis.conf SHUTDOWN not connected> #指定配置文件启动 redis-server /etc/redis/6379/redis.conf 3.写入数据 #连接redis，如果设置了密码，使用-a参数 redis-cli 127.0.0.1:6379> set name hehe OK 4.查看AOF文件 appendonly.aof就是AOF持久化日志文件 $ ll /etc/redis/6379/ total 20 -rw-r--r-- 1 root root 56 Dec 10 19:49 appendonly.aof -rw-r--r-- 1 root root 92 Dec 10 19:49 dump.rdb -rw-r--r-- 1 root root 182 Dec 10 19:49 redis.conf -rw-r--r-- 1 root root 6358 Dec 10 19:49 redis.log 5.AOF文件内容 $ cat /etc/redis/6379/appendonly.aof *2 $6 SELECT $1 0 *3 $3 set $4 name $4 hehe *3 $3 set $4 test $4 test 3.如何选择RDB和AOF 1.一般来说,如果想达到足以媲美 PostgreSQL 的数据安全性， 你应该同时使用两种持久化功能 2.如果你非常关心你的数据,但仍然可以承受数分钟以内的数据丢失， 那么你可以只使用 RDB 持久化 3.有很多用户单独使用AOF，但是我们并不鼓励这样，因为时常进行RDB快照非常方便于数据库备份，启动速度也较之快，还避免了AOF引擎的bug 4.在企业中，通常都使用RDB来做持久化，因为一般redis是在做MySQL的缓存，就算缓存数据丢失，真实的数据还是在MySQL中，之所以用缓存是为了速度，性能而考虑，所以还是建议使用RDB持久化，相对来说会好一些，除非专门用redis来做一个key:value的数据库，而且数据很重要，那么可以考虑使用AOF 注意：基于这些原因，将来我们可能会统一AOF和RDB为一种单一的持久化模型(长远计划) RDB和AOF持久化对比 4.RDB与AOF工作方式 4.1 RDB 4.1.1 RDB快照工作方式 1.默认情况下，Redis保存数据集快照到磁盘，名为dump.rdb的二进制文件。你可以设置让Redis在N秒内至少有M次数据集改动时保存数据集，或者你也可以手动调用SAVE或者BGSAVE命令 2.在上文中我们已经在配置文件中做过对应的配置： 例如，这个配置会让Redis在每个60秒内至少有1000次键改动时自动转储数据集到磁盘： save 60 1000 3.当 Redis 需要保存 dump.rdb 文件时，服务器执行以下操作： Redis 调用 fork() ，同时拥有父进程和子进程 子进程将数据集写入到一个临时的 RDB 文件中。当子进程完成对新 RDB 文件的写入时， Redis 用新RDB 文件替换原来的 RDB 文件，并删除旧的 RDB 文件 4.这种方式使得 Redis 可以从写时复制机制中获益 RDB是先将数据集写入到一个临时文件，当临时文件全部写完时，再覆盖dump.rdb文件 4.1.2 RDB持久化方式 save、bgsave、自动化 save 该命令会阻塞当前Redis服务器，执行save命令期间，Redis不能处理其他命令，直到RDB过程完成为止。具体流程如下： bgsave 执行该命令时，Redis会在后台异步进行快照操作，快照同时还可以响应客户端请求。具体流程如下： 自动化 自动触发是由我们的配置文件来完成的。在redis.conf配置文件中，里面有如下配置，我们可以去设置： ①save：这里是用来配置触发 Redis的 RDB 持久化条件，也就是什么时候将内存中的数据保存到硬盘。比如save m n。表示m秒内数据集存在n次修改时，自动触发bgsave。 默认如下配置： save 900 1 表示900 秒内如果至少有 1 个 key 的值变化，则保存save 900 1 save 300 10 表示300 秒内如果至少有 10 个 key 的值变化，则保存save 300 10 save 60 10000 表示60 秒内如果至少有 10000 个 key 的值变化，则保存save 60 10000 不需要持久化，那么你可以注释掉所有的 save 行来停用保存功能。 ②stop-writes-on-bgsave-error ：默认值为yes。当启用了RDB且最后一次后台保存数据失败，Redis是否停止接收数据。这会让用户意识到数据没有正确持久化到磁盘上，否则没有人会注意到灾难（disaster）发生了。如果Redis重启了，那么又可以重新开始接收数据了 ③rdbcompression ；默认值是yes。对于存储到磁盘中的快照，可以设置是否进行压缩存储。 ④rdbchecksum ：默认值是yes。在存储快照后，我们还可以让redis使用CRC64算法来进行数据校验，但是这样做会增加大约10%的性能消耗，如果希望获取到最大的性能提升，可以关闭此功能。 ⑤dbfilename ：设置快照的文件名，默认是 dump.rdb ⑥dir：设置快照文件的存放路径，这个配置项一定是个目录，而不能是文件名。 save于bgsave对比 4.2 AOF 4.2.1 AOF重写功能 1.因为 AOF 的运作方式是不断地将命令追加到文件的末尾，所以随着写入命令的不断增加， AOF 文件的体积也变得越来越大。举个例子，如果你对一个计数器调用了 100 次 INCR ，那么仅仅是为了保存这个计数器的当前值， AOF 文件就需要使用 100 条记录。然而在实际上，只使用一条 SET 命令已经足以保存计数器的当前值了，其余 99 条记录实际上都是多余的 2.为了处理这种情况， Redis 支持一种有趣的特性：可以在不断服务客户端的情况下，对 AOF 文件进行重建。执行 BGREWRITEAOF 命令， Redis 将生产一个新的 AOF 文件，这个文件包含重建当前数据集所需的最少命令 AOF重写原理 4.2.2 AOF有多持久 你可以配置 Redis 多久才将数据 fsync 到磁盘一次。 有三个选项： always：每次有新命令追加到 AOF 文件时就执行一次 fsync ：非常慢，也非常安全。 everysec：每秒 fsync 一次：足够快（和使用 RDB 持久化差不多，）并且在故障时只会丢失1秒钟的数据。 no：从不 fsync，将数据交给操作系统来处理。更快，也更不安全的选择。 推荐（并且也是默认）的措施为每秒 fsync 一次，这种 fsync 策略可以兼顾速度和安全性。 总之fsync 的策略在实际使用中非常慢，即使在 Redis2.0 对相关的程序进行了改进之后仍是如此。频繁调用 fsync 注定了这种策略不可能快得起来。 4.2.3 AOF文件出错解决方法 服务器可能在程序正在对AOF文件进行写入时停机，如果停机造成了AOF文件出错，那么 Redis 在重启时会拒绝载入这个 AOF 文件，从而确保数据的一致性不会被破坏 当发生 AOF 文件出错时，可以用以下方法来修复出错的 AOF 文件： 1.为现有的 AOF 文件创建一个备份 2.使用 Redis 附带的 redis-check-aof 程序，对原来的AOF文件进行修复。redis-check-aof --fix 3.使用 diff -u 对比修复后的 AOF 文件和原始 AOF 文件的备份，查看两个文件之间的不同之处 4.重启 Redis 服务器，等待服务器载入修复后的 AOF 文件，并进行数据恢复 4.3 RDB和AOF之间的相互作用 在版本号大于等于 2.4 的 Redis 中， BGSAVE 执行的过程中，不可以执行 BGREWRITEAOF 。 反过来说，在 BGREWRITEAOF 执行的过程中，也不可以执行 BGSAVE 这可以防止两个 Redis 后台进程同时对磁盘进行大量的 I/O 操作。如果 BGSAVE 正在执行，并且用户显示地调用 BGREWRITEAOF 命令，那么服务器将向用户回复一个 OK 状态，并告知用户， BGREWRITEAOF 已经被预定执行； 一旦 BGSAVE 执行完毕， BGREWRITEAOF 就会正式开始 当 Redis 启动时，如果 RDB 持久化和 AOF 持久化都被打开了，那么程序会优先使用 AOF 文件来恢复数据集，因为 AOF 文件所保存的数据通常是最完整的 5.redis备份 5.1 备份策略 1.Redis 对于数据备份是非常友好的，因为你可以在服务器运行的时候对 RDB 文件进行复制： RDB 文件一旦被创建，就不会进行任何修改 2.当服务器要创建一个新的 RDB 文件时，它先将文件的内容保存在一个临时文件里面，当临时文件写入完毕时，程序才使用临时文件替换原来的 RDB 文件 3.这也就是说，无论何时， 复制 RDB 文件都是绝对安全的 以下是我们的建议： 创建一个定期任务（cron job）， 每小时将一个 RDB 文件备份到一个文件夹， 并且每天将一个 RDB 文件备份到另一个文件夹 确保快照的备份都带有相应的日期和时间信息， 每次执行定期任务脚本时， 使用 find 命令来删除过期的快照： 比如说， 你可以保留最近 48 小时内的每小时快照， 还可以保留最近一两个月的每日快照 至少每天一次， 将 RDB 备份到你的数据中心之外， 或者至少是备份到你运行 Redis 服务器的物理机器之外 5.2 redis备份、恢复 redis持久化就是redis备份 将备份文件 (dump.rdb) 移动到 redis 安装目录并启动服务即可，使用CONFIG GET dir命令获取redis目录 6.redis持久化高级配置 RDB持久化高级配置 #后台备份进程出错时,主进程停不停止写入? 主进程不停止容易造成数据不一致 stop-writes-on-bgsave-error yes #导出的rdb文件是否压缩 如果rdb的大小很大的话建议这么做 rdbcompression yes #导入rbd恢复数据时,要不要检验rdb的完整性 验证版本是不是一致 rdbchecksum yes AOF持久化高级配置 #正在导出rdb快照的过程中,要不要停止同步aof no-appendfsync-on-rewrite yes/no #aof文件大小比起上次重写时的大小,增长率100%时重写,缺点:业务开始的时候，会重复重写多次 auto-aof-rewrite-percentage 100 #aof文件,至少超过64M时,重写 auto-aof-rewrite-min-size 64mb 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/redis/redis命令.html":{"url":"db/redis/redis命令.html","title":"redis命令","keywords":"","body":"[toc] redis命令 redis命令官方文档 ⚠️ keys * 命令不要在生产环境中使用！！！ 这里仅仅是为了实验使用这个命令 redis-cli 语法 redis-cli 选项 -h 指定连接的redis服务端IP地址，不写默认本机 -p 指定端口 -a 指定密码 示例 redis没有配置密码 $ redis-cli 127.0.0.1:6379> redis配置了密码 redis-cli -h 127.0.0.1 -p 6379 -a \"mypass\" redis设置密码 #方法一 配置文件 在redis的配置文件中写入以下一行即为设置redis密码 requirepass password #方法二 命令行设置密码，如果需要取消密码则设置为空即可 CONFIG set requirepass \"password\" 命令行设置redis密码后通过redis-cli命令进入redis就需要输入auth命令进行验证 redis-server 示例：指定配置文件启动redis redis-server /etc/redis/6379/redis.conf keys命令 del key 含义：该命令用于在key存在时删除 key 示例： 127.0.0.1:6379> get str \"6abc\" 127.0.0.1:6379> DEL str (integer) 1 127.0.0.1:6379> get str (nil) dump key 含义：序列化key，并返回被序列化的值 示例： 127.0.0.1:6379> GET str \"abc\" 127.0.0.1:6379> DUMP str \"\\x00\\x03abc\\a\\x00&\\x9e\\xe5\\xceI\\xb8w\\xf8\" exists key 含义：检查指定key是否存在，存在返回1，不存在返回0 示例： 127.0.0.1:6379> EXISTS str (integer) 1 127.0.0.1:6379> EXISTS str1 (integer) 0 expire key seconds 含义：为给定key设置过期时间，以秒为单位 示例： 127.0.0.1:6379> GET str \"abc\" 127.0.0.1:6379> EXPIRE str 5 (integer) 1 127.0.0.1:6379> GET str \"abc\" 127.0.0.1:6379> GET str \"abc\" 127.0.0.1:6379> GET str (nil) expireat key timestamp 含义：EXPIREAT 的作用和 EXPIRE 类似，都用于为key设置过期时间。 不同在于 EXPIREAT 命令接受的时间参数是 UNIX 时间戳(unix timestamp) 示例： 127.0.0.1:6379> EXPIREAT str 1688666880 (integer) 0 1688666880是unix时间戳秒，对应的时间是 2023/7/7 2:8:0 pexpire key milliseconds 含义：设置key的过期时间以毫秒计 示例： 127.0.0.1:6379> PEXPIRE str 3000 (integer) 1 127.0.0.1:6379> GET str \"abc\" 127.0.0.1:6379> GET str (nil) pexpireat key milliseconds-timestamp 含义：设置key过期时间的时间戳(unix timestamp) 以毫秒计 示例： 127.0.0.1:6379> PEXPIREAT str 1655555555005 (integer) 1 1655555555005是unix时间戳毫秒，对应的时间是 2022/6/18 20:32:35 keys pattern 含义：查找所有符合给定模式( pattern)的 key 示例： 127.0.0.1:6379> SET str1 aaa OK 127.0.0.1:6379> SET str2 bbb OK 127.0.0.1:6379> SET str3 ccc OK #查找以str开头的key 127.0.0.1:6379> KEYS str* 1) \"str1\" 2) \"str3\" 3) \"str2\" move key db 含义：将当前数据库的key移动到给定的数据库db当中 示例： ⚠️当从一个数据库移动一个不存在的key到另一个数据库时会失败 ⚠️当两个数据库存在相同的key时，则无法移动 #redis默认使用数据库0，这里再显示指定一次 127.0.0.1:6379> SELECT 0 OK 127.0.0.1:6379> SET str aaa OK #把str移动到数据库1 127.0.0.1:6379> MOVE str 1 (integer) 1 #在数据库0检查str是否存在，返回0表示不存在 127.0.0.1:6379> EXISTS str (integer) 0 #切换到库1，然后检查str是否存在 127.0.0.1:6379> SELECT 1 OK 127.0.0.1:6379[1]> EXISTS str (integer) 1 persist key 含义：移除key的过期时间，key将持久保持 示例： #设置过期时间为10秒 127.0.0.1:6379> EXPIRE str 10 (integer) 1 #取消过期时间，返回值为1表示取消成功 127.0.0.1:6379> PERSIST str (integer) 1 pttl key 含义：以毫秒为单位返回key的剩余的过期时间 示例： #设置过期时间 127.0.0.1:6379> EXPIRE str 10 (integer) 1 #返回结果为还有7109毫秒过期 127.0.0.1:6379> PTTL str (integer) 7109 ttl key 含义：以秒为单位，返回给定 key 的剩余生存时间(TTL, time to live) 示例： 127.0.0.1:6379> EXPIRE str 10 (integer) 1 127.0.0.1:6379> TTL str (integer) 8 randomkey 含义：从当前数据库中随机返回一个 key 示例： 127.0.0.1:6379> MSET str1 a str2 b str3 c OK 127.0.0.1:6379> RANDOMKEY \"str1\" 127.0.0.1:6379> RANDOMKEY \"str2\" 127.0.0.1:6379> RANDOMKEY \"str1\" 127.0.0.1:6379> RANDOMKEY \"str2\" 127.0.0.1:6379> RANDOMKEY \"str1\" 127.0.0.1:6379> RANDOMKEY \"str1\" 127.0.0.1:6379> RANDOMKEY \"str3\" pename key newkey 含义：修改key的名称 示例： 127.0.0.1:6379> GET str \"aaa\" 127.0.0.1:6379> RENAME str str1 OK 127.0.0.1:6379> KEYS * 1) \"str1\" renamenx key newkey 含义：仅当newkey不存在时，将key改名为newkey 示例： 127.0.0.1:6379> KEYS * 1) \"str2\" 127.0.0.1:6379> RENAMENX str2 str3 (integer) 1 127.0.0.1:6379> KEYS * 1) \"str3\" type key 含义：返回key所存储值的类型 示例： 127.0.0.1:6379> type str3 string 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/redis/redis危险命令.html":{"url":"db/redis/redis危险命令.html","title":"redis危险命令","keywords":"","body":"[toc] redis危险命令 keys * 虽然其模糊匹配功能使用非常方便也很强大，在小数据量情况下使用没什么问题，数据量大会导致 Redis 锁住及 CPU 飙升，在生产环境建议禁用或者重命名！ flushdb 删除Redis中当前所在数据库中的所有记录，并且该命令是原子性的，不会终止执行，一旦执行，将不会执行失败。 flushall 删除Redis中所有数据库中的所有记录，并且该命令是原子性的，不会终止执行，一旦执行，将不会执行失败。 config 修改redis配置命令，一般在redis配置文件中做配置 禁用redis危险命令 编辑redis配置文件，引号中内容为空则为禁用命令，有内容则为给命令起别名 rename-command KEYS \"\" rename-command FLUSHALL \"\" rename-command FLUSHDB \"\" rename-command CONFIG \"\" 示例 #禁用keys * 127.0.0.1:6379> KEYS * (error) ERR unknown command 'KEYS' #使用命令别名，keys命令别名设置为了hehe 127.0.0.1:6379> hehe * 1) \"str2\" 2) \"str1\" 3) \"str3\" 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/redis/redis数据类型/1.redis数据类型-string字符串.html":{"url":"db/redis/redis数据类型/1.redis数据类型-string字符串.html","title":"string 字符串","keywords":"","body":"[toc] redis数据类型 string 字符串 redis命令官方文档 string(字符串) 简介： 字符串 可以存储的值： 字符串，整数或者浮点数，还有jpg图片或者序列化对象 操作： 对整个字符串或者字符串的其中一部分执行操作，对整数和浮点数执行自增或者自减操作 应用场景： 做简单的键值对缓存 使用示例： 127.0.0.1:6379> set a 1 OK 127.0.0.1:6379> get a \"1\" 127.0.0.1:6379> del a (integer) 1 127.0.0.1:6379> get a (nil) redis字符串常用命令 set key value 含义：设置指定key的值 示例： 127.0.0.1:6379> SET str a OK get key 含义：获取指定key的值 示例： 127.0.0.1:6379> GET str \"a\" setrange key offset value 含义：用value参数覆写给定key所储存的字符串值，从偏移量offset开始 示例： 127.0.0.1:6379> GET str \"abc\" 127.0.0.1:6379> SETRANGE str 1 123 (integer) 4 127.0.0.1:6379> GET str \"a123\" getrange key start end 含义：返回key中字符串值的子字符 示例： 127.0.0.1:6379> GET str \"abcdef\" 127.0.0.1:6379> GETRANGE str 0 3 \"abcd\" getset key value 含义：将给定key的值设为value ，并返回key的旧值(old value) 示例： 127.0.0.1:6379> GET str \"abcdef\" 127.0.0.1:6379> GETSET str 123 \"abcdef\" 127.0.0.1:6379> GET str \"123\" getbit key offset 含义：对key所储存的字符串值，获取指定偏移量上的位(bit) 示例： 127.0.0.1:6379> GET str \"abcd\" 127.0.0.1:6379> GETBIT str 0 (integer) 0 127.0.0.1:6379> GETBIT str 1 (integer) 1 127.0.0.1:6379> GETBIT str 2 (integer) 1 127.0.0.1:6379> GETBIT str 3 (integer) 0 setbit key offset value 含义：对key所储存的字符串值，设置或清除指定偏移量上的位(bit) 示例： 27.0.0.1:6379> GET str \"abcd\" 127.0.0.1:6379> GETBIT str 0 (integer) 0 127.0.0.1:6379> GETBIT str 1 (integer) 1 127.0.0.1:6379> GETBIT str 2 (integer) 1 127.0.0.1:6379> GETBIT str 3 (integer) 0 127.0.0.1:6379> SETBIT str 0 1 (integer) 0 127.0.0.1:6379> GETBIT str 0 (integer) 1 mset key value [key value] 含义：同时设置一个或多个key-value对 示例： 127.0.0.1:6379> MSET str1 aaa str2 bbb OK mget key1 [key2] 含义：获取所有(一个或多个)给定key的值 示例： 127.0.0.1:6379> MGET str1 str2 1) \"a\" 2) \"b\" setex key seconds value 含义：将值value关联到key ，并将key的过期时间设为seconds (以秒为单位) 示例： 127.0.0.1:6379> SETEX str 10 abc OK #10秒后str的值失效 127.0.0.1:6379> GEt str (nil) setnx key value 含义：只有在key不存在时设置key的值 示例： 127.0.0.1:6379> GET str \"abc\" 127.0.0.1:6379> SETNX str 123 (integer) 0 127.0.0.1:6379> GET str \"abc\" 127.0.0.1:6379> SETNX str2 123 (integer) 1 127.0.0.1:6379> GET str2 \"123\" msetnx key value [key value] 含义：同时设置一个或多个key-value对，当且仅当所有给定key都不存在，如果key存在则不改变 示例： 127.0.0.1:6379> MSETNX str5 5 str6 6 (integer) 1 127.0.0.1:6379> MGET str5 str6 1) \"5\" 2) \"6\" psetex key milliseconds value 含义：这个命令和 SETEX 命令相似，但它以毫秒为单位设置key的生存时间，而不是像 SETEX 命令那样，以秒为单位 示例： 127.0.0.1:6379> PSETEX str 3000 a123 OK #str 3000毫秒(3秒)后失效 127.0.0.1:6379> GET str (nil) incr key 含义：将key中储存的数字值增一 示例： 127.0.0.1:6379> GET str \"10\" 127.0.0.1:6379> INCR str (integer) 11 127.0.0.1:6379> GET str \"11\" incrby key increment 含义：将key所储存的值加上给定的增量值（increment） 示例： 127.0.0.1:6379> GET str \"11\" 127.0.0.1:6379> INCRBY str 2 (integer) 13 127.0.0.1:6379> GET str \"13\" incrbyfloat key increment 含义：将key所储存的值加上给定的浮点增量值（increment） 示例： 127.0.0.1:6379> GET str \"13\" 127.0.0.1:6379> INCRBYFLOAT str 0.5 \"13.5\" 127.0.0.1:6379> GET str \"13.5\" decr key 含义：将key中储存的数字值减一 示例： 127.0.0.1:6379> GET str \"10\" 127.0.0.1:6379> DECR str (integer) 9 127.0.0.1:6379> GET str \"9\" decrby key decrement 含义：key所储存的值减去给定的减量值（decrement） 示例： 127.0.0.1:6379> GET str \"9\" 127.0.0.1:6379> DECRBY str 3 (integer) 6 127.0.0.1:6379> GET str \"6\" append key value 含义：如果key已经存在并且是一个字符串， APPEND 命令将指定的value追加到该key原来值（value）的末尾 示例： 127.0.0.1:6379> GET str \"6\" 127.0.0.1:6379> APPEND str abc (integer) 4 127.0.0.1:6379> GET str \"6abc\" strlen key 含义：返回key所储存的字符串值的长度 示例： 127.0.0.1:6379> GET str \"6abc\" 127.0.0.1:6379> STRLEN str (integer) 4 在生产环境中使用``keys *`` 这个命令之后会有意想不到的结果 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/redis/redis数据类型/2.redis数据类型-list列表.html":{"url":"db/redis/redis数据类型/2.redis数据类型-list列表.html","title":"list 列表","keywords":"","body":"[toc] redis数据类型 list 列表 list(列表) 简介： 链表（双向链表） 可以存储的值： 列表 操作： 从两端压入或者弹出元素，对单个或者多个元素进行修改，只保留一个范围内的元素 应用场景：最新消息排行；消息队列 范围：一个列表最多可以包含 2^32 - 1 个元素 (4294967295, 每个列表超过40亿个元素) 使用示例： 127.0.0.1:6379> LPUSH test a (integer) 1 127.0.0.1:6379> LPUSH test b (integer) 2 127.0.0.1:6379> LPUSH test c (integer) 3 127.0.0.1:6379> LRANGE test 0 -1 1) \"c\" 2) \"b\" 3) \"a\" 127.0.0.1:6379> LPOP test \"c\" redis列表常用命令 lpush/rpush key value1 [value2] 含义：从左/从右向列表中添加一个或多个值 示例： lpush 从左边开始插入值，先插入的值在后边 127.0.0.1:6379> LPUSH lst a (integer) 1 127.0.0.1:6379> LPUSH lst b (integer) 2 127.0.0.1:6379> LPUSH lst c (integer) 3 127.0.0.1:6379> LRANGE lst 0 -1 1) \"c\" 2) \"b\" 3) \"a\" rpush 从右边开始插入值，先插入的值在前边 127.0.0.1:6379> RPUSH l a (integer) 1 127.0.0.1:6379> RPUSH l b (integer) 2 127.0.0.1:6379> RPUSH l c (integer) 3 127.0.0.1:6379> LRANGE l 0 -1 1) \"a\" 2) \"b\" 3) \"c\" blpop key1 [key2] timeout 含义：移出并获取列表的第一个元素， 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止 示例： 127.0.0.1:6379> LPUSH lst a b c (integer) 3 127.0.0.1:6379> BLPOP lst 3 1) \"lst\" 2) \"c\" brpoplpush source destination timeout 含义：从列表中弹出一个值，将弹出的元素插入到另外一个列表中并返回它； 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止 示例： #从列表lst中弹出一个值插入到lst1中，并且返回lst中的这个值 127.0.0.1:6379> BRPOPLPUSH lst lst1 3 \"a\" #弹出lst1中的值 127.0.0.1:6379> LPOP lst1 \"a\" 127.0.0.1:6379> LPOP lst1 (nil) lindex key index 含义：通过索引获取列表中的元素 示例： 127.0.0.1:6379> LPUSH lst a b c (integer) 3 127.0.0.1:6379> LINDEX lst 0 \"c\" linsert key befor|after pivot value 含义：在列表的元素前或者后插入元素 示例： #向列表lst中插入3个值，因为是lpush，所以第一个元素是c 127.0.0.1:6379> LPUSH lst a b c (integer) 3 #在元素c之前插入d 127.0.0.1:6379> LINSERT lst before c d (integer) 4 #原先c是第一个元素，现在第一个元素是d 127.0.0.1:6379> LINDEX lst 0 \"d\" llen key 含义：获取列表长度 示例： 127.0.0.1:6379> LLEN lst (integer) 4 lpop key 含义：移出并获取列表的第一个元素 示例： 127.0.0.1:6379> RPUSH lst a b c d e f (integer) 6 127.0.0.1:6379> LPOP lst \"a\" rpop key 含义：移除列表的最后一个元素，返回值为移除的元素 示例： 127.0.0.1:6379> RPUSH lst a b c d e f (integer) 6 127.0.0.1:6379> RPOP lst \"f\" lpush key value1 [value2] 含义：将一个或多个值插入到列表头部 示例： 127.0.0.1:6379> LPUSH lst aaa bbb (integer) 5 lpushx key value 含义：将一个值插入到已存在的列表头部 示例： 127.0.0.1:6379> LPUSHX lst 123 (integer) 6 #插入一个不存在的列表，返回值为0 127.0.0.1:6379> LPUSHX lst1 123 (integer) 0 lrange key start stop 含义：获取列表指定范围内的元素，下标从0开始 示例： 127.0.0.1:6379> LRANGE lst 1 3 1) \"bbb\" 2) \"aaa\" 3) \"c\" 127.0.0.1:6379> LRANGE lst 0 3 1) \"123\" 2) \"bbb\" 3) \"aaa\" 4) \"c\" lrem key count value 含义：移除列表元素 count的值 count > 0 : 从表头开始向表尾搜索，移除与 VALUE 相等的元素，数量为 COUNT 。 count count = 0 : 移除表中所有与 VALUE 相等的值。 示例： #rpush插入元素，因此元素从左往右为a b c a b c 127.0.0.1:6379> RPUSH lst a b c a b c (integer) 6 #从表头开始向表尾搜索，移除2个与a相等的元素，所以列表中剩下的元素是b c b c 127.0.0.1:6379> LREM lst 2 a (integer) 2 127.0.0.1:6379> LRANGE lst 0 -1 1) \"b\" 2) \"c\" 3) \"b\" 4) \"c\" lset key index value 含义：通过索引设置列表元素的值 示例： 127.0.0.1:6379> RPUSH lst a b c a b c (integer) 6 127.0.0.1:6379> LSET lst 0 hehe OK 127.0.0.1:6379> LRANGE lst 0 -1 1) \"hehe\" 2) \"b\" 3) \"c\" 4) \"a\" 5) \"b\" 6) \"c\" ltrim key start stop 含义：对一个列表进行修剪(trim)，就是说，让列表只保留指定区间内的元素，不在指定区间之内的元素都将被删除 示例： #插入元素 127.0.0.1:6379> RPUSH lst a b c d e f (integer) 6 #保留从下标1开始到结尾，即把下标为0的第一个元素删除 127.0.0.1:6379> LTRIM lst 1 -1 OK #下标为0的元素被删除 127.0.0.1:6379> LRANGE lst 0 -1 1) \"b\" 2) \"c\" 3) \"d\" 4) \"e\" 5) \"f\" RPOPLPUSH source destination 含义：移除列表的最后一个元素，并将该元素添加到另一个列表并返回 示例： 127.0.0.1:6379> RPUSH lst1 aaa (integer) 1 127.0.0.1:6379> RPUSH lst2 bbb ccc (integer) 2 127.0.0.1:6379> RPOPLPUSH lst1 lst2 \"aaa\" 127.0.0.1:6379> LRANGE lst2 0 -1 1) \"aaa\" 2) \"bbb\" 3) \"ccc\" rpushx key value 含义：为已存在的列表添加值，每次只能添加一个值 示例： 127.0.0.1:6379> RPUSH lst a b c (integer) 3 127.0.0.1:6379> RPUSHX lst d (integer) 4 127.0.0.1:6379> LRANGE lst 0 -1 1) \"a\" 2) \"b\" 3) \"c\" 4) \"d\" 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/redis/redis数据类型/3.redis数据类型-hash字典.html":{"url":"db/redis/redis数据类型/3.redis数据类型-hash字典.html","title":"hash 字典","keywords":"","body":"[toc] redis数据类型 hash 字典 hash(字典) 简介： 键值对集合，即编程语言中的map类型 可以存储的值： 适合存储对象，并且可以像数据库中的update一样，只修改某一项的属性值 操作： 添加、获取、移除单个键值对，获取所有键值对，检查某个键是否存在 应用场景： 存储、读取、修改用户属性 范围：Redis 中每个 hash 可以存储 2^32 - 1 键值对（40多亿） 使用示例: #定义一个哈希man，有名字、年龄、体重描述信息 127.0.0.1:6379> HMSET man name \"xiaoming\" age 18 weight \"60kg\" OK 127.0.0.1:6379> HGETALL man 1) \"name\" 2) \"xiaoming\" 3) \"age\" 4) \"18\" 5) \"weight\" 6) \"60kg\" redis哈希常用命令 hdel key field1 [field2] 含义：删除一个或多个哈希表字段 示例： 127.0.0.1:6379> HMSET man name \"xiaoming\" age 18 weight \"60kg\" OK 127.0.0.1:6379> HGETALL man 1) \"name\" 2) \"xiaoming\" 3) \"age\" 4) \"18\" 5) \"weight\" 6) \"60kg\" #删除哈希中的weight字段 127.0.0.1:6379> HDEL man weight (integer) 1 127.0.0.1:6379> HGETALL man 1) \"name\" 2) \"xiaoming\" 3) \"age\" 4) \"18\" hexists key field 含义：查看哈希表key中，指定的字段是否存在 示例： 127.0.0.1:6379> HMSET man name \"xiaoming\" age 18 weight \"60kg\" OK 127.0.0.1:6379> HEXISTS man name (integer) 1 127.0.0.1:6379> HEXISTS man sex (integer) 0 hget key fidle 含义：获取存储在哈希表中指定字段的值 示例： 127.0.0.1:6379> HMSET man name \"xiaoming\" age 18 weight \"60kg\" OK 127.0.0.1:6379> HGET man name \"xiaoming\" hincrby key fidle increment 含义：为哈希表key中的指定字段的整数值加上增量 increment 示例： 127.0.0.1:6379> HMSET man name \"xiaoming\" age 18 weight \"60kg\" OK 127.0.0.1:6379> HINCRBY man age 2 (integer) 20 127.0.0.1:6379> HGET man age \"20\" hincrbyfloat key field increment 含义：为哈希表key中的指定字段的浮点数值加上增量 increment 示例： 127.0.0.1:6379> HMSET man name \"xiaoming\" age 18 weight \"60kg\" OK 127.0.0.1:6379> HINCRBYFLOAT man age 0.5 \"18.5\" 127.0.0.1:6379> HGET man age \"18.5\" hkeys key 含义：获取所有哈希表中的字段 示例： 127.0.0.1:6379> HMSET man name \"xiaoming\" age 18 weight \"60kg\" OK 127.0.0.1:6379> HKEYS man 1) \"name\" 2) \"age\" 3) \"weight\" hvals key 含义：获取哈希表中所有值 示例： 127.0.0.1:6379> HMSET man name \"xiaoming\" age 18 weight \"60kg\" OK 127.0.0.1:6379> HVALS man 1) \"xiaoming\" 2) \"18\" 3) \"60kg\" hlen key 含义：获取哈希表中字段的数量 示例： 127.0.0.1:6379> HMSET man name \"xiaoming\" age 18 weight \"60kg\" OK 127.0.0.1:6379> HLEN man (integer) 3 hmget key field1 [field2] 含义：获取所有给定字段的值 示例： 127.0.0.1:6379> HMSET man name \"xiaoming\" age 18 weight \"60kg\" OK 127.0.0.1:6379> HMGET man name age 1) \"xiaoming\" 2) \"18\" hmset key field1 value1 [field2 value2] 含义：同时将多个field-value (域-值)对设置到哈希表key中 示例： 127.0.0.1:6379> HMSET man name \"xiaoming\" age 18 weight \"60kg\" OK hset key field value 含义：将哈希表key中的字段field的值设为 value 示例： 127.0.0.1:6379> HSET man name \"xiaoming\" (integer) 1 hsetnx key field value 含义：只有在字段field不存在时，设置哈希表字段的值 示例： 127.0.0.1:6379> HSETNX hehe name \"hehe\" (integer) 1 scan、sscan、hscan、zscan SSCAN 命令、 HSCAN 命令和 ZSCAN 命令的第一个参数总是一个数据库键。 而 SCAN 命令则不需要在第一个参数提供任何数据库键 —— 因为它迭代的是当前数据库中的所有数据库键。 scan cursor [MATCH pattern] [COUNT count] 含义：用于迭代当前数据库中的数据库键 示例： sscan 含义：用于迭代集合键中的元素 示例： hscan 含义：用于迭代哈希键中的键值对 示例： zscan 含义：用于迭代有序集合中的元素（包括元素成员和元素分值） 示例： 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/redis/redis数据类型/4.redis数据类型-set集合.html":{"url":"db/redis/redis数据类型/4.redis数据类型-set集合.html","title":"set 集合","keywords":"","body":"[toc] redis数据类型 set 集合 简介： hash表实现，元素不重复 可以存储的值： 无序集合 操作： 添加、获取、移除单个元素，检查一个元素是否已经存在于集合中，计算交集、并集、差集从集合里面随机获取元素 应用场景： 共同好友；利用唯一性，统计访问网站的所有IP 范围：集合中最大的成员数为 2^32 - 1 (4294967295, 每个集合可存储40多亿个成员) 使用示例： 127.0.0.1:6379> SADD s a (integer) 1 127.0.0.1:6379> SADD s b (integer) 1 127.0.0.1:6379> SADD s c (integer) 1 127.0.0.1:6379> SMEMBERS s 1) \"b\" 2) \"c\" 3) \"a\" redis集合常用命令 sadd key member1 [member2] 含义：向集合添加一个或多个成员 示例： 127.0.0.1:6379> SADD s aaa bbb ccc (integer) 3 scard key 含义：获取集合的成员数 示例： 127.0.0.1:6379> SADD s aaa bbb ccc (integer) 3 127.0.0.1:6379> SCARD s (integer) 3 sdiff key1 [key2] 含义：返回给定所有集合的差集，⚠️返回的差集来自第一个key，而不是后边的key 示例： 127.0.0.1:6379> SADD s aaa bbb ccc (integer) 3 127.0.0.1:6379> SADD s1 aaa ccc ddd eee fff (integer) 5 127.0.0.1:6379> SDIFF s s1 1) \"bbb\" #差集结果是bbb，s在s1中有的元素是aaa ccc，没有bbb，因为差集结果来自第一个key，也就是s，即aaa bbb ccc，肯定是这3个元素中的某一个或者多个或者没有 127.0.0.1:6379> SDIFF s s1 1) \"bbb\" sdiffstore destination key1 [key2] 含义：返回给定所有集合的差集并存储在 destination 中 示例： #向集合s1、s2中插入元素 127.0.0.1:6379> SADD s1 aaa bbb ccc (integer) 3 127.0.0.1:6379> SADD s2 aaa ccc ddd (integer) 3 #把s1和s2的差集存储在s3中 127.0.0.1:6379> SDIFFSTORE s3 s1 s2 (integer) 1 127.0.0.1:6379> SMEMBERS s3 1) \"bbb\" sinter key1 [key2] 含义：返回给定所有集合的交集 示例： 127.0.0.1:6379> SADD s1 aaa bbb ccc (integer) 3 127.0.0.1:6379> SADD s2 bbb ddd fff (integer) 3 127.0.0.1:6379> SADD s3 ggg bbb kkk (integer) 3 127.0.0.1:6379> SINTER s1 s2 s3 1) \"bbb\" sinterstore destination key1 含义：返回给定所有集合的交集并存储在 destination 中 示例： 27.0.0.1:6379> SADD s1 aaa bbb ccc (integer) 3 127.0.0.1:6379> SADD s2 bbb ddd fff (integer) 3 127.0.0.1:6379> SADD s3 ggg bbb kkk (integer) 3 127.0.0.1:6379> SINTERSTORE s4 s1 s2 s3 (integer) 1 127.0.0.1:6379> SMEMBERS s4 1) \"bbb\" sismember key member 含义：判断member元素是否是集合 key 的成员 示例： 127.0.0.1:6379> SADD s1 aaa bbb ccc (integer) 3 127.0.0.1:6379> SISMEMBER s1 bbb (integer) 1 127.0.0.1:6379> SISMEMBER s1 ddd (integer) 0 smembers key 含义：返回集合中的所有成员 示例： 127.0.0.1:6379> SADD s1 aaa bbb ccc (integer) 3 127.0.0.1:6379> SMEMBERS s1 1) \"bbb\" 2) \"ccc\" 3) \"aaa\" smove source destination member 含义：将member元素从source集合移动到destination集合 示例： 127.0.0.1:6379> SADD s1 aaa bbb ccc (integer) 3 127.0.0.1:6379> SMOVE s1 s2 bbb (integer) 1 127.0.0.1:6379> SMEMBERS s1 1) \"ccc\" 2) \"aaa\" 127.0.0.1:6379> SMEMBERS s2 1) \"bbb\" spop key 含义：移除并返回集合中的一个随机元素 示例： 127.0.0.1:6379> SADD s1 aaa bbb ccc (integer) 3 127.0.0.1:6379> SPOP s1 \"bbb\" 127.0.0.1:6379> SPOP s1 \"aaa\" 127.0.0.1:6379> SPOP s1 \"ccc\" 127.0.0.1:6379> SPOP s1 (nil) srandmember key [count] 含义：返回集合中一个或多个随机数，count不写默认返回一个 示例： 127.0.0.1:6379> SRANDMEMBER s1 2 1) \"bbb\" 2) \"ccc\" 127.0.0.1:6379> SRANDMEMBER s1 \"bbb\" srem key member1 [member2] 含义：移除集合中一个或多个成员 示例： 127.0.0.1:6379> SADD s1 aaa bbb ccc (integer) 3 127.0.0.1:6379> SREM s1 aaa (integer) 1 127.0.0.1:6379> SMEMBERS s1 1) \"bbb\" 2) \"ccc\" sunion key1 [key2] 含义：返回所有给定集合的并集 示例： 127.0.0.1:6379> SADD s1 aaa bbb ccc (integer) 3 127.0.0.1:6379> SADD s2 ccc ddd eee (integer) 3 127.0.0.1:6379> SUNION s1 s2 1) \"aaa\" 2) \"ccc\" 3) \"bbb\" 4) \"eee\" 5) \"ddd\" sunionstore destination key1 [key2] 含义：所有给定集合的并集存储在destination集合中 示例： 127.0.0.1:6379> SADD s1 aaa bbb ccc (integer) 3 127.0.0.1:6379> SADD s2 ccc ddd eee (integer) 3 127.0.0.1:6379> SUNIONSTORE s3 s1 s2 (integer) 5 127.0.0.1:6379> SMEMBERS s3 1) \"aaa\" 2) \"ccc\" 3) \"bbb\" 4) \"eee\" 5) \"ddd\" SSCAN key cursor [MATCH pattern] [COUNT count 含义：迭代集合中的元素 示例： 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/redis/redis数据类型/5.redis数据类型-zset有序集合.html":{"url":"db/redis/redis数据类型/5.redis数据类型-zset有序集合.html","title":"zset 有序集合","keywords":"","body":"[toc] redis数据类型 zset 有序集合 简介： 将 set 中的元素增加一个权重参数score，元素按score有序排列 可以存储的值： 有序集合 操作： 添加、获取、删除元素，根据分值范围或者成员来获取元素，计算一个键的排名 应用场景： 排行榜；带权重的消息队列 范围：集合中最大的成员数为 2^32 - 1 (4294967295, 每个集合可存储40多亿个成员)。 特点：每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。有序集合的成员是唯一的，但分数(score)却可以重复。 使用示例： 127.0.0.1:6379> ZADD z1 1 aaa (integer) 1 127.0.0.1:6379> ZADD z1 2 bbb (integer) 1 127.0.0.1:6379> ZADD z1 3 ccc (integer) 1 127.0.0.1:6379> ZRANGE z1 0 -1 1) \"aaa\" 2) \"bbb\" 3) \"ccc\" redis有序集合常用命令 zadd key score1 member1 含义：向有序集合添加一个或多个成员，或者更新已存在成员的分数 示例： 127.0.0.1:6379> ZADD z1 1 a 2 b 3 c (integer) 3 127.0.0.1:6379> ZRANGE z1 0 -1 1) \"a\" 2) \"b\" 3) \"c\" zcard key 含义：获取有序集合的成员数 示例： 127.0.0.1:6379> ZADD z1 1 a 2 b 3 c (integer) 3 127.0.0.1:6379> ZCARD z1 (integer) 3 zcount key min max 含义：计算在有序集合中指定区间分数的成员数 示例： 127.0.0.1:6379> ZADD z1 1 aaa (integer) 1 127.0.0.1:6379> ZADD z1 1 bbb (integer) 1 127.0.0.1:6379> ZADD z1 2 ccc (integer) 1 127.0.0.1:6379> ZADD z1 3 ddd (integer) 1 127.0.0.1:6379> ZADD z1 4 eee (integer) 1 127.0.0.1:6379> ZCOUNT z1 1 3 (integer) 4 zincrby key increment member 含义：有序集合中对指定成员的分数加上增量 increment 示例： 127.0.0.1:6379> ZADD z1 1 aaa (integer) 1 127.0.0.1:6379> ZINCRBY z1 5 1 \"5\" zinterstore destination numkeys key [key ...] 含义：计算给定的一个或多个有序集的交集并将结果集存储在新的有序集合 key 中 示例： 127.0.0.1:6379> ZADD z1 1 aaa 2 bbb 3 ccc 4 ddd 5 eee 6 fff (integer) 6 127.0.0.1:6379> ZADD z2 1 aaa 2 bbb 3 ccc 5 hhh 6 ttt 9 wwww (integer) 6 #这里必须要指定一下有序集合的个数 127.0.0.1:6379> ZINTERSTORE z3 2 z1 z2 (integer) 3 127.0.0.1:6379> ZRANGE z3 0 -1 1) \"aaa\" 2) \"bbb\" 3) \"ccc\" zlexcount key min max 含义：在有序集合中计算指定字典区间内成员数量 示例： zrange key start stop [WITHSCORES] 含义：通过索引区间返回有序集合指定区间内的成员 示例： 127.0.0.1:6379> ZADD z1 1 a 2 b 3 c (integer) 3 127.0.0.1:6379> ZRANGE z1 0 -1 1) \"a\" 2) \"b\" 3) \"c\" zrangebylex key min max [LIMIT offset count] 含义：通过字典区间返回有序集合的成员 示例： 127.0.0.1:6379> ZADD z1 1 a 2 b 3 c 4 d 5 e 6 f (integer) 6 127.0.0.1:6379> ZRANGEBYLEX z1 [b [e 1) \"b\" 2) \"c\" 3) \"d\" 4) \"e\" zrangebyscore key min max [WITHSCORES] [LIMIT] 含义：通过分数返回有序集合指定区间内的成员 示例： 127.0.0.1:6379> ZADD z1 1 a 2 b 3 c 4 d 5 e 6 f (integer) 6 127.0.0.1:6379> ZRANGEBYSCORE z1 2 5 1) \"b\" 2) \"c\" 3) \"d\" 4) \"e\" zrank key member 含义：返回有序集合中指定成员的索引 示例： 127.0.0.1:6379> ZADD z1 1 a 2 b 3 c 4 d 5 e 6 f (integer) 6 127.0.0.1:6379> ZRANGE z1 0 3 1) \"a\" 2) \"b\" 3) \"c\" 4) \"d\" zrem key member1 [member2 ...] 含义：移除有序集合中的一个或多个成员 示例： 127.0.0.1:6379> ZADD z1 1 a 2 b 3 c 4 d 5 e 6 f (integer) 6 127.0.0.1:6379> ZREM z1 a c (integer) 2 127.0.0.1:6379> ZRANGE z1 0 -1 1) \"b\" 2) \"d\" 3) \"e\" 4) \"f\" zremrangebylex key min max 含义：移除有序集合中给定的字典区间的所有成员 示例： 127.0.0.1:6379> ZADD z1 1 a 2 b 3 c 4 d 5 e 6 f (integer) 6 127.0.0.1:6379> ZREMRANGEBYLEX z1 [c [e (integer) 3 127.0.0.1:6379> ZRANGE z1 0 -1 1) \"a\" 2) \"b\" 3) \"f\" zremrangebyrank key start stop 含义：移除有序集合中给定的排名区间（下标）的所有成员 示例： 127.0.0.1:6379> ZADD z1 1 a 2 b 3 c 4 d 5 e 6 f (integer) 6 127.0.0.1:6379> ZREMRANGEBYRANK z1 0 3 (integer) 4 127.0.0.1:6379> ZRANGE z1 0 -1 1) \"e\" 2) \"f\" zremrangebyscore key min max 含义：移除有序集合中给定的分数区间的所有成员 示例： 127.0.0.1:6379> ZADD z1 1 a 2 b 3 c 4 d 5 e 6 f (integer) 6 127.0.0.1:6379> ZREMRANGEBYSCORE z1 2 5 (integer) 4 127.0.0.1:6379> ZRANGE z1 0 -1 1) \"a\" 2) \"f\" zrevrange key start stop [WITHSCORES] 含义：返回有序集合中指定区间内的成员，通过索引，分数从高到低，索引为0的元素在最后 示例： 127.0.0.1:6379> ZADD z1 1 a 2 b 3 c 4 d 5 e 6 f (integer) 6 #这里的下标也是从0开始，但是下标为0的元素在最后 127.0.0.1:6379> ZREVRANGE z1 1 3 1) \"e\" 2) \"d\" 3) \"c\" 127.0.0.1:6379> ZREVRANGE z1 0 1 1) \"f\" 2) \"e\" zrevrangebyscore key max min [WITHSCORES] 含义：返回有序集中指定分数区间内的成员，分数从高到低排序 示例： 127.0.0.1:6379> ZADD z1 1 a 2 b 3 c 4 d 5 e 6 f (integer) 6 127.0.0.1:6379> ZREVRANGEBYSCORE z1 3 1 1) \"c\" 2) \"b\" 3) \"a\" zrevrank key member 含义：返回有序集合中指定成员的排名，有序集成员按分数值递减(从大到小)排序 示例： 127.0.0.1:6379> ZADD z1 1 a 2 b 3 c 4 d 5 e 6 f (integer) 6 127.0.0.1:6379> ZREVRANK z1 c (integer) 3 zscore key member 含义：返回有序集中，成员的分数值 示例： 127.0.0.1:6379> ZADD z1 1 a 2 b 3 c 4 d 5 e 6 f (integer) 6 127.0.0.1:6379> ZSCORE z1 f \"6\" zunionstore destination numkeys key1 [key2 ...] 含义：计算给定的一个或多个有序集的并集，并存储在新的 key 中 示例： 127.0.0.1:6379> ZADD z1 1 aaa 2 bbb 3 ccc (integer) 3 127.0.0.1:6379> ZADD z2 1 aaa 2 ttt 3 vvv (integer) 3 127.0.0.1:6379> ZUNIONSTORE z3 2 z1 z2 (integer) 5 127.0.0.1:6379> ZRANGE z3 0 -1 1) \"aaa\" 2) \"bbb\" 3) \"ttt\" 4) \"ccc\" 5) \"vvv\" zscan key cursor [MATCH pattern] [COUNT count] 含义：迭代有序集合中的元素（包括元素成员和元素分值） 示例： 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"db/mongodb/centos7安装mongodb.html":{"url":"db/mongodb/centos7安装mongodb.html","title":"centos7安装mongodb","keywords":"","body":"[toc] centos7安装mongodb mongodb官网 mongodb官方文档 mongodb社区版官方下载地址 mongodb github地址 1.yum安装 1.1 添加清华yum源 cat > /etc/yum.repos.d/mongodb.repo yum makecache 1.2 安装社区版mongodb 安装最新版 yum -y install mongodb-org 安装指定版本 yum -y install mongodb-org-4.2.8 mongodb-org-server-4.2.8 mongodb-org-shell-4.2.8 mongodb-org-mongos-4.2.8 mongodb-org-tools-4.2.8 1.3 启动mongodb systemctl enable mongod && systemctl start mongod mongodb监听tcp/27017 $ netstat -ntpl|grep 27017 tcp 0 0 127.0.0.1:27017 0.0.0.0:* LISTEN 17976/mongod mongodb默认以mongod用运行 $ ps aux|grep [m]ongo mongod 17976 0.6 7.8 1552904 78904 ? Sl 22:04 0:00 /usr/bin/mongod -f /etc/mongod.conf 1.4 mongodb相关目录文件 mongodb配置文件/etc/mongod.conf mongodb日志文件/var/log/mongodb/mongod.log mongodb PID文件/run/mongodb/mongod.pid mongodb SOCKET文件/tmp/mongodb-27017.sock mongodb数据目录/var/lib/mongo/ 2.二进制安装 mongodb4.2二进制安装官方文档 2.1 安装依赖包 yum -y install libcurl openssl 2.2 下载二进制包 wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-4.2.8.tgz 2.3 解压缩包 tar xf mongodb-linux-x86_64-rhel70-4.2.8.tgz -C /usr/local/ 2.4 做软连接 ln -s /usr/local/mongodb-linux-x86_64-rhel70-4.2.8/ /usr/local/mongodb-4.2.8 2.5 导出PATH环境变量 echo 'export PATH=/usr/local/mongodb-4.2.8/bin:$PATH' >/etc/profile.d/mongodb.sh && source /etc/profile 2.6 创建相关目录 #创建数据、日志、pid、配置文件目录 mkdir -p /data/db/mongodb/{data,log,pid,conf} 2.7 创建mongod用户 useradd mongod -s /sbin/nologin -M 2.8 编辑mongodb配置文件 cat > /data/db/mongodb/conf/mongod.conf 2.9 设置目录所有者为mongod chown -R mongod:mongod /data/db/mongodb 2.10 使用supervisor管理mongodb cat >/etc/supervisor/config.d/mongodb.ini $ supervisorctl update mongodb mongodb: added process group 2.11 查看mongodb运行状态 查看mongodb进程 $ ps aux|grep [m]ongodb mongod 30352 1.5 2.2 1550840 89140 ? Sl 20:18 0:00 /usr/local/mongodb-4.2.8/bin/mongod -f /data/db/mongodb/conf/mongod.conf mongodb监听TCP/27010端口 $ netstat -ntpl|grep mongod tcp 0 0 10.0.0.31:27010 0.0.0.0:* LISTEN 30352/mongod 2.12 连接mongodb $ mongo 10.0.0.31:27010 MongoDB shell version v4.2.8 connecting to: mongodb://10.0.0.31:27010/test?compressors=disabled&gssapiServiceName=mongodb Implicit session: session { \"id\" : UUID(\"f87cbfff-d5b5-4f72-b378-15547f61fdca\") } MongoDB server version: 4.2.8 Server has startup warnings: 2020-07-01T20:18:05.560+0800 I CONTROL [initandlisten] 2020-07-01T20:18:05.560+0800 I CONTROL [initandlisten] ** WARNING: /sys/kernel/mm/transparent_hugepage/enabled is 'always'. 2020-07-01T20:18:05.560+0800 I CONTROL [initandlisten] ** We suggest setting it to 'never' 2020-07-01T20:18:05.560+0800 I CONTROL [initandlisten] 2020-07-01T20:18:05.560+0800 I CONTROL [initandlisten] ** WARNING: /sys/kernel/mm/transparent_hugepage/defrag is 'always'. 2020-07-01T20:18:05.560+0800 I CONTROL [initandlisten] ** We suggest setting it to 'never' 2020-07-01T20:18:05.560+0800 I CONTROL [initandlisten] --- Enable MongoDB's free cloud-based monitoring service, which will then receive and display metrics about your deployment (disk utilization, CPU, operation statistics, etc). The monitoring data will be available on a MongoDB website with a unique URL accessible to you and anyone you share the URL with. MongoDB may use this information to make product improvements and to suggest MongoDB products and deployment options to you. To enable free monitoring, run the following command: db.enableFreeMonitoring() To permanently disable this reminder, run the following command: db.disableFreeMonitoring() --- > show databases admin 0.000GB config 0.000GB local 0.000GB 生产环境配置 bind_ip = 172.20.1.40 logpath = /data/db/mongodb/logs/mongod.log logappend = true pidfilepath = /data/db/mongodb/pid/mongod.pid dbpath = /data/db/mongodb/data storageEngine = wiredTiger directoryperdb = true #replSet = replset #rest = true oplogSize = 61440 #fork = true auth = false shardsvr = true port = 27010 journal = true maxConns = 30000 master = true #slave = true #source = 10.31.133.145:27010 #source = 10.47.125.99:27010 autoresync=true 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/1.shell简介.html":{"url":"linux/shell/1.shell简介.html","title":"shell简介","keywords":"","body":"[toc] shell简介 一、什么是shell Shell 是一个用 C 语言编写的程序，它是用户使用 Linux 的桥梁。Shell 既是一种命令语言，又是一种程序设计语言。 Shell 是指一种应用程序，这个应用程序提供了一个界面，用户通过这个界面访问操作系统内核的服务。 Ken Thompson 的 sh 是第一种 Unix Shell，Windows Explorer 是一个典型的图形界面Shell。 kernel：为软件服务，接收用户或软件指令驱动硬件，完成工作 shell：命令解释器 user：用户接口，对接用户 上图可以看出，shell在操作系统中起到了承接用户和系统内核的作用。那为什么不直接用户对内核呢？ 原因很简单，因为内核处理的都是二进制，而用户处理的都是高级语言。 Shell 脚本 Shell 脚本（shell script），是一种为 shell 编写的脚本程序。 业界所说的 shell 通常都是指 shell 脚本，但是shell 和 shell script 是两个不同的概念。 Shell 环境 Shell 编程跟 JavaScript、php 编程一样，只要有一个能编写代码的文本编辑器和一个能解释执行的脚本解释器就可以了。 Linux 的 Shell 种类众多，常见的有： Bourne Shell（/usr/bin/sh或/bin/sh） Bourne Again Shell（/bin/bash） C Shell（/usr/bin/csh） K Shell（/usr/bin/ksh） Shell for Root（/sbin/sh） …… 在一般情况下，人们并不区分 Bourne Shell 和 Bourne Again Shell，所以，像 #!/bin/sh，它同样也可以改为 #!/bin/bash。 #! 是告诉系统其后路径所指定的程序即是解释此脚本文件的 Shell 程序。 二、shell分类 2.1 交互分类 2.1.1 交互式shell 交互式：顾名思义就是 shell 与用户存在交互， 用户登录后，在终端上输入命令，shell 立即执行用户提交的命令。 当用户退出后，shell 也终止了。 2.1.2 非交互式shell 非交互式：即 shell 与用户不存在交互，而是以 shell script 的方式执行的。 shell 读取存放在文件中的命令, 并且执行它们，类似如下 cat > test.sh 2.2 登陆分类 2.2.1 登陆式shell 登录 shell 是指需要用户名、密码登录后进入的 shell，或者通过 --login 选项生成的 shell su - username 2.2.2 非登陆式shell 非登录式 shell 是指不需要输入用户名和密码即可打开的 shell，比如输入命令 bash或者sh 就能进入一个全新的非登录 shell 图形终端下打开的命令窗口 自动执行的shell脚本 su username 2.2.3 区分登陆式shell和非登陆式shell 区分方法一 可以通过查看 $0 的值，登录式 shell 返回 -bash，而非登录式 shell 返回的是 bash #通过终端输入用户名和密码登录的登陆式shell $ echo $0 -bash #通过su username登陆的非登陆式shell $ su www $ echo $0 bash 区分方法二 执行exit命令和logout命令 执行 exit 命令， 退出的 shell 可以是登录式shell 或者 非登录式 shell ； 执行 logout 命令，则只能退出登录式 shell，不能退出非登录式 shell 执行exit命令 #通过终端输入用户名和密码登录的登陆式shell $ exit 登出 Connection to 10.0.0.10 closed. #通过su username登陆的非登陆式shell $ su www [www@test1 root]$ exit exit 执行logout命令 #通过终端输入用户名和密码登录的登陆式shell $ logout Connection to 10.0.0.10 closed. #通过su username登陆的非登陆式shell $ logout bash: logout: 不是登录shell: 使用 `exit' 三、shell的配置文件 登录式shell 读取配置 文件过程： /etc/profile –> /etc/profile.d/*.sh –> ~ /.bash_profile –> ~ /bashrc –> /etc/bashrc 非登录式shell 读取配置 文件过程： ~ /.bashrc –> /etc/bashrc –> /etc/prodile.d/*.sh 3.1 bash的配置文件 全局配置文件 /etc/profile /etc/profile.d/* /etc/bashrc 个人配置文件 ~/.bash_profile ~/.bashrc profile类文件作用 1.设定环境变量 2.运行命令或脚本（登录时运行的脚本） bashrc类文件配置作用 1.设定本地变量 2.定义命令别名 3.2 各shell读取配置文件过程 3.2.1 bash 1、交互式的登录shell （bash –il test.sh） 载入的信息： /etc/profile ~/.bash_profile（ -> ~/.bashrc -> /etc/bashrc） ~/.bash_login ~/.profile 2、非交互式的登录shell （bash –l test.sh） 载入的信息： /etc/profile ~/.bash_profile （ -> ~/.bashrc -> /etc/bashrc） ~/.bash_login ~/.profile $BASH_ENV 3、交互式的非登录shell （bash –i test.sh） 载入的信息： ~/.bashrc （ -> /etc/bashrc） 4、非交互式的非登录shell （bash test.sh） 载入的信息： $BASH_ENV 3.2.2 sh 1、交互式的登录shell （sh –il test.sh） 载入的信息： /etc/profile ~/.profile 2、非交互式的登录shell （sh –l test.sh） 载入的信息： /etc/profile ~/.profile 3、交互式的非登录shell （sh –i test.sh） 载入的信息： $ENV 4、非交互式的非登录shell （sh test.sh） 载入的信息： nothing 综上可知， 交互/非交互/登录/非登录，这四种 shell 主要区别在于：是否载入相关配置文件！ 这些配置的载入与否，导致了 Linux 很多默认选项的差异。 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/2.1shell变量综述.html":{"url":"linux/shell/2.1shell变量综述.html","title":"shell变量综述","keywords":"","body":"[toc] shell变量综述 一、shell变量分类 环境变量（全局变量） 可以在当前shell及子shell中使用 本地变量（局部变量） 只能在子shell中使用 二、shell查看变量 set 输出所有的变量，包括环境变量和本地变量 env 只显示全局变量 declare 输出所有的变量、函数、整数和已经导出的变量 三、设置环境变量 3.1设置临时性环境变量 export 变量名=value 变量名=value;export 变量名 declare -x 变量名=value 3.2 设置永久性环境变量 写入/etc/profile中 然后使用source命令生效 3.3 取消变量 unset 变量名 四、环境变量生效顺序 /etc/profile ~/.bashrc_profile ~/.bashrc /etc/bashrc 五、shell特殊位置环境变量 变量 含义 $0 脚本名称 $n 位置变量，n代表数字，超过10要用{}括起来 $# 参数个数 $*、$@ 列出参数 $? 上一个命令的执行结果返回值 $() 表示先执行里边的内容 ${} \"金庸新著\" 和 \"金庸新\"著 用于区分变量 $! 获取上一个脚本的PID $_ 获取上一个脚本的最后一个参数 $- 显示shell使用的当前选项，与set命令功能相同 $0 当前脚本的文件名，如果执行脚本包含了路径，那么就包括脚本路径 #编辑脚本内容 $ cat >a.sh $n 位置变量，获取当前执行的脚本的第n个参数，n=1..9,如果n大于9，用大括号括起来${10} #编辑脚本内容 $ cat > b.sh $# 获取当前执行脚本后接的参数总个数 #编辑脚本内容 $ cat > c.sh $*和¥@ 列出所有参数 $* 以\"参数1 参数2 参数3 ...\"的形式列出所有参数 $@ 以\"参数1\" \"参数2\" \"参数3\"的形式列出所有参数 不加引号时，$*和$@输出一样 #$* $ set -- a b c $ for i in $*;do echo $i;done a b c #$@ $ set -- a b c $ for i in $@;do echo $i;done a b c 加入引号时，$*和$@输出不一样 #$* $ set -- a b c $ for i in \"$*\";do echo $i;done a b c #$@ $ set -- a b c $ for i in \"$@\";do echo $i;done a b c $? 上一个命令的执行结果返回值 常见返回值 #命令错误，返回值127 $ lp -bash: lp: 未找到命令 $ echo $? 127 #参数不正确或者文件目录不存在 $ ls -e ls：无效选项 -- e Try 'ls --help' for more information. $ echo $? 2 #权限拒绝，不是目录或文件等等 $ touch /opt/txt touch: cannot touch ‘/opt/txt’: Permission denied $ echo $? 1 #命令正确执行 $ ls $ echo $? 0 $() 表示先执行里边的内容 $()等同于`` #脚本中想要把一个命令的输出复值给一个变量 $ ip=$(ip a s eth0|awk -F'[ /]'+ 'NR==3{print $3}') $ echo $ip 10.0.0.10 ${} \"金庸新著\" 和 \"金庸新\"著 用于区分变量 #想要输出你好a，但是因为没有加{}，所以aa就成了最终的变量，但是变量名是a $ a='你好' $ echo $aa 输出内容为空 #用{}解决以上问题 $ a='你好' $ echo ${a}a 你好a $! 获取最后运行的后台进程的PID ⚠️必须是后台进程 #下载nginx官方的一张图片 $ nohup wget nginx.org/nginx.png & $ echo $! 1065 $_ 获取上一个脚本的最后一个参数 #编辑脚本内容 $ cat > b.sh $- 显示shell使用的当前选项，与set命令功能相同 #shell默认选项是himBH $ echo $- himBH # $ set -x ++ printf '\\033]0;%s@%s:%s\\007' root test1 '~' $ echo $- + echo himxBH himxBH ++ printf '\\033]0;%s@%s:%s\\007' root test1 '~' shell默认选项himBH，每个字母都代表了一个 shell 选项 h - hashall i - interactive-comments m - monitor B - braceexpand H- history ⚠️⚠️⚠️注意这里的 \"设置-\" 和 \"取消+\" 是反人类的：设置用 -，关闭反而是用 + h - hashall bash 的 hash 功能，可以实现让某些 command 和 具体路径 绑定在一起 查看默认选项 $ echo $- himBH $ hash -p /tmp/aaadate date $ hash -l |grep aaadate builtin hash -p /tmp/aaadate date #此时再执行命令date，会发现原先的/usr/bin/date变成了/tmpaaadate $ date -bash: /tmp/aaadate: No such file or directory #执行set +h +h表示去掉h，也就是imBH 命令不能和具体路径绑定，因此date命令能正确执行 $ set +h $ echo $- imBH $ date 2020年 06月 24日 星期三 00:26:50 CST #加上h 命令可以和具体路径绑定 $ set -h $ echo $- himBH $ date -bash: /tmp/aaadate: 没有那个文件或目录 #恢复 $ hash -d date $ date 2020年 06月 24日 星期三 00:29:14 CST i - interactive-comments 配置在交互 shell 模式下，是否允许注释 恢复默认选项 $ echo $- himBH 必须使用set +o interactive-comments，set +i不好使，原因未知 #设置命令行不允许注释 $ set +o interactive-comments $ echo $- himBH #在命令行加上# 此时是不被允许的 $ #testcomment -bash: #testcomment: 未找到命令 #取消设置 $ set -o interactive-comments $ echo $- himBH $ #testcomment m - monitor 配置是否打开控制 Job control 功能 恢复默认选项 $ echo $- himBH Job control 是什么？ 即可以控制进程的停止、继续，后台或者前台执行等。 开启 job control 后，如果执行了一个比较耗时的命令，可以按下 CTRL+Z 让它在后台运行： $ sleep 50 ^Z [1]+ 已停止 sleep 50 然后， 可以用 fg 命令将后台运行的任务恢复到前台执行 $ fg 1 sleep 50 如果关闭这个选项，就会失去控制 Job 的能力 $ set +m $ echo $- hiBH #此时使用ctrl+z不管用 $ sleep 50 ^Z^Z^Z^Z^Z^C $ fg -bash: fg: 无任务控制 B - braceexpand 关于大括号使用的flag，打开后可以快捷地实现某些效果 恢复默认选项 $ echo $- himBH 利用大括号输出文件 echo {1..5}.txt 1.txt 2.txt 3.txt 4.txt 5.txt 关闭大括号效果 $ set +B $ echo $- himH 再次执行命令发现{}不生效了 echo {1..5}.txt {1..5}.txt H - histexpand 是否允許用 \"感叹号 ！+ history number\" 来执行历史命令 !! 返回并执行最近的一个历史命令 !n 返回并执行第 n 个历史命令 恢复默认选项 $ echo $- himBH #执行命令 $ ls /tmp/ ks-script-2R8Xnq yum.log #使用!运行上一次以l开头的命令 $ !l ls /tmp/ ks-script-2R8Xnq yum.log #取消!功能 $ set +H #再次执行就会报错 $ !l -bash: !l: 未找到命令 问题 由于 histexpand 打开的时候，\"!\" 带特殊含义； 因此 histexpand 打开状态下，\"!\" 不能出现在双引号中， 否则会报错 -bash: !\": event not found $ echo $- himBH #想要输出hehe!，但是却报错了，原因是命令行下，双引号里面用了!的话，Shell 会以为要执行历史展开，从而导致报错 $ echo \"hehe!\" -bash: !\": event not found 解决方法一 关闭histexpand $ echo $- himBH $ set +H $ echo $- himB $ echo \"hehe!\" hehe! 解决方法二 使用单引号 $ echo $- himBH $ echo 'hehe!' hehe! 六、环境变量总结 变量名通常要大写 变量可以在自身的shell及子shell中使用 常用export来定义环境变量 执行env默认可以显示所有的环境变量名称及对应的值 输出时用\"$变量名\"，取消时用\"unset 变量名\" 书写crond定时任务是要注意，脚本要用到的环境变量最好先在所执行的shell脚本中重新定义 如果希望环境变量永久生效，则可以将其放在用户环境变量文件或全局环境变量文件中 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/2.2shell变量子串.html":{"url":"linux/shell/2.2shell变量子串.html","title":"shell变量子串","keywords":"","body":"[toc] shell变量子串 1.返回变量 ${var} 返回变量var的内容 $ a=10 $ echo ${a} 10 2.返回字符长度 ${#var} 返回变量var的长度，按照字符 a=10 $ echo ${#a} 2 3.截取子串 ${var:n} 在变量var中，从位置n(n为数字)之后开始提取子串到结尾 $ a=\"i am boy\" $ echo ${a:3} m boy ${var:n:length} 在变量var中，从位置n之后开始提取长度为length的子串 $ a=\"i am boy\" $ echo ${a:3:3} m b 4.删除子串 4.1 从开头删除 4.1.1 最短匹配 ${var#word} 从变量var开头开始删除最短匹配的word子串 $ a=abc123ABCabc123ABC $ echo ${a#a*b} c123ABCabc123ABC #匹配了ab，最短匹配 $ echo ${a#a*C} abc123ABC #匹配了abc123ABC 4.1.2 最长匹配 ${var##word} 从变量var开头开始删除最长匹配的word子串 $ a=abc123ABCabc123ABC $ echo ${a##a*b} c123ABC #匹配了abc123ABCab，最长匹配 $ echo ${a##a*C} #全部匹配，全部删除 4.2 从结尾删除 4.2.1 最短匹配 ${var%word} 从变量var结尾开始删除最短匹配的word子串 $ a=abc123ABCabc123ABC $ echo ${a%1*C} abc123ABCabc #从结尾开始匹配了123ABC 4.2.2 最长匹配 ${var%%word} 从变量var结尾开始删除最长匹配的word子串 $ a=abc123ABCabc123ABC $ echo ${a%%1*C} abc #从结尾匹配了123ABCabc123ABC 5.替换子串 ${var/A/B} 用B代替第一个匹配的A $ a=abc111 $ echo ${a/1/9} abc911 ${var//A/B} 用B代替所有匹配的A $ a=abc111 $ echo ${a//1/9} abc999 6.特殊扩展变量 6.1 ${var:-word} ${var:-word},冒号可以忽略 如果var的变量值为空或为未赋值，则会返回word字符串并替代变量的值 $ echo $C #变量C没有赋值，所以为空 $ B=${C:-hehe} #如果C没有赋值，则将hehe赋值给B $ echo $B hehe $ A=abc $ B=${A:-haha} #因为变量A有值，所以将变量A的值赋予B $ echo $B abc #冒号可以不写 $ echo $D $ E=${D-hehe} $ echo $E hehe 6.2 ${var:=word} ${var:=word}，冒号可以忽略 如果var的变量值为空或未赋值，则设置这个变量值为word,并返回其值 $ echo $A #变量A没有赋值 $ B=${A=hehe} #如果变量A为空或未赋值，则设置变量A的值为hehe $ echo $A hehe #与var-word不同，变量A也会有值 $ echo $B hehe 6.3 ${var:?word} ${var:?word}，冒号可以忽略 如果var的变量值为空或未赋值，那么word字符串将被作为标准错误输出，否则输出变量的值 $ echo $A #变量A为空或未赋值 $ echo ${A?hehe} -bash: A: hehe #将hehe作为标准错误输出 $ A=abc $ echo ${A?hehe} abc #因为变量A有值，所以输出变量A的值 6.4 ${var:+word} ${vrt:+word}，冒号可以忽略 如果var变量值为空或未赋值，则什么都不做，否则word字符串将替代变量的值 $ echo $A #变量A为空或未赋值 $ echo ${A+hehe} #不做任何操作 $ A=abc $ echo ${A+hehe} hehe #因为变量A有值，所以用hehe替代变量A的值 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/2.3shell变量自增.html":{"url":"linux/shell/2.3shell变量自增.html","title":"shell变量自增","keywords":"","body":"shell变量自增 a++是先执行表达式后再自增，执行表达式时使用的是a的原值 ++a是先自增再执行表达示，执行表达式时使用的是自增后的a a++示例 $ a=1 $ let num=a++ #a++ 先执行表达式，num=a，所以num的值为1，然后a再自增，值为2 $ echo $num 1 $ echo $a 2 ++a示例 $ a=1 $ let num=++a #a先自增，因此a的值为2，然后执行表达式，num=2，所以num的值为2 $ echo $num 2 $ echo $a 2 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/2.4shell特殊位置环境变量.html":{"url":"linux/shell/2.4shell特殊位置环境变量.html","title":"shell特殊位置环境变量","keywords":"","body":"[toc] shell特殊位置环境变量 变量 含义 $0 脚本名称 $n 位置变量，n代表数字，超过10要用{}括起来 $# 参数个数 $*、$@ 列出参数 $? 上一个命令的执行结果返回值 $() 表示先执行里边的内容 ${} \"金庸新著\" 和 \"金庸新\"著 用于区分变量 $! 获取上一个脚本的PID $_ 获取上一个脚本的最后一个参数 $- 显示shell使用的当前选项，与set命令功能相同 $0 当前脚本的文件名，如果执行脚本包含了路径，那么就包括脚本路径 #编辑脚本内容 $ cat >a.sh $n 位置变量，获取当前执行的脚本的第n个参数，n=1..9,如果n大于9，用大括号括起来${10} #编辑脚本内容 $ cat > b.sh $# 获取当前执行脚本后接的参数总个数 #编辑脚本内容 $ cat > c.sh $*和¥@ 列出所有参数 $* 以\"参数1 参数2 参数3 ...\"的形式列出所有参数 $@ 以\"参数1\" \"参数2\" \"参数3\"的形式列出所有参数 不加引号时，$*和$@输出一样 #$* $ set -- a b c $ for i in $*;do echo $i;done a b c #$@ $ set -- a b c $ for i in $@;do echo $i;done a b c 加入引号时，$*和$@输出不一样 #$* $ set -- a b c $ for i in \"$*\";do echo $i;done a b c #$@ $ set -- a b c $ for i in \"$@\";do echo $i;done a b c $? 上一个命令的执行结果返回值 常见返回值 #命令错误，返回值127 $ lp -bash: lp: 未找到命令 $ echo $? 127 #参数不正确或者文件目录不存在 $ ls -e ls：无效选项 -- e Try 'ls --help' for more information. $ echo $? 2 #权限拒绝，不是目录或文件等等 $ touch /opt/txt touch: cannot touch ‘/opt/txt’: Permission denied $ echo $? 1 #命令正确执行 $ ls $ echo $? 0 $() 表示先执行里边的内容 $()等同于`` #脚本中想要把一个命令的输出复值给一个变量 $ ip=$(ip a s eth0|awk -F'[ /]'+ 'NR==3{print $3}') $ echo $ip 10.0.0.10 ${} \"金庸新著\" 和 \"金庸新\"著 用于区分变量 #想要输出你好a，但是因为没有加{}，所以aa就成了最终的变量，但是变量名是a $ a='你好' $ echo $aa 输出内容为空 #用{}解决以上问题 $ a='你好' $ echo ${a}a 你好a $! 获取最后运行的后台进程的PID ⚠️必须是后台进程 #下载nginx官方的一张图片 $ nohup wget nginx.org/nginx.png & $ echo $! 1065 $_ 获取上一个脚本的最后一个参数 #编辑脚本内容 $ cat > b.sh $- 显示shell使用的当前选项，与set命令功能相同 #shell默认选项是himBH $ echo $- himBH # $ set -x ++ printf '\\033]0;%s@%s:%s\\007' root test1 '~' $ echo $- + echo himxBH himxBH ++ printf '\\033]0;%s@%s:%s\\007' root test1 '~' shell默认选项himBH，每个字母都代表了一个 shell 选项 h - hashall i - interactive-comments m - monitor B - braceexpand H- history ⚠️⚠️⚠️注意这里的 \"设置-\" 和 \"取消+\" 是反人类的：设置用 -，关闭反而是用 + h - hashall bash 的 hash 功能，可以实现让某些 command 和 具体路径 绑定在一起 查看默认选项 $ echo $- himBH $ hash -p /tmp/aaadate date $ hash -l |grep aaadate builtin hash -p /tmp/aaadate date #此时再执行命令date，会发现原先的/usr/bin/date变成了/tmpaaadate $ date -bash: /tmp/aaadate: No such file or directory #执行set +h +h表示去掉h，也就是imBH 命令不能和具体路径绑定，因此date命令能正确执行 $ set +h $ echo $- imBH $ date 2020年 06月 24日 星期三 00:26:50 CST #加上h 命令可以和具体路径绑定 $ set -h $ echo $- himBH $ date -bash: /tmp/aaadate: 没有那个文件或目录 #恢复 $ hash -d date $ date 2020年 06月 24日 星期三 00:29:14 CST i - interactive-comments 配置在交互 shell 模式下，是否允许注释 恢复默认选项 $ echo $- himBH 必须使用set +o interactive-comments，set +i不好使，原因未知 #设置命令行不允许注释 $ set +o interactive-comments $ echo $- himBH #在命令行加上# 此时是不被允许的 $ #testcomment -bash: #testcomment: 未找到命令 #取消设置 $ set -o interactive-comments $ echo $- himBH $ #testcomment m - monitor 配置是否打开控制 Job control 功能 恢复默认选项 $ echo $- himBH Job control 是什么？ 即可以控制进程的停止、继续，后台或者前台执行等。 开启 job control 后，如果执行了一个比较耗时的命令，可以按下 CTRL+Z 让它在后台运行： $ sleep 50 ^Z [1]+ 已停止 sleep 50 然后， 可以用 fg 命令将后台运行的任务恢复到前台执行 $ fg 1 sleep 50 如果关闭这个选项，就会失去控制 Job 的能力 $ set +m $ echo $- hiBH #此时使用ctrl+z不管用 $ sleep 50 ^Z^Z^Z^Z^Z^C $ fg -bash: fg: 无任务控制 B - braceexpand 关于大括号使用的flag，打开后可以快捷地实现某些效果 恢复默认选项 $ echo $- himBH 利用大括号输出文件 echo {1..5}.txt 1.txt 2.txt 3.txt 4.txt 5.txt 关闭大括号效果 $ set +B $ echo $- himH 再次执行命令发现{}不生效了 echo {1..5}.txt {1..5}.txt H - histexpand 是否允許用 \"感叹号 ！+ history number\" 来执行历史命令 !! 返回并执行最近的一个历史命令 !n 返回并执行第 n 个历史命令 恢复默认选项 $ echo $- himBH #执行命令 $ ls /tmp/ ks-script-2R8Xnq yum.log #使用!运行上一次以l开头的命令 $ !l ls /tmp/ ks-script-2R8Xnq yum.log #取消!功能 $ set +H #再次执行就会报错 $ !l -bash: !l: 未找到命令 问题 由于 histexpand 打开的时候，\"!\" 带特殊含义； 因此 histexpand 打开状态下，\"!\" 不能出现在双引号中， 否则会报错 -bash: !\": event not found $ echo $- himBH #想要输出hehe!，但是却报错了，原因是命令行下，双引号里面用了!的话，Shell 会以为要执行历史展开，从而导致报错 $ echo \"hehe!\" -bash: !\": event not found 解决方法一 关闭histexpand $ echo $- himBH $ set +H $ echo $- himB $ echo \"hehe!\" hehe! 解决方法二 使用单引号 $ echo $- himBH $ echo 'hehe!' hehe! 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/3.1shell产生随机数方法.html":{"url":"linux/shell/3.1shell产生随机数方法.html","title":"shell产生随机数方法","keywords":"","body":"[toc] shell产生随机数方法 方法1：通过系统环境变量（$RANDOM）实现 示例 $ echo $RANDOM 9010 $ echo $RANDOM 21762 $ echo $RANDOM 10826 RANDOM的随机数范围为0-32767，因此，加密性不好，可以通过在输出的随机数后增加加密字符串（就是和密码生成有关的一个字符串）的方式解决，最后再一起执行md5sum操作并截取结果的后n位，这样安全性就提高了 $ echo hehe$RANDOM|md5sum a0c30ca1ee50e4ad15a103d18b59bd16 - 方法2：通过openssl产生随机数 -base64 使用-base64位编码格式 后边的数字不知道是什么 $ openssl rand -base64 1 Og== $ openssl rand -base64 2 49U= $ openssl rand -base64 3 ii94 $ openssl rand -base64 4 ZdFgGQ== $ openssl rand -base64 5 g+6lQaw= $ openssl rand -base64 6 6TL7NHBd $ openssl rand -base64 7 vI3vCFMJzA== $ openssl rand -base64 8 dE+LENuXc3Y= $ openssl rand -base64 9 ciL/KwjUQnR5 $ openssl rand -base64 10 nAFsTVLvVBtREw== 方法3：通过date获得随机数 示例 $ date +%N N表示纳秒 519916150 $ date +%N 660161239 $ date +%N 352563239 $ date +%N 198867394 $ date +%N 872287833 $ date +%N 698481118 $ date +%N 988858815 方法4：通过/dev/urandom配合chksum生成随机数 /dev/random设备存储着系统当前运行环境的实时数据，它可以看作系统在某个时候的唯一值，因此可以用作随机数元数据，可以通过文件读取的方式读到里面的数据，/dev/urandom这个设备的数据与random里的一样，只是它是非阻塞的随机数发生器，读取操作不会不会产生阻塞 示例 $ head /dev/urandom | cksum 185767657 2818 $ head /dev/urandom | cksum 2888724895 3156 $ head /dev/urandom | cksum 1734919599 1634 $ head /dev/urandom | cksum 3186002271 2797 $ head /dev/urandom | cksum 3511808856 1282 方法5：通过UUID生成随机数 UUID码全称是通用唯一标识码（Universally Unique Identifier,UUID）,它是一个软件建库的标准，亦为自由软件基金会（Open Software Foundation,OSF）的组织在分布式计算环境（Distributed Computing Enviroment,DCE）领域的一部分，UUID的目的是让分布式系统中的所有元素都能有唯一的的辨识信息，而不需要通过中央控制端来做辨识信息的指定 示例 $ cat /proc/sys/kernel/random/uuid f541c0ee-bbe7-4257-8733-2f58cc0ef27f $ cat /proc/sys/kernel/random/uuid 9a136a5c-5397-40df-bea4-d6cf60db5d78 $ cat /proc/sys/kernel/random/uuid 6548c058-7d40-4013-9ce2-a6503c3010a4 方法6：使用expect附带的mkpasswd生成随机数 mkpasswd依赖于包expect，因此需要先安装expect包 mkpasswd -l 指定密码长度 -d 指定密码中数字的数量 -c 指定密码中小写字母的数量 -C 指定密码中大写字母的数量 -s 指定密码中特殊字符的数量 示例 $ mkpasswd -l 10 -d 2 -c 2 -C 2 -s 2 Ly6Jl7|kq{ $ mkpasswd -l 10 -d 2 -c 2 -C 2 -s 2 raBT2x?4c) $ mkpasswd -l 10 -d 2 -c 2 -C 2 -s 2 Ata$\"P03op 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/3.2shell获取奇数行和偶数行.html":{"url":"linux/shell/3.2shell获取奇数行和偶数行.html","title":"shell获取奇数行和偶数行","keywords":"","body":"[toc] shell获取奇数行和偶数行 方法一 awk 文件内容 $ cat test.txt 1 2 3 4 5 6 获取奇数行 方法一 NR为行，每一行与2取模，奇数与2取模结果为1，1为真，所以输出奇数行 $ awk 'NR%2' test.txt 1 3 5 方法二 ++i表示i先自加1，因此i的值为1 $ awk '++i%2' test.txt 1 3 5 方法三 行数与2取模，等于1的就是奇数行 $ awk '{if(NR%2==1)print $0}' test.txt 1 3 5 获取偶数行 方法一 NR为行，每一行与2取模，奇数与2取模结果为1，1为真，所以输出奇数行，取反则输出偶数行 $ awk '!(NR%2)' test.txt 2 4 6 方法二 i++表示先赋值再自加，因此i的值为0 $ awk 'i++%2' test.txt 2 4 6 方法三 行数与2取模，等于0的就是偶数行 $ awk '{if(NR%2==0)print $0}' test.txt 2 4 6 方法二 sed 获取奇数行 n表示换行，此命令为先打印P，然后再换行，会把下一行覆盖，因此为先打印1行，然后换行，覆盖第二行，再打印第3行，覆盖第四行。。。 $ sed -n '1,$p;n' test.txt 1 3 5 获取偶数行 与奇数行相反，先换行，覆盖第一行，再打印，打印第二行，然后再换行，覆盖第三行，然后再打印，打印第四行。。。 $ sed -n '1,$n;p' test.txt 2 4 6 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/3.3shell脚本小操作.html":{"url":"linux/shell/3.3shell脚本小操作.html","title":"shell脚本小操作","keywords":"","body":"[toc] shell脚本小操作 一、获取本机公网IP 场景： 云主机使用ip或者ifconfig命令是看不到本机公网IP的，想要获取本机公网IP 获取本地网络公网IP curl ifconfig.me $ curl ifconfig.me 8.8.8.8 curl icanhazip.com $ curl icanhazip.com 8.8.8.8 curl ident.me $ curl ident.me 8.8.8.8 curl ipecho.net/plain $ curl ipecho.net/plain 8.8.8.8 curl whatismyip.akamai.com $ curl whatismyip.akamai.com 8.8.8.8 curl myip.dnsomatic.com $ curl myip.dnsomatic.com 8.8.8.8 curl myip.dnsomatic.com $ curl myip.dnsomatic.com 8.8.8.8 二、获取脚本绝对路径 #编辑脚本 $ cat >/usr/src/test.sh 三、获取脚本执行时间 #编辑脚本 $ cat >/opt/test.sh 四、shell脚本中精准过滤进程 示例：过滤crond进程，会把grep命令同样显示出来 $ ps aux|grep crond root 771 0.0 0.0 126388 1616 ? Ss 08:57 0:00 /usr/sbin/crond -n root 20996 0.0 0.0 112828 980 pts/0 S+ 21:11 0:00 grep --color=auto crond 精准过滤 $ ps aux|grep '[c]rond' root 771 0.0 0.0 126388 1616 ? Ss 08:57 0:00 /usr/sbin/crond -n $ ps aux|grep crond |grep -v grep root 771 0.0 0.0 126388 1616 ? Ss 08:57 0:00 /usr/sbin/crond -n 五、shell显示ok或者faild #编辑脚本 $ cat >test.sh 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/3.4shell脚本执行方法.html":{"url":"linux/shell/3.4shell脚本执行方法.html","title":"shell脚本执行方法","keywords":"","body":"shell脚本执行方法 shell脚本5种执行方法 执行方法 是否需要执行权限 是否在当前进程执行 . 不需要 当前进程 source 不需要 当前进程 ./ 需要 子进程，不能读取当前shell中的变量 sh 不需要 子进程，不能读取当前shell中的变量 bash 不需要 子进程，不能读取当前shell中的变量 脚本内容 cat >test.sh 执行脚本 $ pwd /root $ sh test.sh $ pwd #会发现使用sh执行脚本切换路径未生效 /root 问题：执行脚本后，理应切换到/opt，但是实际并没有 原因：执行脚本的时候，只是在当前的shell下开了一个子进程，切换目录的操作只对该进程中相关后续指令有效，但改变不了父进程的目录 解决方法：使用.或者source执行脚本 脚本的执行方法 . source 脚本可以没有执行权限，会在当前进程中执行 ./ sh bash 执行脚本时，会启动一个子进程来运行脚本，不能读取当前shell中的变量 以上五种方法中，只有./需要脚本有执行权限，其他四种不需要 测试 . 切换目录成功 $ pwd /root $ . test.sh $ pwd /opt source 切换目录成功 $ pwd /root $ source test.sh $ pwd /opt ./ 切换目录失败 $ pwd /root $ ./test.sh $ pwd /root sh 切换目录失败 $ pwd /root $ sh test.sh $ pwd /root bash 切换目录失败 $ pwd /root $ bash test.sh $ pwd /root 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/3.5shell打印字符长度.html":{"url":"linux/shell/3.5shell打印字符长度.html","title":"shell打印字符串长度","keywords":"","body":"[toc] shell打印字符长度 编写shell脚本以打印下面语句中字符数小于6的单词。 The hard part isn’t making the decision. It’s living with it. 思路：首先取出所有单词，计算每个单词的长度，然后依次进行判断 计算变量内容的长度，常见的方法有四种： 1.变量自带的获取长度的方法 echo $ $ str=abc $ echo ${#str} 3 2.管道加wc -L方法 $ str=abc #-L 打印行长度 $ echo $str|wc -L 3 3.利用expr自带的length方法 $ str=abc $ expr length $str 3 4.利用awk自带的length函数方法 $ str=abc $ echo $str|awk '{print length ($0)}' 3 结合for循环截取字符串 结合for循环截取字符串，例如截取给定字符串中长度大于某一个值或小于某一个值 例句：The hard part isn't making the decision. It's living with it. 截取例句中单词长度大于5的单词 方法一 $ #编辑脚本 cat >test.sh 方法二 wc -L #编辑脚本 cat >test.sh 方法三 利用expr自带的length方法 #编辑脚本 cat >test.sh 方法四 利用awk自带的length函数方法 #编辑脚本 cat >test.sh 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/4.shell脚本条件测试.html":{"url":"linux/shell/4.shell脚本条件测试.html","title":"shell脚本条件测试","keywords":"","body":"[toc] shell脚本条件测试 一、sehll脚本的条件测试 条件测试语法 说明 test 测试表达式 test命令和测试表达式之间至少有一个空格 [ 测试表达式 ] []边界和内容之间至少有一个空格 [[ 测试表达式 ]] [[]]边界和内容之间至少有一个空格 ((测试表达式)) 一般用于if语句里,(())俩边不需要有空格 &&、||、>、等操作符可以应用于[[]]中，但不能应用于[]中，在[]中一般用-a、-o、-gt、-lt代替上述操作 二、文件测试表达式 文件测试操作符 说明 -d (directory) 是否为目录 -f (file) 是否为文件 -e (exist) 文件是否存在 -r (read) 是否只读 -w (write) 是否可写 -x (executable) 是否可执行 -s (size) 文件大小不为0 -L (link) 是否为链接文件 -nt (new than) f1 -nt f2 文件f1是否比f2新 -ot (older than) f1 -ot f2 文件f1是否比f2旧 -r和-w都是判断文件属主是否有权限(root用户下都会返回1)，属组和其他人即使有权限也返回0 -x 普通用户下判断属主是否有执行权限，即使属组和其他人有权限也返回0 -x root用户下只要文件有执行权限就返回1 文件测试表达式示例 判断文件属主是否有读权限 #创建文件 $ touch hehe $ ll hehe -rw-r--r-- 1 root root 0 Nov 22 13:30 hehe #将hehe的权限改为000 $ chmod 000 hehe $ ll hehe ---------- 1 root root 0 Nov 22 13:30 hehe #判断文件是否只读 $ [ -r hehe ] && echo 1 || echo 0 #文件hehe没有任何权限，但是返回了1，因为当前用户是root 1 #切换到普通用户测试 $ su - www $ touch hehe $ chmod 000 hehe $ ll total 0 ---------- 1 www www 0 Jun 25 10:14 hehe #普通用户下，当文件没有读权限的时候返回是0 $ [ -r hehe ] && echo 1 || echo 0 0 判断文件属主是否有写权限 #判断文件是否可写 $ [ -w hehe ] && echo 1 || echo 0 #文件hehe没有任何权限，但是返回了1，因为当前用户是root 1 #切换到普通用户测试 $ su - www $ touch hehe $ chmod 000 hehe $ ll total 0 ---------- 1 www www 0 Jun 25 10:14 hehe #普通用户下，当文件没有写权限的时候返回是0 $ [ -w hehe ] && echo 1 || echo 0 0 判断文件是否有执行权限 #判断文件是否可执行 $ [ -x hehe ] && echo 1 || echo 0 #文件所有者是root的情况下，只有执行权限没有的时候会返回0，读写权限即使没有也会返回1 0 -f示例 $ echo $hehe #变量hehe为空 $ [ -f $hehe ] && echo 1 || echo 0 1 #不加引号返回的结果不正确 $ [ -f \"$hehe\" ] && echo 1 || echo 0 0 #加引号返回才正确 三、字符串测试表达式 字符串测试操作符 说明 -n 字符串 若字符串长度不为0，则为真 no zero -z 字符串 若字符串长度为0，则为真 zero 字符串1=字符串2 若字符串1等于字符串2，则为真 字符串1!=字符串2 若字符串1不等于字符串2，则为真 字符串测试表达式特殊示例 示例1 $ [ \"abc\"=\"1\" ] && echo 1 || echo 0 1 #结果不正确，进行字符串比较时，等号两端如果没有空格则会错误 $ [ \"abc\" = \"1\" ] && echo 1 || echo 0 0 示例2 $ var=\"\" $ echo $var $ [ -n \"$var\" ] && echo 1 || echo 0 0 $ [ -n $var ] && echo 1 || echo 0 1 #结果不正确，因为变量var为空，字符串比较时，如果不加双引号则结果不正确 示例3 (大于号)在[]中使用时，需要用\\转义 $ [ 1 四、整数二元比较操作符 整数二元比较操作符 说明 -eq 相等 equal -ne 不相等 not equal -gt 大于 granter than -ge 大于等于 granter equal -lt 小于 less than -le 小于等于 less equal 五、逻辑操作符 在[]和test中使用的操作符 在[[]]和(())中使用的操作符 说明 -a && and,与 俩端都为真则结果为真 -o **\\ \\ ** or,或 俩端有一个为真则为真 ！ ！ not,非 俩端相反则为真 逻辑运算符示例 -a -o不能用在[[]]中，&& ||等不能用在[]中 #&&不能用在[]中，需要使用-a $ [ -f /etc/hosts && -f /etc/services ] && echo 1 || echo 0 #bash: [: missing `]' 0 $ [ -f /etc/hosts -a -f /etc/services ] && echo 1 || echo 0 1 #-a不能用在[[]]中，需要使用&& $ [[ -f /etc/hosts -a -f /etc/services ]] && echo 1 || echo 0 -bash: syntax error in conditional expression #-bash: syntax error near `-a' $ [[ -f /etc/hosts && -f /etc/services ]] && echo 1 || echo 0 1 六、测试表达式test、[]、[[]]、(())的区别总结 其中[]最为常用 测试表达式符号 边界是否需要空格 逻辑操作符 整数比较操作符 字符串比较操作符 是否支持通配符匹配 test 需要 !、-a、-o -eq、-gt、-lt、ge、-le =、==、!= 不支持 [] 需要 !、-a、-o -eq、-gt、-lt、ge、-le =、==、!= 不支持 [[]] 需要 **!、&&、\\ \\ ** -eq、-gt、-lt、ge、-le或=、>、=、 =、==、!= 支持 (()) 不需要 !、&&、 =、>、=、 =、==、!= 不支持 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/5.shell数值运算.html":{"url":"linux/shell/5.shell数值运算.html","title":"shell数值运算","keywords":"","body":"[toc] shell数值运算 一、算术运算符表 二、运算操作符与运算命令 三、双小括号\"(())\"数值运算 示例 $ echo $((1+10)) 11 $ echo $((1-10)) -9 示例1 用(())做数值运算 $ echo $((1+2**3-4%3)) #先乘除后加减，先算2**3=8 4%3=1，最后1+8-1=8 8 $ ((a=1+2**3-4%3)) $ echo $a 8 示例2 在变量前后使用--和++特殊运算符的表达式 $ a=10 $ echo $((a++)) #如果a在运算符++的前面，那么在输出整个表达式时，会输出a的值 10 $ echo $a #执行上面的表达式后，因为有a++，因此a会增加1 11 $ a=10 $ echo $((a--)) #如果a在运算符--的前面，那么在输出整个表达式时，会输出a的值 10 $ echo $a #执行上面的表达式后，因为有a--，因此a会减少1 9 $ a=10 $ echo $((++a)) #如果a在运算符++的后面，那么在输出整个表达式时，先进行自增 11 $ echo $a 11 $ a=10 $ echo $((--a)) #如果a在运算符--的后面，那么在输出整个表达式时，先进行自减 9 $ echo $a 9 示例3 通过(())运算后赋值给变量 $ a=100 $ b=$((a+1)) $ echo $b 101 四、let运算命令的用法 let运算命令语法格式 let 赋值表达式 相当于 ((赋值表达式)) 示例1 用let做数值运算 $ a=1 $ let a=a+8 $ echo $a 9 五、expr命令的用法 expr命令既可以用于整数运算,也可以用于相关字符串长度、匹配等的运算处理 示例 expr命令用于计算 $ expr 2+2 2+2 $ expr 2 + 2 #注意，运算符左右必须有至少一个空格 4 $ expr 2 * 2 expr: syntax error #注意，做乘法运算需要转义* $ expr 2 \\* 2 4 示例2 利用expr判断一个变量值或字符串是否为整数 实现原理:利用expr做计算时变量或字符串必须是整数的原则，把一个变量或字符串和一个已知的整数(非0)相加，看命令返回的值是否为0，如果为0，就认为做加法的变量或字符串为整数，否则就不是 $ i=1 $ expr $i + 1 &>/dev/null $ echo $? 0 #返回0，证明i的值为整数 $ i=hehe $ expr $i + 1 &>/dev/null $ echo $? 2 #返回为非0，证明i的值不是整数 示例3 利用expr判断参数是否为整数 #编辑脚本 cat >expr1.sh /dev/null 2>&1 [ $? -eq 0 ] && echo int || echo chars done EOF #执行脚本 $ sh expr1.sh please input: 1 int please input: 2 int please input: a chars please input: / chars 示例4 利用expr判断文件扩展名是否符合要求 #编辑脚本 cat > string.sh /dev/null;then echo \"you are using $1\" else echo \"please use *.pub file\" fi EOF #执行脚本 $ sh string.sh hehe please use *.pub file $ sh string.sh hehe.pub you are using hehe.pub 示例5 利用expr计算字符串的长度 #当变量有空格时，expr length 后的变量必须加引号 $ char=\"i am a boy\" $ expr length $char expr: syntax error $ expr length \"$char\" 10 #当变量没有空格时，expr length 后的变量可以不加引号 $ char=\"iamboy\" $ expr length $char 6 五、bc命令的用法 示例1 bc交互式用法 $ bc bc 1.06.95 Copyright 1991-1994, 1997, 1998, 2000, 2004, 2006 Free Software Foundation, Inc. This is free software with ABSOLUTELY NO WARRANTY. #For details type `warranty'. scale=5 #表示小数后保留5位 9/5 1.80000 9/1.3 6.92307 1+1 2 9-3 6 示例2 bc用在命令行中 $ echo 3+5 | bc 8 $ echo 9-3 | bc 6 $ echo 9*9 | bc 81 $ echo 9.9+6.6 | bc 16.5 示例3 利用bc通过一条命令计算输出1+2+3+..+10的值 #生成表达式方法一 $ seq -s \"+\" 10 1+2+3+4+5+6+7+8+9+10 $ seq -s \"+\" 10 | bc 55 #生成表达式方法二 $ echo {1..10} | tr \" \" \"+\" 1+2+3+4+5+6+7+8+9+10 $ echo {1..10} | tr \" \" \"+\" | bc 55 六、$[]符号的运算 $[]只能做整数运算 $ echo $[2**3] 8 $ echo $((3%5)) 3 $ echo $[1+1] 2 $ echo $[2*2] 4 $ echo $[10/3] 3 $ echo $[10%3] 1 $ echo $[10**3] 1000 七、用awk实现计算 #方法1 $ echo \"9.9 8.8\" | awk '{print ($1-$2)}' 1.1 $ echo \"358 113\" | awk '{print ($1-3)/$2}' 3.14159 #方法2 $ awk 'BEGIN{print (9.9-8.8)}' 1.1 $ awk 'BEGIN{print (358-3)/113}' 3.14159 八、declare(同typeset)命令用法 使用typeset定义整数变量，直接进行计算，此方法不常用，因为需要定义才能生效 $ declare -i A=10 B=20 $ A=A+B $ echo $A 30 $ typeset -i C=30 D=50 $ C=C+D $ echo $C 80 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/6.1shell流程控制语句之if.html":{"url":"linux/shell/6.1shell流程控制语句之if.html","title":"if","keywords":"","body":"[toc] shell流程控制语句之if 一、if单分支 1.1 if单分支语法 #写法一 if 条件表达式;then 命令 fi #写法二 if 条件表达式 then 命令 fi 1.2 示例 if [ 1 -eq 1 ];then echo \"equality\" fi 二、if双分支 2.1 if双分支语法 #写法一 if 条件表达式;then 命令 else 命令 fi #写法二 if 条件表达式 then 命令 else 命令 fi 2.2 示例 if [ 1 -eq 1 ];then echo \"equality\" else echo \"not equality\" fi 三、if多分支 3.1 if多分支语法 elif后面还可以加条件，即elif可以有多个 写法一 if 条件表达式;then 命令 elif 命令 else 命令 fi 写法二 if 条件表达式 then 命令 elif 命令 else 命令 fi 3.2 示例 if [ 1 -eq 1 ];then echo \"equality\" elif [ 2 -eq 1 ];then echo \"equality\" else echo \"not equality\" fi 猜数字脚本示例 系统产生一个随机数，然后用户输入一个数字，与随机数做比较，输入的数字比随机数大则提示输入数字大了，输入的数字比随机数小则提示输入数字小了，猜对提示你猜对了 #!/usr/bin/env bash SJ=`echo $((RANDOM%100+1))` i=1 for ((;;)) do read -p \"please input a num: \" NUM if [[ $NUM =~ ^[0-9]+$ ]];then if [ $NUM -eq $SJ ];then echo -e \"\\033[32myou guess it!!! \\033[0m\" echo -e \"\\033[34mGuess the total is $i times!!!\\033[0m\" exit 0 elif [ $NUM -gt $SJ ];then echo \"Larger than the random number\" else echo \"Smaller than the random number\" fi else echo \"Please enter the correct number: \" fi let i++ done 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/6.2shell流程控制语句之case.html":{"url":"linux/shell/6.2shell流程控制语句之case.html","title":"case","keywords":"","body":"shell流程控制语句之case case语句语法 case 变量名 in 模式1) 命令1 ;; 模式2) 命令2 ;; 模式3) 命令3 ;; *) 其它命令 esac 使用示例 read -p \"请输入一个数字：\" NUM case $NUM in 1) echo \"你输入的是1\" ;; 2) echo \"你输入的是2\" ;; 3) echo \"你输入的是3\" ;; *) echo \"请输入以下数字{1|2|3}\" esac 购物脚本示例 打印本店菜单，然后提示输入商品的编号，购买的数量，并计算消费多少，如果输入不正确则退出 #!/bin/bash echo -e \"\\033[36m这是本店的菜单:\\033[0m \\n\\t\\033[32m1.汉堡/13￥\\033[0m\\n\\t\\033[35m2.>鸡腿/9￥\\033[0m\\n\\t\\033[31m3.可乐/6￥\\033[0m\" xunhuan (){ read -p '请输入要购买的商品编号,按\"q\"退出,按\"y\"打印消费清单: ' menu case \"$menu\" in q) break ;; 1) read -p \"请输入要购买的汉堡数量: \" hb echo -e \"\\033[34m\\n您购买了$hb个汉堡\\n\\033[0m\" ;; 2) read -p \"请输入要购买的鸡腿数量: \" jt echo -e \"\\033[34m\\n您购买了$jt个鸡腿\\n\\033[0m\" ;; 3) read -p \"请输入要购买的可乐数量: \" kl echo -e \"\\033[34m\\n您购买了$kl个可乐\\n\\033[0m\" ;; y) a=$hb*13 b=$jt*9 c=$kl*6 let sum=$a+$b+$c echo -e \"\\033[32m消费清单:\\n\\t商品名称\\t单价\\t数量\\t总价\\n\\t汉堡\\t\\t13￥\\t$hb\\t$a\\n\\t鸡腿\\t\\t9￥\\t$jt\\t$b\\n\\t可乐\\t\\t6￥\\t$kl\\t$c\\n\\n\\t总计:$sum元\\t\\t收银员：李骚峰\\033[0m\" ;; *) echo \"输入不正确,请输入正确的编号{1|2|3}\" esac } while : ;do xunhuan done 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/7.1shell循环控制语句之for.html":{"url":"linux/shell/7.1shell循环控制语句之for.html","title":"for","keywords":"","body":"shell循环控制语句之for 语法 for 变量名 in 取值列表; do 命令 done 示例 #!/bin/bash data='a b c d' IFS=, for i in $data;do echo $i done for循环高并发执行脚本 for循环ping脚本，执行效果很慢，因为会一个一个IP去ping #!/usr/bin/env bash #export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin for i in {1..254} do ping -c1 10.0.0.$i &>/dev/null if [ $? -eq 0 ];then echo \"10.0.0.$i\" >> /root/ping.txt fi done 高并发执行 #!/usr/bin/env bash #export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin for i in {1..254} do { ping -c1 10.0.0.$i &>/dev/null if [ $? -eq 0 ];then echo \"10.0.0.$i\" >> /root/ping.txt fi }& done wait echo -e\"online ip is: \\n`cat /root/ping.txt`\" waite 等待并发全部执行完成才往后执行 for循环高并发 在要执行的命令外加上 {命令}& 即在do跟done之间的命令加{}& 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/7.2shell循环控制语句之while、until.html":{"url":"linux/shell/7.2shell循环控制语句之while、until.html","title":"while、until","keywords":"","body":"shell循环控制语句之while、until 一、while 语法 while 条件表达式 do 命令 done 说明 while循环语句会对紧跟在while命令后的条件表达式进行判断，如果该条件表达式成立，则执行while循环体里的命令，每一次执行到done时就会重新判断while条件表达式是否成立，直到条件表达式不成立时才会跳出while循环体，如果一开始条件表达式就不成立，那么程序就不会进入循环体中执行命令了 示例1 while true表示条件永远为真，因此会一直执行 #!/bin/bash while true do uptime sleep 2 done 执行结果如下 22:15:04 up 11:00, 2 users, load average: 0.00, 0.01, 0.05 22:15:06 up 11:00, 2 users, load average: 0.00, 0.01, 0.05 22:15:08 up 11:00, 2 users, load average: 0.00, 0.01, 0.05 。。。 ctrl+c停止 示例2 while循环竖向打印54321 #!/bin/bash i=5 while ((i>0)) do echo \"$i\" ((i--)) done 二、until 语法 until 条件表达式 do 命令 done 当条件表达式不成立时，进入循环执行命令，条件表达式成立时，终止循环，until应用较少 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/7.3循环控制及状态返回值.html":{"url":"linux/shell/7.3循环控制及状态返回值.html","title":"循环控制及状态返回值","keywords":"","body":"[toc] 循环控制及状态返回值 一、break、continue、exit、return的区别和对比 break、continue在条件语句及循环语句(for、while)中用于控制程序的走向 exit用于终止所有语句并退出当前脚本，除此之外exit还可以返回上一次程序或命令的执行状态值给当前shell return类似于exit，只不过return仅用于在函数内部返回函数执行的状态值 命令 说明 break [n] 跳出整个循环，n表示跳出循环的层数 continue [n] 跳过本次循环，忽略本次循环的剩余代码，进入下一次循环，n表示退到第n层继续循环 exit [n] 退出当前shell程序，n为上一次程序执行的状态返回值，n也可以省略，在下一个shell里可通过$?接收exit n的n的值 return [n] 退出当前函数，并且在函数里作为函数的返回值，以判断函数执行是否正确，在下一个shell里可通过$?接收return n的n的值 二、break、continue、exit功能执行流程图 2.1 break 2.2.1 while循环中break的功能执行流程图 2.2.2 for循环中break的功能执行流程图 2.2 continue 2.2.1 while循环中continue的功能执行流程图 2.2.2 for循环中continue的功能执行流程图 2.3 exit 2.3.1 while循环中exit的功能执行流程图 2.3.2 for循环中exit的功能执行流程图 三、break、continue、exit、return基础示例 3.1 编辑综合示例脚本 #!/bin/bash if [ $# -ne 1 ];then #如果传入的参数个数不为1，则打印下面的使用提示给用户 echo $\"usage:$0 {break|continue|exit|return}\" exit 1 fi #定义测试函数 test(){ for((i=0; i 3.2 break 含义 跳出整个循环 当参数是break时 当i等于3时，符合条件，执行break，跳出整个循环，之前循环打印的变量i依次为0、1、2，并执行后续的echo，因为没有return所以执行echo \"I am in func.\"和echo \"ok\" $ sh test.sh break 0 1 2 I am in func. ok 3.3 continue 含义 跳过本次循环，忽略本次循环的剩余代码，进入下一次循环，n表示退到第n层继续循环 当参数是continue时 当i等于3时，符合条件，执行continue，跳出本次循环，继续下一次循环，因此打印的变量i没有3，执行后续的echo，因为没有return所以执行echo \"I am in func.\"和echo \"ok\" $ sh test.sh continue 0 1 2 4 5 I am in func. ok 3.4 exit 含义 退出当前shell程序，n为上一次程序执行的状态返回值，n也可以省略，在下一个shell里可通过$?接收exit n的n的值 当参数是exit时 当i等于3时，符合条件，执行exit，退出脚本，所有的echo都不执行，之前循环打印的变量i依次为0、1、2，默认exit的退出码是0，如果指定了则打印指定的exit退出码 $ sh test.sh exit 0 1 2 $ echo $? 0 #手动设置exit返回值是100 $ sh test.sh \"exit 100\" 0 1 2 $ echo $? 100 3.5 return 含义 退出当前函数，并在函数里作为函数的返回值，以判断函数执行是否正确，在下一个shell里可通过$?接收return n的n的值 当参数是return时 当i等于3时，符合条件，执行return，并且返回一个返回值，执行后续的echo，因为是return，所以函数中的echo不执行，最后执行echo \"return's exit status:$func_ret\"和echo \"ok\" $ sh test.sh return 0 1 2 return's exit status:0 ok #手动设置return返回值是100 $ sh test.sh \"return 100\" 0 1 2 return's exit status:100 ok 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/8.shell数组.html":{"url":"linux/shell/8.shell数组.html","title":"shell数组","keywords":"","body":"[toc] shell数组 一、shell数组概念 概念 shell数组就是一个元素集合，它把有限个元素(变量或字符组合)用一个名字来命名，然后用编号对他们进行区分，这个名字就称为数组名 数组下标 用于区分不同内容的编号 数组元素 组成数组的各个元素,也称为变量 二、shell数组定义 2.0 shell数组分类 普通数组：数组下标只能是数字 关联数组：数组下标可以是字符，通过declare -A 数组名定义 2.1 普通数组 方法一 用小括号将变量值括起来赋值给数组变量，每个变量之间要用空格进行分隔,常用定义方法 语法 array=(value1 value2 value3 ... ) 使用示例 $ array=(1 2 3) $ echo ${array[*]} 1 2 3 方法二 用小括号将变量值括起来，同时采用键值对的形式复制，不常用 语法 array=( [1]=one [2]=two [3]=three ) 使用示例 $ array=( [1]=one [2]=two [3]=three ) $ echo ${array[*]} one two three $ echo ${array[1]} one $ echo ${array[2]} two $ echo ${array[3]} three 方法三 通过分别定义数组变量的方法来定义，不常用 语法 array[0]=a;array[1]=b;array[2]=c 使用示例 $ array[0]=a;array[1]=b;array[2]=c $ echo ${array[0]} a $ echo ${array[1]} b $ echo ${array[2]} c 方法四 动态的定义数组变量，并使用命令的输出结果作为数组的内容 语法 array=($(命令)) 或 array=(`命令`) 使用示例 $ mkdir /array $ touch /array/{1..3}.txt $ ll /array/ total 0 -rw-r--r-- 1 root root 0 Nov 12 08:37 1.txt -rw-r--r-- 1 root root 0 Nov 12 08:37 2.txt -rw-r--r-- 1 root root 0 Nov 12 08:37 3.txt $ array=($(ls /array)) $ echo ${array[*]} 1.txt 2.txt 3.txt array=(`ls /array`) $ echo ${array[*]} 1.txt 2.txt 3.txt 2.2 关联数组 2.2.1 定义关联数组 普通数组的下标不能为字符 $ array=([a]=hehe [b]=haha [c]=xixi) #输出不正确 $ echo ${array[*]} xixi 定义关联数组，需要提前用declear -A 数组名定义 $ declare -A array $ array=([a]=hehe [b]=haha [c]=xixi) $ echo ${array[*]} hehe haha xixi 关联数组使用示例 #定义了一个关联数组array $ declare -A array #让下标为m的数组元素自增1，此时m为1 $ let array[m]++ #让下标为m的数组元素自增1，此时m为2 $ let array[m]++ #让下标为f的数组元素自增1，此时f为1 $ let array[f]++ #打印下标为m出现的次数 $ echo ${array[m]} 2 #打印下标为f出现的次数 $ echo ${array[f]} 1 2.2.2 关联数组脚本示例 统计/etc/passwd中每个bash出现的次数 思路: 1.将/etc/passwd的最后一列取出来,因为最后一列是bash类型 2.定义关联数组，将取出来的bash类型存放到数组中 3.for循环遍历数组，输出相同下标的数组元素 编辑脚本 #!/usr/bin/env bash #定义一个关联数组 declare -A array #使用while循环从/etc/passwd文件中读取每一行 while read line do #把/etc/passwd中bash类型取出来 PASSWD=`echo $line|awk -F: '{print $NF}'` #让每一行bash自增1,这样最后就会把相同bash相加 let array[$PASSWD]++ done 执行结果如下 /sbin/nologin 21 /bin/sync 1 /bin/bash 2 /sbin/shutdown 1 /sbin/halt 1 2.2.3 关于关联数组的问题 2.2.3.1 关联数组赋值时必须指定下标 #先定义一个关联数组 $ declare -A array #在给数组赋值的时候提示为关联数组赋值时必须使用下标 $ array=(1 2 3) bash: array: 1: must use subscript when assigning associative array bash: array: 2: must use subscript when assigning associative array bash: array: 3: must use subscript when assigning associative array #此时必须给数组元素指定下标 $ array=([1]=1 [2]=2 [3]=3) //查看元素显示 $ echo ${array[*]} 1 2 3 //查看数组下标 $ echo ${!array[*]} 1 2 3 //依据数组下标查看数组元素正确 $ echo ${array[1]} 1 $ echo ${array[2]} 2 $ echo ${array[3]} 3 2.2.3.2 普通数组在只有一个元素的情况下，下标可以是字符串，但是超过2个元素就不可以 第一种情况，指定普通数组的下标是字符串，但是只给数组赋值一个元素 #定义一个普通数组 $ array=([a]=aa) //数组内容可以正常显示 $ echo ${array[*]} aa #依据数组下标查看数组元素正确 $ echo ${array[a]} aa #数组下标也显示正确 $ echo ${!array[*]} 0 第二种情况，指定普通数组的下标是字符串，并给数组赋值2个元素 #定义一个普通数组 $ array=([a]=aa [b]=bb) #此时数组内容显示不正确 $ echo ${array[*]} bb #依据数组下标查看数组元素也不正确 $ echo ${array[a]} bb $ echo ${array[b]} bb #同时数组下标显示也不正确 $ echo ${!array[*]} 0 三、shell数组的打印及输出 3.1 打印数组元素 语法 ${数组名 [下标]} 数组下标从0开始 使用示例 $ array=(1 2 3) $ echo ${array[0]} 1 $ echo ${array[1]} 2 $ echo ${array[2]} 3 #使用*或@打印整个数组内容 $ echo ${array[*]} 1 2 3 #使用*或@打印整个数组内容 $ echo ${array[@]} 1 2 3 3.2 打印数组元素个数 语法 ${#数组名 [*]} 或 ${#数组名 [@]} 使用示例 $ array=(1 2 3) $ echo ${array[*]} 1 2 3 $ echo ${array[@]} 1 2 3 $ echo ${#array[@]} 3 $ echo ${#array[*]} 3 3.3 查看数组下标 语法 ${!数组名[*]} 使用示例 $ array=(1 2 3) $ echo ${!array[*]} 0 1 2 3.4 数组赋值 语法 #如果下标不存在，则会自动添加一个新的数组元素 数组名[下标]=值 使用示例 $ array=(1 2 3) $ echo ${array[*]} 1 2 3 #增加下标为3的数组元素，即增加数组第4个元素 $ array[3]=4 $ echo ${array[*]} 1 2 3 4 $ array[0]=hehe $ echo ${array[*]} hehe 2 3 4 查看数组赋值 命令: declare -a #查看普通数组 declare -A #查看关联数组 使用示例: #创建一个普通数组 $ array=(1 2 3 4 5) $ declare -a | tail -1 declare -a array='([0]=\"1\" [1]=\"2\" [2]=\"3\" [3]=\"4\" [4]=\"5\")' #创建一个关联数组 $ declare -A array $ array=([a]=hehe [b]=haha [c]=xixi) $ declare -A | tail -1 declare -A array='([a]=\"hehe\" [b]=\"haha\" [c]=\"xixi\" )' 3.5 数组及数组元素的删除 语法 #删除相应下标的数组元素 unset 数组[下标] #删除整个数组 unset 数组 使用示例 $ array=(1 2 3) $ echo ${array[*]} 1 2 3 #删除下标为0的数组元素 $ unset array[0] $ echo ${array[*]} 2 3 #删除整个数组 $ unset array $ echo ${array[*]} #数组已经删除，返回为空 数组元素删除扩展示例 $ array=(one one one one one) $ echo ${array[*]} one one one one one #从左边开始匹配最短的数组元素并删除 $ echo ${array[*]#o*} ne ne ne ne ne #从左边开始匹配最长的数组元素并删除 $ echo ${array[*]##o*} #全部匹配并删除 #从右边开始匹配最短的数组元素并删除 $ echo ${array[*]%%e*} on on on on on #从右边开始匹配最长的数组元素并删除 $ echo ${array[*]%e*} on on on on on 3.6 数组内容截取 语法 #数字1:数字2表示截取下标1到下标2的元素 $[array[*]:数字1:数字2] 使用示例 $ array=(1 2 3 4 5 6) $ echo ${array[*]} 1 2 3 4 5 6 #截取下标1到下标3的元素 $ echo ${array[*]:1:3} 2 3 4 $ array=(1 2 3 4 5 6) #这里理解为从下标0开始,截取3个元素 $ echo ${array[*]:0:3} 1 2 3 $ array=(`echo {a..z}`) $ echo ${array[*]} a b c d e f g h i j k l m n o p q r s t u v w x y z 3.7 数组内容替换 语法 $[数组名[*]/要替换掉数组元素/替换为什么] 使用示例 $ array=(1 2 3 4 5 6) #将数组中的1替换为hehe，但不改变原数组内容 $ echo ${array[*]/1/hehe} hehe 2 3 4 5 6 #数组内容并没有改变 $ echo ${array[*]} 1 2 3 4 5 6 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/9.shell函数.html":{"url":"linux/shell/9.shell函数.html","title":"shell函数","keywords":"","body":"[toc] shell函数 一、shell函数简介 函数概念 类似于alias别名 函数作用 将程序里多次被调用的相同代码组合起来(函数体)，并为其取一个名字，即函数名，其他所有想重复调用这部分代码的地方都只需要调用这个名字就可以了 函数优势 把相同程序段定义为函数,可以减少整个程序的代码量,提升开发效率 增加程序的可读性,易读性,提升管理效率 可以实现程序功能模块化,使得程序具备通用性 二、shell函数的定义 2.1 标准写法 语法 function 函数名 () { 命令 } 2.2 简化写法 简化写法一 函数名 () { 命令 } 简化写法二 function 函数名 { 命令 } 三、shell函数参数 参数列表 参数 说明 $n 位置参数，第n个参数，n是整数 $# 脚本参数总数 $* 参数列表 $@ 参数列表 四、shell函数的调用 直接在脚本中写函数名即可 #!/usr/bin/env bash #test为函数名称 test (){ echo \"function test\" } #调用函数 test 五、shell函数的执行 1.执行shell函数时，函数名前的function和函数后的小括号都不要带 2.函数的定义必须在要执行的程序前面定义或加载 3.shell执行系统中各种程序的执行顺序为: 系统别名-->函数-->系统命令-->可执行文件 4.在shell函数里面，return命令的功能于exit类似，return的作用是退出函数，而exit是退出脚本文件 5.return语句会返回一个退出值(即返回值)给调用函数的当前程序，而exit会返回一个退出值(即返回值)给执行程序的当前shell 6.如果函数存放在独立的文件中，被脚本加载使用时，需要使用source或\".\"来加载 7.在函数内一般使用local定义局部变量,这些变量离开函数后就会消失 六、shell函数使用示例 6.1 基本使用示例 编辑脚本 #!/usr/bin/env bash test1 (){ echo test1 } test2 (){ echo test2 } test1 test2 执行结果如下 test1 test2 6.2 分离函数体和执行函数的脚本示例 ⚠️这个方法在centOS7中不能用，原因未知!!! 建立函数库脚本，往/etc/init.d/functions文件中追加一个函数 cat >>/etc/init.d/functions 编写脚本，调用刚才往/etc/init.d/functions文件中追加的函数 cat > test-functions.sh 执行结果如下 $ sh test-functions.sh test functions 6.3 函数传参示例 6.3.1 通过脚本传参 编辑脚本 cat > test-functions.sh 执行结果如下 $ sh test-functions.sh 123 you input 123 6.3.2 通过/etc/init.d/functions文件实现 ⚠️这个方法在centOS7中不能用，原因未知!!! 建立函数库脚本，往/etc/init.d/functions文件中追加一个函数 cat >>/etc/init.d/functions 编写脚本，调用刚才往/etc/init.d/functions文件中追加的函数 cat >test-functions.sh 执行结果如下 $ sh test-functions.sh 123 you input 123 七、shell函数脚本使用示例 url检测脚本示例 cat >check_url.sh 执行结果如下 $ sh check_url.sh www.baidu.com www.baidu.com is yes 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/10.expect自动化交互式程序.html":{"url":"linux/shell/10.expect自动化交互式程序.html","title":"expect自动化交互程序","keywords":"","body":"[toc] expect自动化交互式程序 一、expect简介 什么是expect expect是一个用来实现自动交互功能的软件套件(expect is a software suite for automating interactive tools)，是基于TCL的脚本编程工具语言，方便学习，功能强大 为什么要使用expect linux中执行系统命令或程序时，系统会以交互式的形式要求输入指定的字符串，之后才能继续执行命令，如果在shell脚本中，这样就不能实现脚本完全自动化执行，因此需要用到expect自动化交互 linux交互式操作的场景 #修改用户密码 $ passwd Changing password for user root. New password: #ssh连接服务器 $ ssh root@10.0.0.10 root@10.0.0.10's password: #生成ssh密钥 ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): expect自动交互工作流程 spawn启动指定进程--->expect获取期待的关键字--->send向指定进程发送指定字符--->进程执行完毕，退出结束 二、expect简单使用示例 示例：免交互ssh连接服务器 先手动交互ssh连接一台服务器 $ ssh root@10.0.0.33 The authenticity of host '10.0.0.33 (10.0.0.33)' can't be established. ECDSA key fingerprint is SHA256:wrYIb1Ou6Yjp70e/3Tz9LSkjNBkW9flmmZjN80wwufU. ECDSA key fingerprint is MD5:34:61:48:e5:71:3c:11:a0:53:ea:73:e9:4c:cd:e0:ea. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '10.0.0.33' (ECDSA) to the list of known hosts. root@10.0.0.33's password: Last login: Sun May 16 23:13:49 2021 from 10.0.0.2 安装expect yum -y install expect 编辑expect脚本 脚本开头的#!/usr/bin/expect是脚本开头解析器，和shell类似，表示 程序使用expect解析 #扩展名使用.exp代表是expect脚本 cat >test.exp 使用expect命令执行脚本，因为之前已经ssh连接过了，所以这里就能不需要输入yes连接了 $ expect test.exp spawn ssh root@10.0.0.31 uptime root@10.0.0.31's password: 10:29:02 up 44 min, 1 user, load average: 0.00, 0.01, 0.03 这里仅仅是一个简单的不需要输入密码的示例，但是如果没有ssh连接过的话还是需要手动输入第一次连接需要的yes 三、expect程序自动交互的重要命令 3.1 spawn命令 在expect自动交互程序执行的过程中，spawn命令是一个开始就需要使用的命令，通过spawn执行一个命令或程序，之后所有的expect操作都会在这个执行过的命令或程序进程中进行，包括自动交互功能，因此如果没有spawn命令，expect程序将会无法实现自动交互 语法 spawn [选项] [需要自动交互的命令或程序] 示例 在spawn命令的后面，直接挤上要执行的命令或程序 spawn ssh root@10.0.0.100 uptime 使用spawn命令是expect程序实现自动交互工作流程中的第一步，也是最关键的一步 3.2 expect命令 3.2.1 expect命令说明 在expect自动交互程序的执行过程中，当使用spawn命令执行一个命令或者程序之后，会提示某些交互式信息，expect命令的作用就是获取spawn命令执行后的信息，查看是否和其事先自定的相匹配，一旦匹配上指定的内容就执行expect后面的动作，expect命令也有一些选项，相对用的多的是-re，表示使用正则表达式的方式来匹配 语法 expect 表达式 [动作] 示例 ⚠️以下命令不能在命令行中执行，需要放入expect脚本中执行 spawn ssh root@10.0.0.100 uptime expect \"*password\" (send 1\\r) 3.2.2 expect命令实践 3.2.2.1 实践示例1 执行ssh命令远程获取服务器负载值和eth0网卡，并自动输入yes及用户名密码 编辑expect脚本 cat >test.exp 说明 exp_send和send类似，后面的\\r(回车)，和前文的\\n(换行)类似 expect {}，类似多行expect 匹配多个字符串，需要在每次匹配并执行动作后，加上exp_continue 执行脚本 $ expect test.exp spawn ssh root@10.0.0.31 uptime && ip a s eth0 The authenticity of host '10.0.0.31 (10.0.0.31)' can't be established. ECDSA key fingerprint is SHA256:o9tdhhhgM3KPZytcz16k/Wt6gfEkDp1FCD3Q7VmLnoE. ECDSA key fingerprint is MD5:0e:a2:0c:a6:42:e4:95:b4:77:44:14:36:ba:11:2b:d8. #可以看到这里expect会自动输入yes Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '10.0.0.31' (ECDSA) to the list of known hosts. #expect自动输入密码 root@10.0.0.31's password: 10:32:18 up 8:45, 1 user, load average: 0.08, 0.03, 0.05 2: eth0: mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:1c:42:1b:e4:d7 brd ff:ff:ff:ff:ff:ff inet 10.0.0.31/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::21c:42ff:fe1b:e4d7/64 scope link valid_lft forever preferred_lft forever 3.2.2.2 实践示例2 利用expect相应shell脚本中的多个read读入 编辑shell脚本 cat > read.sh 编辑expect脚本 ⚠️expect脚本中的exp_send也可以改成send cat > read.exp 执行脚本 $ expect read.exp spawn /bin/sh /root/read.sh please input your username:xiaoming please input your password:1 please input your email:1@qq.com your name is xiaoming,your password is 1,your email is 1@qq.com. 3.3 send命令 send命令和exp_send命令用法类似，即在expect命令后匹配指定的字符串后，发送指定的字符串给系统，这些命令可以支持一些特殊转义符号，例如\\r(回车)、\\n(换行)、\\t(制表符) send命令使用示例 ⚠️expect脚本中的send也可以改成exp_send cat > read.exp send命令参数 -i 指定spawn_id，用来向不同的spawn_id进程发送命令，是进行多程序控制的参数 -s s代表slpwly，即控制发送的速度，使用的时候要与expect中的变量send_slow相关联 3.4 exp_continue命令 exp_continue命令一般处于expect命令中，属于一种动作命令，一般用在匹配多次字符串的动作中，从命令的拼写就可以看出命令的作用，即让expect程序继续匹配的意思 exp_continue命令使用示例 cat > read.exp 如果需要一次性匹配多个字符串，那么不同的匹配之间就要加上exp_continue，否则expect将不会自动输入指定的字符串，最后一个结尾就不需要加上exp_continue了，因为匹配全部完成了 3.5 send_user命令 send_user命令可以用来打印expect脚本信息，类似shell里的echo命令，并且有echo -e的功能，而默认的send、exp_send命令都是将字符串输出到expect程序中去 send_user命令使用示例 编辑expect脚本 cat >send_user.exp 执行脚本 $ expect send_user.exp i am boy. i like play basketbal. 3.6 exit命令 exit命令的功能类似于shell中的exit，即直接退出expect脚本，除了最基本的退出脚本功能外，还可以利用这个命令对脚本做一些关闭前的清理和提示等工作 exit命令使用示例 cat > exit.exp 执行脚本 $ expect exit.exp i am boy. i like play basketbal. goog bye. 3.7 expect常用命令总结 expect命令 作用 spawn spawn命令是一个在expect自动交互程序的开始就需要使用的命令，通过spawn执行一个命令或程序，之后所有的expect操作都在这个执行过的命令或程序进程中进行，包括自动交互功能 expect 在expect自动交互程序的执行过程中，在使用spawn命令执行一个命令或程序之后，会提示某些交互式信息，expect命令的作用就是获取这些信息，查看是否和其事先指定的信息相匹配，一旦匹配上指定的内容，就执行expect后面的动作 send expect中的动作命令，当expect匹配了指定的字符串后，发送指定的的字符串给系统，这些命令可以支持一些特殊的转义符号，例如\\r表示回车、\\n表示换行、\\t表示制表符等，还有一个类似的exp_send命令 exp_continue 属于一种动作命令，在一个expect命令中，用于多次匹配字符串并执行不同的动作中，从命令的拼写格式就可以看出该命令的作用，即让expect程序继续匹配 send_user 用来打印expect脚本信息，类似shell里的echo命令，并且带-e(支持转义)功能 exit 退出expect脚本，以及在退出脚本前做一些关闭前的清理和提示等工作 四、expect程序变量 4.1 普通变量 定义语法 set 变量名 变量值 定义普通变量示例 set pwd 123 打印变量 puts $变量名 使用示例 编辑expect脚本 cat > var.exp 执行脚本 $ expect var.exp 123 password is 123 4.2 特殊参数变量 在expect里也有与shell脚本里的$0、$1、$#等类似的特殊参数变量，用于接收及控制expect脚本传参 在expect中$argv表示参数数组，可以使用[lindex $argv n]接收expect脚本传递参，n从0开始，分别表示第一个[lindex $argv 0]参数、第二个[lindex $argv 1]参数、第三个[lindex $argv 2]参数。。。 定义及输出特殊参数变量 cat >special-var.exp 执行脚本 $ expect special-var.exp access.log 10.0.0.100 /tmp access.log 10.0.0.100 /tmp 文件是: access.log 主机IP是: 10.0.0.100 目录是: /tmp expect接收参数的方式和bash脚本的方式区别 类型 接收传参方式 bash $0...$n expect set 变量名 [lindex $argv 0...n] 除了基本的位置参数外，expect也支持其他的特殊参数，例如$argc表示传参的个数，$argv0表示脚本的名字 使用示例 编辑expect脚本 cat > special-other.exp 执行脚本 $ expect special-other.exp access.log 10.0.0.100 /tmp 文件是: access.log 主机IP是: 10.0.0.100 目录是: /tmp 传参个数是: 3 脚本名是: special-other.exp 五、expect程序中的if条件语句 语法 if关键字后面要有空格，else关键字前后都要有空格，{条件表达式}大括号里边靠近大括号处可以没有空格，将指令括起来的起始大括号\"{\"前要有空格 if {条件表达式} { 命令 } 或 if {条件表达式} { 命令 } else { 命令 } 使用示例 使用if语句判断脚本传参的个数，如果不符合则给予提示 cat > if.exp 执行脚本 未给脚本传参数 $ expect if.exp usage: expect if.exp file ip dir 给脚本传参 $ expect if.exp access.log 10.0.0.100 tmp 文件是: access.log 主机IP是: 10.0.0.100 目录是: tmp 六、expect中的关键字 expect中的特殊关键字用于匹配过程，代表某些特殊的含义或状态，一般只用于expect命令中而不能在expect命令外面单独使用 6.1 eof关键字 eof(end-of-line)关键字用于匹配结束符，即在expect脚本中的最后声明 示例 #!/usr/bin/expect spawn ssh root@10.0.0.100 uptime expect \"*password\" {send \"1\\n\"} #eof关键字用于结束expect脚本 expect eof 6.2 timeout关键字 timeout是expect中的一个控制时间的关键字变量，它是一个全局性的时间控制开关，可以通过为这个变量赋值来规定整个expect操作的时间，注意这个变量是服务于expect全局的，而不是某一条命令，即使命令没有任何错误，到了时间仍然会激活这个变量，此外，到时间后还会激活一个处理及提示信息开关 timeout超时功能使用示例 #timeout语法1 cat >timeout.exp timeout.exp 执行脚本 执行脚本后提示需要输入密码，这里等待5秒后如果没有输入密码则提示请求超时，并退出脚本 $ expect timeout.exp spawn ssh root@10.0.0.31 uptime root@10.0.0.31's password: request timeout 七、expect综合示例 批量分发ssh密钥示例 本地生成密钥对 ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa &>/dev/null 编辑expect脚本 cat >ssh.exp 分发单个主机示例 $ expect ssh.exp ~/.ssh/id_rsa.pub 10.0.0.31 spawn ssh-copy-id -i /root/.ssh/id_rsa.pub -p 22 root@10.0.0.31 /usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \"/root/.ssh/id_rsa.pub\" The authenticity of host '10.0.0.31 (10.0.0.31)' can't be established. ECDSA key fingerprint is SHA256:o9tdhhhgM3KPZytcz16k/Wt6gfEkDp1FCD3Q7VmLnoE. ECDSA key fingerprint is MD5:0e:a2:0c:a6:42:e4:95:b4:77:44:14:36:ba:11:2b:d8. Are you sure you want to continue connecting (yes/no)? yes /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys root@10.0.0.31's password: Number of key(s) added: 1 Now try logging into the machine, with: \"ssh -p '22' 'root@10.0.0.31'\" and check to make sure that only the key(s) you wanted were added. 分发多个主机示例 编辑shell脚本循环执行expect脚 cat > ssh.sh 执行脚本 $ sh ssh.sh spawn ssh-copy-id -i /root/.ssh/id_rsa.pub -p 22 root@10.0.0.31 /usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \"/root/.ssh/id_rsa.pub\" The authenticity of host '10.0.0.31 (10.0.0.31)' can't be established. ECDSA key fingerprint is SHA256:o9tdhhhgM3KPZytcz16k/Wt6gfEkDp1FCD3Q7VmLnoE. ECDSA key fingerprint is MD5:0e:a2:0c:a6:42:e4:95:b4:77:44:14:36:ba:11:2b:d8. Are you sure you want to continue connecting (yes/no)? yes /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys root@10.0.0.31's password: Number of key(s) added: 1 Now try logging into the machine, with: \"ssh -p '22' 'root@10.0.0.31'\" and check to make sure that only the key(s) you wanted were added. spawn ssh-copy-id -i /root/.ssh/id_rsa.pub -p 22 root@10.0.0.52 /usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \"/root/.ssh/id_rsa.pub\" The authenticity of host '10.0.0.52 (10.0.0.52)' can't be established. ECDSA key fingerprint is SHA256:o9tdhhhgM3KPZytcz16k/Wt6gfEkDp1FCD3Q7VmLnoE. ECDSA key fingerprint is MD5:0e:a2:0c:a6:42:e4:95:b4:77:44:14:36:ba:11:2b:d8. Are you sure you want to continue connecting (yes/no)? yes /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys root@10.0.0.52's password: Number of key(s) added: 1 Now try logging into the machine, with: \"ssh -p '22' 'root@10.0.0.52'\" and check to make sure that only the key(s) you wanted were added. 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/各种脚本/大神杰作/shell脚本 俄罗斯方块.html":{"url":"linux/shell/各种脚本/大神杰作/shell脚本 俄罗斯方块.html","title":"shell脚本 俄罗斯方块","keywords":"","body":"shell脚本 俄罗斯方块 #!/bin/bash #颜色定义 cRed=1 cGreen=2 cYellow=3 cBlue=4 cFuchsia=5 cCyan=6 cWhite=7 colorTable=($cRed $cGreen $cYellow $cBlue $cFuchsia $cCyan $cWhite) #位置和大小 iLeft=3 iTop=2 ((iTrayLeft = iLeft + 2)) ((iTrayTop = iTop + 1)) ((iTrayWidth = 10)) ((iTrayHeight = 15)) #颜色设置 cBorder=$cGreen cScore=$cFuchsia cScoreValue=$cCyan #控制信号 #改游戏使用两个进程，一个用于接收输入，一个用于游戏流程和显示界面; #当前者接收到上下左右等按键时，通过向后者发送signal的方式通知后者。 sigRotate=25 sigLeft=26 sigRight=27 sigDown=28 sigAllDown=29 sigExit=30 #七中不同的方块的定义 #通过旋转，每种方块的显示的样式可能有几种 box0=(0 0 0 1 1 0 1 1) box1=(0 2 1 2 2 2 3 2 1 0 1 1 1 2 1 3) box2=(0 0 0 1 1 1 1 2 0 1 1 0 1 1 2 0) box3=(0 1 0 2 1 0 1 1 0 0 1 0 1 1 2 1) box4=(0 1 0 2 1 1 2 1 1 0 1 1 1 2 2 2 0 1 1 1 2 0 2 1 0 0 1 0 1 1 1 2) box5=(0 1 1 1 2 1 2 2 1 0 1 1 1 2 2 0 0 0 0 1 1 1 2 1 0 2 1 0 1 1 1 2) box6=(0 1 1 1 1 2 2 1 1 0 1 1 1 2 2 1 0 1 1 0 1 1 2 1 0 1 1 0 1 1 1 2) #所有其中方块的定义都放到box变量中 box=(${box0[@]} ${box1[@]} ${box2[@]} ${box3[@]} ${box4[@]} ${box5[@]} ${box6[@]}) #各种方块旋转后可能的样式数目 countBox=(1 2 2 2 4 4 4) #各种方块再box数组中的偏移 offsetBox=(0 1 3 5 7 11 15) #每提高一个速度级需要积累的分数 iScoreEachLevel=50 #be greater than 7 #运行时数据 sig=0 #接收到的signal iScore=0 #总分 iLevel=0 #速度级 boxNew=() #新下落的方块的位置定义 cBoxNew=0 #新下落的方块的颜色 iBoxNewType=0 #新下落的方块的种类 iBoxNewRotate=0 #新下落的方块的旋转角度 boxCur=() #当前方块的位置定义 cBoxCur=0 #当前方块的颜色 iBoxCurType=0 #当前方块的种类 iBoxCurRotate=0 #当前方块的旋转角度 boxCurX=-1 #当前方块的x坐标位置 boxCurY=-1 #当前方块的y坐标位置 iMap=() #背景方块图表 #初始化所有背景方块为-1, 表示没有方块 for ((i = 0; i elif [[ $key == \"B\" ]]; then sig=$sigDown # elif [[ $key == \"D\" ]]; then sig=$sigLeft # elif [[ $key == \"C\" ]]; then sig=$sigRight # fi elif [[ $key == \"W\" || $key == \"w\" ]]; then sig=$sigRotate #W, w elif [[ $key == \"S\" || $key == \"s\" ]]; then sig=$sigDown #S, s elif [[ $key == \"A\" || $key == \"a\" ]]; then sig=$sigLeft #A, a elif [[ $key == \"D\" || $key == \"d\" ]]; then sig=$sigRight #D, d elif [[ \"[$key]\" == \"[]\" ]]; then sig=$sigAllDown #空格键 elif [[ $key == \"Q\" || $key == \"q\" ]] #Q, q then MyExit fi if [[ $sig != 0 ]] then #向另一进程发送消息 kill -$sig $pidDisplayer fi done } #退出前的恢复 function MyExitNoSub() { local y #恢复终端属性 stty $sTTY ((y = iTop + iTrayHeight + 4)) #显示光标 echo -e \"\\033[?25h\\033[${y};0H\" exit } function MyExit() { #通知显示进程需要退出 kill -$sigExit $pidDisplayer MyExitNoSub } #处理显示和游戏流程的主函数 function RunAsDisplayer() { local sigThis InitDraw #挂载各种信号的处理函数 trap \"sig=$sigRotate;\" $sigRotate trap \"sig=$sigLeft;\" $sigLeft trap \"sig=$sigRight;\" $sigRight trap \"sig=$sigDown;\" $sigDown trap \"sig=$sigAllDown;\" $sigAllDown trap \"ShowExit;\" $sigExit while (( 1 )) do #根据当前的速度级iLevel不同，设定相应的循环的次数 for ((i = 0; i = iTrayHeight || x = iTrayWidth)) then #撞到墙壁了 return 1 fi if ((${iMap[y * iTrayWidth + x]} != -1 )) then #撞到其他已经存在的方块了 return 1 fi done return 0; } #将当前移动中的方块放到背景方块中去, #并计算新的分数和速度级。(即一次方块落到底部) function Box2Map() { local j i x y xp yp line #将当前移动中的方块放到背景方块中去 for ((j = 0; j = j; i--)) do if ((${iMap[$i]} == -1)); then break; fi done if ((i >= j)); then continue; fi ((line++)) for ((i = j - 1; i >= 0; i--)) do ((x = i + iTrayWidth)) iMap[$x]=${iMap[$i]} done for ((i = 0; i $k )); then iDown=$k; fi done s=`DrawCurBox 0` #将旧的方块抹去 ((boxCurY += iDown)) s=$s`DrawCurBox 1` #显示新的下落后的方块 echo -ne $s Box2Map #将当前移动中的方块贴到背景方块中 RandomBox #产生新的方块 } #旋转方块 function BoxRotate() { local iCount iTestRotate boxTest j i s iCount=${countBox[$iBoxCurType]} #当前的方块经旋转可以产生的样式的数目 #计算旋转后的新的样式 ((iTestRotate = iBoxCurRotate + 1)) if ((iTestRotate >= iCount)) then ((iTestRotate = 0)) fi #更新到新的样式, 保存老的样式(但不显示) for ((j = 0, i = (${offsetBox[$iBoxCurType]} + $iTestRotate) * 8; j i)); then i=${boxCur[$j]}; fi if ((${boxCur[$j]} 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/各种脚本/实际应用脚本/shell脚本判断系统类型.html":{"url":"linux/shell/各种脚本/实际应用脚本/shell脚本判断系统类型.html","title":"shell脚本判断系统类型","keywords":"","body":"[toc] shell脚本判断系统类型 一、场景说明 在生产环境中服务器的系统类型可能会有多种，例如有CentOS、Ubuntu、Debian等等，在做批量操作(如shell脚本、ansible等)的时候就需要对服务器等系统类型做判断，然后执行不同的命令，例如执行安装命令，CentOS系统执行 yum 命令，Ubuntu、Debian执行 apt 命令 二、判断方法 这里以常用的Ubuntu和CentOS为例 2.1 CentOS 方法一 查看文件 /etc/redhat-release $ cat /etc/redhat-release CentOS Linux release 7.8.2003 (Core) /etc/centos-release $ cat /etc/centos-release CentOS Linux release 7.8.2003 (Core) /etc/issue # CentOS6中会显示具体版本，CentOS7中显示如下 $ cat /etc/issue \\S Kernel \\r on an \\m /etc/os-release $ cat /etc/os-release NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" 方法二 通过命令 命令 lsb_release，通过 yum -y install redhat-lsb-core 安装 $ lsb_release -a LSB Version: :core-4.1-amd64:core-4.1-noarch Distributor ID: CentOS Description: CentOS Linux release 7.8.2003 (Core) Release: 7.8.2003 Codename: Core 2.2 Ubuntu 方法一 查看文件 $ cat /etc/issue Ubuntu 16.04.1 LTS \\n \\l 方法二 通过命令 命令 lsb_release ，通过 apt -y install lsb-core 安装 $ lsb_release LSB Version: core-9.20160110ubuntu0.2-amd64:core-9.20160110ubuntu0.2-noarch:security-9.20160110ubuntu0.2-amd64:security-9.20160110ubuntu0.2-noarch 三、最佳判断方法 在生产我们常用的方法就是根据 /etc/os-release 返回的内容进行判断 openvpn一键安装脚本 中有写对系统类型的判断 # Detect OS # $os_version variables aren't always in use, but are kept here for convenience if grep -qs \"ubuntu\" /etc/os-release; then os=\"ubuntu\" os_version=$(grep 'VERSION_ID' /etc/os-release | cut -d '\"' -f 2 | tr -d '.') group_name=\"nogroup\" elif [[ -e /etc/debian_version ]]; then os=\"debian\" os_version=$(grep -oE '[0-9]+' /etc/debian_version | head -1) group_name=\"nogroup\" elif [[ -e /etc/centos-release ]]; then os=\"centos\" os_version=$(grep -oE '[0-9]+' /etc/centos-release | head -1) group_name=\"nobody\" elif [[ -e /etc/fedora-release ]]; then os=\"fedora\" os_version=$(grep -oE '[0-9]+' /etc/fedora-release | head -1) group_name=\"nobody\" else echo \"This installer seems to be running on an unsupported distribution. Supported distributions are Ubuntu, Debian, CentOS, and Fedora.\" exit fi 判断示例 $ grep -w NAME /etc/os-release |awk -F'[=\"]+' '{print $2}' CentOS Linux $ grep -w NAME /etc/os-release |awk -F'[=\"]+' '{print $2}' Ubuntu 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/各种脚本/实际应用脚本/shell脚本实现企业微信告警.html":{"url":"linux/shell/各种脚本/实际应用脚本/shell脚本实现企业微信告警.html","title":"shell脚本实现企业微信告警","keywords":"","body":"shell脚本实现企业微信告警 一、背景需求说明 背景说明 公司官网由4个服务模块组成，分别监听4个不同的端口，每个服务都是随机启动在5台机器中的某一台上，虽然nginx中配置了upstream但实际上还是属于单点运行(不明白为何要这么做。。。) 需求说明 5台机器分别为dn1-5，4个服务分别随机运行在其中某一台机器上，需要对端口进行存活监控 二、外部脚本 2.1 调用企业微信机器人 #!/bin/bash wx(){ # 将下面的webhook地址替换成你的企业微信机器人地址，$1为告警消息 $2为@人的手机号 $2可以为空 cat > $0.msg 二、初级第一版 能发送告警但不能发送恢复 监控脚本 #!/bin/bash # # nc命令检测端口，存在返回0，不存在返回1 dn1=`nc -z dn1.com 8888 ; echo $?` dn2=`nc -z dn2.com 8888 ; echo $?` dn3=`nc -z dn3.com 8888 ; echo $?` dn4=`nc -z dn4.com 8888 ; echo $?` dn5=`nc -z dn5.com 8888 ; echo $?` # 加载外部脚本，内容为调用企业微信机器人 . ./qiyewx.sh if [[ $dn1 -eq $dn2 && $dn2 -eq $dn3 && $dn3 -eq $dn4 && $dn4 -eq $dn5 && $dn5 -eq 1 ]];then # 调用户外部脚本，实现企业微信告警 wx \"集群数据接口8888异常\" fi 三、初级第二版 能发送告警也能发送恢复 #!/bin/bash # # nc命令检测端口，存在返回0，不存在返回1 dn1=`nc -z dn1.com 8888 ; echo $?` dn2=`nc -z dn2.com 8888 ; echo $?` dn3=`nc -z dn3.com 8888 ; echo $?` dn4=`nc -z dn4.com 8888 ; echo $?` dn5=`nc -z dn5.com 8888 ; echo $?` ALARM_FILE=alarm_txt # 加载外部脚本，内容为调用企业微信机器人 . ./qiyewx.sh # 如果5个返回值都相等并且都为1，则说明服务挂了 if [[ $dn1 -eq $dn2 && $dn2 -eq $dn3 && $dn3 -eq $dn4 && $dn4 -eq $dn5 && $dn5 -eq 1 ]];then wx \"集群数据接口8888异常\" # 当服务挂掉的时候向一个文件中写入一行内容 echo '集群数据接口18891异常' > $ALARM_FILE # 判断文件内容行数是否为1，如果为1，则说明是服务挂掉过，如果为空则说明告警恢复已发过了 elif [[ `wc -l $ALARM_FILE|awk '{print $1}'` -eq 1 ]];then wx \"集群数据接口8888异常已恢复\" # 当服务恢复的时候告警并清空文件内容，这样就只发送一次告警恢复 > $ALARM_FILE fi 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/各种脚本/优化脚本/系统优化脚本.html":{"url":"linux/shell/各种脚本/优化脚本/系统优化脚本.html","title":"系统优化脚本","keywords":"","body":"centos7系统优化脚本 #!/usr/bin/env bash # # 修改系统yum源为aliyun并添加epel源 mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak [ ! -e /etc/yum.repos.d/CentOS-Base.repo ] && curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo && sed -i -e '/mirrors.cloud.aliyuncs.com/d' -e '/mirrors.aliyuncs.com/d' /etc/yum.repos.d/CentOS-Base.repo [ ! -e /etc/yum.repos.d/epel.repo ] && curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo yum makecache yum -y install tar wget net-tools git vim tree lrzsz htop iftop iotop psmisc python36 python3-devel zlib zlib-devel gcc gcc-c++ conntrack-tools jq socat bash-completion telnet nload strace tcpdump lsof sysstat telnet # 关闭防火墙、selinux、NetworkManager systemctl disable firewalld NetworkManager sed -i '7s/enforcing/disabled/' /etc/selinux/config # 同步时间计划任务 sed -i '/*\\/10 \\* \\* \\* \\* \\/usr\\/sbin\\/ntpdate ntp2\\.aliyun\\.com &>\\/dev\\/null/d' /var/spool/cron/root echo \"*/10 * * * * /usr/sbin/ntpdate ntp2.aliyun.com &>/dev/null\" >>/var/spool/cron/root # 历史命令显示时间 sed -i '/HISTFILESIZE=2000/d' /etc/bashrc sed -i '/HISTSIZE=2000/d' /etc/bashrc sed -i '/HISTTIMEFORMAT=\"%Y-%m-%d %H:%M:%S \"/d' /etc/bashrc sed -i '/export HISTTIMEFORMAT/d' /etc/bashrc cat >>/etc/bashrc>/etc/security/limits.conf~/.pip/pip.conf> ～/.bashrc centos8系统优化脚本 #!/usr/bin/env bash # # 修改系统yum源为aliyun并添加epel源 mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak [ ! -e /etc/yum.repos.d/CentOS-Base.repo ] && curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-8.repo dnf clean all dnf makecache yum -y install https://mirrors.aliyun.com/epel/epel-release-latest-8.noarch.rpm sed -i 's|^#baseurl=https://download.fedoraproject.org/pub|baseurl=https://mirrors.aliyun.com|' /etc/yum.repos.d/epel* sed -i 's|^metalink|#metalink|' /etc/yum.repos.d/epel* dnf -y install tar wget net-tools git vim tree lrzsz htop iftop iotop psmisc python36 python3-devel zlib zlib-devel gcc gcc-c++ conntrack-tools jq socat bash-completion telnet nload strace tcpdump lsof sysstat telnet # 关闭防火墙、selinux、NetworkManager systemctl disable firewalld NetworkManager sed -i '7s/enforcing/disabled/' /etc/selinux/config # 同步时间计划任务 rpm -ivh http://mirrors.wlnmp.com/centos/wlnmp-release-centos.noarch.rpm && dnf -y install wntp sed -i '/*\\/10 \\* \\* \\* \\* \\/usr\\/sbin\\/ntpdate ntp2\\.aliyun\\.com &>\\/dev\\/null/d' /var/spool/cron/root echo \"*/10 * * * * /usr/local/bin/ntpdate ntp2.aliyun.com &>/dev/null\" >>/var/spool/cron/root # 历史命令显示时间 sed -i '/HISTFILESIZE=2000/d' /etc/bashrc sed -i '/HISTSIZE=2000/d' /etc/bashrc sed -i '/HISTTIMEFORMAT=\"%Y-%m-%d %H:%M:%S \"/d' /etc/bashrc sed -i '/export HISTTIMEFORMAT/d' /etc/bashrc cat >>/etc/bashrc>/etc/security/limits.conf~/.pip/pip.conf> ～/.bashrc 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"linux/shell/各种脚本/优化脚本/登录提示脚本.html":{"url":"linux/shell/各种脚本/优化脚本/登录提示脚本.html","title":"登录提示脚本","keywords":"","body":"登录提示脚本 登陆提示效果 Last login: Sun May 16 22:37:30 2021 from 10.0.0.2 ================================================================= Private IP: 10.0.0.222 CPU Cores: 2C Memory: 986M Disk: 47G CPU Model Name: Intel(R) Core(TM) i7-8750H CPU @ 2.20GHz OS: CentOS Linux release 7.9.2009 (Core) Hostname: localhost.localdomain Kernel: 3.10.0-1160.el7.x86_64 Uptime: 22:44:50 up 51 min, 3 users, load average: 0.00, 0.01, 0.01 ================================================================= 脚本内容 cat > /etc/profile.d/motd.sh 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/1.python基础一 变量、常量、注释.html":{"url":"python/python基础/1.python基础一 变量、常量、注释.html","title":"python基础一 变量、常量、注释","keywords":"","body":"[toc] python基础一 变量、常量、注释 1.变量 1.1 定义 ​ 将程序中运行的中间值，临时存储起来，以便再次使用 1.2 命名规范 数字、字母、下划线组成 变量名要具有可描述性 不能以数字开头 禁止使用python中的关键字 ['False', 'None', 'True', 'and', 'as', 'assert', 'break', 'class', 'continue', 'def', 'del', 'elif', 'else', 'except', 'finally', 'for', 'from', 'global', 'if', 'import', 'in', 'is', 'lambda', 'nonlocal', 'not', 'or', 'pass', 'raise', 'return', 'try', 'while', 'with', 'yield'] 驼峰命名法 单词首字母大写 AisMan 下划线命名法 单词之间以下划线分割 a_is_man 2.常量 2.1 定义 变量名大写的就是常量 2.2 说明 python中本没有常量，为了迎合别的语言 python中的常量可以修改，但是不建议修改 2.3 用途 用于配置文件中 3.注释 3.1 定义 给一些晦涩难懂的代码进行标注或者解释 3.2 分类 单行注释 一个#号 多行注释 3个单引号或者3个双引号(推荐) 4.用户输入 4.1 语法 input(提示语句) 4.2 说明 ⚠️python中input输入的内容都是字符串 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/2.python基础二 流程控制语句.html":{"url":"python/python基础/2.python基础二 流程控制语句.html","title":"python基础二 流程控制语句","keywords":"","body":"[toc] python基础二 流程控制语句 1. if流程控制语句 1.1 语法 1.1.1 单分支 if 条件: 执行命令 1.1.2 双分支 if 条件: 执行命令 else: 执行命令 1.1.3 多分支 多选一或零 if 条件: 执行命令 elif 条件: 执行命令 elif 条件: 执行命令 。。。 多选一 if 条件: 执行命令 elif 条件: 执行命令 else 条件: 执行命令 2. while循环语句 2.1 语法 #无限循环 while True: 循环体 #有限循环 while 条件: 循环体 2.2 关键字 break 终止当前循环 contiune 跳出本次循环，继续下次循环 ⚠️break和continue下方的代码不会执行 3. for循环语句 3.1 语法 for 变量名 in 条件: 循环体 3.2 for循环删除的坑 #for循环删除列表中的内容，同样循环删除列表也适用于字典 //错误演示 lst = [1,2,3,4,5] for i in lst: lst.remove(i) print(lst) [2, 4] //结果错误，因为for循环删除一个元素后，后边的元素会往前补，当i是0的时候，删除1，此时2补上，2的下标变为了0，下一次for循环，会删除下标为1的，但是此时原本下标为1的元素2已经补前了，所以会删除3，依次类推，会隔空删除元素 //方法1 lst = [1,2,3,4,5] for i in range(len(lst)): lst.pop(0) #列表中的元素删除一个，后边的会继续补上，所以只删除第一个 print(lst) [] //方法2 lst = [1,2,3,4,5] lst1 = lst.copy() #浅拷贝是两个变量各自占不同的内存空间，但是值还是共用 for i in lst1: #因此循环lst1删除值，lst也删除 lst.remove(i) print(lst) [] 3.3 可迭代对象与不可迭代对象说明 #可迭代对象 str -- 字符串 list -- 列表 tuple -- 元祖 set -- 集合 dict -- 字典 range -- 范围 #不可迭代对象 int -- 数字 bool -- 布尔值 3.4 代码示例 #代码示例1，for循环会便利可迭代对象中的每一个字符，然后依次执行print，因为可迭代对象为4个字符，因此打印4行123 for i in \"hehe\" print (123) 打印结果 123 123 123 123 #代码示例2 for i in \"abced\" print (i) 打印结果 a b c d e 3.5 面试题 ⚠️ #for循环面试题1 for i in \"abcde\": pass print (i) 打印结果 e ⚠️for循环中遇到pass，只会打印可迭代对象中的最后一个字符 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/3.python基础三 字符串详解.html":{"url":"python/python基础/3.python基础三 字符串详解.html","title":"python基础三 字符串详解","keywords":"","body":"[toc] python基础三 字符串 1. 索引(下标) 1.1 定义 方便查找字符串 1.2 格式说明 变量名[下标] 1.3 代码说明 name = \"abcd\" 计算机从0开始数数 0123 #从左向右 -1-2-3-4 #从右向左 #正向取值 print (name[2]) c #反向取值 print (name[-3]) b 2. 切片 2.1 定义 截取从一个位置到另一个位置 2.2 格式说明 变量名[起始位置:终止位置] ⚠️顾头不顾尾，就是python中的切片从起始位置开始截取，到终止位置结束，不包含终止位置 2.3 代码说明 name = \"hehe_haha_xixi\" #正向打印 要打印hehe print (name[0:4]) #0表示起始位置，4表示结束位置，但是不包含结束位置 #反向 要打印xixi print (name[-4:]) #默认到结束位置 #正向全部打印 print (name[:]) hehe_haha_xixi #反向全部打印 需要用到步长！！！ ⚠️默认从头开始，到结尾 3. 步长 3.1 定义 配合切片使用，表示切片时的规则，例如步长为2，切割完第一个位置后开始截取第3个位置 3.2 格式说明 变量名[切片起始位置:切片终止位置:步长] name[1:5:2] #name为变量名，1表示从字符下标1开始，下标5结束，步长为2 3.3 代码说明 #例如要截取ace name = \"abcde\" print (name[0::2]) ace #例如要截取hgfed name = \"abcdefghi\" print (name[-2:2:-1]) hgfed ⚠️步长默认为1 ⚠️正向取结尾要加1，反向取结尾要减1 #例如要截取bcd name = \"abcde\" #正向取bcd，d的下标为3，正向取，要加1，因此为4 name = \"abcde\" print (name[1:4]) #反向取bcd即dcb，b的下标为1，反向取，要减1，因此为0 name = \"abcde\" print (name[-2:0:-1]) ⚠️⚠️⚠️索引超出最大范围会报错 ⚠️⚠️⚠️切片超出最大范围不会报错 4. 字符串的方法 upper #全部大写 lower #全部小写 startswith #以什么开头 支持切片 endswith #以什么结尾 支持切片 count #统计 strip #去除头尾两端的空格,换行符,制表符,还可指定去除内容 split #分割, 默认以空格,换行符,制表符进行分割,可以指定分割内容, 返回是列表 replace #替换 参数1(旧值),参数2(新值),参数3(次数) 默认全换 # is系列 str.isalnum #判断数字,中文,字母 str.isalpha #判断中文,字母 str.isdigit #判断阿拉伯数字 str.isdecimal #判断十进制 4.1 upper 说明 ​ 全部大写 代码示例 name = \"abc\" print (name.upper()) ABC 4.2 lower 说明 ​ 全部小写 代码示例 name = \"ABC\" print (name.lower()) abc 4.3 startswith 说明 ​ 以。。。开头，支持切片，返回布尔值 代码示例 //无切片示例 name = \"abcdefg\" print (name.startswith(\"a\")) True print (name.startswith(\"b\")) False //有切片示例1 name = \"abcdefg\" print (name[1:3]) bc print (name.startswith(\"a\",1,3)) # 切片结果为bc，不是以a开头，因此结果为False False //有切片示例2 name = \"abcdefg\" print (name[-1:3:-1]) gfe //反向取值无法正确匹配 print (name.startswith(\"g\",-1,3)) False //startswith最多3个参数，第一个参数：匹配内容，第二、三个参数：切片范围 print (name.startswith(\"g\",-1,3,-1)) Traceback (most recent call last): File \"\", line 1, in TypeError: startswith() takes at most 3 arguments (4 given) 4.4 endswith 说明 ​ 以。。。结尾，支持切片，返回布尔值 代码示例 //无切片示例 name = \"abcdefg\" print (name.endswith(\"g\")) True print (name.endswith(\"f\")) False //有切片示例1 name = \"abcdefg\" print (name[1:3]) bc print (name.endswith(\"a\",1,3)) # 切片结果为bc,不是以a结尾，因此结果为False False print (name.endswith(\"c\",1,3)) # 切片结果为bc,以c结尾,因此结果为True True //有切片示例2 name = \"abcdefg\" print (name[-1:3:-1]) gfe //反向取值无法正确匹配 print (name.endswith(\"e\",-1,3)) False 4.5 strip 说明 ​ 去除头尾两端的空格,换行符,制表符,还可指定去除内容 代码示例 //去除头尾两端的空格、换行符、制表符 name = \" ab c\\td a \" print (name.strip()) ab c d a //strip只会去除头尾两端的空格、换行符、制表符，中间的空格、换行符、制表符不会去除 //指定去除内容 name = \"ab c\\td a\" print (name.strip(\"a\")) b c d //指定去除的内容\"a\",strip只会去除开头和结尾的a 4.6 split 说明 ​ 分割, 默认以空格,换行符,制表符进行分割,可以指定分割内容, 返回是列表 代码示例 //默认以空格、换行符、制表符进行分割，返回列表 name = \"hehe haha\" //中间的空格会销毁 print (name.split()) ['hehe', 'haha'] //指定分割内容，返回列表 name = \"hehe:haha\" print (name.split(\":\")) ['hehe', 'haha'] print (name.split(\"h\")) ['', 'e', 'e:', 'a', 'a'] 4.7 replace 说明 ​ 替换 参数1(旧值),参数2(新值),参数3(次数) 默认全换 代码示例 name = \"hehe hehe hehe\" //默认全部替换示例，替换hehe为haha print (name.replace(\"e\",\"a\")) haha haha haha //只替换第一个hehe为haha print (name.replace(\"e\",\"a\",2)) haha hehe hehe 4.8 count 说明 ​ 计算字符出现次数 代码示例 //统计变量name中a出现的次数 name = \"abcdeabcde\" print (name.count(\"a\")) 2 4.9 str.isalnum 说明 ​ 判断是否只包含数字,中文,字母，返回布尔值 代码示例 //只包含数字、中文、字母，返回结果True name = \"123呵呵haha\" print (name.isalnum()) True //包含数字、中文、字母，同时包含特殊符号*，返回结果False name = \"123呵呵haha*\" print (name.isalnum()) False 4.10 str.isalpha 说明 ​ 判断中文,字母,返回布尔值 代码示例 //只包含中文、字母，返回结果为True name = \"呵呵hehe\" print (name.isalpha()) True //包含中文、字母，同时包含数字，返回结果为False name = \"呵呵hehe123\" print (name.isalpha()) False 4.11 str.isdigit 说明 ​ 判断阿拉伯数字,返回布尔值 代码示例 //isdigit有bug，圆圈5也算作是阿拉伯数字，因此用isdecimal做判断更好 name = \"1234⑤\" print (name.isdigit()) True name = \"12345\" print (name.isdigit()) True //赋值方式错误 name = 12345 print (name.isdigit()) Traceback (most recent call last): File \"\", line 1, in AttributeError: 'int' object has no attribute 'isdigit' 4.12 str.isdecimal 说明 ​ 判断十进制，返回布尔值 代码示例 //判断十进制数字 name = \"10\" print (name.isdecimal()) True //用isdecimal判断圆圈数字更准确 name = \"1234⑤\" print (name.isdecimal()) False 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/4.python基础四 格式化.html":{"url":"python/python基础/4.python基础四 格式化.html","title":"python基础四 格式化","keywords":"","body":"[toc] python基础四 格式化 1. %占位填位表示法 1.1 说明 占位 %s 字符串占位 ⚠️字符串可以填充数字，后边的补位可以是整型 %d 与%i相同，整型占位 ⚠️后边的补位必须为整型 补位 变量名%(打印的内容) ⚠️补位必须与占位位数相同 1.2 代码示例 #msg中的变量名后的%s表示给对应的变量占位，而在最后的print中%表示取位 name = input(\"name\") age = input(\"age\") sex = input(\"sex\") hobby = input(\"hobby\") msg = \"\"\" ------info------ name:%s age:%s sex:%s hobby:%s -------end------ \"\"\" print(msg%(name,int(age),sex,hobby)) 2. f+{}表示法 python3.6以上支持 2.1 说明 用f表示格式化 msg =f\"myname is {input('name')}\" 2.2 代码示例 msg = f\"my name is {input('请输入姓名：')} I'm {input('请输入年龄: ')} years old\" print (msg) 3. 条件格式化 3.1 按照位置 s = \"a{}b\" s1 = s.format(\"你好\") print (s1) a你好b 3.2 按照索引 s = \"a{1}b\" s1 = s.format(\"你好\",\"呵呵\") print (s1) a呵呵b 3.3 按照关键字 s = \"a{A}b\" s = s.format(A=\"你好\") print (s) a你好b 4.其他格式化 4.1 格式化函数使用示例 def func(a,b): return a + b msg = f\"运行结果:{func(1,2)}\" print(msg) 运行结果:3 4.2 格式化列表、字典使用示例 #f-string支持列表 lst = [1,2,3,4,5,6] msg = f\"运行结果:{lst[0:3]}\" print(msg) [1,2,3] #f-string支持字典 dic = {\"key\":1,\"key1\":22} msg = f\"运行结果:{dic['key1']}\" print(msg) 运行结果:22 4.3 三木运算符使用示例 a = 100 b = 20 msg = f\"{a if a 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/5.python基础五 运算符.html":{"url":"python/python基础/5.python基础五 运算符.html","title":"python基础五 运算符","keywords":"","body":"[toc] python基础五 运算符 1. 比较运算符 > 大于 = 大于等于 2. 赋值运算符 = 等于 += 加等于 -= 减等于 *= 乘等于 /= 除等于 //= 整除等于 **= 幂等于 %= 模等于 3. 逻辑运算符 3.1 说明 与 and 或 or 非 not 3.2 优先级和查找顺序 #优先级 () > not > and > or #查找顺序 从左往右 3.3 True和False进行逻辑运算 and: 一真一假,就是假 同真为真,同假为假 or: 一真一假,就是真 同真为真,同假为假 3.4 逻辑运算符与数字运算时的规则 逻辑运算符与数字运算时的规则 # and数字进行逻辑运算时: # 数字不为 0 时和不为 False # and运算选择and后边的内容 # and运算都为假时选择and前的内容 # and 运算一真一假选择假 示例 ⚠️ 任何时候，0都可以看作是False ⚠️ 任何时候，除0外的其余数字都可以看作是True print(1 and 3) 结果：3 原因：数字不为0的时候选择and后边的内容 print(3 and 1) 结果：1 原因：数字不为0的时候选择and后边的内容 print(0 and 8) 结果：0 原因：and运算一真一假选择假 print(8 and 0) 结果：0 原因：and运算一真一假选择假 print(False and 5) 结果：False 原因：为False选择False print(5 and False) 结果：False 原因：and运算符一真一假选择假 print(0 and False) 结果：0 原因：将0看作False，and运算符都为假选择and前边的内容 print(False and 0) 结果：False 原因：and运算符都为假时选择and前的内容 # or 数字进行逻辑运算时: # 数字不为 0 时和不为 False # or运算选择or前边的内容 # or运算都为假时选择or后边的内容 # or 运算一真一假选择真 示例 print(1 or 3) 结果：1 原因：数字不为0的时候选择or前边的内容 print(3 or 1) 结果：3 原因：数字不为0的时候选择or前边的内容 print(0 or 8) 结果：8 原因：or运算一真一假选择真，0为假，8为真，选择8 print(8 or 0) 结果：8 原因：or运算一真一假选择真，0为假，8为真，选择8 print(False or 5) 结果：5 原因：or运算一真一假选择真，0位假，5为真，选择5 print(5 or False) 结果：5 原因：or运算一真一假选择真，0位假，5为真，选择5 print(0 or False) 结果：False 原因：0为假，or运算都为假时选择or后边的内容 print(False or 0) 结果：0 原因：0为假，or运算都为假时选择or后边的内容 4. 成员运算符 4.1 说明 in 在。。。中 not in 不在。。。中 4.2 代码示例 #以下代码，如果用户输入的内容包含hehe即为真 name = \"hehe\" msg = input(\"请输入字符\") if name in msg: print(111) else: print(222) 5. 算术运算符 + 加 - 减 * 乘 / 除 // 整除|地板除（向下取整） 例如 5//3 = 1 ** 幂 % 取余 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/6.python基础六 编码.html":{"url":"python/python基础/6.python基础六 编码.html","title":"python基础六 编码","keywords":"","body":"[toc] python基础六 编码 1.编码集 ascii 不支持中文，每一个字符占8位，1个字节 gbk 国标，支持中文，一个字符占16位，2个字节 unicode 中文、英文都是4个字节 utf-8 英文1个字节、欧洲2个字节、亚洲3个字节 2.二次编码 2.1 编码作用 1.存储 -- 文件操作 2.传输 -- 网编 2.2 编码 s = \"呵呵\" s1 = s.encode(\"utf8\") print (s1) b'\\xe5\\x91\\xb5\\xe5\\x91\\xb5' 2.3 解码 s2 = s1.decode(\"utf8\") print (s2) 呵呵 2.4 注意点 ⚠️ 1.python3内存中使用的就是unicode（万国码） 2.硬盘中存储时的编码方式 gbk utf-8 3.用什么编码，就用什么解码 3.单位转换 bit = 1byte 1024byte = 1KB 1024KB = 1MB 1024MB = 1GB 1024GB = 1TB 1024TB = 1PB 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/7.1python基础七 基础数据类型-整型、布尔值.html":{"url":"python/python基础/7.1python基础七 基础数据类型-整型、布尔值.html","title":"整型、布尔值","keywords":"","body":"[toc] python基础七 基础数据类型-整型、布尔值 1.整型 1.1 定义 用于计算和比较 1.2 进制转换 1.2.1 10进制 --> 2进制 算法 整除2，获取余数，从下往上读取 #示例 15转换为二进制 15整除2 商 余数 7 1 3 1 1 1 0 1 15转换为二进制 --> 1111 转换关键字 bin() #十进制转换二进制 示例 print (bin(15)) 0b1111 1.2.2 2进制 --> 10进制 算法 从右向左，计算机从0开始 #示例 1010转换为十进制 1010 =0*0**2 + 1*2**1 + 0*2**2 + 1*2**3 =0 + 2 + 0 + 8 =10 转换关键字 int() #二进制转换十进制 示例 print (int(\"1010\"),2) 2表示括号中的数字是二进制 10 1.3 最大位数 bit_length 求十进制最大位数 //示例1 a = 10 print (a.bit_length()) 4 #说明 10转换为2进制为1010 -->4位 //示例2 a = 30 print (a.bit_length()) 5 #说明 30转换为2进制为11110 -->5位 2.布尔值 2.1 作用 判断对错 2.2 说明 只有python的True和False的首字母是大写，其余语言都是小写 2.3 代码示例 print(1>2) print(10>5) False True 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/7.2python基础七 基础数据类型-字符串.html":{"url":"python/python基础/7.2python基础七 基础数据类型-字符串.html","title":"字符串","keywords":"","body":"[toc] python基础七 基础数据类型-字符串 1.字符串 1. 字符串的方法 upper #全部大写 lower #全部小写 startswith #以什么开头 支持切片 endswith #以什么结尾 支持切片 count #统计 strip #去除头尾两端的空格,换行符,制表符,还可指定去除内容 split #分割, 默认以空格,换行符,制表符进行分割,可以指定分割内容, 返回是列表 replace #替换 参数1(旧值),参数2(新值),参数3(次数) 默认全换 capitalize #首字母大写 title #每个单词首字母大写 index #根据元素查找索引 查找不到报错 find #根据元素查找索引 查找不到返回-1 join #将列表转换为字符串 split #将字符串转换为列表 center #居中 format #格式化 swapcase #大小写转换 # is系列 str.isalnum #判断数字,中文,字母 str.isalpha #判断中文,字母 str.isdigit #判断阿拉伯数字 str.isdecimal #判断十进制 1.1 upper 全部大写 name = \"abc\" print (name.upper()) ABC 1.2 lower 全部小写 name = \"ABC\" print (name.lower()) abc 1.3 startswith 以。。。开头，支持切片，返回布尔值 //无切片示例 name = \"abcdefg\" print (name.startswith(\"a\")) True print (name.startswith(\"b\")) False //有切片示例1 name = \"abcdefg\" print (name[1:3]) bc print (name.startswith(\"a\",1,3)) # 切片结果为bc，不是以a开头，因此结果为False False //有切片示例2 name = \"abcdefg\" print (name[-1:3:-1]) gfe //反向取值无法正确匹配 print (name.startswith(\"g\",-1,3)) False //startswith最多3个参数，第一个参数：匹配内容，第二、三个参数：切片范围 print (name.startswith(\"g\",-1,3,-1)) Traceback (most recent call last): File \"\", line 1, in TypeError: startswith() takes at most 3 arguments (1 given) 1.4 endswith 以。。。结尾，支持切片，返回布尔值 //无切片示例 name = \"abcdefg\" print (name.endswith(\"g\")) True print (name.endswith(\"f\")) False //有切片示例1 name = \"abcdefg\" print (name[1:3]) bc print (name.endswith(\"a\",1,3)) # 切片结果为bc,不是以a结尾，因此结果为False False print (name.endswith(\"c\",1,3)) # 切片结果为bc,以c结尾,因此结果为True True //有切片示例2 name = \"abcdefg\" print (name[-1:3:-1]) gfe //反向取值无法正确匹配 print (name.endswith(\"e\",-1,3)) False 1.5 strip 去除头尾两端的空格,换行符,制表符,还可指定去除内容 //去除头尾两端的空格、换行符、制表符 name = \" ab c\\td a \" print (name.strip()) ab c d a //strip只会去除头尾两端的空格、换行符、制表符，中间的空格、换行符、制表符不会去除 //指定去除内容 name = \"ab c\\td a\" print (name.strip(\"a\")) b c d //指定去除的内容\"a\",strip只会去除开头和结尾的a 1.6 split 作用1:分割, 默认以空格,换行符,制表符进行分割,可以指定分割内容, 返回是列表 作用2:将字符串转换为列表 1.分割 //默认以空格、换行符、制表符进行分割，返回列表 name = \"hehe haha\" //中间的空格会销毁 print (name.split()) ['hehe', 'haha'] //指定分割内容，返回列表 name = \"hehe:haha\" print (name.split(\":\")) ['hehe', 'haha'] print (name.split(\"h\")) ['', 'e', 'e:', 'a', 'a'] 2.将字符串转换为列表 s = \"hehe\" print (s.split()) ['hehe'] print (type(s.split())) 1.7 replace 替换 参数1(旧值),参数2(新值),参数3(次数) 默认全换 name = \"hehe hehe hehe\" //默认全部替换示例，替换hehe为haha print (name.replace(\"e\",\"a\")) haha haha haha //只替换第一个hehe为haha print (name.replace(\"e\",\"a\",2)) haha hehe hehe 1.8 count 计算字符出现次数 //统计变量name中a出现的次数 name = \"abcdeabcde\" print (name.count(\"a\")) 2 1.9 capitalize 首字母大写 s = \"hehe\" print (s.capitalize()) Hehe 1.10 title 每个单词首字母大写 s = \"hehe,haha\" print (s.title()) Hehe,Haha 1.11 index 根据元素查找索引 查找不到报错 #通过元素查找索引 s = [1,2,3,\"b\"] s = s.index(\"b\") print (s) 3 查找不到报错 s = [1,2,3,\"b\"] s = s.index(\"c\") print (s) ValueError: 'c' is not in list 1.12 find 根据元素查找索引 查找不到返回-1 #列表不支持find s = [1,2,3,\"b\"] s = s.find(\"c\") print (s) AttributeError: 'list' object has no attribute 'find' #查找不到返回-1 s = \"abc\" s = s.find(\"d\") print (s) -1 1.13 join 将列表转换为字符串 //join() 将列表转换为字符串 lst = ['a','b','c'] s = \"_\".join(lst) print (s) a_b_c print (type(s)) 1.14 center 居中 //示例1 s = \"abc\" s = s.center(20) print (s) abc //总长度20 //示例2 s = \"abc\" s.center(20,\"_\") //总长度20 左右两边为_ s = \"abc\" s = s.center(20,\"_\") print (s) ________abc_________ 1.15 format 格式化 1.按照位置格式化 s = \"a{}b\" s1 = s.format(\"你好\") print (s1) a你好b 2.按照索引格式化 s = \"a{1}b\" s1 = s.format(\"你好\",\"呵呵\") print (s1) a呵呵b 3.按照关键字格式化 s = \"a{A}b\" s = s.format(A=\"你好\") print (s) a你好b 1.16 swapcase 大小写转换 s = \"abc\" print (s.swapcase()) ABC s = \"abcD\" print (s.swapcase()) ABCd is系列 1.17 str.isalnum 判断是否只包含数字,中文,字母，返回布尔值 //只包含数字、中文、字母，返回结果True name = \"123呵呵haha\" print (name.isalnum()) True //包含数字、中文、字母，同时包含特殊符号*，返回结果False name = \"123呵呵haha*\" print (name.isalnum()) False 1.18 str.isalpha 判断中文,字母,返回布尔值 //只包含中文、字母，返回结果为True name = \"呵呵hehe\" print (name.isalpha()) True //包含中文、字母，同时包含数字，返回结果为False name = \"呵呵hehe123\" print (name.isalpha()) False 1.19 str.isdigit 判断阿拉伯数字,返回布尔值 //isdigit有bug，圆圈5也算作是阿拉伯数字，因此用isdecimal做判断更好 name = \"1231⑤\" print (name.isdigit()) True name = \"12315\" print (name.isdigit()) True //赋值方式错误 name = 12315 print (name.isdigit()) Traceback (most recent call last): File \"\", line 1, in AttributeError: 'int' object has no attribute 'isdigit' 1.20 str.isdecimal 判断十进制，返回布尔值 //判断十进制数字 name = \"10\" print (name.isdecimal()) True //用isdecimal判断圆圈数字更准确 name = \"1231⑤\" print (name.isdecimal()) False 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/7.3python基础七 基础数据类型-列表.html":{"url":"python/python基础/7.3python基础七 基础数据类型-列表.html","title":"列表","keywords":"","body":"[toc] python基础七 基础数据类型-列表 1. 列表 1.1 含义 ​ python中数据结构之一 1.2 关键字 list 1.3 定义 列表名 = [元素1,元素2,元素3,...] #每个元素以逗号分隔 以逗号分隔称为一个元素 1.4 作用 1.存储大量数据 2.存储不同数据类型的数据 1.5 特点 可变、可迭代、有序 1.6 列表增加 lst.append() #追加列表,只能在末尾添加 //代码示例 lst = [\"a\",\"b\"] lst.append(c) print (lst) ['a', 'b', 'c'] 1.7 列表插入 ⚠️⚠️⚠️以后尽量少使用列表插入insert，有性能消耗 lst.insert(下标,插入的内容) #参数1，要插入的位置的下标，参数2，要插入的内容 //代码示例 lst = [\"a\",\"b\"] lst.insert(1,\"c\") //1表示要插入的位置，即下标，c表示要插入的内容 print (lst) ['a', 'c', 'b'] 1.8 列表扩展 迭代添加 ⚠️extend原理是for循环迭代添加 lst.extend() #extend是迭代添加，例如添加\"abc\"，会添加\"a\",\"b\",\"c\" //代码示例 lst = [\"a\",\"b\"] lst.extend(\"hehe\") print (lst) ['a', 'b', 'h', 'e', 'h', 'e'] //for循环代码示例 lst = [\"a\",\"b\"] for i in \"hehe\": lst.append(i) print (lst) ['a', 'b', 'h', 'e', 'h', 'e'] 1.9 列表删除 #方式一 lst.remove() //一次只能删除一个 //通过元素名进行删除 lst = [\"a\",\"b\",\"c\"] lst.remove(\"b\") ['a', 'c'] #方式二 lst.pop() //弹出，默认从列表最后弹出内容，并且有返回值，返回的内容是删除的内容 //代码示例,默认删除 lst = [\"a\",\"b\",\"c\"] lst.pop() 'c' print (lst) ['a', 'b'] //代码示例，通过下标删除 lst = [\"a\",\"b\",\"c\"] lst.pop(0) 'a' print (lst) ['b', 'c'] #方式三 del lst //慎用，删除列表全部 //代码示例,删除列表全部 lst = [\"a\",\"b\",\"c\"] del lst print (lst) Traceback (most recent call last): File \"\", line 1, in NameError: name 'lst' is not defined //代码示例，根据下标删除 lst = [\"a\",\"b\",\"c\"] del lst[0] print (lst) ['b', 'c'] //代码示例，切片删除 lst = [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\"] del lst[1:3] print (lst) ['a', 'd', 'e', 'f'] #方式四 clear lst //清除列表 //代码示例 lst = [\"a\",\"b\",\"c\"] lst.clear print (lst) [] 1.10 列表修改 lst[下标] = \"修改为。。。\" #方式一，修改单个元素 //代码示例 lst = [\"a\",\"b\",\"c\"] lst[0] = \"hehe\" print (lst) ['hehe', 'b', 'c'] #方式二，修改多个元素 //代码示例1，超出列表范围添加多个元素 lst = [\"a\",\"b\",\"c\"] lst[1:4] = 1,2,3 print (lst) ['a', 1, 2, 3] //代码示例2，超出列表范围添加一个元素 lst = [\"a\",\"b\",\"c\"] lst[1:4] = 1, //⚠️这里后边必须要加一个逗号，否则会变成不可迭代的数字 print (lst) ['a', 1] ⚠️列表修改元素时，修改的内容必须为可迭代的 lst[1:4] = 1 Traceback (most recent call last): File \"\", line 1, in TypeError: can only assign an iterable #方式三，切片修改多个元素 //代码示例1,将a、c、e修改为A、C、E lst = [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\"] lst[0:5:2] = \"A\",\"B\",\"C\" print (lst) ['A', 'b', 'B', 'd', 'C', 'f'] //代码示例2,修改列表中不连续的元素，修改的内容必须一一对应 lst = [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\"] lst[0:5:2] = \"A\",\"B\" Traceback (most recent call last): File \"\", line 1, in ValueError: attempt to assign sequence of size 2 to extended slice of size 3 1.10.1列表修改总结 1.使用切片时，获取的内容就是原数据本身，即列表切片后获取的是列表 2.切片获取的内容是连续的，修改内容时可多可少 3.如果切片获取的内容不是连续的，修改的内容必须一一对应 1.11 列表查看 #方法一 for循环 lst = [\"a\",\"b\",\"c\"] for i in lst: print (i) a b c #方法二 索引查询 lst = [\"a\",\"b\",\"c\"] print (lst[0]) a 1.11 列表嵌套 #列表嵌套就是列表中的元素是列表 //代码示例，要打印8 lst = [1,2,3,[4,5,[6,7,8]]] lst1 = lst[3] print (lst1) [4, 5, [6, 7, 8]] lst2 = lst1[2] print (lst2) [6, 7, 8] print (lst2[2]) 8 //代码示例，要打印8 lst = [1,2,3,[4,5,[6,7,8]]] print (lst[3][2][2]) 8 1.12 列表方法 1.12.1 列表倒序 reverse() 将列表倒序显示 #方法1 lst[::-1] //此方式为新开辟内存空间 lst = [1,2,3,4,5,6] print (lst[::-1]) [6, 5, 4, 3, 2, 1] print (id(lst),id(lst[::-1])) 140193101789000 140193101800136 #方法2 lst.reverse() //此方式为原地修改 lst = [1,2,3,4,5,6] lst.reverse() print (lst) [6, 5, 4, 3, 2, 1] //⚠️这样写会返回空 lst = [1,2,3,4,5,6] lst = lst.reverse() print (lst) None 1.12.2 列表排序 #升序 sort() lst = [1,3,2,7,5,6] lst.sort() print (lst) [1, 2, 3, 5, 6, 7] #降序 sort(reverse=True) lst = [1,2,3,7,5,6] lst.sort(reverse=True) print (lst) [7, 6, 5, 3, 2, 1] 1.13 列表总结 1.列表是可变数据类型，可迭代数据类型，列表是有序的 2.列表的作用存储大量数据，并且可以存储不同数据类型 3.列表就是一个容器 1.14 列表面试题 1.列表整合面试题 lst1 = [1,2,3,[4]] lst2 = [5,6,7] lst1和lst2整合 #方法1 extend lst1 = [1,2,3,[4]] lst2 = [5,6,7] lst1.extend(lst2) print (lst1) [1, 2, 3, [4], 5, 6, 7] #方法2 列表相加 lst1 = [1,2,3,[4]] lst2 = [5,6,7] print (lst1 + lst2) [1, 2, 3, [4], 5, 6, 7] 2.列表相乘面试题 ⚠️列表进行乘法时，元素都是共用的 同样的，元组也适用 lst = [1,2,[]] lst1 = lst * 2 lst1[-1].append(8) lst、lst1结果？ ⚠️ lst与lst1中元素相同，因为列表进行乘法时，元素都是共用的，因此当lst1最后一个元素追加了一个8之后，所有的元素变成了1,2,[8] lst = [1,2,[]] lst1 = lst * 2 lst1[-1].append(8) print (lst) [1, 2, [8]] print (lst1) [1, 2, [8], 1, 2, [8]] 3.定义列表 lst = [] lst1 = list() //次方式同样适用于其他数据类型 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/7.4python基础七 基础数据类型-字典.html":{"url":"python/python基础/7.4python基础七 基础数据类型-字典.html","title":"字典","keywords":"","body":"[toc] python基础七 基础数据类型-字典 1.字典 1.1 含义 python中的一种数据类型 唯一的一种{} 键值对的数据 1.2 关键字 dict 1.3 定义 dic = {key:value} 1.4 哈希说明 可变数据类型就不可哈希 不可变数据类型就可哈希 1.5 字典说明 字典的键是不可变数据类型，可哈希 字典的键是唯一的，不可重复 字典的值可以是任意的 字典是一个可变数据类型 1.6 字典的增 1.6.1 方式一 dic[键] = 值 dic = {\"key1\":1,\"key2\":2,\"key3\":3} //定义一个字典 print (dic) {'key1': 1, 'key2': 2, 'key3': 3} dic[\"key4\"] = 4 //增加一个键 print (dic) {'key1': 1, 'key2': 2, 'key3': 3, 'key4': 4} 1.6.2 方式二 dic.setdefault(\"键\",值) 参数1是键，参数2是值 dic = {\"key1\":1,\"key2\":2,\"key3\":3} //定义一个字典 dic.setdefault(\"key4\",4) //增加一个键 print (dic) {'key1': 1, 'key2': 2, 'key3': 3, 'key4': 4} #特殊说明1 使用setdefault增加字典值时，如果键值存在就不增加 dic = {\"key1\":1,\"key2\":2,\"key3\":3} //定义一个字典 dic.setdefault(\"key1\",5) //向已有的键key1中插入值 print (dic) {'key1': 1, 'key2': 2, 'key3': 3} //已存在的键不会改变 #特殊说明2 如果键不存在，则返回None dic = {\"key1\":1} print (dic.setdefault(\"key5\")) None #特殊说明3 键和值添加成功后返回的是添加的值 dic = {\"key1\":1} print (dic.setdefault(\"key5\",5)) 5 #特殊说明4 如果键存在，返回的是键的值 dic = {\"key1\":1} print (dic.setdefault(\"key1\")) 1 #setdefault工作流程说明 1.setdefault在字典中先根据键值查找，如果返回的结果为None，在进行第二步 2.将键和值添加到字典中 1.7 字典的删 1.7.1 clear() 清空 dic = {\"key1\":1,\"key2\":2,\"key3\":3} dic.clear() print (dic) {} 1.7.2 pop dic = {\"key1\":1,\"key2\":2,\"key3\":3} dic.pop(\"key1\") print (dic) {'key2': 2, 'key3': 3} 1.7.3 popitem() 随机删除(官方名称，py3.6版本后默认删除最后一个) #随机删除（官方名称）python3.6版本后默认删除最后一个 dic = {\"key1\":1,\"key2\":2,\"key3\":3} dic.popitem() print (dic) {'key1': 1, 'key2': 2} 1.7.4 del 删除字典 #直接删除字典 dic = {\"key1\":1,\"key2\":2,\"key3\":3} del dic sprint (dic) Traceback (most recent call last): File \"\", line 1, in NameError: name 'dic' is not defined #根据键删除,del删除，键只能有一个 dic = {\"key1\":1,\"key2\":2,\"key3\":3} del dic[\"key1\"] print (dic) {'key2': 2, 'key3': 3} 1.8 字典的改 1.8.1 dic[\"key\"] = \"value\" dic = {\"key1\":1,\"key2\":2,\"key3\":3} #修改方式 暴力增加，当键在字典中存在时就修改，不存在就是增加 dic[\"key1\"] = \"5\" //键key1存在，此时为修改 print (dic) {'key1': '5', 'key2': 2, 'key3': 3} dic[\"key6\"] = \"6\" //键key6不存在，此时为增加 print (dic) {'key1': 1, 'key2': 2, 'key3': 3, 'key6': '6'} 1.8.2 update dic = {\"key1\":1,\"key2\":2,\"key3\":3} dic.update({\"key1\":\"hehe\"}) //修改单个值 print (dic) {'key1': 'hehe', 'key2': 2, 'key3': 3} dic = {\"key1\":1,\"key2\":2,\"key3\":3} dic.update({\"key1\":\"hehe\",\"key2\":\"haha\"}) //修改多个值 print (dic) {'key1': 'hehe', 'key2': 'haha', 'key3': 3} 1.9 字典的查 #查找方式1 有键返回值，没有键报错 dic = {\"key1\":1,\"key2\":2,\"key3\":3} print (dic[\"key1\"]) //有键返回值 1 print (dic[\"key1\"]) //没有键报错 KeyError: 'key10' #查找方式2 dic.get(\"key\") dic = {\"key1\":1,\"key2\":2,\"key3\":3} print (dic.get(\"key1\")) //有键返回值 1 print (dic.get(\"key10\")) //没有键返回None None print (dic.get(\"key10\",\"呵呵\")) //没有键还可以返回自定义值 呵呵 #查找方式3 获取字典所有的键、值、键和值 dic = {\"key1\":1,\"key2\":2,\"key3\":3} print (dic.keys()) //返回字典所有的键，高仿列表 dict_keys(['key1', 'key2', 'key3']) for i in dic.keys(): print (i) key1 key2 key3 print (dic.values()) //返回字典所有的值 dict_values([1, 2, 3]) for i in dic.values(): print (i) 1 2 3 print (dic.items()) //返回字典所有的键和值 dict_items([('key1', 1), ('key2', 2), ('key3', 3)]) for i in dic.items(): print (i) ('key1', 1) ('key2', 2) ('key3', 3) 2.解构 2.1 解构示例1 两个变量互换值 a,b = 10,20 print (a,b) #a与b的值互换 a = 10 b = 20 a,b = b,a print (a,b) 20 10 #解构说明1 a,b = \"你好\" //将字符串分别赋值给a和b print (a) print (b) 你 好 a,b = \"你好啊\" //后边的值必须与变量的数量相同 print (a) print (b) 2.2 解构示例2 字典示例 字典本身是无序的，所以不支持索引，python3.6以上版本按照定义的顺序显示 #将字典转换为列表从而可以以索引查询字典的键和值 dic = {\"key1\":1,\"key2\":2,\"key3\":3} lst = list(dic.items()) //将字典所有键和值显示并转换为列表，从而可以取键和值 print (lst[0]) ('key1', 1) print (lst[0][0]) //打印出字典的键和值后再根据索引取键或者值 key1 #进阶，字典中的值比较少的时候可以用转换为列表的方式然后根据索引获取键或者值解决，如果字典中的值比较多，可以用for循环解决 dic = {\"key1\":1,\"key2\":2,\"key3\":3,\"key4\":4,\"key5\":5,\"key6\":6,\"key7\":7,\"key8\":8,\"key9\":9} //获取字典中的键和值 for i in dic.items(): print (i) ('key1', 1) ('key2', 2) ('key3', 3) ('key4', 4) ('key5', 5) ('key6', 6) ('key7', 7) ('key8', 8) ('key9', 9) //分别获取字典中的键和值 for i in dic.items(): print (i[0],i[1]) key1 1 key2 2 key3 3 key4 4 key5 5 key6 6 key7 7 key8 8 key9 9 3.字典的嵌套 字典的嵌套就是字典中还有字典 #示例1 dic = {\"dic1\":1,\"dic2\":2,\"dic3\":3,\"dic4\":4,\"dic5\":{\"dic6\":6,\"dic7\":{\"dic8\":8,\"dic9\":9,\"dic10\":{\"dic11\":11,\"dic12\":12}}}} //要获取dic12的值 print (dic[\"dic5\"][\"dic7\"][\"dic10\"][\"dic12\"]) 12 #示例2 dic = {\"dic1\":1,\"dic2\":2,\"dic3\":3,\"dic4\":4,\"dic5\":{\"dic6\":6,\"dic7\":{\"dic8\":8,\"dic9\":9,\"dic10\":{\"dic11\":\"么么哒\",\"dic12\":\"呵哈嘻\"}}}} //要获取哈 print (dic[\"dic5\"][\"dic7\"][\"dic10\"][\"dic12\"][1]) #获取到字典的键dic12后，值是字符串，因此可以根据索引取值 哈 4.字典骚操作 4.1 字典批量创建键值对 fromkeys fromkeys(参数1，参数2) 参数1:键（可迭代） 参数2:值（共用）批量创建键值对 dic = {} dic = dic.fromkeys(\"abc\",'hehe') print (dic) {'a': 'hehe', 'b': 'hehe', 'c': 'hehe'} 字典批量创建键值对，值必须可迭代！！！ //字典批量创建键值对，值是可变数据类型的坑 dic = {} dic = dic.fromkeys(\"abc\",[]) dic[\"c\"].append(6) print (dic) {'a': [6], 'b': [6], 'c': [6]} 本意是想修改c的值，但是结果却全变了 正确做法 dic = {} dic = dic.fromkeys(\"abc\",[]) dic[\"c\"] = 6 print (dic) {'a': [], 'b': [], 'c': 6} 4.2 字典另类定义方式 //字典另类定义方式1 print (dict(k1=1,k2=2)) {'k1': 1, 'k2': 2} //字典另类定义方式2 print (dict([(1,2),(3,4),(5,6)])) {1: 2, 3: 4, 5: 6} 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/7.5python基础七 基础数据类型-元组.html":{"url":"python/python基础/7.5python基础七 基础数据类型-元组.html","title":"元组","keywords":"","body":"[toc] python基础七 基础数据类型-元组 1.元组 1.1 含义 python中的数据类型之一 1.2 关键字 tuple 1.3 定义 tu = () 1.4 作用 通过元素的名称获取元素的索引 1.5 应用场景 1.不可变：为了防止误操作时修改重要数据 2.配置文件中存储数据 1.6 特点 1.元组就是一个不可变的列表 2.元组不能增删改 3.元组是一个有序，可迭代，不可变的数据类型 4.当小括号中没有出现逗号时，数据类型就是括号中数据本身的数据类型 5.列表和元组进行乘法时，元素都是共用的 1.7 使用示例 //定义一个元组 tu = (1,2,3) print (tu) (1, 2, 3) //支持索引 tu = (1,2,3) print (tu[0]) 1 //支持切片 tu = (1,2,3) print (tu[0:1]) (1,) print (tu[0:2]) (1, 2) //统计元组中元素个数 tu.count tu = (1,2,3,1) print (tu.count(1)) 2 //通过元素的名称获取元素的索引 tu.index tu = (1,2,3) print (tu.index(1)) 0 //for循环打印元组中的内容 tu = (1,2,3) for i in tu: print (i) 1 2 3 1.8 元组面试题 a = (10) # 当小括号中没有出现逗号时,数据类型就是括号中数据类型本身 a = (\"abc\") # 当小括号中没有出现逗号时,数据类型就是括号中数据类型本身 a = (\"abc\",) # 这是一个元组 a = () # 这是元组 a = (1,2,3) # 这是元组 //代码示例 a = (10) print (type(a)) a = (\"abc\") print (type(a)) a = (\"abc\",) print (type(a)) a = () print (type(a)) a = (1,2,3) print (type(a)) 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/7.6python基础七 基础数据类型-集合.html":{"url":"python/python基础/7.6python基础七 基础数据类型-集合.html","title":"集合","keywords":"","body":"[toc] python基础七 基础数据类型-集合 1.集合 1.1 含义 集合是python中的一种数据类型 1.2 关键字 set 1.3 定义 空集合 set() #定义集合示例 s = {1,2,3} 1.4 特点 1.去重，集合中的元素都是唯一的 2.集合关系 3.集合无序、可变、可迭代 1.5 集合使用示例 1.集合去重 lst = [1,2,3,1,4] print (set(lst)) {1, 2, 3, 4} 1.6 集合面试题示例 一行代码去重 ⚠️需要注意的是原先的数据类型是什么，在做完去重处理后需要转换为原先的数据类型 lst = [1,2,3,5,6,1,5,6,7,8,9,33,1,2,33,66] print (set(lst)) //不完美做法，没有将数据类型转换为原来的列表 print (list(set(lst))) [1, 2, 3, 33, 5, 6, 7, 8, 9, 66] 1.7 集合核心说明 ⚠️集合就是一个没有值的字典 ⚠️集合就是一个没有值的字典 为什么说集合是一个没有值的字典？？？ 字典的值：任意 字典的键：不可变、可哈希 \"\"\"示例说明，列表是可变的 如果集合中能打印列表，那么说明集合中的元素相当于字典中的值,也就是集合是一个没有键的字典 如果集合中不能打印列表，那么说明集合中的元素相当于字典中的键，也就是集合是一个没有值的字典 \"\"\" #打印一个可变类型 s = {1,2,[1,2,3]} print (s) TypeError: unhashable type: 'list' //打印结果报错列表不可哈希 #打印一个不可变类型 s = {1,2,(1,2,3)} print (s) {1, 2, (1, 2, 3)} 1.8 集合的增删改查 1.8.1 集合的增 #方式1 update s = set() s.update('play') print (s) {'l', 'a', 'p', 'y'} //集合中的update会把增加的内容迭代添加 #方式2 add s = set() s.add('play') //add在集合中是常用的 print (s) {'play'} 1.8.2 集合的删 #方式1 pop 随机删除 s = {1,2,3,'呵呵',5} s.pop() //⚠️集合中的pop是随机删除 print (s) {2, 3, 5, '呵呵'} {1, 2, 3, 5} #方式2 remove 通过元素名称删除 s = {1,2,3,'呵呵',5} s.remove(\"呵呵\") print (s) {1, 2, 3, 5} #方式3 clear s = {1,2,3,'呵呵',5} s.clear() print (s) set() //⚠️ 空集合为set() 1.8.3 集合的改 1.先删后加 2.转换数据类型进行修改 1.8.4 集合的查 #for循环 s = {1,2,3,'呵呵',5} for i in s: print (i) 1 2 3 5 呵呵 1.9 集合关系 1.9.1 集合并集 | == or a = {'a','b','c','d','e'} b = {'a','c','e','f'} print (a | b) {'b', 'a', 'c', 'f', 'd', 'e'} 1.9.2 集合交集 & == and a = {'a','b','c','d','e'} b = {'a','c','e','f'} print (a & b) {'a', 'c', 'e'} 1.9.3 集合差集 - #差集就是用自身的元素减去对方的元素，相同的元素消除，但是自身没有的不会出现 a = {'a','b','c','d','e'} b = {'a','c','e','f'} print (a - b) {'b', 'd'} 1.9.4 集合补集、反差集、对称集 ^ #补集就是用自身的元素减去对方的元素，相同的元素消除，同时自身没有的元素会出现 a = {'a','b','c','d','e'} b = {'a','c','e','f'} print (a ^ b) {'b', 'd', 'f'} 1.9.5 超集 就是父集 > a = {'a','b','c','d','e'} b = {'a','c','e'} print (a > b) //判断a是否是b的父集 True 1.9.6 子集 a = {'a','b','c','d','e'} b = {'a','c','e'} print (b 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/8.python基础八 小数据池、代码块.html":{"url":"python/python基础/8.python基础八 小数据池、代码块.html","title":"python基础八 小数据池、代码块","keywords":"","body":"[toc] python基础八 小数据池、代码块 1.小数据池、代码块（了解，实际开发中用不到） 1.1 小数据池、代码块含义 小数据池、代码块属于python中的驻留机制，目的用于节省内存空间 代码块：一个py文件、一个函数、一个模块、一个类、交互模式（终端）下的每一行都是一个代码块 小数据池：python中的内存驻留机制，优先级比代码块低 ⚠️先执行代码块然后执行小数据池 1.2 小数据池缓存机制 数字：-5 ～ 256 字符串： 1.定义时内容（不包含中文、特殊符号）长度不限，内容相同，就进行驻留 2.python3.6解释器字符串进行乘法时（不包含中文、特殊符号），总长度不能超过20 3.python3.7解释器字符串进行乘法时（不包含中文、特殊符号），总长度不能超过4096 ⚠️验证小数据池机制必须在终端中，不能在IDE(例如pycharm)中验证，因为IDE中是一个py文件，是一个代码块，会优先执行 1.小数据池数字验证 //-5 ~ 256验证，返回结果为True，驻留，内存空间地址相同 >>> a = -5 >>> b = -5 >>> a is b True >>> print (id(a),id(b)) 4331026288 4331026288 >>> a = 256 >>> b = 256 >>> a is b True >>> print (id(a),id(b)) 4331034640 4331034640 //-5 ~ 256范围外验证，返回结果为False，不驻留，内存空间地址不同 >>> a = -6 >>> b = -6 >>> a is b False >>> print (id(a),id(b)) 140558040632912 140557772390192 >>> a = 257 >>> b = 257 >>> a is b False >>> print (id(a),id(b)) 140558039687984 140557772390256 2.小数据池字符串验证 2.1定义内容验证 //不包含中文、特殊符号,定义时内容相同，驻留，内存空间地址相同 >>> a = \"abc\" >>> b = \"abc\" >>> a is b True >>> print (id(a),id(b)) 140558040616664 140558040616664 //包含中文、特殊符号，定义时内容相同，不驻留，内存空间地址不同 >>> a = \"abc#\" >>> b = \"abc#\" >>> a is b False >>> print (id(a),id(b)) 140557772689168 140557772689112 >>> a = \"abc#你好\" >>> b = \"abc#你好\" >>> a is b False >>> print (id(a),id(b)) 140557772596880 140557772596688 >>> a = \"abc你好\" >>> b = \"abc你好\" >>> a is b False >>> print (id(a),id(b)) 140557772596688 140557772596880 2.1字符串乘法验证(python3.6) //不包含中文和特殊符号，总长度不超过20，驻留，内存空间地址相同 >>> a = \"hehe\" * 3 >>> b = \"hehe\" * 3 >>> print (a is b) True >>> print (id(a),id(b)) 140557772708656 140557772708656 >>> a = \"hehe\" * 5 >>> b = \"hehe\" * 5 >>> print (a is b) True >>> print (id(a),id(b)) 140557772718488 140557772718488 //字符串相乘，等于20时驻留 //不包含中文和特殊符号，总长度超过20，不驻留，内存空间地址不同 >>> a = \"hehe\" * 6 >>> b = \"hehe\" * 6 >>> print (a is b) False >>> print (id(a),id(b)) 140557772714992 140557772714912 //包含中文或者特殊符号，不管总长度是否超过20，都不驻留，内存空间地址不同 >>> a = \"abc#你好\" * 3 >>> b = \"abc#你好\" * 3 >>> print (a is b) False >>> print (id(a),id(b)) 140231892539968 140231893044000 //总长度不超过20，但是内存地址还是不同 >>> a = \"abc#你好\" * 5 >>> b = \"abc#你好\" * 5 >>> print (a is b) False >>> print (id(a),id(b)) 140231893461712 140231893461848 1.3 代码块缓存机制 数字：-5 ～ 正无穷 字符串： 1.定义时内容长度不限，内容相同，就进行驻留 2.字符串进行乘法时（不包含中文和特殊符号），总长度不能超过20 1.代码块数字验证 （pycharm中验证） //-5 ~ 正无穷验证，返回结果为True，驻留，内存空间地址相同 a = -5 b = -5 print (a is b) True print (id(a),id(b)) 4453603184 4453603184 a = 1000000 b = 1000000 print (a is b) True print (id(a),id(b)) 140434424966800 140434424966800 //-5 ~ 正无穷范围外验证，返回结果为False，不驻留，内存空间地址不同 a = -6 b = -6 print (a is b) False print (id(a),id(b)) 140442480390000 140442480390032 2.字符串验证 2.1定义内容验证 //只要内容相同，就驻留，可以包含中文和字符串,小数据池中就不可以 a = \"abc#你好\" b = \"abc#你好\" print (a is b) True print (id(a),id(b)) 140481266762928 140481266762928 2.2字符串乘法验证 //不包含中文、特殊符号，总长度不超过20，驻留，内存空间地址相同，与小数据池相同 a = \"abc\" * 6 b = \"abc\" * 6 print (a is b) True print (id(a),id(b)) 140679911152496 140679911152496 //包含中文、特殊符号，无论总长度是否超过20，都不驻留，内存空间地址不同 a = \"abc#你好\" * 3 b = \"abc#你好\" * 3 print (a is b) False print (id(a),id(b)) 140361947581552 140361947581216 //总长度不超过20，内存空间地址不同 a = \"abc#你好\" * 5 b = \"abc#你好\" * 5 print (a is b) False print (id(a),id(b)) 140438049459712 140437243696104 //总长度超过20，内存空间地址不同 1.4 小数据池和代码块对比 小数据池 代码块 数字 -5 ～ 256 驻留 -5 ～ 正无穷 驻留 字符串定义内容 不能包含中文、特殊符号，长度不限，内容相同就驻留 可以包含中文、特殊符号，长度不限，内容相同就驻留 字符串乘法 不能包含中文、特殊符号，总长度不超过20就驻留 不能包含中文、特殊符号，总长度不超过20就驻留 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/9.python基础九 深浅拷贝.html":{"url":"python/python基础/9.python基础九 深浅拷贝.html","title":"python基础九 深浅拷贝","keywords":"","body":"[toc] python基础九 深浅拷贝 1.深浅拷贝（⚠️重要） 1.1 浅拷贝 1.1.1 定义 只拷贝第一层的内存地址 1.1.2 浅拷贝示例 赋值 示意浅拷贝前先说明赋值 1.赋值共用同一块内存空间，一个变量的值改变，另一个赋值的变量同时也改变 2.多个变量名指向同一块内存空间 //列表示例 lst = [1,1,3] lst1 = lst lst1.append(4) //正常思维理解应该是lst1更改后，lst不会改变，但是赋值是多个变量名指向同一个内存 空间，因此只要一个变量改变，另一个变量也改变 print (lst,lst1) [1, 1, 3, 4] [1, 1, 3, 4] //字典示例 dic = {'k1':1,'k1':1} dic1 = dic dic1.update({'k1':111}) print (dic,dic1) {'k1': 1, 'k1': 111} {'k1': 1, 'k1': 111} 赋值示意图 浅拷贝 只拷贝第一层的内存地址 列表中浅拷贝有两种方式 1. lst = [1,1,3] new_lst = lst.copy() 1. lst = [1,1,3] new_lst = lst[:] 浅拷贝只拷贝第一层的内存空间地址,浅拷贝的两个变量是单独的内存空间，不再是共用同一个内存空间地址 //列表示例1 列表中未嵌套第二层元素 lst = [1,1,3] new_lst = lst.copy() new_lst.append(4) print (lst,new_lst) [1, 1, 3] [1, 1, 3, 4] //浅拷贝中lst和new_lst结果不同 print (id(lst),id(new_lst)) 140584614767176 140584614774344 //浅拷贝中lst和new_lst内存空间地址不同 //列表示例1 列表中嵌套了第二层元素 lst = [1,1,[3,4,5]] new_lst = lst.copy() new_lst.append(6) print (lst,new_lst) [1, 1, [3, 4, 5]] [1, 1, [3, 4, 5], 6] print (id(lst),id(new_lst)) 140431875130951 140431874963911 print (id(lst[1]),id(new_lst[1])) //浅拷贝只拷贝第一层内存空间，因此两个列表的第二个嵌套的元素的值内存空间相同 140633470413384 140633470413384 浅拷贝单层元素示意图（增加元素） 浅拷贝多层元素示意图（修改元素） 浅拷贝多层元素示意图（第二层增加元素） 1.1.3 浅拷贝总结 1.浅拷贝只复制第一层内存空间地址 1.浅拷贝，修改第一层元素或者追加元素，都是将旧指向改变为新指向，两个变量互不影响 3.浅拷贝修改第二层及以下元素或者追加元素，修改的是两个变量共用的值，此时修改会影响两个变量 1.1.4 浅拷贝坑 lst = [1,3,[4,5],6] lst1 = lst lst1 = lst[:] lst1[-1] = [8,9] lst1[-1].append([0]) #数字无法进行追加操作 print (lst,lst1,lst1) AttributeError: 'int' object has no attribute 'append' 1.2深拷贝 1.2.1 定义 不可变数据类型共用内存空间，可变数据类型开辟新的内存空间，不管嵌套多少层都是这样的原理 1.2.2 深拷贝语法 Import copy copy.deepcopy() 1.2.3 深拷贝示例 import copy lst = [1,1,[3,4]] new_lst = copy.deepcopy(lst) print (id(lst[0]),id(new_lst[0])) #lst[0]为1，1是整型，是不可变数据类型 --> 共用内存空间地址 4464777164 4464777164 print (id(lst[-1]),id(new_lst[-1])) 140663166316984 140663166494088 #lst[-1]为[3,4]，[3,4]是列表，是可变数据类型 --> 新开辟内存空间地址 1.2.4 深拷贝原理图 1.2.5 深拷贝总结 深拷贝中，不可变数据类型共用内存空间地址，可变数据类型开辟新的内存空间，不管嵌套多少层都是这样 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/10.python基础十 文件操作.html":{"url":"python/python基础/10.python基础十 文件操作.html","title":"python基础十 文件操作","keywords":"","body":"[toc] python基础十 文件操作 1.文件操作 1.1 作用 持久化存储 1.2 文件操作总结 f = open(file=\"文件名字或文件路径\",mode=\"操作模式\",encoding=\"编码\") r 只读 w 清空写 a 追加写 rb 只读字节 wb 清空写字节 ab 追加写字节 r+ 读写 w+ 清空写读 a+ 追加写读 1.3 文件操作 1.3.1 只读文件 --> r r: 读文本 f = open(\"test.txt\",mode=\"r\",encoding=\"utf-8\") #参数说明 f 变量名，句柄 open 表示打开文件，通过python向操作系统发送指令 test.txt 表示要操作的文件 mode 指定对文件的操作方式，r表示读取 r 表示读取文件 encoding 指定字符集 ⚠️文件读取时只能读一遍 在py同级目录下创建一个文件txt.txt，并写入以下内容 一只奔跑的草泥马 abc 123 1.全部读取 print(f.read()) f = open(\"txt.txt\",mode=\"r\",encoding=\"utf-8\") print(f.read()) 一只奔跑的草泥马 abc 123 2.r模式下按照字符读取 print(f.read(3)) f = open(\"txt.txt\",mode=\"r\",encoding=\"utf-8\") print(f.read(5)) 一只奔跑的 3.读取一行 print(f.readline().strip()) 读取一行默认最后有换行符，需要去掉 f = open(\"txt.txt\",mode=\"r\",encoding=\"utf-8\") print (f.readline().strip()) 一只奔跑的草泥马 4.读取多行，以列表的形式存储 print(f.readlines()) f = open(\"txt.txt\",mode=\"r\",encoding=\"utf-8\") print (f.readlines()) ['一只奔跑的草泥马\\n', 'abc\\n', '123'] //读取总结 print(f.read()) #全部读取 print(f.read(3)) #模式的r的情况下按照字符读取 print(f.readline().strip()) #读取一行 print(f.readlines()) #读取多行,以列表的形式存储 1.3.2 清空写 --> w w: 清空写文本 f = open(\"txt.txt\",mode=\"w\",encoding=\"utf-8\") 清空写会把要操作的文件先清空，然后再写入 //w模式打开文件后会清空文件内容 f = open(\"txt.txt\",mode=\"w\",encoding=\"utf-8\") print (f) txt.txt文件中的内容会被清空 //写入内容 f = open(\"txt.txt\",mode=\"w\",encoding=\"utf-8\") print (f.write(\"呵呵\\n哈哈\")) 5 txt.txt文件中的内容为如下，并且光标在最开头 呵呵 哈哈 //连续写会从文件最后开始写 f = open(\"txt.txt\",mode=\"w\",encoding=\"utf-8\") print (f.write(\"呵呵\\n哈哈\")) print (f.write(\"嘻嘻\\n嘿嘿\")) 5 5 txt.txt文件中的内容如下，并且光标在最开头 呵呵 哈哈嘻嘻 嘿嘿 //对文件写操作后，需要刷新和关闭文件 f = open(\"txt.txt\",mode=\"w\",encoding=\"utf-8\") print (f.write(\"呵呵\\n哈哈\")) print (f.write(\"嘻嘻\\n嘿嘿\")) print (f.write(\"啦啦\\n吼吼\")) f.flush() f.close() 5 5 5 txt.txt文件中的内容如下，并且光标在最开头 呵呵 哈哈嘻嘻 嘿嘿啦啦 吼吼 1.3.3 追加写 --> a a: 追加写文本 f = open(\"txt.txt\",mode=\"a\",encoding=\"utf-8\") //追加写文本 只会在同一行追加 f = open(\"txt.txt\",mode=\"a\",encoding=\"utf-8\") print (f.write(\"呵呵\")) print (f.write(\"哈哈\")) print (f.write(\"嘿嘿\")) txt.txt文件内容如下，并且光标在最开头 呵呵哈哈嘿嘿 1.3.4 只读字节 --> rb ⚠️二进制方式读取文件不能指定字符集 f = open(\"txt.txt\",mode=\"rb\",encoding=\"utf-8\") print (f.read()) ValueError: binary mode doesn't take an encoding argument //txt.txt文件中的内容为 呵呵哈哈嘿嘿 f = open(\"txt.txt\",mode=\"rb\") print (f.read()) b'\\xe5\\x91\\xb5\\xe5\\x91\\xb5\\xe5\\x93\\x88\\xe5\\x93\\x88\\xe5\\x98\\xbf\\xe5\\x98\\xbf' 以上的字节的内容就是 呵呵哈哈嘿嘿 1.3.5 清空写字节 --> wb 1. 字节内容说明b'\\xe5\\x91\\xb5\\xe5\\x91\\xb5\\xe5\\x93\\x88\\xe5\\x93\\x88\\xe5\\x98\\xbf\\xe5\\x98\\xbf' txt.txt的内容是 呵呵哈哈嘿嘿 2. 清空txt.txt文件 3. f.write()括号中只能写字节 b'\\xxx'⚠️ f = open(\"txt.txt\",mode=\"wb\") f.write(b'\\xe5\\x91\\xb5\\xe5\\x91\\xb5\\xe5\\x93\\x88\\xe5\\x93\\x88\\xe5\\x98\\xbf\\xe5\\x98\\xbf') 4.此时txt.txt文件内容如下 呵呵哈哈嘿嘿 1.3.6 追加写字节 --> ab 1. 字节内容说明b'\\xe5\\x91\\xb5\\xe5\\x91\\xb5\\xe5\\x93\\x88\\xe5\\x93\\x88\\xe5\\x98\\xbf\\xe5\\x98\\xbf' 的内容是 呵呵哈哈嘿嘿 2. txt.txt文件内容是 呵呵哈哈嘿嘿 3. f.write()括号中只能写字节 b'\\xxx'⚠️ f = open(\"txt.txt\",mode=\"ab\") f.write(b'\\xe5\\x91\\xb5\\xe5\\x91\\xb5\\xe5\\x93\\x88\\xe5\\x93\\x88\\xe5\\x98\\xbf\\xe5\\x98\\xbf') 4.此时txt.txt文件内容如下,已成功追加 呵呵哈哈嘿嘿呵呵哈哈嘿嘿 1.3.7 读写 --> r+ txt.txt文件内容为 呵呵哈哈嘿嘿 f = open(\"txt.txt\",\"r+\",encoding=\"utf-8\") a = f.read() f.write(\"这是读写\") 呵呵哈哈嘿嘿 此时txt.txt文件内容为 呵呵哈哈嘿嘿这是读写 1.3.8 清空写读 --> w+ txt.txt文件内容为 呵呵哈哈嘿嘿 f = open(\"txt.txt\",\"w+\",encoding=\"utf-8\") print (f.write(\"清空写读\")) print (f.read()) 4 此时txt.txt文件内容如下，并且光标在最后边 清空写读 1.3.9 追加写读 --> a+ txt.txt文件内容为 呵呵哈哈嘿嘿 f = open(\"txt.txt\",\"a+\",encoding=\"utf-8\") print (f.write(\"追加写读\")) print (f.read()) 此时txt.txt文件内容如下，并且光标在追字前边 呵呵哈哈嘿嘿追加写读 1.4 光标操作 txt.txt内容为 呵呵哈哈嘿嘿 光标操作总结 f = open(\"txt.txt\",\"r\",encoding=\"utf-8\") f.seek(0,0) #移动到文件头部 f.seek(0,1) #移动到光标当前位置 f.seek(0,2) #移动到文件末尾 f.seek(3) #移动3个字节,根据编码不同决定移动的字节大小 print(f.read()) print(f.tell()) #查看光标 返回的是字节 1.5 with open with open 1.自动关闭文件 2.可以同时操作多个文件 3.as 起别名 现在有两个文件 t1 t2，t1内容为t1，t2内容为t2 with open(\"t1\",\"r\",encoding=\"utf-8\") as f1, \\ open(\"t2\",\"r\",encoding=\"utf-8\") as f2: print (f1.read()) print (f2.read()) t1 t2 ⚠️两个open之间必须以逗号分隔，print必须在with的下一级 1.6 修改文件名及文件内容 1.6.1 修改文件名 现在有两个文件 t1 t2，t1内容为t1，t2内容为t2 现在想把两个文件名互换 即t1 --> t2 t2 --> t1 转换思路，将t1改名为临时文件t3，然后把t2改名为t1，最后把t3改名为t1，即 t1 --> t3 t2 --> t1 t3 --> t2 import os # 与操作系统做交互 os.rename(\"t1\",\"t3\") os.rename(\"t2\",\"t1\") os.rename(\"t3\",\"t2\") 此时t1内容为t2，t2内容为t1 1.6.2 修改文件内容 t1的内容如下 abc okm tgb 现在要把t1中的b替换成“呵呵” 替换思路，拷贝t1为t2，⚠️尽量不在原文件修改 ⚠️文本中存储的都是字符串 with open(\"t1\",\"r\",encoding=\"utf-8\")as f,\\ open(\"t2\",\"w\",encoding=\"utf-8\")as f1: for i in f: f1.write(i.replace(\"b\",\"呵呵\")) f1.flush() 此时t2内容如下，已经将t1中的b替换成了呵呵 a呵呵c okm tg呵呵 因为t1是读的，t2才是写的文件，现在的需求是t1中修改，因此还需要转换一下文件名 即把t1与t2的文件名相互替换，这样才能达到需求，同时改名后的源文件t2不要删除 import os # 与操作系统做交互 os.rename(\"t1\",\"t3\") os.rename(\"t2\",\"t1\") os.rename(\"t3\",\"t2\") 此时t1文件内容如下 a呵呵c okm tg呵呵 t2文件内容如下 abc okm tgb #文件替换内容步骤总结 1.不要在原文件操作，需要拷贝一个文件 2.读原文件，改拷贝的文件 3.改完拷贝的文件后再替换文件名 4.保留替换文件名后的原文件 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/11.python基础十一 函数.html":{"url":"python/python基础/11.python基础十一 函数.html","title":"python基础十一 函数","keywords":"","body":"[toc] python基础十一 函数 1.函数的作用 封装代码,大量的减少重复代码,重用性高 2.函数的定义 def 函数名(): 函数体 def 是一个关键字，申明要定义一个函数 ():是固定写法 函数体中写的是需要用到的重复代码 3.函数的调用 函数名() 4.函数的返回值 #函数返回值总结 函数体中不写return默认返回None,或者写了return不写值返回的也是None return 能够返回任意数据类型(python中所有对象) return 能够返回多个数据类型,以元组的形式接收 return 能够终止函数,return下方的代码不执行 return 将返回值返回给调用者 //return示例 def func(): a = 10 return a a = func() print(a) 5.函数的参数 5.1位置参数 //函数参数示例 def hehe(app): //形参数 print (\"打开:\",app) //注意必须用逗号隔开 hehe(\"微信\") //实参 执行结果返回如下： 打开: 微信 //位置参数示例 def ball(web, players, age, addr): #形参 print(\"登陆NBA官网\") print(f\"打开{web}\") print(f\"找一位{players},要求年龄:{age},地区:{addr}的人\") print(\"看看视频\") print(\"学学球技\") ball(\"视频专区\", \"球员\", 28, \"洛杉矶\") #实参 按照位置传参 登陆NBA官网 打开视频专区 找一位球员,要求年龄:28,地区:洛杉矶的人 看看视频 学学球技 5.2 默认参数 //默认参数示例 def ball(web, players, age, addr=\"洛杉矶\"): #addr=\"洛杉矶\"表示默认参数 print(\"登陆NBA官网\") print(f\"打开{web}\") print(f\"找一位{players},要求年龄:{age},地区:{addr}的人\") print(\"看看视频\") print(\"学学球技\") ball(\"视频专区\", \"球员\", 28) #实参这里可不写形参中定义的默认值 ball(\"视频专区\",\"球员\",30,\"波士顿\") #实参写内容会覆盖形参中定义的默认参数 登陆NBA官网 打开视频专区 找一位球员,要求年龄:28,地区:洛杉矶的人 看看视频 学学球技 登陆NBA官网 打开视频专区 找一位球员,要求年龄:30,地区:波士顿的人 看看视频 学学球技 5.3 关键字传参 //关键字传参示例 def fun(a,b,c=1,d=2): print (a,b,c,d) fun(1,2,3,4) //结果是1 2 3 4 fun(a=\"呵呵\",b=\"哈哈\") //结果是呵呵 哈哈 1 2 fun(1,2,d=\"呵呵\") //结果是1 2 1 呵呵 5.4 动态参数 *args 5.4.1 动态参数作用 1.能够接受不固定长度的参数 2.位置参数过多时可以使用动态参数 5.4.2 动态参数使用方法 def func(*c): #形参位置上的*是聚合 print(*c) #函数体中的*就是打散 func(1,2,3,4,5,6,7,8,9,0) 结果如下： 1 2 3 4 5 6 7 8 9 0 def func(a,b,*c): # *c就表示动态参数 print (a,b,*c) func(1,2,3,4,5,6,7,8) 1 2 3 4 5 6 7 8 5.5 动态关键字参数 **kwages 5.5.1 动态关键字参数作用 只接收多余的动态位置参数 注意是只接收 5.5.2 动态关键字参数使用方法 //动态关键字使用方法示例1 def func(a,b,**kwargs): print (kwargs) func(a=1,b=2,c=3,d=4,e=5) 结果如下： {'c': 3, 'd': 4, 'e': 5} //动态关键字参数使用方法示例2 dic = {\"key\": 1, \"key2\": 2} def func(**kwargs): #聚合 print(kwargs) #打散 func(**dic) #打散 func(key=1,key2=2) 结果如下： {'key': 1, 'key2': 2} 5.6 函数参数面试题 #万能传参 def func(*args, **kwargs): print(args, kwargs) func(12, 2, 121, 12, 321, 3, a=1, b=2) 结果如下 (12, 2, 121, 12, 321, 3) {'a': 1, 'b': 2} *xargs获取的是一个元组 **kwargs获取的是一个字典 #*args和**kwargs def func(*args, a1=8,**kwargs): #万能传参 print(args, kwargs) #函数体中的*args是将元组打散,*kwargs是将字典的键获取到 func(12, 2, a1=1, b1=2) 结果如下： (12, 2) {'b1': 2} 5.7 函数参数总结 #参数的总结: 1.形参 在定义函数的阶段就是形参 可以单独使用位置参数,也可以单独使用默认参数,也可以混合使用 位置传参:必须一一对应 默认参数:可以不传参,可以传参,传参就是把默认的值给覆盖 混合使用:位置参数,默认参数 2.实参 在调用函数的阶段就是实参 可以单独使用位置参数,也可以单独使用关键字参数,也可以混合使用 位置传参:必须一一对应 关键字参数:指名道姓的方式进行传参 混合使用:位置参数,默认参数 3.参数总结 位置参数,动态位置,默认参数,动态关键字参数 *args 程序员之间约定俗称(可以更换但是不建议更换) **kwargs 程序员之间约定俗称(可以更换但是不建议更换) *args 获取的是一个元组 **kwargs 获取的是一个字典 *args 只接受多余的位置参数 **kwargs 只接受多余的关键字参数 4.函数参数优先级 数参数优先级: 位置参数 > 动态位置参数(可变位置参数) > 默认参数 > 动态关键字参数(可变关键字参数) 6.函数的注释 #注释方法1 def a(a,b): \"\"\" 数字加法运算 :param a: int :param b: int :return: int \"\"\" return a + b print (a(1,2)) #注释方法2 def func(a:int,b:int): #这里的a:int只是做到一个约束，并没有实际作用 \"\"\" 加法运算 :param a: :param b: :return: \"\"\" return a + b print (func(1,2)) //查看函数的注释 func.__doc__ print (func.__doc__) 加法运算 :param a: :param b: :return: //查看函数的名字 a.__name__ 代码在编写过程中会设计到函数名赋值，例如 def func(a,b): return a + b a = func print (a(1,2)) print (a.__name__) func 7.函数的名称空间 7.1函数名称空间分类 1.内置空间 -- 存放pyhton自带一些函数 2.全局空间 -- 当前py文件顶格编写的代码开辟的空间 3.局部空间 -- 函数开辟的空间 程序加载顺序: 内置空间 > 全局空间 > 局部空间 程序取值顺序: 局部空间 > 全局空间 > 内置空间 //代码示例 a = 10 #这里的10是全局变量 def func(): a = 5 #这里的5是局部变量 print (a) print (func()) 5 None 7.2函数的作用域 1.全局作用域: 内置 + 全局 globals() #查看全局作用域 2.局部作用域: 局部 locals() #查看当前作用域(建议查看局部) //代码示例 查看全局 a = 10 b = 20 print (globals()) {'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': , '__spec__': None, '__annotations__': {}, '__builtins__': , '__file__': '/jetBrains/pycharm/python-works/python基础/test.py', '__cached__': None, 'a': 10, 'b': 20} //代码示例 查看局部 def func(): a = 10 print (locals()) func() {'a': 10} 8.函数的第一类对象 8.1总概 1.函数名可以当做值,赋值给一个变量 2.函数名可以当做另一个函数的参数来使用 3.函数名可以当做另一个函数的返回值 4.函数名可以当做元素存储在容器中 8.2 函数名可以当作值，赋值给一个变量 //代码示例 def func(): print(1) a = func #函数名是func，然后赋值给了a，打印a与func的函数内存空间地址一样，并且执行a()相当于执行func() print(func) #函数的内存地址 print(a) #函数的内存地址 a() #结果 1 8.3 函数名可以当作另一个函数的参数来使用 //代码示例 def func(): print(1) def foo(a): #a = func print(a) #func这个函数的内存地址 foo(func) func函数当作了foo函数的参数 8.4 函数名可以当作另一个函数的返回值 //代码示例 def func(): return 1 #print def foo(a): #a = func函数的内存地址 return a #return func函数的内存地址 cc = foo(func) print(cc) #func函数的内存地址 8.5 函数名可以当作元素存储在容器中 容器：字典、元组、列表、集合 //代码示例 def func(): print(1) def foo(): print(2) def f(): print(3) lst = [func,foo,f] ⚠️将函数放在容器中的好处，可以使用for循环批量执行函数，而不是一个一个执行 for i in lst: i() 1 2 3 //代码示例 购物平台 ⚠️整体思路 1.定义5个函数，模拟购物平台注册、登陆 2.将这5个步骤的函数存入字典中 3.让用户选择要执行的操作，然后执行函数完成功能实现 def register(): print(\"注册\") def login(): print(\"登录\") def shopping(): print(\"浏览商品\") def add_shopping_car(): print(\"加入购物车\") def buy_goods(): print(\"购买\") msg = \"\"\" 1.注册 2.登录 3.浏览商品 4.加入购物车 5.购买 请输入您要选择的序号: \"\"\" func_dic = {\"1\": register, \"2\": login, \"3\": shopping, \"4\": add_shopping_car, \"5\": buy_goods} while True: choose = input(msg) if choose in func_dic: func_dic[choose]() else: print(\"退出商城\") 9.函数的嵌套 9.1 分类 1.交叉嵌套 2.嵌套 9.2 交叉嵌套 9.2.1 交叉嵌套示例1 def func(foo): print (1) foo() print (3) def a(): print (1) func(a) 1 1 3 上述代码进一步说明 def func(foo): print(1) v = foo() #第一个功能是调用函数,第二个功能是接收返回值 #v = None #print(v) #None print(3) 1 1 None 3 9.2.2 交叉嵌套示例2 //代码示例 def func(): print(1) print(\"太难\") print(2) def foo(b): print(3) b() print(4) def f(a,b): a(b) f(foo,func) 最终结果 3 1 太难 2 4 9.2.3 交叉嵌套示例3 def func(a,b): def foo(b,a): print(b,a) return foo(a,b) a = func(4,7) print(a) 结果 4 7 None 9.2.4 交叉嵌套示例4 //代码示例1 def func(a,b): a = a + b b = a + 10 def foo(a,d): def f(e,f): print(f,e) return \"呵呵\" f(d,a) foo(b,a) print(func(2,3)) 结果 15 5 None ⚠️return \"呵呵\"返回给了f，f是foo函数的最后一行，默认返回None，foo又是func函数的最后一行，默认返回None //代码示例2 def func(a,b): a = a + b b = a + 10 def foo(a,d): def f(e,f): print(f,e) return \"呵呵\" return f(d,a) return foo(b,a) print(func(2,3)) 结果 15 5 呵呵 10. 函数关键字global与nonlocal 10.1 global global 1.global只修改全局空间中的变量 2.在局部空间中可以使用全局中的变量,但是不能修改,如果要强制修改需要添加global 3.当变量在全局存在时global就是申明我要修改全局的变量,并且会在局部开辟这个变量 4.当变量在全局中不存在时global就是申明要在全局创建一个变量,并且会在局部开辟这个变量 1.函数中加关键字global对全局变量进行修改 //不加global关键字 a = 10 def func(): a += 1 func() print(a) 结果报错 UnboundLocalError: local variable 'a' referenced before assignment 当全局变量a存在时 //使用glboal关键字，global关键字申明变量a是全局变量 a = 10 def func(): global a #申明要修改全局的变量a a += 1 func() print(a) 结果： 11 当全局变量a不存在时 //使用global关键字申明并创建全局变量a def func(): global a a = 10 func() print(a) 结果： 10 当变量在全局存在时globla就是申明我要修改全局变量，并且会在局部开辟同名变量 当变量在全局不存在时global就是申明要在全局创建一个变量 10.2 nonlocal nonlocal 1.nonlocal只修改局部空间中的变量,最外层的一个函数 2.只修改离nonlocal最近的一层,如果这一层没有就往上一层查找,只能在局部 3.nonlocal 不能进行创建 nonlocal只修改局部空间中的变量，最外层的一个函数 只修改离nonlocal最近的一层函数，如果这一层没有就往上一层查找，只能在局部 //代码示例1 def func(): a = 10 def foo(): a = 20 def f(): nonlocal a a += 1 print(a) f() foo() func() 结果： 21 //代码示例2 a = 15 def func(): a = 10 def foo(): def f(): nonlocal a a += 1 print(a) #11 print(a) #10 f() foo() print(a) #11 func() print(a) #15 结果： 10 11 11 15 global与nonlocal结合示例 a = 10 def func(): a = 5 def foo(): a = 3 def f(): nonlocal a a += 1 def aa(): a = 1 def bb(): global a a += 1 print(a) #11 bb() print(a) #1 aa() print(a) #4 f() print(a) #4 foo() print(a) #5 func() print(a) #11 结果： 11 1 4 4 5 11 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/12.python基础十二 迭代器、生成器.html":{"url":"python/python基础/12.python基础十二 迭代器、生成器.html","title":"python基础十二 迭代器、生成器","keywords":"","body":"[toc] python基础十二 迭代器、生成器 1.迭代器 1.1 含义 一个一个取值 1.2 可迭代对象 #Python中规定,只要是具有__iter__()方法就是可迭代对象 str.__iter__() list.__iter__() tuple.__iter__() dict.__iter__() set.__iter__() ⚠️可迭代对象能够重复取值 1.3 迭代器使用示例 //使用示例1 将迭代器赋值给一个变量，这样就能重复取值了 可迭代对象能够重复取值 lst = [1,1,2] 将可迭代对象转换成迭代器 l = lst.__iter__() print(l) #结果：，迭代器的内存空间地址 print(l.__next__()) #结果：1 print(l.__next__()) #结果：1 print(l.__next__()) #结果：2 print(1.__next__()) #结果：StopIteration 停止迭代，不能超过元素个数 ⚠️有多少个元素就只能next多少次 //使用示例1 单独执行迭代器，这样每次只能取第一个值 lst = [1,1,2,4,5] ⚠️以下两个lst.__iter__()迭代器内存地址，mac中显示的是一样，win本有的一样，有的不一样！！！ print(lst.__iter__()) # print(lst.__iter__()) # print(lst.__iter__().__next__()) #结果：1 #lst.__iter__() 是一个迭代器1 print(lst.__iter__().__next__()) #结果：1 #lst.__iter__() 是一个迭代器1 ⚠️这里迭代器是多个 1.4 for循环本质 ⚠️⚠️⚠️for循环就是一个迭代器 s = \"hehe\" for i in s: print (i) h e h e s = \"hehe\" s1 = s.__iter__() while True: print (s1.__next__()) 结果会报错如下：停止迭代 StopIteration s = \"hehe\" s1 = s.__iter__() while True: try: #尝试着运行一下缩进体中的内容，如果运行有问题用except接收一下 print (s1.__next__()) except StopIteration: break 结果如下： h e h e 1.5 python3中迭代器用法 //python1和python2中迭代器共同用法 lst = ['a',1,2,4,5] print (iter(lst)) print (iter(lst)) print (iter(lst).__next__()) print (iter(lst).__next__()) 结果如下： a a //python1中支持__iter__()方法 //python1中不支持 __next__()方法 2.生成器 2.1 生成器含义 控制循环迭代行为，一边循环一边计算的特殊程序 例如，创建一个包含100万元素的列表，不仅占空间。而且如果只访问前边几个元素，后续的元素空间就白白浪费了，生成器可以根据规则生成后续元素 2.2 生成器定义 1.基于函数实现的生成器 2.表达式实现生成器 2.3 生成器本质 生成器的本质就是一个迭代器 迭代器：文件句柄，通过数据转换，python自带提供 生成器：程序员自己实现 2.4 生成器使用示例 2.4.1 定义及创建生成器 #定义一个函数 def func(): print (1) return 5 print (func()) 1 5 ⚠️函数题中存在关键字yield就是定义一个生成器 #定义一个生成器 def func(): print (1) yield 5 print (func()) #⚠️这一步才算是创建一个生成器对象 2.4.2 语法及词法 ⚠️代码执行的时候有多个对象在工作 语法检查 词法检查 //示例1 def func(): print (foo) 返回结果为空 原因： 函数没有被调用，因此不报错 //示例1 def func(): if 2 > 1 结果： SyntaxError: invalid syntax 原因： 首先进行语法检查，语法错误 //示例2 def func(): foo() func() 结果： 报错,语法检查没有问题，但是词法检查有问题 2.4.3 生成器使用 2.4.3.1 生成器使用示例1 ⚠️⚠️⚠️生成器最大特点：惰性机制 //示例1 def func(): yield 1 #记录执行位置，当第一次next的时候记录，第二次next的时候就开始从下边取值 yield 1 yield 2 g = func() #获取的是生成器的内存地址 print (next(g)) #取值 1 print (next(g)) #取值 1 print (next(g)) #取值 2 print (next(g)) #取值 会报错 StopIteration 2.4.3.2 生成器使用示例2 //示例1 def func(): yield 1 #记录执行位置，当第一次next的时候记录，第二次next的时候就开始从下边取值 yield 1 yield 2 g = func() g1 = func() g1 = func() print (g) print (g1) print (g1) print (next(func())) #这是一个生成器1 print (next(func())) #这是一个生成器1 print (next(func())) #这是一个生成器2 1 1 1 ⚠️⚠️⚠️yield和return部分功能很像 def func(): yield print(next(func())) None #yeild后边不写内容返回的是None 2.4.3.3 生成器使用示例3 //示例2 def func(): #1 def foo(): #1 print (1) #2 yield foo #4 g = func().__next__() #5 print (g) #结果 .foo at 0x7fdb08082400> print (g()) #结果 1 None print (type(g)) #结果 🐷示例2执行过程 第一步，定义一个函数func()： 第1行 第二步，执行第5行的func()，创建一个生成器 第三步，执行第5行func().__next__()，进行去值，获取的是生成器的内存空间地址 第四步，执行第1、2行的foo函数，只是定义一个foo函数，没有实际调用 第五步，执行第4行的yield foo，yield foo获取的是foo函数的内存空间地址，并且返回给变量g 此时g就是foo函数的内存地址 第六步，print (g)，返回的是foo的函数内存地址 第七步，print (g())，g() == foo()，返回1，函数体中默认返回None 第八步，print (type(g))，返回的是foo函数的类型 2.4.3.4 生成器使用示例4 //示例4 def func(): yield 1,1,2,4,5 print (112) yield 1111 yield 666 g = func() print (next(g)) #结果 (1, 1, 2, 4, 5) print (next(g)) #结果 112 1111 2.5 时间 空间 2.5.1 空间换时间 //概念 例如，一个列表要产生50000个元素，一次性创建，这样就是用了空间换了时间 用一次性生成占用大量空间的元素换取快速读取时间 2.5.2 时间换空间 //概念 例如，还是一个列表创建50000个元素，这次不一次性创建，而是创建一个监控者，用一个元素由监控者取一个，这样就节省了大量内存空间，但是需要读取的时间变长 //代码示例 def func(): for i in range(1,50001): yield i g = func() print (next(g)) #结果1 print (next(g)) #结果1 这样就是需要一个然后读取一个，节省了内存空间地址，但是需要消耗大量时间 2.6 yield from yield from 逐个返回对象 //示例1 将元素整体返回 def func(): yield [1,1,2,4,5] #将元素整体返回 g = func() print (next(g)) [1, 1, 2, 4, 5] //示例1 将元素逐个返回 def func(): yield from [1,1,2,4,5] g = func() print (next(g)) print (next(g)) print (next(g)) 1 1 2 //示例2 多个yield，逐个返回一个yield的元素再返回下一个yield的 def func(): yield from [1,1,2] yield from ['a','b','c','d','e'] g = func() print (next(g)) print (next(g)) print (next(g)) print (next(g)) 1 1 2 a #yield总结 yield 能返回多个,以元组的形式存储 yield 能返回各种数据类型(Python的对象) yield 能够写多个并且都执行 yield 能够记录执行位置 yield 后边不写内容 默认返回None yield 都是将数据一次性返回 3.区分迭代器和生成器及迭代对象说明 #方法1 具有send()方法的就是一个生成器 #方法1 查看内存地址 //示例 lst = [1,1,2] print (lst.__iter__()) #结果 def func(): yield 1 print (func()) #结果 ⚠️⚠️⚠️ 生成器一定是一个迭代器，但是迭代器不一定是一个生成器 //迭代对象 具有__iter__()方法的就是一个可迭代对象 //迭代器: 具有__iter__() 和 __next__()方法就是一个迭代器 //生成器: 基于函数创建的生成器,函数体中必须存在yield 4.迭代器和生成器优缺点总结 #优点 节省空间 #缺点 1.不能直接使用元素 1.不能直观查看元素的个数 2.使用不灵活 4.稍微消耗时间 5.一次性执行，不能逆行执行 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/14.python基础十四 模块.html":{"url":"python/python基础/14.python基础十四 模块.html","title":"python基础十四 模块","keywords":"","body":"[toc] python基础十四 模块 重点 ##os模块 #文件夹相关 os.makedirs('dirname1/dirname2') 可生成多层递归目录 *** os.removedirs('dirname1') 若目录为空，则删除，并递归到上一级目录，如若也为空，则删除，依此类推 *** os.mkdir('dirname') 生成单级目录；相当于shell中mkdir dirname *** os.rmdir('dirname') 删除单级空目录，若目录不为空则无法删除，报错；相当于shell中rmdir dirname *** #文件相关 os.remove() 删除一个文件 *** os.rename(\"oldname\",\"newname\") 重命名文件/目录 *** #路径相关 os.path.abspath(path) 返回path规范化的绝对路径 *** os.path.split(path) 将path分割成目录和文件名二元组返回 *** os.path.exists(path) 如果path存在，返回True；如果path不存在，返回False *** os.path.isfile(path) 如果path是一个存在的文件，返回True。否则返回False *** os.path.isdir(path) 如果path是一个存在的目录，则返回True。否则返回False *** os.path.join(path1[, path2[, ...]]) 将多个路径组合后返回，第一个绝对路径之前的参数将被忽略 *** os.path.getsize(path) 返回path的大小 *** #sys模块 sys.path 返回模块的搜索路径，初始化时使用PYTHONPATH环境变量的值 *** 1. 模块介绍 py文件就是一个模块 2. 模块分类 2.1 内置模块 内置模块也叫标准库 2.2 第三方模块 别人写好的功能 2.3 自定义模块 自己写的特定功能 3. 模块作用 1.开发效率高，有大量的内置函数和模块 2.拿来主义，大量的第三方模块，拿来既能用，不需要知道原理 3.减少重复代码，分文件管理，有助于修改和维护 4. 模块使用 4.1 语法 import 模块名 4.2 模块的两种用法 1.当作普通模块执行 2.被当作脚本执行 5. 自定义模块 自定义模块使用示例 //非同级目录示例 1.在当前需要调用模块的py文件test.py的不同路径新建一个文件i.py，一会test.py调用这个文件i.py 2.在/Users/baixuebing/Desktop下创建一个i.py文件，文件内如下 print(\"非同级目录自定义模块路径导入练习\") a = 10 b = 20 3.test.py文件内容如下 import sys #需要导入sys模块 sys.path.append('/Users/baixuebing/Desktop') #这里写要导入的自定义模块的路径 from i import a,b #自定义模块名是i，导入a,b两个功能 print(a,b) #打印功能 非同级目录自定义模块路径导入练习 10 20 //同级目录示例 说明：test.py文件与自定义模块i.py在同一个路径下 from i import a,b print(a,b) 同级目录自定义模块导入练习 10 20 5.1 自定义模块特殊说明 应用场景： 1.现在有一个自定义模块i.py，文件内容如下 def login(): print('登陆') def register(): print('注册') print('自定义模块保留自定义内容不被调用') print(123456) 调用者文件test.py，文件内容如下 import sys from i import login login() 2.导入这个自定义模块并执行，返回结果如下，但是自定义模块中调用函数的123456不想被调用，需要做一下特殊处理，让调用者只能看到 ’自定义模块保留自定义内容不被调用‘、登陆、注册，不能看到123456 自定义模块保留自定义内容不被调用 123456 3.修改自定义模块i.py，在a()和b()的上方加入一行代码 if __name__ == \"__main__\": def a(): print('登陆') def register(): print('注册') print('自定义模块保留自定义内容不被调用') if __name__ == \"__main__\": #不可外传功能 print(123456) 4.调用者再次执行，结果如下，结果123456无法获取 自定义模块保留自定义内容不被调用 登陆 🌟🌟🌟 原因： 在当前文件i.py中执行__name__获取的值是'__main__' 当前文件i.py被当作模块导入时，__name__获取的是当前文件名 6. 模块导入 6.1 模块导入过程 1.将模块存储到当前名称空间中，可以使用globals查看 2.以模块的名字命名，并开辟一个独立空间 3.通过模块名来使用模块中的功能 6.2. 导入说明 6.2.1 全部导入 import 模块名 是将模块中所有的内容全部倒入 6.2.2 导入部分 from 模块名 import 功能1，功能2 从模块中导入部分功能 from 模块名 import 功能1 as 别名 6.3 模块导入注意点⚠️ 1.同一个模块，写多次，只执行一次 6.4 模块导入顺序 内存 --> 内置 --> sys.path 7. 模块路径 模块与py文件在同一路径可以直接调用 模块与py文件不在同一路径，需要导入sys模块及导入模块路径 import sys sys.path.append('路径') 8. 模块查找顺序 内存 --> 内置 --> sys.path 9. time模块 9.1 时间分类 9.1.1 时间戳 用于计算 通常的叫法,时间戳表示的是格林尼治时间是从1970年1月1日00:00:00开始按秒计算的偏移量。这个是实时变化的。我们运行“type(time.time())”，返回的是float类型 #获取时间戳 import time print(time.time()) 1569543159.206561 9.1.2 结构化时间 用于程序员获取数据结构 命名元组 #获取结构化时间 import time t = time.time() #获取时间戳 print(time.localtime(t)) #将时间戳转换为结构化时间 time.struct_time(tm_year=2019, tm_mon=9, tm_mday=27, tm_hour=8, tm_min=22, tm_sec=41, tm_wday=4, tm_yday=270, tm_isdst=0) 9.1.3 字符串时间 给用户查看的 python中时间日期格式化符号： %y 两位数的年份表示（00-99） %Y 四位数的年份表示（000-9999） %m 月份（01-12） %d 月内中的一天（0-31） %H 24小时制小时数（0-23） %I 12小时制小时数（01-12） %M 分钟数（00=59） %S 秒（00-59） %a 本地简化星期名称 %A 本地完整星期名称 %b 本地简化的月份名称 %B 本地完整的月份名称 %c 本地相应的日期表示和时间表示 %j 年内的一天（001-366） %p 本地A.M.或P.M.的等价符 %U 一年中的星期数（00-53）星期天为星期的开始 %w 星期（0-6），星期天为星期的开始 %W 一年中的星期数（00-53）星期一为星期的开始 %x 本地相应的日期表示 %X 本地相应的时间表示 %Z 当前时区的名称 %% %号本身 #获取字符串时间 import time print(time.strftime(\"%Y-%m-%d %X\")) 2017-09-27 08:18:32 9.2 三种时间相互转换 9.2.1 字符串时间结构化时间 import time str_time = \"2019-9-1 12:23:06\" print(time.strptime(str_time,\"%Y-%m-%d %H:%M:%S\")) # 将字符串时间转换成结构化时间 time.struct_time(tm_year=2019, tm_mon=9, tm_mday=1, tm_hour=12, tm_min=23, tm_sec=6, tm_wday=6, tm_yday=244, tm_isdst=-1) t = time.localtime() #先获取结构化时间 print(time.strftime(\"%Y-%m-%d %H:%M:%S\",t)) #将结构化时间转换成字符串时间 9.2.2 时间戳结构化时间 import time t = time.time() #获取时间戳 print(t) 1569543714.778055 print(time.localtime(t)) #将时间戳转成结构化时间 time.struct_time(tm_year=2019, tm_mon=9, tm_mday=27, tm_hour=8, tm_min=22, tm_sec=41, tm_wday=4, tm_yday=270, tm_isdst=0) t = time.localtime() #先获取结构化时间 print(time.mktime(t)) #将机构化时间转换成时间戳 1569564152.0 9.2.3 字符串时间时间戳 需要先转换为中间人结构化时间再转换，不能直接转化 字符串时间结构化时间时间戳 10. datetime模块 # import datetime ⚠️此写法不正确 1.获取当前时间，是一个对象，不是字符串 from datetime import datetime print(datetime.now()) 2017-10-03 18:58:09.455678 print(type(datetime.now())) 2.自定义时间 print(datetime(2016,11,11,11,11,11)) 2016-11-11 11:11:11 3.时间戳转换成对象 import time print(datetime.fromtimestamp(time.time())) 2017-10-07 18:27:43.683452 4.将对象转换成时间戳 print(datetime.timestamp(datetime.now())) 1570444141.19223 5.将对象转换成字符串 print(datetime.strftime(datetime.now(),\"%Y-%m-%d %H:%M:%S\")) 2017-10-07 18:29:50 6.将字符串转换成对象 print(datetime.strptime(\"2016/11/11\",\"%Y/%m/%d\")) 2016-11-11 00:00:00 7.对象时间与自定义时间做运算 print(datetime.now() - datetime(9999,11,1,12,13,14)) -2914660 days, 6:20:23.854611 8.计算从当前对象时间到某一天 from datetime import datetime,timedelta print(datetime.now()) print(datetime.now() - timedelta(days=1)) print(datetime.now() + timedelta(days=1)) 2017-10-07 18:37:18.013572 2017-10-06 18:37:18.013611 2017-10-08 18:37:18.013628 11. random模块 随机数 1.获取0-1之间的小数 import random random.random() #0-1之间的小数 2.获取范围的随机整数 random.randint(1,5) #获取1-5之间的整数 3.获取范围内的随机偶数 random.randrange(0,10,2) 4.获取列表中单个元素 lst = [1,2,3,4,5,6] random.choice(lst) 5.获取列表中指定个数元素 lst = [1,2,3,4,5,6] random.choices(lst,k=5) #随机获取5个数，会出现重复元素 random.sample(lst,k=5) #随机获取5个数，不会出现重复元素 6.打乱顺序 lst = [1,2,3,4,5,6,7,8] random.shuffle(lst) 12. sys模块 与python解释器做交互 1.获取系统路径 import sys sys.path 2.获取系统平台 sys.platform windowns --> win32 mac --> darwin 3.获取当前py文件路径 sys.argv #返回列表 4.获取python解释器版本 sys.version 5.退出码 sys.exit(1) #0是正常退出，可以修改 6.获取所有模块 sys.modules() 13. os模块 13.1 文件 1.修改文件名 import os os.rename(\"旧名\",\"新名\") 2.删除文件 os.remove(\"要删除的文件名\") 13.2 文件夹 1.创建文件夹 os.makedirs('a/b/c') #递归创建 os.mkdir('a') #单独创建 2.删除文件夹 os.removedirs('a') #递归删除 os.rmdir('a') #删除文件夹 3.查看文件夹 os.listdir() #查看当前路径下所有的文件 13.3 路径 1.获取路径 os.getcwd() #获取当前工作路径 2.切换路径 os.chdir() #切换路径 3.获取当前路径 os.curdir() 4.获取上级目录 os.pardir() 5.获取绝对路径 ⚠️重要 注意是当前路径下的文件或者文件夹 os.path.abspath('当前文件或者文件夹名') 6.路径分割 ⚠️只分一刀 获得的是元组 os.path.split('路径') 7.父级目录 ⚠️⚠️⚠️ 🌟 os.path.dirname('路径') 8.获取路径最外层 os.path.basename('路径') 9.将多个路径组合后返回，第一个绝对路径之前的参数将被忽略 🌟🌟🌟 os.path.join('路径1','路径2'.'路径3') 10.获取文件大小 os.path.getsize() is系列 1.判断路径是否存在 返回布尔值 os.path.exists() 2.判断是否是绝对路径 返回布尔值 os.path.isabs() 3.判断是否是存在的文件 os.path.isfile('绝对路径') 4.判断是否是目录 os.path.isdir() 13.4 其他 1.执行shell命令 os.system() os.popen('dir').read() 2.获取环境变量 os.environ() 14.hashlib模块 14.1 含义 摘要算法，加密算法 14.2 功能 加密，校验一致性 14.3 加密说明 1.内容相同，密码一定相同 2.加密的密文是不可逆的 3.明文-->字节-->密文 4.加密方法 md5、sha1、sha256、sha512 14.4 加密示例 import hashlib #导入hashlib模块 s = \"123abc\" md5 = hashlib.md5() #选择加密的方式，初始化一个加密 md5.update(s.encode(\"utf-8\")) #将要加密的内容添加到变量md5中 print(md5.hexdigest()) #进行加密 14.5 加盐 14.5.1 固定加盐 user = input(\"user:\") pwd = input(\"pwd:\") import hashlib md5 = hashlib.md5(\"abc\".encode(\"utf-8\")) #这里的abc就是盐 md5.update(pwd.encode(\"utf-8\")) print(md5.hexdigest()) 14.5.1 动态加盐 user = input(\"user:\") pwd = input(\"pwd:\") import hashlib md5 = hashlib.md5(user.encode(\"utf-8\")) #这里的user就是动态盐 md5.update(pwd.encode(\"utf-8\")) print(md5.hexdigest()) 15.collections模块 15.1 统计 🌟🌟🌟 //示例 统计列表中每个元素出现的次数 #方法1 for循环 lst = [1,1,2,5,6,7,7,9,99,99,1,3,7] dic = {} for i in lst: dic[i] = lst.count(i) print(dic) {1: 3, 2: 1, 5: 1, 6: 1, 7: 3, 9: 1, 99: 2, 3: 1} #方法2 collections记数 lst = [1,1,2,5,6,7,7,9,99,99,1,3,7] from collections import Counter print(dict(Counter(lst))) {1: 3, 2: 1, 5: 1, 6: 1, 7: 3, 9: 1, 99: 2, 3: 1} 15.2 有序字典 🌟🌟🌟 python2中使用 from collections import OrderedDict a = OrderedDict({\"key1\":1,\"key2\":2}) print(a) print(a[\"key1\"]) OrderedDict([('key1', 1), ('key2', 2)]) 1 15.3 默认字典 from collections import defaultdict dic = defaultdict(list) #括号中写不加括号的方法 例如列表只写list，不写list()，这里字典默认值就是空列表 dic['k1'].append(10) #指明字典的值为10 dic['k2'] #不指定字典的值，默认是空列表 print(dic) defaultdict(, {'k1': [10], 'k2': []}) //示例题 将列表中元素大于66的追加到字典的key1中，小于66的追加到字典的key2中 #方法1 lst = [11,22,33,44,55,77,88,99] dic = {\"key1\":[],\"key2\":[]} for i in lst: if i > 66: dic['key2'].append(i) else: dic['key1'].append(i) print(dic) {'key1': [11, 22, 33, 44, 55], 'key2': [77, 88, 99]} #方法2 lst = [11,22,33,44,55,77,88,99] dic = {} for i in lst: if i > 66: dic.setdefault('key2',[]).append(i) else: dic.setdefault('key1', []).append(i) print(dic) {'key1': [11, 22, 33, 44, 55], 'key2': [77, 88, 99]} #方法3 from collections import defaultdict lst = [11,22,33,44,55,77,88,99] dic = defaultdict(list) for i in lst: if i > 66: dic['key2'].append(i) else: dic['key1'].append(i) print(dic) defaultdict(, {'key1': [11, 22, 33, 44, 55], 'key2': [77, 88, 99]}) 15.4 双端队列 1.队列 先进先出 deque 🌟🌟🌟 from collections import deque lst = deque([11,22,33,44,55]) lst.appendleft(00) #从左侧开始追加 print(lst) deque([0, 11, 22, 33, 44, 55]) lst.popleft() #从左侧开始删除 print(lst) deque([22, 33, 44, 55]) 2.栈 先进后出 lst = [] lst.append(1) lst.append(2) lst.append(3) print(lst) lst.pop() print(lst) lst.pop() print(lst) lst.pop() 15.5 命名元组 namedtuple 🌟🌟🌟 from collections import namedtuple dg = namedtuple('name',[\"aa\",\"bb\",\"cc\"]) print(dg('hehe','haha','xixi')) name(aa='hehe', bb='haha', cc='xixi') 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/15.python基础十五 闭包.html":{"url":"python/python基础/15.python基础十五 闭包.html","title":"python基础十五 闭包","keywords":"","body":"[toc] python基础十五 闭包 1.闭包 1.1作用 保护数据安全，保护数据干净性 1.2 定义 1.在嵌套函数内，使用非全局变量(且不使用本层变量) 2.将嵌套函数返回 ⚠️不能加函数的() 1.3 闭包注意点⚠️ 1.⚠️⚠️⚠️ 没有将嵌套的函数返回也是一个闭包，但是这个闭包不能使用！！！ 2.闭包不能传可变类型数据 1.4 示例 //闭包示例1 def func(): a = 10 #自由变量 def foo(): print(a) return foo f = func() print(f.__closure__) #验证是否是闭包 (,) ⚠️在嵌套函数内，使用非全局变量(且不使用本层变量) //示例2 未返回嵌套函数值，虽然是一个闭包，但是不能使用 def func(): a = 10 def foo(): print (a) print (foo.__closure__) #(,) func() //示例3 def func(): a = 10 def foo(): print (a) return foo func()() #⚠️此时func() == foo == foo() func()() f = func() f() //示例4 ⚠️⚠️⚠️函数在接受参数的时候会在下边定义参数的变量 因此，这是一个闭包 def wrapper(a,b): #a = 2 ⚠️函数接受参数就相当于在函数中再次定义这个变量 #b = 3 ⚠️函数接受参数就相当于在函数中再次定义这个变量 def inner(): print (a) print (b) return inner a = 2 b = 3 ret = wrapper(a,b) print(ret.__closure__) #(, ) > 说明，虽然变量a和b是在wrapper同级定义的，但是wrapper中是接受ab两个参数的，会在wrapper下级定义ab两个变量 1.5 应用场景 1.装饰器 2.防止数据被误改动 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/16.python基础十六 推导式.html":{"url":"python/python基础/16.python基础十六 推导式.html","title":"python基础十六 推导式","keywords":"","body":"[toc] python基础十六 推导式 1.推导式 1.1 作用 做一些有规律的数据结构 1.2 列表推导式 普通循环 [加工后的变量 for循环] 筛选模式 [加工后的变量 for循环 条件] 1.2.1 普通循环 [变量 for循环] //代码示例1 要循环输出1-10 原先的代码 lst = [] for i in range(1,11): lst.append(i) print(lst) [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 使用列表推导式 lst = [ i for i in range(1,11)] print (lst) [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] //代码示例2 将1-10内的数的平方追加到列表中 原先代码 l1 = [] for i in range(1,11): i *= i l1.append(i) print (l1) [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 使用列表推导式 l1 = [i*i for i in range(1,11)] print(l1) [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 1.2.2 筛选模式 [加工后的变量 for循环 条件] //代码示例1 找出30以内可以被3整除的数 原先的代码 lst = [] for i in range(1,31): if i % 3 == 0: lst.append(i) print (lst) [3, 6, 9, 12, 15, 18, 21, 24, 27, 30] 列表推导式 f = [i for i in range(1,31) if i % 3 == 0] print (f) ⚠️推导式外边加括号就是返回生成器的内存空间地址 f = (i for i in range(1,31) if i % 3 == 0) print (f) at 0x7fa7281a7d00> 1.3 字典推导式 普通循环 筛选模式 1.3.1 普通循环 print({i:i+1 for i in range(3)}) {0: 1, 1: 2, 2: 3} 1.3.2 筛选模式 print({i:i+1 for i in range(3) if i > 1}) {加工后的变量:加工的后的变量 for循环 加工条件} {2: 3} 1.4 集合推导式 普通循环 筛选模式 1.4.1 普通循环 print({i for i in range(3)}) {0, 1, 2} 1.4.2 筛选模式 print({i for i in range(3) if i > 1}) {2} 1.5 生成器推导式 普通循环 筛选模式 1.5.1 普通模式 g = (i for i in range(3)) print(next(g)) print(next(g)) 0 1 1.5.2 筛选模式 g = (i for i in range(3) if i+1 == 2) print (next(g)) 1 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/17.python基础十七 匿名函数.html":{"url":"python/python基础/17.python基础十七 匿名函数.html","title":"python基础十七 匿名函数","keywords":"","body":"[toc] python基础十七 匿名函数 1. 定义 示例：给函数传两个参数并计算和 f = lambda a,b:a+b 其中 a,b表示形参，可以传多个，冒号后边的表示函数体中要执行的代码 2. lambda函数简单示例说明 //代码示例 给函数传两个参数并计算和 普通函数写法 def func(a,b): c = a + b return c print(func(1,2)) 3 匿名函数写法1 f = lambda a,b:a+b print(f(1,2)) 3 匿名函数写法2 print((lambda a,b:a+b)(1,2)) 3 3. lambda函数与普通函数对比说明 普通函数写法 def func(a,b): c = a + b return c print(func(1,2)) 匿名函数写法 print((lambda a,b:a+b)(1,2)) 1.lambda和def是一样的 2.lambda中的 a,b 和def中的（a,b）是一样的 3.lambda中的 a+b 和def中的 return a + b 是一样的 4.lambda中a,b是形参，a+b是返回值，即冒号前边的是形参，冒号后边的返回值 形参：可以接受位置参数、动态位置参数、默认参数、动态关键字参数 返回值：只能返回一个数据，如果想返回多个数据，需要用()括起来 4. 匿名函数风骚走位 4.1 lambda+列表 示例1 //示例1 这种写法结果是3个函数地址 print([lambda i:i+1 for i in range(3)]) [. at 0x7f9bf0068620>, . at 0x7f9bf00681e0>, . at 0x7f9bf0068048>] 拆分写法 lst = [] for i in range(3): def func(i): return i+1 lst.append(func) print(lst) [, , ] 示例1进阶版 //错误写法示例 g = [lambda i:i+1 for i in range(3)] #lambda后边的i与for循环中的i没有关系 print([em() for em in g]) 结果报错，因为lambda后的i是形参，em()没有传参，因此报错 拆分写法 lst = [] for i in range(3): def func(i): return i+1 lst.append(func) new_lst = [] for em in lst: new_lst.append(em()) #这里的em()就是func() 结果报错，因为em()没有传参 //正确写法示例 g = [lambda i:i+1 for i in range(3)] #lambda后边的i与for循环中的i没有关系 print([em(3) for em in g]) [4, 4, 4] 拆分写法 lst = [] for i in range(3): def func(i): return i+1 lst.append(func) new_lst = [] for em in lst: #此时lst = [func,func,func] new_lst.append(em(3)) #这里的em()就是func() print(new_lst) [4, 4, 4] 示例2 🦙🦙🦙这个题一般人能想到？？？ g = [lambda x:x*i for i in range(3)] for j in [2,10]: g1 = (em(3) for em in g) print([e+j for e in g1]) [16, 16, 16] 代码拆分 #g = [lambda x:x*i for i in range(3)]拆分如下 lst = [] #循环完后这里是3个函数 [func,func,func] for i in range(3): def func(x): return x*i lst.append(func) for j in [2,10]: #这里执行完后j就是10 def g1(): #生成器存放于g1中,先循环2，然后循环10，会覆盖 for em in lst: yield em(3) new_lst = [] for e in g1(): #g1()产生了一个生成器，一执行就触发for em in lst，lst是3个func，这里就是yield em(3)，执行3次func(3),就是执行3次return 3*2，因为i的for循环已经执行完成，最后的值i是2 new_lst.append(e+j) #6+10，循环3次 print(new_lst) [16, 16, 16] 4.2 lambda+生成器 //示例1 g = (lambda i:i+1 for i in range(3)) #lambda后边的i与for循环中的i没有关系 print([em(3) for em in g]) [4, 4, 4] #代码解析 lambda i:i+1与for i in range(3)没有任何关系！！！ 只是借助for循环执行了3次 return i+1 em(3)就是给i传递了参数，因此执行3次 i+1 拆分写法 def foo(): for j in range(3): def func(i): return i+1 yield func g = foo() lst = [] for i in g: #这里的i就是func lst.append(i(3)) print(lst) [4, 4, 4] //示例2 g = [lambda :i+1 for i in range(3)] print([em() for em in g]) [3, 3, 3] #g = [lambda :i+1 for i in range(3)]拆分后如下 g = [] #循环3次后，列表中是3个func [func,func,func] for i in range(3): def func(): return i+1 g.append(func) #print([em() for em in g])拆分后如下 new_lst = [] for em in g: #这里的g是[func,func,func] new_lst.append(em()) #em()就是调用函数func()，因为上边的for循环已经执行完成了，因此return i+1就是2+1=3,所以这里追加3次func()，func()就是执行return 2+1，所以结果是3次3 print(new_lst) 4.3 lambda+列表与lambda+生成器对比 //不传参示例 g = [lambda :i+1 for i in range(3)] print([em() for em in g]) [3, 3, 3] g = (lambda :i+1 for i in range(3)) print([em() for em in g]) [1, 2, 3] //传参示例 g = [lambda x:x*i for i in range(3)] print([em(3) for em in g]) [6, 6, 6] g = (lambda x:x*i for i in range(3)) print([em(3) for em in g]) [0, 3, 6] 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/18.python基础十八 内置函数.html":{"url":"python/python基础/18.python基础十八 内置函数.html","title":"python基础十八 内置函数","keywords":"","body":"[toc] python基础十八 内置函数 内置函数 1. 含义 python帮助我们写了很多的功能供使用 2. 了解函数 all() any() bytes() callable() chr() complex() divmod() eval() exec() frozenset() globals() hash() help() id() input() int() iter() locals() next() oct() ord() pow() repr() round() 2.1 all 判断元素是否都为True，全部为True才返回True print(all(['a','b','c'])) True print(all(['a','b','c',0])) False 2.2 any 判断元素中是否含有True，有一个True即为True print(any(['a','b','c',0])) True 2.3 bytes 字节串 print(bytes('呵呵',encoding='utf-8')) b'\\xe5\\x91\\xb5\\xe5\\x91\\xb5' 2.4 callable 判断是否可调用，返回布尔值 print(callable([1,1])) False def func(): pass print(callable(func)) True 2.5 chr 根据当前编码（unicode，兼容所有）查对应的内容 print (chr(15678)) 摎 2.6 ord 查看内容对应的编码 print (ord('摎')) 15678 2.7 complex 复数 print(complex(10)) (10+0j) 2.8 divmod 获取的是元组，第一个是商，第二个是余数 print(divmod(10,3)) (3, 1) 2.9 hash 查看内容是否可哈希 print(hash(111)) #结果111 print(hash([1,1,3])) #结果报错TypeError: unhashable type: 'list' 2.10 help 查看帮助 print (help(str)) 2.11bin 十进制转换成二进制 print(bin(10)) 0b1010 2.12 oct 十进制转换成八进制 print(oct(10)) 0o11 2.13 hex 十进制转换成十六进制 print(hex(10)) 0xa 2.14 int 其他进制转换为十进制 int(\"xxx\",16) 十六进制转换为10进制 int(\"xxx\",8) 八进制转换为10进制 2.15 pow 幂 print(pow(3,1)) 9 2.16 repr 显示数据类型 原形必漏 s = \"113\" s1 = 113 print(repr(s)) #原形必漏 '113' print(s1) #113 2.17 round 保留小数位，默认取值 print(round(1.431341314,3)) 2.18 frozenset 冻结集合 //创建可变集合 s={'a','b'} print(s) {'a', 'b'} 修改集合 s.add('c') print(s) {'a', 'b', 'c'} //创建不可变集合 s=frozenset('abc') print(s) frozenset({'b', 'c', 'a'}) print(type(s)) 尝试修改不可变集合 s.add('d') print(s) 结果报错 AttributeError: 'frozenset' object has no attribute 'add' 2.19 eval ⚠️禁用 eval会将字符串转成表达式并执行，比较危险 这样就可以利用执行系统命令，执行删除文件等操作，因此禁用 //代码示例1 eval函数会将字符串转换成表达式执行 msg = \"1+2+3\" print(eval(msg)) 6 //代码示例2 危险用法，可以执行用户输入的任何内容 如果用在了用户评论中，则用户评论输入的内容就会执行，例如输入死循环、删除等操作，非常危险 msg = \"input('>>>')\" print(eval(msg)) 2.20 exec ⚠️禁用 比较危险，msg中的任何代码都会执行 //代码示例 msg = \"\"\" def func(): print(\"这么牛逼\") func() \"\"\" print(exec(msg)) 这么牛逼 None 3. 重点函数 enumerate() open() range() len() str() list() tuple() dict() set() print() sum() abs() dir() zip() format() reversed() filter() map() sorted() max() min() reduce() 3.1 abs 绝对值 不管是正数还是负数都是正数 print(abs(-6)) print(abs(6)) 6 6 3.2 format 格式转换 1.对齐方式 s = \"你好\" s1 = format(s,\">10\") s1 = format(s,\" 3.3 enumerate 枚举 enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。 s = ['a','b','c'] lst = list(enumerate(s)) print (lst) [(0, 'a'), (1, 'b'), (1, 'c')] lst = list(enumerate(s,start=1)) print (lst) [(1, 'a'), (3, 'b'), (4, 'c')] 3.4 sum 求和 print(sum([1,1,3,4])) 10 3.5 print 打印 1.文件流 f = open('a','a',encoding=\"utf-8\") print ('hehe',file=f) 会在当前py文件目录下创建文件a，文件a中的内容为hehe 2.修改print默认换行 //print默认有换行符 print ('a') print ('b') a b //修改print默认换行符 print ('a',end=\"|\") print ('b') a|b //修改print默认分隔符 print默认以空格分隔 print ('a','b') a b print ('a','b',sep='?') a?b 3.6 zip 拉链 lst1 = [1,2,3,4,5] lst2 = [5,4,3,2,1] 将lst1和lst2中下标相同的元素组成一个新的结构 //lambda+map写法 print (list(map(lambda x,y:(x,y),lst1,lst2))) [(1, 5), (2, 4), (3, 3), (4, 2), (5, 1)] //list+zip写法 lst1 = [1,2,3,4,5] lst2 = [5,4,3,2,1] print (list(zip(lst1,lst2))) [(1, 5), (2, 4), (3, 3), (4, 2), (5, 1)] //卧槽 lst1 = [1,2,3,4,5] lst2 = [5,4,3,2,1] print (dict(zip(lst1,lst2))) {1: 5, 2: 4, 3: 3, 4: 2, 5: 1} 4.高阶函数 filter() map() reduce() 以上3个函数必须有条件和参数！！！ max() min() sorted() 4.1 filter 🌟🌟🌟 过滤 筛选列表中数字大于5的 lst = [1,2,3,5,66,7,8,9] 原先代码 new_lst = [] for i in lst: if i > 5: new_lst.append(i) print (new_lst) [66, 7, 8, 9] filter高阶函数写法 //写法1 lst = [1,2,3,5,66,7,8,9] def func(x): return x > 5 print (list(filter(func,lst))) [66, 7, 8, 9] //写法2 lst = [1,2,3,5,66,7,8,9] print (list(filter(lambda x:x>5,lst))) [66, 7, 8, 9] 4.2 map 🌟🌟🌟 映射 映射，将可迭代对象中每个元素执行函数功能 //示例1 将列表中的元素转换成字符串 lst = [1,2,3,4,5] new_lst = [] for i in lst: new_lst.append(str(i)) print (new_lst) ['1', '2', '3', '4', '5'] 高阶函数写法 lst = [1,2,3,4,5] print (list(map(str,lst))) ['1', '2', '3', '4', '5'] //示例2 将两个列表中下标相同的元素相加 原先代码 lst1 = [1,2,3] lst2 = [3,2,1] for i in range(len(lst1)): print (lst1[i] + lst2[i]) 4 4 4 高阶函数写法 //写法1 lst1 = [1,2,3] lst2 = [3,2,1] def func(x,y): return x+y print (list(map(func,lst1,lst2))) [4, 4, 4] //写法2 lst1 = [1,2,3] lst2 = [3,2,1] print (list(map(lambda x,y:x+y,lst1,lst2))) [4, 4, 4] //写法3 当两个列表中的元素个数不同时 lst1 = [1,2,3,4] lst2 = [3,2,1] lst3 = [9,8,7,6,5] print (list(map(lambda x,y,z:x+y+z,lst1,lst2,lst3))) [13, 12, 11] ⚠️只相加最短的 4.3 reduce 🌟🌟🌟 累计算 lst = [1,2,3,4,5] from functools import reduce def func(x,y): return x+y print(reduce(func,lst)) 15 累计算过程 1.将1和2同时赋予x和y，此时x为3，将3赋予y,此时x为6，将4赋予y,此时x为10，将5赋予y，最后x和y相加 //lambda写法 lst = [1,2,3,4,5] from functools import reduce print (reduce(lambda x,y:x+y,lst)) 15 4.4 sorted 排序 用于排序 对以下列表排序 lst = [1,2,9,5,7,8,-6] //写法1 lst = [1,2,9,5,7,8,-6] lst.sort() #⚠️原地修改 print (lst) [-6, 1, 2, 5, 7, 8, 9] //写法2 lst = [1,2,9,5,7,8,-6] print (sorted(lst)) #⚠️新开内存空间 [-6, 1, 2, 5, 7, 8, 9] 对字符串进行排序 print (sorted('hehe,ggsimida')) #升序 [',', 'a', 'd', 'e', 'e', 'g', 'g', 'h', 'h', 'i', 'i', 'm', 's'] print (sorted('hehe,ggsimida',reverse=True)) #降序 ['s', 'm', 'i', 'i', 'h', 'h', 'g', 'g', 'e', 'e', 'd', 'a', ','] 高阶函数写法 按照长度进行排序 //写法1 lst = ['你好啊','呵呵','不好理解','啊'] print (sorted(lst,key=len)) ['啊', '呵呵', '你好啊', '不好理解'] ⚠️sorted这里要先写操作对象名，然后指定key，根据什么进行排序 //写法2 lst = ['你好啊','呵呵','不好理解','啊'] print (sorted(lst,key=lambda x:len(x))) ['啊', '呵呵', '你好啊', '不好理解'] 4.5 max() 最大 min()最小 print (max([1,2,3,5,6,-8])) 6 print (max([1,2,3,5,6,-8],key=abs)) #不管正负数，选择最大 dic = {'a':3,'b':2,'c':1} print (max(dic,key=lambda x:dic[x])) #按值排序拿到键 a print (min([1,2,3,5,6,-8])) 8 print (min([1,2,3,5,6,-8],key=abs)) #不管正负数，选择最小 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/19.python基础十九 序列化.html":{"url":"python/python基础/19.python基础十九 序列化.html","title":"python基础十九 序列化","keywords":"","body":"[toc] python基础十九 序列化 1. 含义 将一个数据类型转换成另一个数据类型 2. 分类 2.1 json 转换成字符串 dump load 用于文件写入存储 dumps(序列) loads(反序列) 用于网络传输 //dumps、loads用法 🌟列表 import json lst = [1,2,3] a = json.dumps(lst) #将列表转换为字符串 print(a) #[1, 2, 3] print(type(a)) # b = json.loads(a) #将字符串转换为列表 print(b) #[1, 2, 3] print(type(b)) # ⚠️列表中有中文 lst = ['呵呵','哈哈'] a = json.dumps(a) print(a) #[\"\\u5475\\u5475\", \"\\u54c8\\u54c8\"]，直接转中文有问题 加参数ensure_ascii=False解决 lst = ['呵呵','哈哈'] a = json.dumps(lst,ensure_ascii=False) print(a) #[\"呵呵\", \"哈哈\"] 🌟字典 import json dic = {\"key\":1,\"key2\":3} a = json.dumps(dic) #将字典转换成字符串 print(a) print(type(a)) {\"key\": 1, \"key2\": 3} b = json.loads(a) #将字符串转换为字典 print(b) print(type(b)) {'key': 1, 'key2': 3} print(json.loads(a)['key']) #字典取值 1 2.2 pickle 几乎支持python中所有的对象(不支持lambda) 转换成字节 pickle写入多行时自动带有换行 dump load 用于文件写入存储 dumps loads 用于网络传输 //将函数转换为字节 import pickle def func(): print(111) a = pickle.dumps(func) #将函数转换为字节 print(a) print(type(a)) b'\\x80\\x03c__main__\\nfunc\\nq\\x00.' b = pickle.loads(a) #将字节转换为函数 b() print(type(b)) 111 //将元组转换为字节 tu = (1,2,3,4,5) import pickle a = pickle.dumps(tu) #将元组转换为字节 print(a) b'\\x80\\x03(K\\x01K\\x02K\\x03K\\x04K\\x05tq\\x00.' b = pickle.loads(a) #将字节转换为元组 print(b) (1, 2, 3, 4, 5) 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/20.python基础二十 正则表达式.html":{"url":"python/python基础/20.python基础二十 正则表达式.html","title":"python基础二十 正则表达式","keywords":"","body":"[toc] python基础二十 正则表达式 1.正则表达式说明 正则就是用一些具有特殊含义的符号组合到一起去匹配相应的内容的方法，在python中，使用正则需要导入re模块正则表达式模式被编译成一系列的字节码，然后用c编写的匹配引擎执行 正则表达式元字符 元字符 匹配内容 \\w 匹配字母（包含中文）或数字或下划线 \\W 匹配非字母（包含中文）或数字或下划线 \\s 匹配任意的空白符 \\S 匹配任意非空白符 \\d 匹配数字 \\D 匹配非数字 \\A 从字符串开头匹配 \\z 匹配字符串的结束，如果是换行，只匹配到换行前的结果 \\n 匹配一个换行符 \\t 匹配一个制表符 ^ 匹配字符串的开始 $ 匹配字符串的结尾 . 匹配任意字符，除了换行符，当re.DOTALL标记被指定时，则可以匹配包括换行符的任意字符。 [...] 匹配字符组中的字符 ... 匹配除了字符组中的字符的所有字符 * 匹配0个或者多个左边的字符。 + 匹配一个或者多个左边的字符。 ？ 匹配0个或者1个左边的字符，非贪婪方式。 {n} 精准匹配n个前面的表达式。 {n,m} 匹配n到m次由前面的正则表达式定义的片段，贪婪方式 ab 匹配a或者b () 匹配括号内的表达式，也表示一个组 2.匹配模式 2.1字符串常用操作 s1 = 'python正则练习' print(s1.find('python')) 0 print(s1.find('正则')) 6 print(s1.find('abc')) -1 2.2正则匹配示例 \\w 匹配中文、字母、数字、下划线 import re s = 'python-正则表达式 练习_1.11' print(re.findall('\\w',s)) 结果： ['p', 'y', 't', 'h', 'o', 'n', '正', '则', '表', '达', '式', '练', '习', '_', '1', '1', '1'] \\W 和\\w相反，不匹配中文、字母、数字、下划线 import re s = 'python-正则表达式 练习_1.11' print(re.findall('\\W',s)) 结果： ['-', ' ', ' ', '.'] \\s 匹配任意空白符 import re s = 'python-正则表达式 练习_1.11' print(re.findall('\\s',s)) 结果，匹配到了练习前边的两个空格： [' ', ' '] \\d 匹配数字 import re s = 'python-正则表达式 练习_1.11' print(re.findall('\\d',s)) 结果： ['1', '1', '1'] \\D 与\\d相反，匹配非数字 import re s = 'python-正则表达式 练习_1.11' print(re.findall('\\D',s)) 结果： ['p', 'y', 't', 'h', 'o', 'n', '-', '正', '则', '表', '达', '式', ' ', ' ', '练', '习', '_', '.'] \\A与^ 以什么开头 import re s = 'python-正则表达式 练习_1.11' print(re.findall('\\Apython',s)) print(re.findall('^python',s)) 结果： ['python'] \\Z与$ 以什么结尾 import re s = 'python-正则表达式 练习_1.11' print(re.findall('1.11\\Z',s)) print(re.findall('1.11$',s)) 结果： ['1.11'] \\n 匹配换行符 import re s = 'python-正则表达式 \\n\\t练习_1.11' print(re.findall('\\n',s)) 结果： ['\\n'] \\t 匹配制表符 import re s = 'python-正则表达式 \\n\\t练习_1.11' print(re.findall('\\t',s)) 结果： ['\\t'] 3.匹配方式 . 匹配除换行符外的任意字符 import re s = 'python-正则表达式 \\n\\t练习_1.11' print(re.findall('.',s)) 结果： ['p', 'y', 't', 'h', 'o', 'n', '-', '正', '则', '表', '达', '式', ' ', ' ', '\\t', '练', '习', '_', '1', '.', '1', '1'] . 如果加了DOTALL标记就可以匹配任意字符 import re s = 'python-正则表达式 \\n\\t练习_1.11' print(re.findall('.',s,re.DOTALL)) 结果，可以看到加了DOTALL标志就匹配到了.原本无法匹配的换行符\\t ['p', 'y', 't', 'h', 'o', 'n', '-', '正', '则', '表', '达', '式', ' ', ' ', '\\n', '\\t', '练', '习', '_', '1', '.', '1', '1'] ? 匹配前边的元素出现0个或者1个 import re s = 'a-b-ab-abab-ababab-ababc' print(re.findall('ab?',s)) 结果，匹配到了全部的8个连续的ab，还可以匹配到一个a，因为b可以没有 ['a', 'ab', 'ab', 'ab', 'ab', 'ab', 'ab', 'ab', 'ab'] * 匹配前边的元素出现0个或者多个(贪婪匹配) import re s = 'a-b-ab-abab' print(re.findall('ab*',s)) 结果： ['a', 'ab', 'ab', 'ab'] + 匹配前边的元素出现1个或者多个(贪婪匹配) import re s = 'a-b-ab-abab' print(re.findall('ab+',s)) 结果： ['ab', 'ab', 'ab'] {n,m} 匹配前边的元素出现n到m个 import re s = 'aa-bb-aaabb-aaaaab' print(re.findall('aaa{1,3}',s)) 结果： ['aaa', 'aaaaa'] #其他匹配 {n} #前边的字符出现n次 {n,} #前边的字符最少出现n次 {,m} #前边的字符最多出现m次 .* 匹配任意内容0个或者多个 import re s = 'aa-bb-aaabb-aaaaab' print(re.findall('b.*',s)) 结果： ['bb-aaabb-aaaaab'] .? 匹配任意内容1个或者多个 import re s = 'ab-bb-aab-aaaaab' print(re.findall('a.?b',s)) 结果： ['ab', 'aab', 'aab'] [] 范围匹配 import re s = 'ab-bb-aab-123' print(re.findall('[a-z]',s)) 结果： ['a', 'b', 'b', 'b', 'a', 'a', 'b'] 与范围匹配相反，不匹配中括号中的内容 import re s = 'ab-bb-aab-123' print(re.findall('[^a-z]',s)) 结果： ['-', '-', '-', '1', '2', '3'] 4.常用方法 4.1findall 全部找到并返回一个列表 import re s = 'ab-bb-aab-abc-abbc' print(re.findall('abc',s)) ['abc'] 4.2search 从字符串任意位置进行匹配，查找到一个就停止 返回的是一个对象，获取匹配内容必须使用.group() import re s = 'ab-bb-abc-abc-abbc' print(re.search('abc',s)) 结果，如果不加.group()获取匹配内容，返回的是一个对象 #使用.group()方法获取匹配内容 print(re.search('abc',s).group()) abc 4.3match 从字符串开始位置进行匹配 import re s = 'ab-bb-abc-abc-abbc' print(re.match('ab ',s).group()) ab #以上示例中必须以a或者ab开始查看，否则会报错 print(re.match('bb ',s).group()) AttributeError: 'NoneType' object has no attribute 'group' 4.4split 分隔，可按照任意分隔符进行分隔 import re s = 'ab-bb-abc-abc-abbc' print(re.split('bb',s)) #以bb为分隔符 结果： ['ab-', '-abc-abc-a', 'c'] 4.5sub 替换 import re s = 'ab-bb-abc-abc-abbc' print(re.sub('bb','呵呵',s)) #将bb替换为呵呵 结果： ab-呵呵-abc-abc-a呵呵c 4.6compile 定义匹配规则 import re s = 'ab-123-abc-abc-a123bbc' obj = re.compile('\\d{2}') #定义匹配规则，数字出现2次 print(obj.findall(s)) 结果： ['12', '12'] 4.7finditer 返回一个迭代器 格式 re.finditer('匹配的内容',操作的对象) import re s = 'ab-123-abc-abc-a123bbc' g = re.finditer('ab',s) #'ab'为匹配的内容，s为操作的对象 print(next(g).group()) 结果： ab 4.8给分组()起名字 格式 (?P) import re s = 'ab-123-abc-abc-a123bbc' ret = re.search('(?P)abc',s) #给分组起名为test_abc print(ret.group()) 结果： abc 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/21.python基础二十一 logging日志.html":{"url":"python/python基础/21.python基础二十一 logging日志.html","title":"python基础二十一 logging日志","keywords":"","body":"python基础二十一 logging日志 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/22.python基础二十二 包.html":{"url":"python/python基础/22.python基础二十二 包.html","title":"python基础二十二 包","keywords":"","body":"python基础二十二 包 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/23.python基础二十三 异常处理.html":{"url":"python/python基础/23.python基础二十三 异常处理.html","title":"python基础二十三 异常处理","keywords":"","body":"[toc] python基础二十三 异常处理 1.异常和错误 1.1错误 1.1.1语法错误 #if后边没有加冒号，属于语法错误，无法通过python解释器，必须运行前修改 num = 10 if num == 11 print('ok') 结果报错： SyntaxError: invalid syntax 1.1.2逻辑错误 #除数为0 num = 10 print(num/0) 结果报错： ZeroDivisionError: division by zero 1.2异常 什么是异常？ 异常即是一个事件，该事件会在程序执行过程中发生，影响了程序的正常执行。 一般情况下，在Python无法正常处理程序时就会发生一个异常。 异常是Python对象，表示一个错误。 当Python脚本发生异常时我们需要捕获处理它，否则程序会终止执行。 1.3python中异常的类 错误示例 #触发IndexError: 索引超出范围 lst = [1,2,3] print(lst[5]) 结果报错： IndexError: list index out of range #触发KeyError: 字典中没有此键 dic = {'k1':1,'k2':2} print(dic['k3']) 结果报错： KeyError: 'k3' 常见异常 异常名称 描述 BaseException 所有异常的基类 SystemExit 解释器请求退出 KeyboardInterrupt 用户中断执行(通常是输入^C) Exception 常规错误的基类 StopIteration 迭代器没有更多的值 GeneratorExit 生成器(generator)发生异常来通知退出 StandardError 所有的内建标准异常的基类 ArithmeticError 所有数值计算错误的基类 FloatingPointError 浮点计算错误 OverflowError 数值运算超出最大限制 ZeroDivisionError 除(或取模)零 (所有数据类型) AssertionError 断言语句失败 AttributeError 对象没有这个属性 EOFError 没有内建输入,到达EOF 标记 EnvironmentError 操作系统错误的基类 IOError 输入/输出操作失败 OSError 操作系统错误 WindowsError 系统调用失败 ImportError 导入模块/对象失败 LookupError 无效数据查询的基类 IndexError 序列中没有此索引(index) KeyError 映射中没有这个键 MemoryError 内存溢出错误(对于Python 解释器不是致命的) NameError 未声明/初始化对象 (没有属性) UnboundLocalError 访问未初始化的本地变量 ReferenceError 弱引用(Weak reference)试图访问已经垃圾回收了的对象 RuntimeError 一般的运行时错误 NotImplementedError 尚未实现的方法 SyntaxError Python 语法错误 IndentationError 缩进错误 TabError Tab 和空格混用 SystemError 一般的解释器系统错误 TypeError 对类型无效的操作 ValueError 传入无效的参数 UnicodeError Unicode 相关的错误 UnicodeDecodeError Unicode 解码时的错误 UnicodeEncodeError Unicode 编码时错误 UnicodeTranslateError Unicode 转换时错误 Warning 警告的基类 DeprecationWarning 关于被弃用的特征的警告 FutureWarning 关于构造将来语义会有改变的警告 OverflowWarning 旧的关于自动提升为长整型(long)的警告 PendingDeprecationWarning 关于特性将会被废弃的警告 RuntimeWarning 可疑的运行时行为(runtime behavior)的警告 SyntaxWarning 可疑的语法的警告 UserWarning 用户代码生成的警告 2.异常处理 捕捉异常可以使用try/except语句。 try/except语句用来检测try语句块中的错误，从而让except语句捕获异常信息并处理。 如果你不想在异常发生时结束你的程序，只需在try里捕获它。 2.1try/except语句 语法 try: 被检测的代码块 except 异常类型： try中一旦检测到异常，就执行这里的代码 2.2try/except...else语句 语法 try: 被检测的代码块 except 异常类型： try中一旦检测到异常，就执行这里的代码 else: 没有发生异常执行的代码 try的工作原理是，当开始一个try语句后，python就在当前程序的上下文中作标记，这样当异常出现时就可以回到这里，try子句先执行，接下来会发生什么依赖于执行时是否出现异常。 如果当try后的语句执行时发生异常，python就跳回到try并执行第一个匹配该异常的except子句，异常处理完毕，控制流就通过整个try语句（除非在处理异常时又引发新的异常）。 如果在try后的语句里发生了异常，却没有匹配的except子句，异常将被递交到上层的try，或者到程序的最上层（这样将结束程序，并打印默认的出错信息）。 如果在try子句执行时没有发生异常，python将执行else语句后的语句（如果有else的话），然后控制流通过整个try语句。 正常运行示例，打开一个文件test.py，在该文件中写入以下内容，且并未发生异常： #!/usr/bin/python # -*- coding: UTF-8 -*- try: fh = open(\"testfile\", \"w\") fh.write(\"这是一个测试文件，用于测试异常!!\") except IOError: print(\"Error: 没有找到文件或读取文件失败\") else: print(\"内容写入文件成功\") fh.close() 运行结果： $ python3 test.py 内容写入文件成功 $ cat testfile # 查看写入的内容 这是一个测试文件，用于测试异常!! 异常运行示例，还是运行以上文件，但文件没有写入权限，发生了异常 先取消test.py文件的执行权限 $ python3 test.py Error: 没有找到文件或读取文件失败 2.3try-finally 语句 try-finally 语句无论是否发生异常都将执行最后的代码。 语法 try: finally: #退出try时总会执行 raise 示例 #!/usr/bin/python # -*- coding: UTF-8 -*- 代码写法一 try: fh = open(\"testfile\", \"w\") fh.write(\"这是一个测试文件，用于测试异常!!\") finally: print(\"Error: 没有找到文件或读取文件失败\") 代码写法二 属于异常嵌套 try: fh = open(\"testfile\", \"w\") try: fh.write(\"这是一个测试文件，用于测试异常!!\") finally: print(\"关闭文件\") fh.close() except IOError: print(\"Error: 没有找到文件或读取文件失败\") 如果文件没有可写权限，会报错 $ python3 test.py Error: 没有找到文件或读取文件失败 当在try块中抛出一个异常，立即执行finally块代码。 finally块中的所有语句执行后，异常被再次触发，并执行except块代码。 参数的内容不同于异常。 2.4万能异常 Exception exception可以捕获所有异常 示例，将字符串转换为整型，会报错 s1 = 'hello' try: int(s1) except Exception as e: print(e) 结果： invalid literal for int() with base 10: 'hello' 2.5异常捕获嵌套示例 a = int(input(\"请输入除数>>>\")) b = int(input(\"请输入被除数>>>\")) try: print(a/b) except Exception: try: b = int(input(\"被除数为0，请重新输入被除数>>>\")) print(a/b) except ZeroDivisionError: print(\"被除数能为0？？？，滚吧！！！\") 2.6raise抛出异常 语法 raise [异常类型[(异常原因)]] 其中，用 [] 括起来的为可选参数，其作用是指定抛出的异常名称，以及异常信息的相关描述。如果可选参数全部省略，则 raise 会把当前错误原样抛出；如果仅省略 (异常原因)，则在抛出异常时，将不附带任何的异常描述信息。 也就是说，raise 语句有如下三种常用的用法： raise：单独一个 raise。该语句引发当前上下文中捕获的异常（比如在 except 块中），或默认引发 RuntimeError 异常。 raise 异常类名称：raise 后带一个异常类名称。该语句引发指定异常类的默认实例。 raise 异常类名称(描述信息)：在引发指定异常的同时，附带异常的描述信息。 上面三种用法最终都是要引发一个异常实例（即使指定的是异常类，实际上也是引发该类的默认实例），raise 语句每次只能引发一个异常实例。 2.6.1 raise def main(): try: # 使用try...except来捕捉异常 # 此时即使程序出现异常，也不会传播给main函数 mtd(3) except Exception as e: print('程序出现异常:', e) # 不使用try...except捕捉异常，异常会传播出来导致程序中止 mtd(3) def mtd(a): if a > 0: raise main() 程序运行结果： 程序出现异常: No active exception to reraise raise RuntimeError: No active exception to reraise 2.6.2 raise 异常类名称 def main(): try: # 使用try...except来捕捉异常 # 此时即使程序出现异常，也不会传播给main函数 mtd(3) except Exception as e: print('程序出现异常:', e) # 不使用try...except捕捉异常，异常会传播出来导致程序中止 mtd(3) def mtd(a): if a > 0: raise ValueError main() 程序运行结果： 程序出现异常: raise ValueError ValueError 2.6.3 raise异常类名称(描述信息) def main(): try: # 使用try...except来捕捉异常 # 此时即使程序出现异常，也不会传播给main函数 mtd(3) except Exception as e: print('程序出现异常:', e) # 不使用try...except捕捉异常，异常会传播出来导致程序中止 mtd(3) def mtd(a): if a > 0: raise ValueError(\"a的值大于0，不符合要求\") main() 程序运行结果： 程序出现异常: a的值大于0，不符合要求 raise ValueError(\"a的值大于0，不符合要求\") ValueError: a的值大于0，不符合要求 2.7自定义异常 可以通过创建一个新的异常类来拥有自己的异常。异常类继承自 Exception 类，可以直接继承，或者间接继承，例如: class MyError(Exception): def __init__(self, value): self.value = value def __str__(self): return repr(self.value) try: raise MyError(3 * 2) except MyError as e: print(f'这是我自定义的异常，自定义运算结果是{e.value}') 程序运行结果： 这是我自定义的异常，自定义运算结果是6 2.8assert断言 Python assert（断言）用于判断一个表达式，在表达式条件为 false 的时候触发异常。 断言可以在条件不满足程序运行的情况下直接返回错误，而不必等待程序运行后出现崩溃的情况，例如我们的代码只能在 Linux 系统下运行，可以先判断当前系统是否符合条件。 语法 assert 异常类型 等价于 if not expression: raise AssertionError assert后边可以紧跟参数 assert expression [, arguments] 等价于 if not expression: raise AssertionError(arguments) 使用示例 #使用示例1，这段代码只能在linux系统中运行 import sys assert ('linux' in sys.platform), \"该代码只能在Linux下执行\" #使用示例2 assert 1 == 2 结果： AssertionError >>> assert True # 条件为true正常执行 >>> assert False # 条件为false触发异常 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/24.1python基础二十四 面向对象初识.html":{"url":"python/python基础/24.1python基础二十四 面向对象初识.html","title":"面向对象初识","keywords":"","body":"[toc] python基础二十四 面向对象初识 1.面向过程与面向对象 1.1什么是面向过程？ 1.1.1面向过程概念 在未学习面向对象之前写的代码都算是面向过程 例如，想要实现一个功能，分析出解决问题所需要的步骤，然后用代码或者函数逐一实现，并按照代码顺序调用，这就是面向过程 1.1.2面过程优缺点 优点 性能比面向对象高，因为类调用时需要实例化，开销比较大，比较消耗资源;比如单片机、嵌入式开发、Linux/Unix等一般采用面向过程开发，性能是最重要的因素 缺点 维护性、复用行、扩展性较差 1.2什么是面向对象？ 1.2.1面向对象概念 面向对象的程序设计的核心是对象（上帝式思维），要理解对象为何物，必须把自己当成上帝，上帝眼里世间存在的万物皆为对象，不存在的也可以创造出来 ⾯向对象思维, 要⾃⼰建立对象. ⾃⼰建立场景. 你是就是⾯向对象世界中的上帝. 你想让⻋⼲嘛就⼲嘛. 你想让⼈⼲嘛⼈就能⼲嘛 1.2.2面向对象优缺点 优点 是一类相似功能函数的集合,使你的代码更清晰化，更合理化 易维护、易复用、易扩展，由于面向对象有封装、继承、多态性的特性，可以设计出低耦合的系统，使系统更加灵活、更加易于维护 缺点 性能比面向过程低 1.2.3面向对象特点 特点 1.程序设计的重点在于数据而不是过程； 2.程序被划分为所谓的对象； 3.数据结构为表现对象的特性而设计； 4.函数作为对某个对象数据的操作，与数据结构紧密的结合在一起； 5.数据被隐藏起来，不能为外部函数访问； 6.对象之间可以通过函数沟通； 7.新的数据和函数可以在需要的时候轻而易举的添加进来； 8.在程序设计过程中遵循由下至上（bottom-up）的设计方法。 2.类与对象 2.1什么是类？ 2.1.1类的概念 对象的抽象，一类事物的总称 具有相同属性和功能的一类事物 2.1.2类的说明 类：就是具有相同属性和功能的一类事物，比如，狗类、猫类、人类 狗类中的金毛、猫类中的橘猫、人类中的男人、女人就是具体的对象 汽车，车有轮胎、发动机、方向盘等等，车就是类 人，人有思想、名字、年龄、爱好、性别，人就是类 2.2什么是对象？ 2.2.1对象的概念 类的具象，即对象为类的具体实现，通过类可以创建对象 2.2.2对象的说明 对象：就是类的具体表现形式，例如狗类中的金毛、猫类中的橘猫、人类中的男人、女人就是具体的对象 3.类操作 3.1创建类 语法 class 类名(): 属性 方法 代码示例 //创建一个人类，人类有属性姓名、年龄、爱好，人类有方法玩 class Person(): name = \"呵呵\" age = 20 hobby = \"玩\" def play(self): #这里的self是固定写法 print(\"人喜欢玩\") 3.1.1类属性 上述代码中的name、age、hobby就是类属性，属于类中的全局属性，后续还有传参方式的私有属性 3.1.2类方法 将函数写在类中加上默认参数self就是类中的方法，self不是固定叫self，只不过大家约定俗称叫self 上述代码中的函数play就是类方法，self代表类实例对象本身，⚠️不是类本身 3.2类名的操作 3.2.1类名操作静态属性 方式一 类名.__dict__() 查看类中所有内容 类名.__dict__()只能做查看操作，不能修改、删除 class Person(): name = \"呵呵\" age = 20 hobby = \"玩\" def play(self): print(\"人喜欢玩\") #查看类中所有的内容 print(Person.__dict__) 结果： {'__module__': '__main__', 'name': '呵呵', 'age': 20, 'hobby': '玩', 'play': , '__dict__': , '__weakref__': , '__doc__': None} 方式二 万能的点 . 操作单个属性(增删改查) 万能的点可以做增、删、改、查操作 class Person(): name = \"呵呵\" age = 20 hobby = \"玩\" def play(self): print(\"人喜欢玩\") #增操作 增加类属性 Person.weight = 100 print(Person.weight) 结果： 100 #删操作 删除类属性 del Person.name print(Person.name) 结果： AttributeError: type object 'Person' has no attribute 'name' #改操作 修改类属性 Person.name = \"哈哈\" print(Person.name) 结果： 哈哈 #查操作 查看类属性 print(Person.name) 结果： 呵呵 3.2.2类名操作动态方法 ⚠️除了类中的属性和类方法之外，一般都是通过对象名操作 class Person(): name = \"呵呵\" age = 20 hobby = \"玩\" def play(self): print(\"人喜欢玩\") Person.play(1) #1是随便传的一个参数，因为类中方法必须有一个参数 结果： 人喜欢玩 3.2.3增加类的静态属性 //类外增加 class Person: def __init__(self,name): self.name = name def func(self,sex): self.sex = sex def func1(self): Person.bbb = 'ccc' Person.aaa = '小明' print(Person.__dict__) 结果： {'__module__': '__main__', '__init__': , 'func': , 'func1': , '__dict__': , '__weakref__': , '__doc__': None, 'aaa': '小明'} 4.对象操作 4.1创建对象 对象是从类中出来的，只要是类名加上（），这就是一个实例化过程，这个就会实例化一个对象。 class Person(): name = \"呵呵\" age = 20 hobby = \"玩\" def play(self): print(\"人喜欢玩\") man = Person() #实例化对象 print(man) 实例化对象发生的事情 1.在内存中开辟了一个对象空间。 2.自动执行类中的__init__方法，并将这个对象空间（内存地址）传给了__init__方法的第一个位置参数self。 3.在__init__ 方法中通过self给对象空间添加属性。 以下为代码示例 class Person(): #self和man指向的是同一个内存地址同一个空间，下面就是通过self给这个对象空间封装四个属性 def __init__(self,name,age,sex): self.name = name self.age = age self.sex = sex def a(self): print(f\"我叫{self.name},我今年{self.age}了,我是{self.sex}的\") man = Person(\"小明\",20,'男') #实例化对象，man现在就是Person类的实例化对象 man.a() #调用类中的方法 结果： 我叫小明,我今年20了,我是男的 4.2对象的操作 4.2.1对象名操作对象空间属性 方式一 对象名查看对象中所有属性 对象名.__dict__ class Person(): def __init__(self,name,age,sex): self.name = name self.age = age self.sex = sex def a(self): print(f\"我叫{self.name},我今年{self.age}了,我是{self.sex}的\") man = Person(\"小明\",20,'男') print(man.__dict__) 结果： {'name': '小明', 'age': 20, 'sex': '男'} 方式二 万能的点 . 万能的点可以做增、删、改、查操作 class Person(): def __init__(self,name,age,sex): self.name = name self.age = age self.sex = sex def a(self): print(f\"我叫{self.name},我今年{self.age}了,我是{self.sex}的\") man = Person(\"小明\",20,'男') #增操作 增加类属性 man.job = \"python开发\" print(man.job) 结果： python开发 #删操作 删除类属性 del man.name print(man.name) 结果： AttributeError: 'Person' object has no attribute 'name' #改操作 修改类属性 man.name = \"小红\" print(man.name) 结果： 小红 #查操作 查看类属性 print(man.name) 结果： 小明 4.2.2对象名查看类中属性 class Person(): size = 36 weight = 100 def __init__(self,name,age,sex): self.name = name self.age = age self.sex = sex def a(self): print(f\"我叫{self.name},我今年{self.age}了,我是{self.sex}的\") man = Person(\"小明\",20,'男') #通过对象名查看类中属性 print(man.size,man.weight) 36 100 4.2.3对象名操作类中方法 类中的方法一般都是通过对象执行的（除去类方法，静态方法外），并且对象执行这些方法都会自动将对象空间传给方法中的第一个参数self class Person(): size = 36 weight = 100 def __init__(self,name,age,sex): self.name = name self.age = age self.sex = sex def a(self): print(f\"我叫{self.name},我今年{self.age}了,我是{self.sex}的\") def f1(self): print(\"我是类中的方法f1\") man = Person(\"小明\",20,'男') man.f1() 结果： 我是类中的方法f1 4.2.4增加对象属性 //类外添加 class Person: def __init__(self,name): self.name = name def func(self,sex): self.sex = sex #类外面可以： obj = Person('小明') obj.age = 20 print(obj.__dict__) 结果： {'name': 'meet', 'age': 18} //类内添加 class Person: def __init__(self,name): self.name = name def func(self,sex): self.sex = sex #类内部也可以： obj = Person('小明') # __init__方法可以。 obj.func(\"男\") #func方法可以 print(obj.__dict__) 结果： {'name': '小明', 'sex': '男'} 5.补充说明 5.1self是什么 self其实就是类中方法（函数）的第一个位置参数，只不过解释器会自动将调用这个函数的对象传给self。所以把类中方法的第一个参数约定俗成设置成self, 代表这个就是对象.这个self可以进行改变但是不建议大家进行修 传参分为隐式传参和显示传参,self这种方式就是隐式传参 class Person(): size = 36 weight = 100 def __init__(self,name,age,sex): self.name = name self.age = age self.sex = sex def a(self): #这里的self就是对象man print(f\"我叫{self.name},我今年{self.age}了,我是{self.sex}的\") man = Person(\"小明\",20,'男') man.a() 结果： 我叫小明,我今年20了,我是男的 5.2 类中__init__()方法 构造方法：类中的__init__()方法（主要用作初始化） 在obj = 类名() 执行后做了两件事 #1.创建对象，对象名叫obj #2.通过对象执行类中的一个特殊方法 __init__ class Bar: def __init__(self): #这里的self就是对象名obj print('123') obj = Bar() 类中__init__()方法示例 //第一版代码 class Man: def __init__(self, name, age): self.n = name self.a = age print(f\"我叫{self.n},我今年{self.a}岁\") b = Man(\"小明\", 20) #打印结果 我叫小明,我今年20岁 #内存中发生的事 内存中有一个空间存放类Man，__init__()方法在类Man中，对象b指向类Man 类中的__init__()方法会自动执行 #代码说明 类Man中的self就是对象b，self.n就是b.n __init__()就叫构造方法，构造方法会在类名()的时候自动执行，即在b = Man()的时候自动执行 //第二版代码 class Man: def __init__(self, name, age): \"\"\" 构造方法，会在类名()的时候自动执行，即在b = Man()的时候自动执行 :param name: :param age: \"\"\" self.n = name self.a = age def show(self): print(f\"我叫{self.n},我今年{self.a}岁\") b = Man(\"小明\", 20) b.show() #打印结果 我叫小明,我今年20岁 5.3对象、类查找属性的顺序 对象查找属性的顺序 对象空间 ------> 类空间 ------> 父类空间 类查找属性的顺序 本类空间 ------> 父类空间 ⚠️⚠️⚠️类名不可能找到对象的属性 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/25.1python基础二十五 并发编程-多进程.html":{"url":"python/python基础/25.1python基础二十五 并发编程-多进程.html","title":"并发编程-多进程","keywords":"","body":"[toc] python基础二十五 并发编程-多进程 1.多任务处理 多任务处理就是使计算机同时处理多个任务 1.1 实现方式 多进程 多线程 1.2 串行、并发、并行示意图 串行 并发 并行 2.多进程 2.1 多进程涉及的概念 程序：是一个指令的集合，例如编写完的代码，还没有运行 进程：正在执行的程序，例如当运行一个程序的时候，就启动了一个进程 2.2 python多进程 1.程序开始运行时，首先会创建一个主进程 2.在主进程(父进程)下，可以创建新的进程(子进程)，子进程依赖于主进程，如果主进程结束，程序会退出 3.python提供了非常好用的进程包multiprocessing，借助这个包，可以轻松完成从单进程到并发执行的转换 2.2.1 multiprocessing模块、类方法创建多进程 方法一：multiprocessing模块提供了一个Process类来创建一个进程对象 //代码示例1 from multiprocessing import Process def run(name): print(f\"子进程 '{name}' 运行中\") if __name__ == \"__main__\": #windowns中防止递归执行创建子进程导致内存不足，Linux、Mac中可以不写 print(\"父进程启动\") p = Process(target=run,args=('我是传入的参数',)) #创建子进程 p.start() #启动进程 print(p.name) #打印进程名字，可以自定义 p.join() #告知主进程等待子进程结束 print(\"子进程结束\") 结果： 父进程启动 Process-1 子进程 '我是传入的参数' 运行中 子进程结束 //代码示例2 创建多个子进程、自定义进程名字 from multiprocessing import Process def run1(name,sex): print(f\"子进程 '{name}' 运行中,我是{sex}的\") def run2(name,sex): print(f\"子进程 '{name}' 运行中,我是{sex}的\") if __name__ == \"__main__\": print(\"父进程启动\") p1 = Process(target=run1,args=('我是子进程1','男',),name='自定义子进程1') p2 = Process(target=run2,args=('我是子进程2','女',),name='自定义子进程2') p1.start() p2.start() print(p1.name) print(p2.name) p1.join() p2.join() print(\"子进程结束\") 结果： 父进程启动 自定义子进程1 自定义子进程2 子进程 '我是子进程1' 运行中,我是男的 子进程 '我是子进程2' 运行中,我是女的 子进程结束 方法二：类方法创建多进程 创建新的进程还可以使用类的方式，可以自定义一个类，继承Process类，每次实例化这个类的时候，就等同于实例化一个进程对象 //multiprocessing模块创建多进程方法 from multiprocessing import Process def run(name): print(f\"我是进程：'{name}'\") if __name__ == \"__main__\": p = Process(target=run,args=('呵呵',)) p.start() p.join() print(\"子进程结束\") 结果： 我是进程：'呵呵' 子进程结束 //基于以上代码，使用类的方式创建多进程 from multiprocessing import Process class Custum(Process): def __init__(self,name): super().__init__() self.name = name def run(self): #重写run方法，只能有一个且必须叫run print(f\"我是进程：'{self.name}'\") if __name__ == \"__main__\": p = Custum(\"类创建进程\") p.start() p.join() print(\"子进程结束\") 结果： 我是进程：'类创建进程' 子进程结束 使用类方法创建多进程 from multiprocessing import Process class A(Process): def __init__(self,name): super().__init__() self.name = name def hehe(self): #名称必须叫run，否则运行结果会有问题 print(f\"子进程 {self.name} 运行中\") if __name__ == \"__main__\": print(\"父进程启动\") p = A(\"类创建进程\") p.start() p.join() print(\"子进程结束\") 结果： 父进程启动 子进程结束 2.2.2 __name == \"__main__\"参数 1.一个python的文件有两种使用的方法，第一是直接作为程序执行，第二是import到其他的python程序 中被调用(模块重用)执行。 2.因此if __name__ == 'main': 的作用就是控制这两种情况执行代码的过程，__name__ 是内置变量，用于表示当前模块的名字 3.在if __name__ == 'main': 下的代码只有在文件作为程序直接执行才会被执行，而import到其他程序中是不会被执行的 4.在Windows 上，子进程会自动 import 启动它的这个文件，而在 import 的时候是会执行这些语句的。 如果不加if __name__ == \"__main__\":的话就会无限递归创建子进程 所以必须把创建子进程的部分用那个 if 判断保护起来 import 的时候 __name__ 不是__main__ ，就不会递归运行了 //错误示例 from multiprocessing import Process def run(name): print(f\"我是进程： '{name}'\") p = Process(target=run,args=('呵呵',)) p.start() p.join() print(\"子进程结束\") 结果： windows会报一堆错，Linux、Mac没有问题 //正确示例 from multiprocessing import Process def run(name): print(f\"我是进程： '{name}'\") if __name__ == \"__main__\": p = Process(target=run,args=('呵呵',)) p.start() p.join() print(\"子进程结束\") 我是进程： '呵呵' 子进程结束 2.2.3 多进程参数 target 表示调用对象，即子进程要执行的任务 p = Process(target=对象名(函数名)) args 表示调用对象的位置参数元组，args=(传入的参数,) ⚠️括号中传入的参数后面必须加逗号 p = Process(target=xxx,args=('传入的参数',)) name 表示进程的名称 p = Process(target=xxx,args=('传入的参数',),name='子进程名称') 2.2.4 Process类常用方法 p.start() 启动进程，并调用该子进程中的p.run() p.run() 进程启动时运行的方法，正是它去调用target指定的函数，我们自定义类中的一定要实现该方法 p.terminate() 强制终止进程p，不会进行任何清理操作 p.is_alive() 如果子进程p仍然运行，返回True，用来判断进程是否还在运行 p.join([超时时间]) 主进程等待p终止，timeout是可选的超时时间 2.2.5 Process类常用属性 name 当前进程实例别名，默认为Process-N,N为从1开始递增的整数，可以指定进程名称 pid 当前进程实例的PID 2.2.6 多进程中的全局变量 全局变量在多个进程中不共享，进程之间的数据是独立的，默认情况下互不影响 from multiprocessing import Process num = 10 def f1(): global num num += 1 print(f\"第一个子进程中的num值为:{num}\") def f2(): global num num += 2 print(f\"第二个子进程中的num值为:{num}\") if __name__ == \"__main__\": p1 = Process(target=f1) p2 = Process(target=f2) p1.start() p2.start() p1.join() p2.join() print(f\"全局变量中的num值为:{num}\") 结果： 第一个子进程中的num值为:11 第二个子进程中的num值为:12 全局变量中的num值为:10 2.3 进程池 进程池：用来创建多个进程 当需要创建多子进程数量不多时，可以直接利用multiprocessing中的Process动态生成多个进程，但如果是大量的进程目标，手动创建进程的工作量巨大，此时就可以利用multiprocessing模块提供的Pool 初始化Pool时，可以指定一个最大进程数，当有新的请求提交到Pool中时，如果池还没有满，那么就会创建一个新的进程用来执行该请求，但是如果池中的进程数已经达到指定的最大值，那么该请求就会等待，直到池中有进程结束，才会创建新的进程来执行 from multiprocessing import Pool import time def r1(): print(\"123\") time.sleep(5) def r2(): print(\"abc\") time.sleep(3) if __name__ == \"__main__\": po = Pool(5) #定义一个进程池，最大进程数为5，不写默认为CPU核心数 for i in range(5): po.apply_async(r1) #apply_async选择要调用的目标，每次循环会用空出来的子进程去调用目标 po.apply_async(r2) po.close() #进程关闭之后不再接受新的请求 po.join() #等待po中所有子进程结束，必须放在close后面 结果： 第一秒会出现以下结果，但是后续不确定 因为进程池中定义了最大进程数为5 123 abc 123 abc 123 2.3.1 进程池常用函数解析 multiprocessing.Pool常用函数解析 apply_async(func[,args[,kwds]]): 使用非阻塞方式调用func(并行执行，堵塞方式必须等待上一个进程退出才能进行下一个进程)，args为传递给func的参数列表，kwds为传递给func的关键字参数列表 apply(func[,args[,kwds]]) 使用阻塞方式调用func close() 关闭Pool，使其不再接受新的任务 join() 主进程阻塞，等待子进程的退出，必须在close或terminate之后使用 2.4 进程间通信 2.4.1 队列Q实现进程间数据传递 多进程之间，默认是不共享数据的 通过Queue(队列Q)可以实现进程间数据传递 Q本身是一个消息队列 Queue方法说明 Queue.put([num]) 存入消息，num不写或者为负数不限制 Queue.qsize() 返回当前队列包含的消息数量 Queue.empty() 如果队列为空，返回True，反之返回False Queue.full() 如果队列满了，返回True，反之返回False Queue.get([block[,timeout]]) 获取队列中的一条消息，然后将其从队列移除，block默认值为True 如果block使用默认值，且没有设置timeout(单位秒)，消息队列如果为空，此时程序将被阻塞(停在读取状态)，直到从消息队列读到消息为止 如果设置了timeout，则会等待timeout秒，若还没有读取到任何消息，则抛出\"Queue.Empty\"异常 如果block值为False，消息队列如果为空，则会立刻抛出\"Queue.Empty\"异常 2.4.1.1 存入消息 Queue.put([num]) #存入消息，num不写或者为负数不限制 //存入消息，最多存入3条，此时运行程序不回报错 from multiprocessing import Queue q = Queue(3) q.put(\"存入消息1\") q.put(\"存入消息2\") q.put(\"存入消息3\") //存入消息，最多存入3条，如果此时存入4条，程序会卡住，知道能够存入为止 from multiprocessing import Queue q = Queue(3) q.put(\"存入消息1\") q.put(\"存入消息2\") q.put(\"存入消息3\") q.put(\"存入消息4\") #这一条消息不会存入到队列中，知道队列有空闲可以存入为止 2.4.1.2 读取消息 Queue.get([block[,timeout]]) #获取队列中的一条消息，然后将其从队列移除，block默认值为True //存入消息 from multiprocessing import Queue q = Queue(3) #初始化一个Queue对象，最多可接受3条消息 q.put(\"存入消息1\") #添加消息，数据类型不限 q.put(\"存入消息2\") q.put(\"存入消息3\") //读取消息，默认方式读取 from multiprocessing import Queue q = Queue(3) #初始化一个Queue对象，最多可接受3条消息 q.put(\"存入消息1\") #添加消息，数据类型不限 q.put(\"存入消息2\") q.put(\"存入消息3\") print(q.get()) print(q.get()) print(q.get()) 结果： 存入消息1 存入消息2 //读取消息，指定block值为False 如果block值为False，消息队列如果为空，则会立刻抛出\"Queue.Empty\"异常 from multiprocessing import Queue q = Queue(3) print(q.get(block=False)) 结果： queue.Empty ⚠️此方法有时会报错队列为空，有时就没有问题，win和mac中一样,linux会始终报错队列为空 from multiprocessing import Queue q = Queue(3) q.put(\"存入消息1\") q.put(\"存入消息2\") q.put(\"存入消息3\") print(q.get(block=False)) //读取消息，指定读取空队列超时时间 from multiprocessing import Queue q = Queue(3) print(q.get(timeout=3)) 结果： 等待3秒后会报错 queue.Empty Queue.get_nowait() 相当于Queue.get(False) from multiprocessing import Queue q = Queue(3) print(q.get_nowait()) 结果： queue.Empty 2.4.1.3 获取队列信息 Queue.empty() #如果队列为空，返回True，反之返回False //队列不为空，返回False from multiprocessing import Queue import time q = Queue(3) q.put(\"1\") q.put(\"2\") q.put(\"3\") time.sleep(0.1) #如果不加sleep可能会返回队列为空 print(q.empty()) 结果： False //队列为空，返回True from multiprocessing import Queue import time q = Queue(3) print(q.empty()) Queue.full() #如果队列满了，返回True，反之返回False //队列满，返回True from multiprocessing import Queue q = Queue(3) q.put(1) q.put(2) q.put(3) print(q.full()) 结果： True //队列不满，返回False from multiprocessing import Queue q = Queue(3) q.put(1) q.put(2) print(q.full()) 结果： False 多接受方代码示例 同时有2个以上的接收方 from multiprocessing import Process,Queue import time def write(q): for i in range(6): print(\"子进程1写入了：\",i) q.put(i) time.sleep(1) def read1(q): while True: if not q.empty(): print(\"子进程2读取了：\",q.get()) time.sleep(1) else: break def read2(q): while True: if not q.empty(): print(\"子进程3读取了：\",q.get()) time.sleep(1) else: break if __name__ == \"__main__\": q = Queue() pw = Process(target=write,args=(q,)) pr1 = Process(target=read1,args=(q,)) pr2 = Process(target=read2,args=(q,)) pw.start() pw.join() pr1.start() pr2.start() pr1.join() pr2.join() print(\"接受完毕！\") 结果： 子进程1写入了： 0 子进程1写入了： 1 子进程1写入了： 2 子进程1写入了： 3 子进程1写入了： 4 子进程1写入了： 5 子进程2读取了： 0 子进程3读取了： 1 子进程2读取了： 2 子进程3读取了： 3 子进程3读取了： 4 子进程2读取了： 5 接受完毕！ ⚠️上述代码的问题之处，子进程2和子进程3不能同时读取队列中的消息，子进程2读取了0、2、4，子进程3读取了1、3、5，需要做一些代码逻辑修改，让子进程2和子进程3能够同时读取消息队列中的消息 //两个接收方读取消息队列 思路： from multiprocessing import Process,Queue import time def write(q1): for i in range(6): print(\"子进程1写入了：\",i) q1.put(i) time.sleep(1) def read1(q1,q2): while True: if not q1.empty(): a = q1.get() #如果q1队列不为空则取值并赋值给a print(\"子进程2读取了：\",a) q2.put(a) #q2从a中读取并写入消息 time.sleep(1) else: break def read2(q2): while True: if not q2.empty(): print(\"子进程3读取了：\",q2.get()) time.sleep(1) else: break if __name__ == \"__main__\": q1 = Queue() q2 = Queue() pw = Process(target=write,args=(q1,)) pr1 = Process(target=read1,args=(q1,q2)) pr2 = Process(target=read2,args=(q2,)) pw.start() pw.join() pr1.start() pr2.start() pr1.join() pr2.join() print(\"接受完毕！\") 结果： 子进程1写入了： 0 子进程1写入了： 1 子进程1写入了： 2 子进程1写入了： 3 子进程1写入了： 4 子进程1写入了： 5 子进程2读取了： 0 子进程3读取了： 0 子进程2读取了： 1 子进程3读取了： 1 子进程2读取了： 2 子进程3读取了： 2 子进程2读取了： 3 子进程3读取了： 3 子进程2读取了： 4 子进程3读取了： 4 子进程2读取了： 5 子进程3读取了： 5 接受完毕！ ⚠️⚠️⚠️mac本中执行结果不正确： 子进程1写入了： 0 子进程1写入了： 1 子进程1写入了： 2 子进程1写入了： 3 子进程1写入了： 4 子进程1写入了： 5 子进程2读取了： 0 子进程2读取了： 1 子进程2读取了： 2 子进程2读取了： 3 子进程2读取了： 4 子进程2读取了： 5 接受完毕！ 进程、线程、协程之间的区别 进程 最小的资源单位，内存级别，如果进程中没有线程只是划分了一个空间 线程 最小的运行单位，cpu级别，进程在执行，实际上是线程在执行 协程 微线程 用户级别 操作系统不知道什么是协程，是程序员yy出来的，欺骗操作系统，因为有IO操作，操作系统会回收权限，协程就是在用户级别将多个任务做成一个任务，但凡有一个线程有IO，手动切换，让线程能获得更大占用系统资源的机会，提高单线程效率 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/25.2python基础二十五 并发编程-多线程.html":{"url":"python/python基础/25.2python基础二十五 并发编程-多线程.html","title":"并发编程-多线程","keywords":"","body":"[toc] python基础二十五 并发编程-多线程 1.线程概念 线程：实现多任务的另一种方式，一个进程中同时运行的多个子任务就成为线程 轻量级进程：线程又被称为轻量级进程，是更小的执行单元 一个进程可拥有多个并行的线程，当中每一个线程，共享当前进程的资源 一个进程中的线程共享相同的内存单元/内存地址空间，这样就可以访问相同的变量和对象，而且他们从同一堆中分配对象，进行通信、数据交换、同步操作 线程间的通信是在同一个地址上进行的，所以不需要额外的通信机制，这就使得通信更简单而且信息传递速度也更快 线程的5种状态 多线程程序的执行顺序是不确定的(操作系统决定)。当执行到sleep语句时，线 程将被阻塞(Blocked) ， 到sleep结束后， 线程进入就绪(Runnable) 状态， 等待调度。 而线程调度将自行选择一个线程执行。 代码中只能保证每个线程都运行 完整个run函数， 但是线程的启动顺序、run函数中每次循环的执行顺序都不能确定 1、新状态:线程对象已经创建，还没有在其上调用start()方法。 2、可运行状态:当线程有资格运行，但调度程序还没有把它选定为运行线程时线程所处的状态。当start()方法调用时，线程首先进入可运行状态。在线程运行之后或者从阻塞、等待或睡眠状态回来后，也返回到可运行状态。 3、运行状态:线程调度程序从可运行池中选择一个线程作为当前线程时线程所处的状态。这也是线程进入运行状态的唯一一种方式。 4、等待/阻塞/睡眠状态:这是线程有资格运行时它所处的状态。实际上这个三状态组合为一种，其共同点是:线程仍旧是活的(可运行的)，但是当前没有条件运行。 但是如果某件事件出现，他可能返回到可运行状态。 5、死亡态:当线程的run()方法完成时就认为它死去。这个线程对象也许是活的，但是，它已经不是一个单独执行的线程，线程一旦死亡，就不能再次执行，如果在一个死去的线程上调用start()方法，会抛出异常 2.线程和进程的区别 区别 进程 线程 根本区别 作为资源分配的单位 调度和执行的单位 开销 每个进程都有独立的代码和数据空间(进程上下文)，进程间的切换会有较大的开销 线程可以看成是轻量级的进程，同一个类线程共享代码和数据空间，每个线程有独立的运行栈和程序计数器(PC)，线程切换的开销小 所处环境 在操作系统中能同时运行多个任务 在同一个应用程序中有多个顺序流同时执行 分配内存 系统在运行的时候会为每个进程分配不同的内存区域 除了CPU之外，不会为线程分配内存，线程使用的资源是它所属的进程的资源，线程只能共享资源 包含关系 没有线程的进程是可以被看作单线程的，如果一个进程内拥有多个线程，则执行过程不是一个线程，而是多个线程完成的 线程是进程的一部分，所以线程有的时候被称为是轻权进程或者轻量进程 3.多线程实现 3.1 通过threading.Thread直接在线程中运行函数 //创建单线程 import threading #导入threading模块 def thread(): print(\"线程启动\") t = threading.Thread(target=thread) #创建线程 t.start() #启动线程 //利用for循环创建多线程 import threading def thread(): print(\"子线程启动\") for i in range(5): t = threading.Thread(target=thread) t.start() 结果： 子线程启动 子线程启动 子线程启动 子线程启动 子线程启动 3.2 通过类继承threading.Thread类来创建线程 import threading class MyThread(threading.Thread): def run(self): for i in range(5): print(i) t1 = MyThread() t2 = MyThread() t1.start() t2.start() 结果： 0 1 2 3 4 0 1 2 3 4 3.3 查看当前线程数量 len(threading.enumerate() import threading import time import random def func(): time.sleep(random.randint(1, 3)) print(threading.current_thread().name,f\"当前活跃线程数量{len(threading.enumerate())}\") lst = [threading.Thread(target=func,name=f\"线程{i}\") for i in range(10)] for i in lst: i.start() 结果： 线程名称不固定 线程2 当前活跃线程数量11 线程8 当前活跃线程数量10 线程7 当前活跃线程数量9 线程4 当前活跃线程数量8 线程3 当前活跃线程数量7 线程1 当前活跃线程数量6 线程0 当前活跃线程数量5 线程9 当前活跃线程数量4 线程6 当前活跃线程数量3 线程5 当前活跃线程数量2 4.线程同步 4.1互斥锁 创建锁 lock = threading.Lock() 锁定 lock.acquire() 释放锁 lock.release() 未加锁之前循环100万次出现的结果不准确BUG from multiprocessing import Queue,Process,Lock import threading num = 0 def f1(): global num for i in range(1000000): num += 1 print(num) def f2(): global num for i in range(1000000): num += 1 print(num) t1 = threading.Thread(target=f1) t2 = threading.Thread(target=f2) t1.start() t2.start() print(\"主线程的num是\",num) 结果1: 主线程的num是 299428 1226857 1536106 结果2: 主线程的num是 263111 1173618 1499331 每一次执行的结果都不一样 加锁解决循环100万次出现结果不准确的BUG from multiprocessing import Queue,Process,Lock import threading num = 0 def f1(): global num lock.acquire() for i in range(1000000): num += 1 print(num) lock.release() def f2(): global num lock.acquire() for i in range(1000000): num += 1 print(num) lock.release() lock = threading.Lock() #创建锁 t1 = threading.Thread(target=f1) t2 = threading.Thread(target=f2) t1.start() t2.start() print(\"主线程的num是\",num) 结果1: 主线程的num是 235674 1000000 2000000 结果2: 主线程的num是 227114 1000000 2000000 主线程的num还是不一致，因为线程间数据是共享的，锁只能保证f1、f2两个函数中完整执行 4.2互斥锁实现线程同步 代码整体思路 1.创建3个线程，分别执行3个函数f1、f2、f3 2.创建3个锁，只有在锁1创建后不上锁，锁2、锁3都上锁 3.由于锁1没有上锁，因此可以先执行，锁1中执行后释放锁2，锁2执行，执行后释放锁3，锁3执行，执行完后释放锁1，锁1执行，这样就可以无限循环顺序执行锁1、锁2、锁3 import threading from multiprocessing import Lock def f1(): while True: lock1.acquire() #抢锁1 print(1) lock2.release() #释放锁2，这样就能执行函数f2 def f2(): while True: lock2.acquire() #抢锁2 print(2) lock3.release() #释放锁3，这样就能执行函数f3 def f3(): while True: lock3.acquire() #抢锁3 print(3) lock1.release() #释放锁1，这样就能执行函数f1 #创建3个线程，并分别执行函数f1、f2、f3 t1 = threading.Thread(target=f1) t2 = threading.Thread(target=f2) t3 = threading.Thread(target=f3) #创建3个锁，并且锁2、锁3创建后就上锁 lock1 = threading.Lock() lock2 = threading.Lock() lock2.acquire() lock3 = threading.Lock() lock3.acquire() #启动线程 t1.start() t2.start() t3.start() 结果： 1 2 3 1 2 3 。。。 4.3消息队列Queue实现线程同步 4.3.1线程中的Queue与进程中的Queue区别 进程中的Queue 进程中的Queue是从multriprocessing模块中导入的， from multiprocessing import Queue，作用是作为消息队列接收消息 线程中的Queue 线程中的Queue是从queue模块中导入的，from queue import Queue，作用是实现线程同步 4.3.2线程中的Queue Python中Queue模块，实现了3种类型的队列来实现线程同步，包括 FIFO(先入先出) 队列Queue，按照先进先出的顺序检索条目 LIFO(后入先出) 栈LifoQueue，最后添加的条目最先检索到 优先级队列 PriorityQueue，条目被保存为有序的(使用heapq模块)并且最小值的条目最先被剪锁 class queue.Queue(maxsize=0) FIFO队列的构造器，maxsize为一个整数，表示队列的最大条目数，可用来限制内存的使用 一旦队列满，插入将被阻塞直到队列中存在空闲时间，如果maxsize小于等于0，队列大小为无限制，maxsize默认为0 import queue import threading def write(): while True: if q1.qsize() = 100: for i in range(20): a = q1.get() print(a) q1 = queue.Queue() #通过Queue隔离开了存放数据的线程与读取数据的线程 for i in range(1,501): q1.put(f\"初始数据f{i}\") for i in range(500): print(q1.get()) t1 = threading.Thread(target=write) t2 = threading.Thread(target=read) t1.start() t2.start() 结果： 循环打印新数据1-50 4.3.3生产者消费者模式 生产者就是生产数据的线程，消费者就是消费数据的线程 生产者消费者模式是通过一个容器(缓冲区)来解决生产者和消费者的强耦合问题 生产者和消费者之间不直接通讯，通过阻塞队列来进行通讯 4.4死锁(错误情况) 死锁属于错误情况，在线程共享多个资源的时候，如果两个线程分别占有一部分资源并同时等待对方的资源，就会造成死锁 #以下代码，锁1再等待锁2，锁2再等待锁1，因此造成了死锁 import threading import time def f1(): if lock1.acquire(): #f1抢到了锁1 print(\"lock1抢到了锁\") time.sleep(1) if lock2.acquire(): #再等待锁2，但是锁2已经被f2抢到，还未释放，因此无法继续执行后续代码 print(\"lock2\") lock2.release() lock1.release() def f2(): if lock2.acquire(): #f2抢到了锁2 print(\"lock2抢到了锁\") time.sleep(1) if lock1.acquire(): #再等待锁1，但是锁1已经被f1抢到，还未释放，因此无法继续执行后续代码 print(\"lock1\") lock1.release() lock2.release() lock1 = threading.Lock() lock2 = threading.Lock() t1 = threading.Thread(target=f1) t2 = threading.Thread(target=f2) t1.start() t2.start() 结果： lock1抢到了锁 lock2抢到了锁 程序会卡住不动 4.5信号量 semaphone 4.5.1概念 1.信号量semaphone用于控制一个时间点内线程进入数量的锁，信号量是用来控制线程并发数的 2.信号量只控制同一时间能并发执行的线程，其余不管 4.5.2使用场景 读写文件 读写文件的时候，一般只有一个线程再写，而读可以有多个线程同时进行，如果需要限制同时读取文件的线程个数，这时候就需要用到信号量 如果使用互斥锁，就是限制同一时间只能有一个线程读取文件 提供访问的web服务 web服务都是跑在服务器中的，而服务器的资源是有限的，如果访问请求数量特别多，不加限制就会导致服务器宕机，此时使用信号量限制同一时间web服务能处理的请求就不会造成因访问量巨大而造成服务器宕机 4.5.3信号量代码示例 //无法控制同时执行的线程数 import time import threading def f1(): time.sleep(1) print(111) for i in range(100): t1 = threading.Thread(target=f1) t1.start() 结果： 100个111 //使用信号量控制同时执行的线程数 import time import threading s = threading.Semaphore(5) #开启信号量 def f1(): s.acquire() #信号量加锁 time.sleep(2) print(111) s.release() #信号量解锁 for i in range(100): t1 = threading.Thread(target=f1) t1.start() 结果： 一次打印5个111，直到循环完成 4.6GIL全局解释器锁 4.6.1概念 Cpython独有的锁，牺牲效率保证数据安全，同一时间只能有一个线程来修改共享的数据 4.6.2GIL锁说明 首先，执行python文件是什么过程？谁把进程运行起来的？ 操作系统将你的应用程序从硬盘加载到内存然后运行python文件，在内存中开辟一个进程空间，将你的python解释器以及p y文件加载进去，解释器运行py文件 python解释器分为两部分，先将你的代码通过编译器编译成c的字节码，然后你的虚拟机拿到你的c字节码，输出机器码，再配合操作系统把你的这个机器仍给CPU处理 py文件有一个主线程，主线程做的就是这个过程，如果开多线程，每个线程都要进行这个过程 Cpython为什么用不了多核？ cpython在所有的线程进入解释器之前加了一个全局解释器锁即GIL锁，这个锁是互斥锁，是加在解释器上的，导致同一时间只有一个线程在执行，所以用不了多核 为什么这么设计？ 因为写python的人只有一个CPU 所以加了一个锁，保证了数据的安全，而且再写python解释器时，更加好写 为什么不取消这个锁？ 解释器内部的管理全部是针对单线程写的，如果要取消锁，需要重构python解释器 能不能不用Cpython？ 官方推荐使用Cpython，处理速度快，相对其他解释器比较完善 多线程无法使用多核，怎么办？ 虽然多线程无法使用多核，但是多进程可以应用多核，但是开销大 GIL全局解释器锁和互斥锁的区别 锁的目的就是为了保护共享的数据，同一时间只能有一个线程来修改共享数据 保护不同的数据就应该加不同的锁 GIL和lock是两种锁，保护的数据不一样，GIL是解释器级别的，保护的是解释器级别的数据，比如垃圾回收的数据，lock互斥锁是保护用户自己开发的应用程序的数据，GIL是不管这样的数据的 示意图 1⃣️xx.py文件中有多个线程，由于GIL锁的存在导致同时只能执行一个线程，因为多个线程操作一个资源会出问题，所以在解释器级别加了GIL锁 2⃣️cpython解释器不支持同时解释多个python线程，原因是为了保证解释器的安全，因此不支持多线程同时并发 3⃣️GIL锁是解释器级别的锁，只能保证解释器的安全，但是无法保证数据安全，我们自己加互斥锁能解决这个问题 5.线程异步 线程异步有多种方式 1.无需等待线程执行(正常写的线程执行代码就是异步，因为操作系统自动调用线程执行，多个线程执行顺序不固定) 2.通过循环控制(方法low) 3.通过回调机制实现线程异步 通过回调机制实现线程异步 from multiprocessing import Pool import random import time def download(name): for i in range(1,6): print(f\"{name}下载文件{i}\") time.sleep(random.randint(1,3)) return \"下载完成\" def alterUser(msg): print(msg) if __name__ == \"__main__\": p = Pool(3) #当func执行完成后，return的东西会给到回调函数callback p.apply_async(func=download,args=(\"线程1\",),callback=alterUser) p.apply_async(func=download,args=(\"线程2\",),callback=alterUser) p.apply_async(func=download,args=(\"线程3\",),callback=alterUser) p.close() #关闭进程池 p.join() 结果： 结果顺序每一次执行都会不同，最终结果为显示3次下载完成 线程1下载文件1 线程2下载文件1 线程3下载文件1 线程1下载文件2 线程2下载文件2 线程3下载文件2 线程3下载文件3 线程1下载文件3 线程3下载文件4 线程2下载文件3 线程3下载文件5 线程1下载文件4 下载完成 线程2下载文件4 线程1下载文件5 线程2下载文件5 下载完成 下载完成 进程、线程、协程之间的区别 进程 最小的资源单位，内存级别，如果进程中没有线程只是划分了一个空间 线程 最小的运行单位，cpu级别，进程在执行，实际上是线程在执行 协程 微线程 用户级别 操作系统不知道什么是协程，是程序员yy出来的，欺骗操作系统，因为有IO操作，操作系统会回收权限，协程就是在用户级别将多个任务做成一个任务，但凡有一个线程有IO，手动切换，让线程能获得更大占用系统资源的机会，提高单线程效率 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/25.3python基础二十五 并发编程-多协程.html":{"url":"python/python基础/25.3python基础二十五 并发编程-多协程.html","title":"并发编程-多协程","keywords":"","body":"[toc] python基础二十五 并发编程-多协程 1.协程 1.1概念 协程是比线程更小的执行单元，也叫微线程 一个线程作为一个容器里面可以放置多个协程 1.2协程作用 只切换函数调用即可完成多线程，可以减少CPU的切换 协程自己主动让出CPU，不需要系统调用 1.3协程实现 1.3.1 greenlet(第三方模块，手动切换函数执行) #协程间来回切换，不需要CPU的调用 #需要先安装greenlet pip install greenlet from greenlet import greenlet import time def t1(): while True: print(\"AAA\") #第一步先打印AAA gr2.switch() #第二步让协程gr2进来执行，gr1保留此处的执行位置，协程gr2执行的是t2函数 time.sleep(1) def t2(): while True: print(\"bbb\") #第三步打印bbb gr1.switch() #第四步让协程gr1进来执行，gr2保留此处的执行位置，协程gr1执行的是t1函数 time.sleep(1) gr1 = greenlet(t1) #创建一个协程对象 gr2 = greenlet(t2) gr1.switch() #此时会执行t1函数 结果： 循环打印 AAA bbb 1.3.2 gevent(第三方模块，自动切换函数执行) 概念 gevent是一个能够自动切换函数执行的协程模块，比greenlet功能强大 原理 gevent通过greenlet实现协程，当一个greenlet遇到IO操作时，就自动切换到其他的greenlet，等待IO操作完成，再在适当的时候切换回来继续执行 特点 gevent只有遇到模块能够识别的IO操作的时候，程序才会进行任务切换，实现并发效果 #需要先安装gevent pip install gevent //代码示例1 无限循环切换协程 import gevent def A(): while True: print(\".........A.........\") gevent.sleep(1)#用来模拟一个耗时操作 #gevent中：当一个协程遇到耗时操作会自动交出控制权给其他协程 def B(): while True: print(\".........B.........\") gevent.sleep(1) #每当遇到耗时操作，会自用转到其他协程 g1 = gevent.spawn(A) #创建一个gevent对象（创建了一个协程），此时就已经开始执行函数A g2 = gevent.spawn(B) g1.join() #等待协程执行结束 g2.join() #会等待协程运行结束后再退出 结果： 无限循环打印 .........A......... .........B......... 。。。 //代码示例2 控制协程循环次数 import gevent def A(): for i in range(10): print(\"AAA\") gevent.sleep(1) def B(): for i in range(10): print(\"BBB\") gevent.sleep(1) g1 = gevent.spawn(A) g2 = gevent.spawn(B) g1.join() g2.join() 结果： 打印10次 AAA BBB 进程、线程、协程之间的区别 进程 最小的资源单位，内存级别，如果进程中没有线程只是划分了一个空间 线程 最小的运行单位，cpu级别，进程在执行，实际上是线程在执行 协程 微线程 用户级别 操作系统不知道什么是协程，是程序员yy出来的，欺骗操作系统，因为有IO操作，操作系统会回收权限，协程就是在用户级别将多个任务做成一个任务，但凡有一个线程有IO，手动切换，让线程能获得更大占用系统资源的机会，提高单线程效率 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/26.1python基础二十六 网络编程-网络基础知识.html":{"url":"python/python基础/26.1python基础二十六 网络编程-网络基础知识.html","title":"网络编程-计算机网络基础","keywords":"","body":"[toc] python基础二十六 网络编程-网络基础知识 1.网络基础中涉及到的几种概念 1.1IP地址 1.1.1概念 IP地址是用来在网络中标记一台电脑的一串数字，在网络上是唯一的 IP地址分类私网IP地址和公网IP地址 1.1.2格式说明 每一个IP地址包括两部分：网络地址和主机地址 A类IP地址由1字节的⽹络地址和3字节主机地址组成， ⽹络地址的最⾼位必须是“0”，地址范围1.0.0.1-126.255.255.254可⽤的A类⽹络有126个， 每个⽹络能容纳1677214个主机 B类IP地址由2个字节的⽹络地址和2个字节的主机地址组成， ⽹络地址的最⾼位必须是“10”，地址范围128.1.0.1-191.255.255.254 可⽤的B类⽹络有16384个， 每个⽹络能容纳65534主机 C类IP地址由3字节的⽹络地址和1字节的主机地址组成， ⽹络地址的最⾼位必须是“110”范围192.0.1.1-223.255.255.254 C类⽹络可达2097152个， 每个⽹络能容纳254个主机 D类IP地址第⼀个字节以“1110”开始， 它是⼀个专⻔保留的地址。它并不指向特定的⽹络， ⽬前这⼀类地址被⽤在多点⼴播（一对多） 中多点⼴播地址⽤来⼀次寻址⼀组计算机 地址范围224.0.0.1-239.255.255.254 E类IP地址以“1111”开始，为将来保留使用，仅做实验和开发用 1.1.3私有IP地址 1.1.3.1含义 私有IP：本地局域网的IP，专门为组织机构内部使用 在这么多网络IP中，国际规定有一部分的Ip地址是用于我们的局域网使用，属于私网IP 1.1.3.2范围 A类 10.0.0.0 ～ 10.255.255.255 B类 172.16.0.0 ～ 172.31.0.0 C类 192.168.0.0 ～ 192.168.255.255 1.1.4回环地址 127.0.0.1 作用 测试当前计算机的网络通信协议，127.0.0.1可以代表本机 用来检测本机网络问题，ping 127.0.0.1即可检测 1.1.5子网掩码 1.1.5.1概念 子网掩码是在IPv4地址资源紧缺的背景下为了解决lP地址分配而产生的虚拟lP技术，通过子网掩码将A、B、C三类地址划分为若干子网，从而显著提高了IP地址的分配效率，有效解决了IP地址资源紧张的局面。另一方面，在企业内网中为了更好地管理网络，网管人员也利用子网掩码的作用，人为地将一个较大的企业内部网络划分为更多个小规模的子网，再利用三层交换机的路由功能实现子网互联，从而有效解决了网络广播风暴和网络病毒等诸多网络管理方面的问题。 1.1.5.2默认子网掩码 类别 子网掩码的二进制数值 子网掩码的十进制数值 A 11111111 00000000 00000000 00000000 255.0.0.0 B 11111111 11111111 00000000 00000000 255.255.0.0 C 11111111 11111111 11111111 000000000 255.255.255.0 1.1.5.3子网掩码功能 子网掩码是一个32位地址，是与IP地址结合使用的一种技术。它的主要作用有两个 一是用于屏蔽IP地址的一部分以区别网络标识和主机标识，并说明该IP地址是在局域网上，还是在远程网上 二是用于将一个大的IP网络划分为若干小的子网络。 1.2网络端口号 1.2.1概念及作用 端口号是网络中通过IP地址+端口号区分不同的服务 1.2.2范围及分类 端口号是一个数字，只有整数，范围是从0到65535(分为知名和动态两种) 知名端口是众所周知的端口号，用来做固定的事情，范围是从0到1023 80端口分配给http服务 21端口分配给ftp服务 动态端口是一般不固定分配某种服务，而是动态分配，动态分配是指当一个系统进程或应用程序进程需要网络通信是，它向主机申请一个端口，主机从可用的端口号中分配一个供它使用 1.3网络协议 1.3.1概念 协议：约定好的规范 早期的计算机⽹络， 都是由各⼚商⾃⼰规定⼀套协议， IBM、Apple和 Microsoft都有各⾃的⽹络协议， 互不兼容（语言、方言、阿帕网） 为了把全世界的所有不同类型的计算机都连接起来， 就必须规定⼀套全球通⽤的协议， 为了实现互联⽹这个⽬标，互联⽹协议簇（Internet Protocol Suite）就是通⽤协议标准。 因为互联⽹协议包含了上百种协议标准，但是最重要的两个协议是TCP和IP协议，所以，⼤家把互联⽹的协议总称TCP/IP协议 1.3.2TCP/IP模型 TCP/IP定义了电子设备如何连入因特网，以及数据如何在它们之间传输的标准 4层的层级结构中，每一层都呼叫它的下一层所提供的网络来完成自己的需求 其中的应用层关注的是应用程序的细节，而不是数据在网络中的传输活动 其他三层主要处理所有的通信细节，对应用程序一无所知； 应用层：应用程序间沟通的层，不同的文件系统有不同的文件命名原则和不同的文本行表示方法等，不同的系统之间传输文件还有各种不兼容问题，这些都将由应用层来处理 传输层：在此层中，它提供了节点间的数据传送服务，如传输控制协议（TCP）、用户数据报协议（UDP）等，这一层负责传送数据，并且确定数据已被送达并接收 网络层：负责提供基本的数据包传送功能，让每一块数据包都能够到达目的主机。网络层接收由更低层发来的数据包，并把该数据包发送到更高层，相反，IP层也把从TCP或UDP层接收来的数据包传送到更低层 网络接口层：对实际的网络媒体的管理，定义如何使用实际网络来传送数据（处理机械的、电气的和过程的接口） 示意图 TCP/IP协议簇中各协议之间的关系 1.3.3OSI七层模型 OSI七层模型示意图 物理层： 网线连接在客户端计算机上，其实是连接在了计算机的一个叫做网卡的设备上，网卡是专门负责与外界通信的。网线一般是双绞线或者光缆，也可以使用无线电波，中间经过交换机，路由器，防火墙等等一堆设备统称为物理连接介质，可以理解为经过互联网，再连接到服务端设备。首先工作的是物理层，发送电信号 数据链路层： 电信号分为两种，高电平和低电平，高电平可以被人定义成数字 1，低电平可以被人定义成数字 0。假如我客户端发送一个 0010101100，服务端相应的就会收到这些数字。但是单纯的一段二进制数字这是没有意义的，一定要明确，从哪里开始到哪里结束这表示一段内容，从哪里开始到哪里结束这又表示另外一段内容。这也就是说，我们要给这些二进制数字进行分组 以太网协议规定： 一组电信号构成一个数据报，叫做‘帧’ 每一数据帧分成：报头head和数据data两部分 这一点和我们写信类似，有信封，有信的内容，信封上面会写明这封信的发送者接受者分别是谁， 信里面的信纸上写的就是信的内容。 head包含：(固定18个字节) 发送者／源地址，6个字节 接收者／目标地址，6个字节 数据类型，6个字节 data包含：(最短46字节，最长1500字节) 注意：头固定长度18个字节，也只有固定长度，接收者才知道按照什么标准来读取 数据报的具体内容 head长度＋data长度＝最短64字节，最长1518字节，超过最大限制就分片发送 以太网规定head里面要有发送者的源地址和接受者的目标地址，源地址可以理解为是发送者的家，目标地址就是接收者的家。那么，在计算机中如何标识家在哪里？ 使用mac地址，注意这个mac地址不是你用的苹果电脑那个mac，只是巧合同名了。MAC地址（Media Access Control Address），直译为媒体访问控制地址，也称为局域网地址（LAN Address），以太网地址（Ethernet Address）或物理地址（Physical Address），它是一个用来确认网上设备位置的地址。 mac地址是计算机上一个唯一的地址，是在计算机的网卡上的，每块网卡出厂时都被烧制上一个 世界唯一的mac地址，长度为48位2进制，通常由12位16进制数表示（前六位是厂商编号，后六位是流水线号） 这样做的目的就是要保证每一个mac地址是全世界独一无二的 网络层： 网络层有一个IP协议，我们常说的IPV4就是IP协议的第四个版本，IPV6就是IP协议的第六个版本 传输层： 传输层有一个TCP协议和UDP协议，这两个协议都是基于端口工作的协议 会话层：负责文件发送/接收 表示层：负责数据压缩、编码 应用层： 应用层就是应用软件，应用软件是你写的，这个标准可以由你来定，当然了你也可以遵循一些大家已经定制好了的应用层协议的标准，常见的有 http，mail，ftp 发送过程： 应用层软件（有协议或无协议）===>传输层（TCP/UDP协议）===>网络层（ip协议）===>数据链路层（以太网协议）===>物理层===>电信号发送（100011001010110） 接收过程则相反 TCP/IP与OSI七层模型对比 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/26.2python基础二十六 网络编程-udp编程.html":{"url":"python/python基础/26.2python基础二十六 网络编程-udp编程.html","title":"网络编程-UDP编程","keywords":"","body":"[toc] python基础二十六 网络编程-udp编程 1.Socket编程简介 1.1含义 socket：套接字，通过网路完成进程间通信的方式(区别于一台计算机之间进程通信) 1.2说明 socket本质是编程接口(API) :socket是对TCP/IP协议的封装，socket只是个编程接口不是协议，通过socket我们才能使用TCP/IP协议簇 TCP/IP也要提供可供程序员做网络开发所用的接口，这就是socket编程接口，socket提供了网络通信的能力 套接字之间的连接过程可分为3个步骤： 1.服务器监听 2.客户端请求 3.连接确认 2.创建socket 语法 #导入套接字模块 from socket import * #创建套接字对象 s = socket(socket.AF_NENT,SOCK_DGRAM) 参数说明 AF_NENT #指明IPV4 SOCKET_DGRAM #套接字类型，SOCKET_DGRAM是tcp协议 SOCKET_STREAM #套接字类型，SOCKET_STREAM是udp协议 3.socket编程-udp 3.1udp说明 3.1.1概念 UDP：User Data Protocol，用户数据报协议，是一个无连接的简单的面向数据报的传输层协议，udp不提供可靠性，它只是把应用层传给IP层的数据报发出去，但是并不能保证它们能到达目的地，由于udp在传输数据报前不用在客户端和服务器之间建立一个连接，且没有超时重发等机制，故而传输速度很快** 3.1.2udp用处 udp一般用于多点通信和实时的数据业务，比如 语音广播 视频 QQ TFTP(简单文件传输) 3.2使用udp发送数据 第一步 发送数据，为看到效果先安装网络调试助手NetAssist(windows安装) NetAssist初始配置，协议选择UDP，IP地址会自动识别本机地址，端口号任意选择一个可使用的，配置完成后点击连接按钮 from socket import * #AF_INET表示IPV4 SOCK_DGRAM表示udp协议 s = socket(AF_INET,SOCK_DGRAM) #NetAssist中的默认编码是gb2312，这里需要指定一下，否则显示的信息会是乱码 s.sendto(\"你好\".encode(\"gb2312\"),(\"192.168.34.90\",8080)) 第二步 运行以上代码，会在NetAssist中看到效果 这里可以看到发送的你好已经在NetAssist中收到 3.3使用udp接收数据 3.3.1udp接收数据 from socket import * s = socket(AF_INET, SOCK_DGRAM) #创建套接字 addr = ('127.0.0.1', 8888) #准备接收方地址 data = input(\"请输入：\") s.sendto(data.encode(),addr) #等待接收数据 redata = s.recvfrom(1024) #1024表示本次接收的最大字节数 print(redata) s.close() mac和linux中运行程序，输入内容后程序会卡住，原因未知⚠️⚠️⚠️ windows中运行程序，输入内容后会返回如下结果 (b'abc', ('127.0.0.1', 8888)) 3.2.2udp绑定信息 bind 如果信息(IP地址、端口号)没有绑定，每发送一次信息，系统会随机分配一个端口，还要避免同一台计算机上的不同进程端口号相同的问题 绑定信息：让一个进程可以使用固定的端口 一般情况下，发送方不绑定端口，接收方会绑定 from socket import * s = socket(AF_INET, SOCK_DGRAM) #创建套接字 s.bind(('', 8788)) #绑定本机一个端口，ip地址和端⼝号，ip⼀般不⽤写 addr = ('192.168.1.17', 8080) #准备接收方地址和端口 data = input(\"请输入：\") s.sendto(data.encode(),addr) redata = s.recvfrom(1024) #1024表示本次接收的最⼤字节数 print(redata) s.close() 3.3.3echo服务器 echo服务器就是发送什么，返回什么 udp接收使用recvfrom方法 from socket import * s = socket(AF_INET,SOCK_DGRAM) port = 8888 s.bind((\"\",port)) rdata = s.recvfrom(1024) print(rdata) 执行以上代码，程序会等待接受消息 NetAssist端发送消息，程序会接收到如下结果，是一个元组 (b'hehe', ('192.168.34.11', 8080)) 结果说明： b'hehe' #接收到的是一个字节码 192.168.34.11 #发送方IP地址 8080 #发送方端口 以下代码为udp无限接收消息 from socket import * s = socket(AF_INET,SOCK_DGRAM) port = 8888 s.bind((\"\",port)) #绑定8888端口，注意这里是一个元组⚠️ while True: rdata = s.recvfrom(1024) #s.recvfrom表示接收的消息，1024表示本次接收的最大字节数 s.sendto(rdata[0],rdata[1]) #发送数据，rdata[0]是接受的信息，rdata[1]是接收的IP地址和端口 4.使用socket进行网络通信的过程 1.导入socket模块 2.创建套接字对象 3.绑定IP地址和端口号(接收数据时要绑定端口，发送时可以不绑定) 4.发送消息，需要写明接收方的IP和端口号 5.接受消息(接受消息前如果没有进行过通信，需要先发送一次) 以下代码为模拟全双工，python程序发送信息给NetAssist,NetASssist发送信息给python程序，如果发送的信息中包含886、在见、再见等就退出程序 from socket import * import time #1创建套接字 udpSocket = socket(AF_INET, SOCK_DGRAM) bindAddr = (\"\",7088) udpSocket.bind(bindAddr)#绑定 while True: lst = ['886','在见','再见'] #接收对方发送的数据 recvData = udpSocket.recvfrom(1024) print(recvData) print(type(recvData[0].strip())) #类型是字节 print(str(recvData[0].strip(), encoding='gb2312')) #类型是字符串 if str(recvData[0].strip(),encoding='gb2312') in lst: break print('[%s] %s.%s' %(time.ctime(),recvData[1],recvData[0].decode(\"gb2312\"))) a = input(\"请输入：\") udpSocket.sendto(a.encode('gb2312'),('192.168.34.11',8080)) if a in lst: break udpSocket.close() 5.udp广播 5.1概念、分类、示意图 概念 udp广播：当前网络上所有电脑的某个进程都收到同一个数据(⚠️tcp没有广播) 分类 单播：点对点 多播：一对多 广播：一对所有 示意图 5.2配置udp广播 发送方 from socket import * #创建udp套接字 s = socket(AF_INET,SOCK_DGRAM) #对这个需要发送广播数据的套接字进行修改设置，固定格式，否则不能发送广播数据 s.setsockopt(SOL_SOCKET,SO_BROADCAST,1) #代表当前网段的广播地址，编码如果不写就是utf-8 s.sendto(\"udp广播信息测试\".encode(),(\"\",8080)) s.close() 接收方 from socket import * s = socket(AF_INET,SOCK_DGRAM) addr = s.bind((\"\",8080)) recv = s.recvfrom(1024) print(recv[0].decode()) s.close() 运行过程 1.先运行接收方程序等待接受，程序会卡住直到接收到信息 2.发送方运行程序，向当前网络中发送udp广播，接收程序就会收到发送方的信息 6.基于udp实现的TFTP 6.1TFTP介绍 概念 TFTP(Trivial File Transfer Protocol，简单文件传输协议)是TCP/IP协议簇中一个用来在客户端和服务器之间进行简单文件传输的协议 作用 使用TFTP协议，就可以实现简单文件的下载 特点 简单 占用资源小 适合传递小文件 适合在局域网进行传递 端口号为69 基于udp实现 6.2TFTP传输过程 第一步、客户端向服务端发送读写请求，服务端默认端口udp69 第二步、服务端响应数据包发送给客户端，TFTP数据包有固定的格式 第三步、客户端收到数据包后向服务端返回确认信息ACK 传输过程中涉及的一些问题 服务端向客户端传数据的时候发生丢包怎么办？ 如果服务端发送给客户端的数据包发生丢失情况，则服务端会重新发送数据给客户端 客户端向服务端返回的确认信息丢失怎么办？ 客户端会重发ACK给服务端，这样服务端才能继续传输数据 客户端如何确定服务端已经全部传输完毕？ TFTP协议中，服务端每次会固定向客户端返回516字节的数据(2字节操作码+2字节块编号+512字节真实数据)，当客户端接收到的数据小于516字节时，就意味着服务端已经发送完毕了 如果恰好最后一次数据长度为516字节，服务端会再发一个长度为0的数据包 TFTP能否保证数据不丢包？ TFTP是可以保证数据不丢包的，因为客户端如果没有收到数据服务端会重发数据，服务端没有收到客户端发送的ACK就不会继续发送数据 TFTP不能保证数据不丢失，例如，客户端收到的数据小于服务端发送的516字节，这种情况无法做校验 6.3TFTP格式要求 TFTP格式要求 6.4TFTP构造下载请求数据 TFTP构造下载请求数据需要根据TFTP读写请求格式来编写 以下代码为构造TFTP请求数据示例 #需要导入struct模块 import struct #构造下载请求 filename = \"abc.jpg\" //将文件名赋值给变量，方便修改 requestData = struct.pack(\"!H%dsb5sb\" %len(filename.encode(\"gb2312\")),1,filename.encode(\"gb2312\"),0,\"octet\".encode(\"gb2312\"),0) struct.pack(\"!H%dsb5sb\" %len(filename.encode(\"gb2312\")),1,filename.encode(\"gb2312\"),0,\"octet\".encode(\"gb2312\"),0) struct.pack是一种打包的方法，打包格式中分为6个部分，第1部分是对后5部分进行的统一说明，后5部分是TFTP读写请求固定格式 !H%dsb5sb\" %len(filename.encode(\"gb2312\")) 1 filename.encode(\"gb2312\"）#编码方式根据实际情况修改 0 \"octet\".encode(\"gb2312\") #编码方式根据实际情况修改 0 第一部分 \"!H%dsb5sb\" %len(filename.encode(\"gb2312\")) !表示按照网络传输数据要求的形式来组织数据 H表示将第二部分的1替换成占2个字节 %d是数字占位符，因为后面写了 %len(filename.encode(\"gb2312\"))，因此这里的%s就是存放文件名的变量filename中的文件的字节长度，%d后边的b表示字节 5sb是指后边的 octet,sb表示的是字节，octet是5个字节，因此是5sb，这里是固定的格式 第二部分 1 这里的1已经由前边的H替换成2个字节，表示的是上传还是下载，1是下载，2是上传 第三部分 filename.encode(\"gb2312\") 这里表示将文件名编码成二进制，⚠️注意，这里的编码方式要根据实际情况做相应的修改 第四部分 0 这里的0是固定格式 第五部分 \"octet\".encode(\"gb2312\") 这里的octet是固定格式 第六部分 0 这里的0是固定格式 6.5实现TFTP下载 6.5.1struct模块说明 作用 struct模块可以按照指定格式将python数据转换为字符串，该字符串为字节流 struct中的三个重要函数 pack 按照给定的格式(fmt)，把数据封装成字符串(实际上是类似于c结构的字节流) pack(fmt,v1,v2,...) struct.pack(\"!H8sb5sb\",1,\"test.jpg\",0,\"octet\",0) unpack 按照给的格式(fmt)解析字节流string，返回解析出来的元组 unpact(fat,string) struct.unpack(\"!HH\",4,p_num) cmdTuple = struct.unpack(\"!HH\",recvData[:4]) calcsize 计算给定的格式(fmt)占用多少字节 struct模块使用说明图 6.5.2TFTP下载程序 第一步、设置TFTP服务端 实现TFTP需要用到一个软件Tftpd32，选择共享的目录用来提供下载，选择本机网卡127.0.0.1 第二步、编写下载器(客户端) 实现TFTP下载器 下载：从服务器上将一个文件复制到本地 下载过程 在本地创建一个空文件，文件名一定要与下载的文件名相同 向空文件中写入接收到的数据，接收一点写入一点 接收完所有数据后关闭文件 编写一个TFTP下载程序 #导入struct模块、socket模块、time模块 import struct from socket import * import time #TFTP中共享的文件及TFTP服务端IP地址分别写入变量中 filename = \"xiaohua.jpg\" serverIP = \"192.168.34.112\" #利用struck模块的pack方法封装请求数据，代码具体含义在6.4TFTP构造下载请求数据 requestData = struct.pack(\"!H%dsb5sb\" %len(filename.encode(\"gb2312\")),1,filename.encode(\"gb2312\"),0,\"octet\".encode(\"gb2312\"),0) #创建套接字对象 s = socket(AF_INET,SOCK_DGRAM) #发送请求数据 s.sendto(requestData,(serverIP,69)) #设置文件句柄，将后续接收的数据写入到文件中，⚠️写入的文件必须与TFTP中共享的文件名相同 f = open(filename,\"ab\") #因为不知道要接受的数据有多大，因此写一个while循环循环接收，知道接收完成 while True: #接收数据，打印一下看看接收到的内容是什么 recvData = s.recvfrom(1024) print(recvData) 收到的数据内容如下，是一个元组，分为两个部分 第一部分是 操作码+块编号+真实数据 一共516字节 操作码：前2个字节 块编号：前2个字节 真实数据：512字节 第二部分是 TFTP服务端IP及TFTP向客户端响应数据时用到的随机端口 ⚠️TFTP向客户端返回数据时不会使用默认的69端口，会使用一个随机端口，因为69端口还需要向其他客户端响应请求，而后续客户端向服务端返回ACK确认信息时，也需要用到这个随机端口⚠️ (b'\\x00\\x03\\x00\\x01FLV\\x01\\此处省略一万字\\x00!modified by youku.com in 20111202\\x00\\x0chasKeyframes\\\\x00\\x00Aj\\xb0\\xa6@\\x00', ('192.168.34.112', 49373)) \\x00\\x03\\x00\\x01FLV 这一部分其实就能看到操作码和块编号，操作码是3(\\x03),块编号是1(\\x01)，文件名是(FLv) 上述代码中已经收到了TFTP响应的数据，接下来获取一下操作码和块编号 #因为不知道要接收的数据有多大，因此写一个while循环循环接收，知道接收完成 while True: #接收数据，打印一下看看接收到的内容是什么 recvData = s.recvfrom(1024) print(recvData) #获取操作码和块编号，这里用到了struct模块中的unpack(解包)方法 caozuoma,kuaibianhao = struct.unpack(\"!HH\",recvData[0][:4]) //获取操作码和块编号代码说明 收到的数据如下 (b'\\x00\\x03\\x00\\x01FLV\\x01\\此处省略一万字\\x00!modified by youku.com in 20111202\\x00\\x0chasKeyframes\\\\x00\\x00Aj\\xb0\\xa6@\\x00', ('192.168.34.112', 49373)) 要获取操作码和代码块，需要截取收到的数据的第一部分中的前4个字节 返回的数据的是一个元组 第一部分是 操作码+块编号+真实数据 一共516字节 操作码：前2个字节 块编号：前2个字节 真实数据：512字节 第二部分是 服务器IP地址和随机端口 获取到的结果如下，因为还没有向服务器发送ACK确认信息，因此块编号会一直收到1 3 1 3 1 。。。 现在已经获取到了操作码和块编号，接下来就可以写入本地文件以及向服务器发送ACK确认信息了 #先判断一下操作码是否是5，如果是5则就是错误信息 if caozuoma == 5: print(\"文件不存在！！！\") break #将收到的数据写入本地文件，收到的数据的第一部分第4个字节后的512字节就是真实数据 f.write(recvData[0][4:]) #TFTP协议中每次传输的数据是512字节，这里做一个判断，如果数据小于512字节则说明客户端接收完毕 ⚠️这里需要注意一下的是，如果最后一次传输的数据恰好等于512字节，则服务端会再次发送一个数据长度为0的包 if len(recvData[0]) 完整代码 import struct from socket import * import time filename = \"xiaohua.jpg\" serverIP = \"192.168.34.112\" requestData = struct.pack(\"!H%dsb5sb\" %len(filename.encode(\"gb2312\")),1,filename.encode(\"gb2312\"),0,\"octet\".encode(\"gb2312\"),0) s = socket(AF_INET,SOCK_DGRAM) s.sendto(requestData,(serverIP,69)) f = open(filename,\"ab\") while True: recvData = s.recvfrom(1024) print(recvData) caozuoma,kuaibianhao = struct.unpack(\"!HH\",recvData[0][:4]) serverPort = recvData[1][1] print(caozuoma,kuaibianhao) if caozuoma == 5: print(\"文件不存在！！！\") break f.write(recvData[0][4:]) if len(recvData[0]) 6.6实现TFTP上传 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/26.3python基础二十六 网络编程-tcp编程.html":{"url":"python/python基础/26.3python基础二十六 网络编程-tcp编程.html","title":"网络编程-TCP编程","keywords":"","body":"[toc] python基础二十六 网络编程-tcp编程 1.TCP介绍 TCP 与 UDP 的区别相当大。它充分地实现了数据传输时各种控制功能，可以进行丢包时的重发控制，还可以对次序乱掉的分包进行顺序控制。而这些在 UDP 中都没有。 此外，TCP 作为一种面向有连接的协议，只有在确认通信对端存在时才会发送数据，从而可以控制通信流量的浪费。 根据 TCP 的这些机制，在 IP 这种无连接的网络上也能够实现高可靠性的通信（ 主要通过检验和、序列号、确认应答、重发控制、连接管理以及窗口控制等机制实现）。 2.TCP三次握手与四次挥手 2.1TCP三次握手 TCP 提供面向有连接的通信传输。面向有连接是指在数据通信开始之前先做好两端之间的准备工作。 所谓三次握手是指建立一个 TCP 连接时需要客户端和服务器端总共发送三个包以确认连接的建立。在socket编程中，这一过程由客户端执行connect来触发。 下面来看看三次握手的流程图： 三次握手过程 第一次握手：客户端将标志位SYN置为1，随机产生一个值seq=J，并将该数据包发送给服务器端，客户端进入SYN_SENT状态，等待服务器端确认。 第二次握手：服务器端收到数据包后由标志位SYN=1知道客户端请求建立连接，服务器端将标志位SYN和ACK都置为1，ack=J+1，随机产生一个值seq=K，并将该数据包发送给客户端以确认连接请求，服务器端进入SYN_RCVD状态。 第三次握手：客户端收到确认后，检查ack是否为J+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=K+1，并将该数据包发送给服务器端，服务器端检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，客户端和服务器端进入ESTABLISHED状态，完成三次握手，随后客户端与服务器端之间可以开始传输数据了。 2.2TCP四次挥手 四次挥手即终止TCP连接，就是指断开一个TCP连接时，需要客户端和服务端总共发送4个包以确认连接的断开。在socket编程中，这一过程由客户端或服务端任一方执行close来触发。 由于TCP连接是全双工的，因此，每个方向都必须要单独进行关闭，这一原则是当一方完成数据发送任务后，发送一个FIN来终止这一方向的连接，收到一个FIN只是意味着这一方向上没有数据流动了，即不会再收到数据了，但是在这个TCP连接上仍然能够发送数据，直到这一方向也发送了FIN。首先进行关闭的一方将执行主动关闭，而另一方则执行被动关闭。 下面来看看四次挥手的流程图： 中断连接端可以是客户端，也可以是服务器端。 第一次挥手：客户端发送一个FIN=M，用来关闭客户端到服务器端的数据传送，客户端进入FIN_WAIT_1状态。意思是说\"我客户端没有数据要发给你了\"，但是如果你服务器端还有数据没有发送完成，则不必急着关闭连接，可以继续发送数据。 第二次挥手：服务器端收到FIN后，先发送ack=M+1，告诉客户端，你的请求我收到了，但是我还没准备好，请继续你等我的消息。这个时候客户端就进入FIN_WAIT_2 状态，继续等待服务器端的FIN报文。 第三次挥手：当服务器端确定数据已发送完成，则向客户端发送FIN=N报文，告诉客户端，好了，我这边数据发完了，准备好关闭连接了。服务器端进入LAST_ACK状态。 第四次挥手：客户端收到FIN=N报文后，就知道可以关闭连接了，但是他还是不相信网络，怕服务器端不知道要关闭，所以发送ack=N+1后进入TIME_WAIT状态，如果Server端没有收到ACK则可以重传。服务器端收到ACK后，就知道可以断开连接了。客户端等待了2MSL后依然没有收到回复，则证明服务器端已正常关闭，那好，我客户端也可以关闭连接了。最终完成了四次挥手。 上面是一方主动关闭，另一方被动关闭的情况，实际中还会出现同时发起主动关闭的情况， 具体流程如下图： 3.TCP其他特性 3.1通过序列号与确认应答提高可靠性 在 TCP 中，当发送端的数据到达接收主机时，接收端主机会返回一个已收到消息的通知。这个消息叫做确认应答（ACK）。当发送端将数据发出之后会等待对端的确认应答。如果有确认应答，说明数据已经成功到达对端。反之，则数据丢失的可能性很大。 在一定时间内没有等待到确认应答，发送端就可以认为数据已经丢失，并进行重发。由此，即使产生了丢包，仍然能够保证数据能够到达对端，实现可靠传输。 未收到确认应答并不意味着数据一定丢失。也有可能是数据对方已经收到，只是返回的确认应答在途中丢失。这种情况也会导致发送端误以为数据没有到达目的地而重发数据。 此外，也有可能因为一些其他原因导致确认应答延迟到达，在源主机重发数据以后才到达的情况也屡见不鲜。此时，源主机只要按照机制重发数据即可。 对于目标主机来说，反复收到相同的数据是不可取的。为了对上层应用提供可靠的传输，目标主机必须放弃重复的数据包。为此我们引入了序列号。 序列号是按照顺序给发送数据的每一个字节（8位字节）都标上号码的编号。接收端查询接收数据 TCP 首部中的序列号和数据的长度，将自己下一步应该接收的序列号作为确认应答返送回去。通过序列号和确认应答号，TCP 能够识别是否已经接收数据，又能够判断是否需要接收，从而实现可靠传输。 3.2重发超时的确定 重发超时是指在重发数据之前，等待确认应答到来的那个特定时间间隔。如果超过这个时间仍未收到确认应答，发送端将进行数据重发。最理想的是，找到一个最小时间，它能保证“确认应答一定能在这个时间内返回”。 TCP 要求不论处在何种网络环境下都要提供高性能通信，并且无论网络拥堵情况发生何种变化，都必须保持这一特性。为此，它在每次发包时都会计算往返时间及其偏差。将这个往返时间和偏差时间相加，重发超时的时间就是比这个总和要稍大一点的值。 在 BSD 的 Unix 以及 Windows 系统中，超时都以0.5秒为单位进行控制，因此重发超时都是0.5秒的整数倍。不过，最初其重发超时的默认值一般设置为6秒左右。 数据被重发之后若还是收不到确认应答，则进行再次发送。此时，等待确认应答的时间将会以2倍、4倍的指数函数延长。 此外，数据也不会被无限、反复地重发。达到一定重发次数之后，如果仍没有任何确认应答返回，就会判断为网络或对端主机发生了异常，强制关闭连接。并且通知应用通信异常强行终止。 3.3以段为单位发送数据 在建立 TCP 连接的同时，也可以确定发送数据包的单位，我们也可以称其为“最大消息长度”（MSS）。最理想的情况是，最大消息长度正好是 IP 中不会被分片处理的最大数据长度。 TCP 在传送大量数据时，是以 MSS 的大小将数据进行分割发送。进行重发时也是以 MSS 为单位。 MSS 在三次握手的时候，在两端主机之间被计算得出。两端的主机在发出建立连接的请求时，会在 TCP 首部中写入 MSS 选项，告诉对方自己的接口能够适应的 MSS 的大小。然后会在两者之间选择一个较小的值投入使用。 3.4利用窗口控制提高速度 TCP 以1个段为单位，每发送一个段进行一次确认应答的处理。这样的传输方式有一个缺点，就是包的往返时间越长通信性能就越低。 为解决这个问题，TCP 引入了窗口这个概念。确认应答不再是以每个分段，而是以更大的单位进行确认，转发时间将会被大幅地缩短。也就是说，发送端主机，在发送了一个段以后不必要一直等待确认应答，而是继续发送。如下图所示： 窗口大小就是指无需等待确认应答而可以继续发送数据的最大值。上图中窗口大小为4个段。这个机制实现了使用大量的缓冲区，通过对多个段同时进行确认应答的功能。 3.5滑动窗口控制 上图中的窗口内的数据即便没有收到确认应答也可以被发送出去。不过，在整个窗口的确认应答没有到达之前，如果其中部分数据出现丢包，那么发送端仍然要负责重传。为此，发送端主机需要设置缓存保留这些待被重传的数据，直到收到他们的确认应答。 在滑动窗口以外的部分包括未发送的数据以及已经确认对端已收到的数据。当数据发出后若如期收到确认应答就可以不用再进行重发，此时数据就可以从缓存区清除。 收到确认应答的情况下，将窗口滑动到确认应答中的序列号的位置。这样可以顺序地将多个段同时发送提高通信性能。这种机制也别称为滑动窗口控制。 3.6窗口控制中的重发控制 在使用窗口控制中， 出现丢包一般分为两种情况： ① 确认应答未能返回的情况。在这种情况下，数据已经到达对端，是不需要再进行重发的，如下图： ② 某个报文段丢失的情况。接收主机如果收到一个自己应该接收的序列号以外的数据时，会针对当前为止收到数据返回确认应答。如下图所示，当某一报文段丢失后，发送端会一直收到序号为1001的确认应答，因此，在窗口比较大，又出现报文段丢失的情况下，同一个序列号的确认应答将会被重复不断地返回。而发送端主机如果连续3次收到同一个确认应答，就会将其对应的数据进行重发。这种机制比之前提到的超时管理更加高效，因此也被称为高速重发控制。 4.TCP编程流程 4.1长连接与短连接 长连接 长连接指的是三次握手与四次挥手之间分多次传递完所有数据，会长时间占用某个套接字，例如在线看视频、打游戏就是长连接 短连接 短连接指的是三次握手与四次挥手之间传递少部分数据，多次握手挥手才传递完所有数据，会短时间占用某个套接字，例如浏览器静态页面就是短连接，因为浏览器静态页面可能会非常多，而浏览者不一定会全部阅读完，因此浏览器会先加载一部分页面，然后断开连接，将连接空闲出来提供其他请求，等到浏览者继续往下流览再把剩余的内容建立连接发送完成 4.2TCP编程流程说明 过程说明 服务端 1.服务端创建socket套接字对象，用于收发数据 socket() 2.服务端绑定自身的IP及端口 bind() 3.服务端设置最大连接数 listen() 这里的最大连接数指的是，服务端达到最大连接数后，后续可以排队等待的请求数，超过这个数字就会被服务端拒绝服务，例如listen(5)，这里设置为5，假如服务器达到了最大连接数1万，则可以排队等待的请求数为5 4.服务端创建accept()，等待接受socket连接，服务端的accept会与客户端创建的connect建立TCP三次握手 客户端 1.客户端创建socket套接字对象，用于收发数据 socket() 2.客户端创建connect，用于连接服务端，connect会与服务端创建的accept建立TCP三次握手 5.创建TCP服务器、客户端 5.1最low版TCP服务器、客户端 编写一个最简单的TCP服务器 #导入socket模块 from socket import * #创建socket套接字，用于tcp监听 tcpSocket = socket(AF_INET,SOCK_STREAM) #绑定服务端IP地址和端口 tcpSocket.bind((\"127.0.0.1\",8080)) #设置最大连接数,这里的最大连接数指的是服务端达到最大连接数后可以排队等待的请求数 tcpSocket.listen(5) #创建一个新的套接字，等待接受socket连接，于收发数据,接受到的数据解构为新套接字和客户端IP地址 newTcpSocket,addr = tcpSocket.accept() #发送数据，编码根据实际情况指定 newTcpSocket.send(\"我是tcp服务端，快来连我！\".encode()) print(newTcpSocket.recv(1024).decode()) #发送完数据后关闭用于收发数据的新套接字对象 newTcpSocket.close() #关闭用于监听的套接字对象，关闭后程序不再接受任何新的客户端连接 tcpSocket.close() 编写一个最简单的TCP客户端 #导入socket模块 from socket import * #创建socket套接字对象 tcpSocket = socket(AF_INET,SOCK_STREAM) #创建connect，用于连接TCP服务器 tcpSocket.connect((\"127.0.0.1\",8080)) #接受数据 recvData = tcpSocket.recv(1024) print(recvData.decode()) #向TCP服务端发送数据 tcpSocket.send(\"我是tcp客户端，我来了！\".encode()) #关闭套接字对象 tcpSocket.close() 5.2单进程TCP服务器 单进程的TCP服务器每次只能服务一个客户端 #导入socket模块 from socket import * #创建只用来监听的套接字对象 serverSocket = socket(AF_INET,SOCK_STREAM) #绑定TCP服务端IP和端口 addr = (\"192.168.34.90\",9999) serverSocket.bind(addr) #设置最大排队等待数 serverSocket.listen(3) #这里要能多次处理客户端连接请求，因此写一个while循环 while True: print(\"主进程等待新客户端连接\") #创建accept，用来等待客户端socket连接 newSocket,clientAddr = serverSocket.accept() print(newSocket) #打印结果 print(clientAddr) #打印结果 ('192.168.34.90', 55255) #clientAddr[0]就是客户端的IP地址，clientAddr[1]就是客户端的端口 print(f\"主进程接下来负责处理{clientAddr[0]},端口{clientAddr[1]}的请求\") #传输过程可能会出错，因此写一个异常处理避免程序崩溃 try: while True: #接受数据并解码，编码类型根据实际情况填写 recvData = newSocket.recv(1024).decode() #做一个判断，如果收到的数据内容长度大于0，则说明是在接受数据，并打印接受的数据，都则就提示客户端已关闭 if len(recvData) > 0: print(f\"接收到来自{clientAddr[0]}，端口{clientAddr[1]}的数据:\",recvData) else: print(f\"{clientAddr[0]}客户端已关闭\") break except Exception: print(\"接收数据出错！\") #无论接受是否报错最后都执行关闭新建的用于收发数据的套接字 finally: newSocket.close() break #关闭用于监听的套接字 serverSocket.close() TCP客户端编写 #导入socket模块 from socket import * #创建socket套接字对象 tcpSocket = socket(AF_INET,SOCK_STREAM) #创建connect，用于连接TCP服务器 tcpSocket.connect((\"192.168.34.90\",9999)) #向TCP服务端发送数据 while True: s = input(\"请输入要发送的内容>>>\") tcpSocket.send(s.encode()) #如果客户端输入的是Q或者q，则关闭套接字对象并退出程序 if s.lower() == \"q\": tcpSocket.close() break 服务端接受本机客户端发送的信息 服务端接收其他机器客户端发送的信息 5.3并发TCP服务器 5.3.1 setsockopt(SOL_SOCKET,SO_REUSEADDR,1）方法 serSocket.setsockopt(SOL_SOCKET,SO_REUSEADDR,1） serSokcet是套接字对象变量名，此选项意思为重新设置套接字选项，重复使用绑定的信息 这么做的原因？ 当有一个有相同本地地址和端口的socket1处于TIME_WAIT状态时，而你启动的程序的socket2要占用该地址和端口，你的程序就要用到SO_REUSEADDR选项。 在多进程中的作用？ 使用多进程编写并发TCP服务，因为有多个子进程可以提供服务，因此每个子进程需要占用主进程的端口，但是端口只能被一个进程占用，这个时候就出现了端口被占用的情况，所以用到了setsockopt(SOL_SOCKET,SO_REUSEADDR,1）方法 在多线程中的作用？ 使用多线程编写并发TCP服务，线程之间是共享数据的，不存在端口被占用情况，但是多线程中每个子线程使用主进程的端口后，系统会保留几分钟端口被占用的状态，不让别的线程使用，使用setsockopt(SOL_SOCKET,SO_REUSEADDR,1）方法让端口不被系统保留 5.3.2多进程TCP服务器 5.3.3多线程TCP服务器 6.sockeserver 6.1sockeserver介绍 概念 socketserver可以实现和多个客户端通信（实现并发处理多个客户端请求的Socket服务端） 作用 可以使用socketserver来创建socket用来简化并发服务器 原理 它是在socket的基础上进行了一层封装，也就是说底层还是调用的socket 处理请求过程 服务器接受客户端连接请求 ➡️ 实例化一个请求处理程序 ➡️ 根据服务器类和请求处理程序类，调用处理方法。 例如：基本请求程序类（BaseRequestHandler）调用方法 handle 。此方法通过属性 self.request来访问客户端套接字 7.远程执行命令subprocess 7.1subprocess介绍 作用 python可以使用subprocess模块下Popen类中封装的方法来执行系统终端命令 语法 obj = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE) //参数说明 shell=True #命令解释器，相当于调用windows中的cmd或者mac中的终端执行指定的命令 stdout=subprocess.PIPE #stdout表示正确结果丢到管道中 stderr=subprocess.PIPE #stderr表示错误结果丢到管道中 PIPE #PIPE表示将结果转移到当前进程 subprocess方法 Popen() 构造方法，用于创建Popen类的实例化对象 stdout.read() stderr.read() 可以获取命令执行的结果(正确的与错误的) 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python基础/27.python基础二十七 python操作mysql.html":{"url":"python/python基础/27.python基础二十七 python操作mysql.html","title":"python基础二十七 python操作mysql","keywords":"","body":"[toc] python基础二十七 python操作mysql 1.安装pymysql pip3 install pymysql 2.python连接mysql 2.1python连接mysql语法 //导入pymysql模块 import pymysql //打开数据库连接 ⚠️⚠️⚠️指定字符集的时候utf-8在这里的写法为utf8 conn = pymysql.connect(\"数据库ip\",\"用户\",\"密码\",\"数据库\",\"端口(不写默认为3306)\",\"字符集\") //使用cursor()方法获取操作游标 cursor = conn.cursor() //使用execute()方法执行SQL操作 cursor.execute(\"SQL语句\") //使用fetchone()方法获取单条数据 data = cursor.fetchone() print (\"Database version : %s \" %data) //关闭游标 cursor.close() //关闭数据库连接 conn.close() 2.2python连接mysql简单查询示例 //数据库db1中有一张t1表，表内容如下 mysql> select * from t1; +------+--------+ | id | name | +------+--------+ | 1 | 小明 | | 2 | 小颖 | | 3 | 小丽 | +------+--------+ 3 rows in set (0.00 sec) //接下来连接数据库进行操作 #导入pymysql模块 import pymysql #打开数据库连接 conn = pymysql.connect( host='xxx', user='xxx', password=\"xxx\", database='db1', port=3306, charset='utf8', ) #创建游标 cursor = conn.cursor() #定义一个变量，存放sql语句 sql = 'select * from t1' #使用execute()方法执行sql操作 cursor.execute(sql) #使用fetchall()方法获取所有数据 data = cursor.fetchall() #打印查询的内容 print(data) ((1, '小明'), (2, '小颖'), (3, '小丽')) #关闭游标 cursor.close() #关闭连接 conn.close() 3.python操作mysql 3.1创建表操作 #导入pymysql模块 import pymysql #打开数据库连接 db = pymysql.connect(\"localhost\",\"testuser\",\"test123\",\"TESTDB\" ) #使用cursor()方法创建一个游标对象cursor cursor = db.cursor() #使用execute()方法执行 SQL，如果表存在则删除 cursor.execute(\"drop table if exists t2\") #使用预处理语句创建表 sql = \"\"\"create table t2( id int not null, age int not null, name char(10) not null, hobby set('唱','跳','rap','篮球'))\"\"\" #执行sql语句 cursor.execute(sql) #查看结果 mysql> show tables; +---------------+ | Tables_in_db1 | +---------------+ | t1 | | t2 | +---------------+ 2 rows in set (0.00 sec) mysql> desc t2; +-------+---------------------------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+---------------------------------+------+-----+---------+-------+ | id | int(11) | NO | | NULL | | | age | int(11) | NO | | NULL | | | name | char(10) | NO | | NULL | | | hobby | set('唱','跳','rap','篮球') | YES | | NULL | | +-------+---------------------------------+------+-----+---------+-------+ 4 rows in set (0.00 sec) #关闭游标 cursor.clos() #关闭数据库连接 conn.close() 3.2数据操作 3.2.1查询操作 python操作mysql时获取数据的方法有3种 cursor.fetchone() #查询返回一条结果 cursor.fetchmany(n) #查询返回指定的条数，n为数字 cursor.fetchall() #查询返回所有的结果 //数据库db1中有一张t1表，表内容如下 mysql> select * from t1; +------+--------+ | id | name | +------+--------+ | 1 | 小明 | | 2 | 小颖 | | 3 | 小丽 | +------+--------+ 3 rows in set (0.00 sec) //使用cursor.fetchone()方法获取一条结果 data = cursor.fetchone() print(data) (1, '小明') //使用cursor.fetchmany(n)方法指定获取的内容个数，n为数字 #fetchmany不指定获取个数时返回的内容 data = cursor.fetchmany() print(data) ((1, '小明'),) #fetchmany指定获取个数时返回的内容 data = cursor.fetchmany(2) print(data) ((1, '小明'), (2, '小颖')) //使用cursor.fetchall()方法获取全部内容 data = cursor.fetchall() print(data) ((1, '小明'), (2, '小颖'), (3, '小丽')) fetch方法返回的结果都是元组，没法看出哪个数据是对应的哪个字段，可以采用字典的方式显示，这样看起来比较明确 可以在创建游标的时候，加上一个参数让返回的结果是字典格式展示 //创建游标的时候加上一个参数cursor=pymysql.cursors.DictCursor让返回的结果是字典格式 cursor = conn.cursor(cursor=pymysql.cursors.DictCursor) data = cursor.fetchall() print(data) [{'id': 1, 'name': '小明'}, {'id': 2, 'name': '小颖'}, {'id': 3, 'name': '小丽'}] 3.2.2增加数据操作 //数据库中t2表内容为空 mysql> desc t2; +-------+---------------------------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+---------------------------------+------+-----+---------+-------+ | id | int(11) | NO | | NULL | | | age | int(11) | NO | | NULL | | | name | char(10) | NO | | NULL | | | hobby | set('唱','跳','rap','篮球') | YES | | NULL | | +-------+---------------------------------+------+-----+---------+-------+ 4 rows in set (0.00 sec) mysql> select * from t2; Empty set (0.00 sec) //现在向t2表中插入数据 import pymysql conn = pymysql.connect( host='xxx', user='pptfz', password=\"xxx\", database='db1', port=3306, charset='utf8', ) cursor = conn.cursor(cursor=pymysql.cursors.DictCursor) sql = \"\"\" insert into t2 values(1,18,'小明','唱,篮球'), (2,20,'小颖','唱,跳'), (3,28,'小坤','唱,跳,rap,篮球') \"\"\" cursor.execute(sql) conn.commit() conn.close() //查看结果 sql = 'select * from t2' cursor.execute(sql) data = cursor.fetchall() print(data) [{'id': 1, 'age': 18, 'name': '小明', 'hobby': '唱,篮球'}, {'id': 2, 'age': 20, 'name': '小颖', 'hobby': '唱,跳'}, {'id': 3, 'age': 28, 'name': '小坤', 'hobby': '唱,跳,rap,篮球'}] 3.2.3删除数据操作 #创建游标 cursor = conn.cursor(cursor=pymysql.cursors.DictCursor) #sql语句 sql = 'delete from t2 where id=3' #执行sql操作，发生错误回滚 try: cursor.execute(sql) conn.commit() except: conn.rollback() #关闭连接 conn.close() 3.2.4修改数据操作 #创建游标 cursor = conn.cursor(cursor=pymysql.cursors.DictCursor) #sql语句 sql = 'update t2 set age=30 where id=2' #执行sql操作，发生错误回滚 try: cursor.execute(sql) conn.commit() except: conn.rollback() #关闭连接 conn.close() 4.execute()之sql注入 4.1先做一个简单的登陆认证 ⚠️这里的示例有sql注入的问题，会在后边的示例中解决 //userinfo表内容如下 mysql> select * from userinfo; +-------+-------+ | uname | pwd | +-------+-------+ | admin | admin | +-------+-------+ 1 row in set (0.00 sec) //接下来做一个单一的判断 #导入pymysql import pymysql #连接mysql conn = pymysql.connect( host='xxx', user='pptfz', password=\"xxx\", database='db1', port=3306, charset='utf8', ) #创建游标 cursor = conn.cursor(cursor=pymysql.cursors.DictCursor) #输入用户民共和密码 uname = input('请输入用户名>>>') password = input('请输入密码>>>') #执行sql语句，返回结果大于1说明用户名和密码存在，否则就是不存在 sql = \"select * from userinfo where uname='%s' and pwd='%s'\"%(uname,password) #执行sql语句 cursor.execute(sql) #查询的结果 data = cursor.fetchone() #做判断 if data: print('登陆成功') else: print('用户名或密码错误') conn.close() 执行结果如下： 当输入的用户名和密码存在于userinfo表中时，这里为固定的用户名amin密码admin返回登陆成功，否则返回用户名或密码错误，这样就做了一个简单的用户登陆认证 登陆成功 登陆失败 4.2sql注入简单示例 4.2.1示例1 接下来做一个操作，在输入用户名的时候，在用户名后边加一个单引号，然后空格，然后写上--，最后--的后面写上任意字符，这样就能在知道用户名的情况下不需要输入密码就可以登陆成功 import pymysql conn = pymysql.connect( host='xxx', user='pptfz', password=\"xxx\", database='db1', port=3306, charset='utf8', ) cursor = conn.cursor(cursor=pymysql.cursors.DictCursor) uname = input('请输入用户名>>>') password = input('请输入密码>>>') # sql = f'select * from userinfo where uname=\"{uname}\" and pwd=\"{password}\"' sql = \"select * from userinfo where uname='%s' and pwd='%s';\"%(uname,password) cursor.execute(sql) # data = cursor.fetchall() data = cursor.fetchone() if data: print('登陆成功') else: print('登陆失败') conn.close() 分析一下结果 此时uname这个变量等于admin' -- xxx，sql语句被字符串替换之后会变成如下 select * from userinfo where uname='admin' -- xxx' and pwd=''; 其中admin后边的'，在进行字符串替换的时候，我们输入的是admin'，这个引号和前边的引号组成了一对，也就是select * from userinfo where uname='admin'' -- xxx and pwd='' --在sql语句中是注释的意思，也就是说后面的语句被注释了，此时的sql语句变成了select * from usrinfo where uname='admin'，后边的' -- xxx and pwd=''变成了注释 这样的话就是知道用户名即可登陆成功，这就是最简单的sql注释 4.2.2示例2 用户名和密码都不需要输入就能登陆成功 or后边跟了一个永真的条件，因此不管输入什么都能登陆成功 有些网站直接在输入内容的时候，就限定了不能输入一些特殊的符号，因为有些特殊符号可以改变sql的执行逻辑，其实不光是--，还有一些其他的符号也能改变sql语句的执行逻辑，这个方案是在客户端给用户输入的地方进行限制，但是别人可不可以模拟你的客户端来发送请求，是可以的，他模拟一个客户端，不按照你的客户端的要求来，就发一些特殊字符，你的客户端是限制不了的。 》所以单纯的在客户端进行这个特殊字符的过滤是不能解决根本问题的，那怎么办？我们服务端也需要进行验证，可以通过正则来将客户端发送过来的内容进行特殊字符的匹配，如果有这些特殊字符，我们就让它登陆失败。 在服务端来解决sql注入的问题：不要自己来进行sql字符串的拼接了，pymysql能帮我们拼接，他能够防止sql注入，所以以后我们再写sql语句的时候按下面的方式写： //之前我们的sql语句是这样写的： sql = \"select * from userinfo where uname='%s' and pwd='%s';\"%(uname,passwdor) //以后再写的时候，sql语句里面的%s左右的引号去掉，并且语句后面的%(uname,pword)这些内容也不要自己写了，按照下面的方式写 sql = \"select * from userinfo where username=%s and password=%s;\" //难道我们不传值了吗，不是的，我们通过下面的形式，在excute里面写参数： 其实它本质也是帮你进行了字符串的替换，只不过它会将uname和password里面的特殊字符给过滤掉 cursor.execute(sql,[uname,password]) 使用cursor.execute方法执行sql语句正确写法 import pymysql conn = pymysql.connect( host='xxx', user='pptfz', password=\"xxx\", database='db1', port=3306, charset='utf8', ) cursor = conn.cursor(cursor=pymysql.cursors.DictCursor) uname = input('请输入用户名>>>') password = input('请输入密码>>>') # sql = f'select * from userinfo where uname=\"{uname}\" and pwd=\"{password}\"' sql = \"select * from userinfo where uname=%s and pwd=%s\" cursor.execute(sql,[uname,password]) # data = cursor.fetchall() data = cursor.fetchone() if data: print('登陆成功') else: print('登陆失败') conn.close() 使用4.2.1示例1中的写法登陆，会提示登陆失败 使用4.2.2示例2中的写法登陆，会提示登陆失败 这样使用cursor.execute方法就解决了简单sql注入的问题，因此以后的任意语句都必须使用此写法来代替我们自己写的字符串替换 cursor.execute(sql,[uname,password]) 4.3sql注入简单总结 4.3.1sql注入的两种方法 两种sql注入方法 1、sql注入之：用户存在，绕过密码 用户名' -- 任意字符 2、sql注入之：用户不存在，绕过用户与密码 用户名' or 1=1 -- 任意字符 4.3.2通过pymysql提供的excute()方法解决了简单sql注入问题 //原来是我们对sql进行字符串拼接 sql=\"select * from userinfo where uname='%s' and pwd='%s'\" %(uname,password) print(sql) data = cursor.execute(sql) //改写为execute帮我们做字符串拼接，我们无需且一定不能再为%s加引号了 sql=\"select * from userinfo where uname=%s and pwd=%s\" //⚠️%s需要去掉引号，因为pymysql会自动为我们加上 data = cursor.execute(sql,[uname,password]) #pymysql模块自动帮我们解决sql注入的问题，只要我们按照pymysql的规矩来。 ⚠️⚠️⚠️sql语句不要自己拼接，交给pymysql提供的execute方法解决 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python进阶/django/1.django创建项目.html":{"url":"python/python进阶/django/1.django创建项目.html","title":"django基础一 创建项目","keywords":"","body":"[toc] 1.django创建项目 1.1方式一 命令创建项目 第一步、安装django pip3 install django==1.11.9 第二步、创建项目 创建一个目录并切换到这个目录下，然后执行以下命令创建项目 django-admin startproject 项目名称 创建项目时遇到的项目名称问题，不能加 -，可以加 _ 第三步、创建应用 进入到创建的项目目录中然后执行以下命令 python3 manage.py startapp 应用名称 第四步、修改配置文件 #创建的项目路径 project01就是项目 /jetBrains/pycharm/django/project01 #创建的应用的路径 app03就是应用 /jetBrains/pycharm/django/project01/app03 #修改的配置文件的路径 创建的项目下同名的目录中的settings.py /jetBrains/pycharm/django/project01/project01 修改以下配置，添加创建的应用名称 大概在40行 INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'app03', #最后一行添加创建的应用名称,将项目和应用关联到一起 ] 第五步、启动项目 如果端口不写，默认是8000 如果IP地址和端口号都不写，默认是127.0.0.1:8000 python3 manage.py runserver 127.0.0.1:8001 第六步、浏览器访问 127.0.0.1:8001 到此，最基本的django创建项目、应用、启动完成 用pycharm打开创建的django项目 1.2方式二 利用pycharm创建项目 File-->New Project 打开pycharm创建的django项目 1.3项目目录文件介绍 manage.py -----> Django项目里面的工具，通过它可以调用django shell和数据库，启动关闭项目与项目交互等，不管你将框架分了几个文件，必然有一个启动文件，其实他们本身就是一个文件 settings.py -----> 包含了项目的默认设置，包括数据库信息，调试标志以及其他一些工作的变量 urls.py -----> 负责把URL模式映射到应用程序 wsgi.py -----> runserver命令就使用wsgiref模块做简单的web server，后面会看到renserver命令，所有与socket相关的内容都在这个文件里面了 1.4MVC和MTV模式 MVC模式 Web服务器开发领域里著名的MVC模式，所谓MVC就是把Web应用分为模型(M)，控制器(C)和视图(V)三层，他们之间以一种插件式的、松耦合的方式连接在一起，模型负责业务对象与数据库的映射(ORM)，视图负责与用户的交互(页面)，控制器接受用户的输入调用模型和视图完成用户的请求，其示意图如下所示： MTV模式(django中的模式) Django的MTV模式本质上和MVC是一样的，也是为了各组件间保持松耦合关系，只是定义上有些许不同，Django的MTV分别是值： M 代表模型（Model）： 负责业务对象和数据库的关系映射(ORM) T 代表模板 (Template)：负责如何把页面展示给用户(html) V 代表视图（View）： 负责业务逻辑，并在适当时候调用Model和Template 　　除了以上三层之外，还需要一个URL分发器，它的作用是将一个个URL的页面请求分发给不同的View处理，View再调用相应的Model和Template，MTV的响应模式如下所示： 过程 1.用户输入url访问，请求发送至视图 2.视图的业务逻辑是我们自己写的，视图判断请求的类型 如果是静态页面，从模版中获取静态html文件返回给视图，再由视图返回给用户 如果是动态页面，视图将请求发送至模型，模型从数据库中获取数据返回用户 3.用户的请求最终由视图返回给用户 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python进阶/django/2.django urls路由.html":{"url":"python/python进阶/django/2.django urls路由.html","title":"django基础二 urls路由","keywords":"","body":"[toc] 2.urls路由 2.1urls路由使用示例 在项目目录中的urls文件中编写路由规则 from django.conf.urls import url from django.contrib import admin from app01 import views #这一行为导入应用中的views文件(用于写业务逻辑) urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^index/$', views.index), #这一行自定义一个访问路径 ] 需要在urls文件中引入应用程序中的views 在应用程序中的views文件中编写业务逻辑 from django.shortcuts import render # Create your views here. def index(request): return render(request,'index.html') #编写函数，返回index.html页面，函数中的request参数为默认写法，名称随意，render方法用于返回给view视图index.html文件，再由view视图返回给用户 自定义的函数中需要传入一个request参数，这个参数封装了所有的请求相关信息，这个request是一个对象 视图中需要用到render()方法，render中需要用到request参数，并且返回templates目录下的html文件 为什么这里render方法中写一个html文件就能返回给用户呢，在项目下同名的目录中的settings配置文件中TEMPLATES一项(关于模版html文件的配置) 'DIRS': [os.path.join(BASE_DIR, 'templates')] os.path.join(BASE_DIR,'templates') BASE_DIR在开头import os处定义如下 BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) __file__是当前文件 os.path.abspath当前文件绝对路径 os.path.dirname找到当前文件上层目录 两个os.path.dirname就找到了项目目录，因此BASE_DIR就是项目目录 再回头看'DIRS': [os.path.join(BASE_DIR, 'templates')] BASE_DIR就是项目目录，和templates目录做了一个拼接，因此在render方法中直接写html文件就能找到 项目/templates/html文件 在templates目录下创建index.html文件 Title 第一次django使用 如果要想给用户返回一个html页面，需要在项目下的templates中编辑html文件 浏览器访问 这样就是一个简单的django使用示例 关于url路径最后边的斜杠说明，例如 index/ url(r'^home/', views.home) 前置导航斜杠不需要写,后面的斜杠是根据django的配置来的,如果在settings配置文件中我们设置了APPEND_SLASH = False,那么浏览器发送来的请求如果没有带着后面的斜杠,也是可以正常请求的,但是如果没有这个配置的话,django要求浏览器必须带着路径后面的斜杠来进行访问,如果你输入路径的时候没有加/,那么django让你的浏览器发一个重定向请求带上/ 比如，访问的时候只写了index，django会让浏览器再次发送一个重定向请求index/ 2.2url别名和命名空间 url别名的作用是当修改了url的访问路径，视图逻辑中返回的url路径也需要改动，其他相关联的地方也可能需要改动，这个时候我们在url中定义一个别名，如果后续更改了url的访问路径，其他地方只需要用到url别名的反向解析就能自动找到别名对应的url路径，这样的话只需要修改url路径而不更改其余地方 写法: url(r'^login/v2/', views.login,name='login'), 视图中反向解析: from django.urls import reverse def login(request): print(reverse('login')) #/login/v2/ if request.method == 'GET': return render(request,'login.html') else: uname = request.POST.get('uname') pwd = request.POST.get('pwd') if uname == 'admin' and pwd == 'admin': return HttpResponse('ok') else: return redirect(reverse('login')) #使用反向解析 html模板渲染时反向解析的语法{% url 别名 %} {% csrf_token %} 用户名: 密码: 2.3url别名反向解析参数 在urls文件中定义的url路径中如果有正则匹配，那么在视图函数中进行url别名反向解析的时候就需要带上参数了，而带参数又分为无名分组和有名分组 2.3.1url无名分组传参方式 args=(参数1,) urls文件中定义url路径如下所示 url(r'book1/1/', views.book,name='book1'), url(r'book2/(\\d+)/', views.book,name='book2'), 现在想做的效果是访问book1/1/然后重定向到book2，视图文件内容如下 def book1(request): return redirect('book2') def book2(request): return HttpResponse('book2') 这个时候访问127.0.0.1/book/1会报错，因为book2的url是有正则匹配的，这个时候没有传参数，所以会报错如下 此时就需要在url反向解析时传参了，urls文件中的正则匹配是无名分组，无名分组的传参方式是args=() from django.urls import reverse def book1(request): #url无名分组传参是args return redirect(reverse('book2',args=(1,))) #urls中book2有无名分组，因此需要传一个参数，参数名任意 def book2(request,n): return HttpResponse('book2') 传参后再访问就可以正确重定向了 2.3.2url有名分组传参方式 kwargs={'分组名称':'参数'} urls文件中定义url路径如下所示 url(r'book1/1/', views.book1,name='book1'), #定义一个有名分组，名称为book2 url(r'book2/(?P\\d+)/', views.book2,name='book2'), 现在想做的效果是访问book1/1/然后重定向到book2，视图文件内容如下 def book1(request): return redirect('book2') def book2(request): return HttpResponse('book2') 这个时候访问127.0.0.1/book/1会报错，因为book2的url是有正则匹配的，这个时候没有传参数，所以会报错如下 此时就需要在url反向解析时传参了，urls文件中的正则匹配是有名分组，有名分组传参方式是kwargs={'分组名称':'参数'} from django.urls import reverse def book1(request): #这里是url有名分组位置传参，page就是urls文件中定义的url有名分组名称 return redirect(reverse('book2',kwargs={'page':1})) #在要重定向的路径函数中也需要传递urls文件中定义的url有名分组名称 def book2(request,page): return HttpResponse('book2') 传参后再访问就可以正确重定向了 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python进阶/django/3.django 视图函数.html":{"url":"python/python进阶/django/3.django 视图函数.html","title":"django基础三 视图函数","keywords":"","body":"[toc] 3.视图函数 3.1request对象 request.path #request.path当前请求路径 request.method #当前请求方法(get,post...) request.GET #获取所有get请求携带过来的数据 request.POST #获取所有post请求携带过来的数据 request.body #获取所有post请求携带过来的数据的原始格式 3.2视图函数使用小示例 urls文件 urlpatterns = [ url(r'^admin/', admin.site.urls), #编写一个login登陆视图文件 url(r'^login/', views.login), ] views文件 from django.shortcuts import render,HttpResponse def login(request): if request.method == 'GET': return render(request,'login.html') else: uname = request.POST.get('username') pwd = request.POST.get('password') if uname == 'admin' and pwd == 'admin': return HttpResponse('登陆成功！') else: return HttpResponse('登陆失败!!!') html文件 Title {##} 用户名： 密码： 为了验证post请求，注释settings文件中的一行 Django项目下同名目录中的settings文件，48行 MIDDLEWARE = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', # 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware', ] 浏览器访问 错误的示例 urls文件 def login(request): if request.method == 'POST': return render(request,'login.html') else: uname = request.GET.get('username') pwd = request.GET.get('password') if uname == 'admin' and pwd == 'admin': return HttpResponse('登陆成功！') else: return HttpResponse('登陆失败!!!') html文件 Title {##} 用户名： 密码： 为了验证post请求，注释settings文件中的一行 Django项目下同名目录中的settings文件，48行 MIDDLEWARE = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', # 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware', ] 按照之前的理解应该是会返回登陆界面，因为我login.html文件中form表单写的请求方法是post，而views视图文件中的逻辑判断是如果请求是post则返回登陆界面，但是实际情况并不是这样 为什么会直接返回登陆失败呢？ 原因：当浏览器打开页面的时候，默认的方法就是GET，当点击登陆那一刻才会根据form表单中的method指定的请求类型而进行转变，但是现在直接就是GET方法，因此在login函数的判断中就直接跳到了第一个else后面，但是此时根本就没有用户登陆界面出现，所以就无法获取用户名和密码，所以会直接提示登陆失败！！！ 3.3响应方法 render返回html页面 HttpResponse返回字符串 redirect 3.3.1render 返回html页面 views文件中的写法 #需要导入HttpResponse from django.shortcuts import render,HttpResponse # Create your views here. def index(request): return render(request,'index.html') 3.3.2HttpResponse 返回字符串 views文件中的写法 #需要导入HttpResponse from django.shortcuts import render,HttpResponse # Create your views here. #HttpResponse方法中直接写字符串 def index(request): # return render(request,'index.html') return HttpResponse('HttpResponse方法返回的是字符串') 3.3.3redirect 重定向 FBV写法 urls文件 urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^home/', views.home), url(r'^login/', views.login), ] views文件 #redirect方法需要导入redirect from django.shortcuts import render,HttpResponse,redirect # Create your views here. def home(request): return HttpResponse('登陆成功') def login(request): if request.method == 'GET': return render(request,'login.html') else: uname = request.POST.get('uname') pwd = request.POST.get('pwd') if uname == 'admin' and pwd == 'admin': # return HttpResponse('登陆成功') return redirect('/home/') else: # return HttpResponse('登陆失败') return redirect('/login/') redict是重定向，之前我们访问的时候如果登陆失败就直接提示登陆失败 现在使用了redict重定向 如果登陆成功就重定向到另一个路径下，这里是/home，也就是登陆成功页面 如果登陆失败就还是重定向到访问路径下，这里是/login，也就是登陆界面 CBV写法 urls文件 urlpatterns = [ url(r'^admin/', admin.site.urls), # url(r'^login/', views.login), url(r'^login/', views.LoginView.as_view()), ] views文件 #redirect方法需要导入redirect from django.shortcuts import render,HttpResponse,redirect from django.views import View #CBV写法中需要导入View # Create your views here. def home(request): return HttpResponse('登陆成功') class LoginView(View): def get(self,request): return render(request, 'login.html') def post(self,request): uname = request.POST.get('username') pwd = request.POST.get('password') if uname == 'admin' and pwd == 'admin': # return HttpResponse('登陆成功') return redirect('/home/') else: # return HttpResponse('登陆失败') return redirect('/login/') redict是重定向，之前我们访问的时候如果登陆失败就直接提示登陆失败 现在使用了redict重定向 如果登陆成功就重定向到另一个路径下，这里是/home，也就是登陆成功页面 如果登陆失败就还是重定向到访问路径下，这里是/login，也就是登陆界面 3.4CBV和FBV 3.4.1含义 FBV:function based view :基于函数的视图逻辑 CBV:class based view :基于类的视图逻辑 3.4.1CBV CBV中url写法 # url(r'^login/', views.login), url(r'^login/', views.LoginView.as_view()), CBV中views写法 from django.shortcuts import render,HttpResponse,redirect from django.views import View #CBV写法中需要导入View # Create your views here. class LoginView(View): def get(self,request): return render(request, 'login.html') def post(self,request): uname = request.POST.get('username') pwd = request.POST.get('password') if uname == 'admin' and pwd == 'admin': # return HttpResponse('登陆成功') return redirect('/home/') else: # return HttpResponse('登陆失败') return redirect('/login/') CBV中源码重点 def dispatch(self, request, *args, **kwargs): #根据请求方法去分发对应的类发放来执行 # Try to dispatch to the right method; if a method doesn't exist, # defer to the error handler. Also defer to the error handler if the # request method isn't on the approved list. if request.method.lower() in self.http_method_names: handler = getattr(self, request.method.lower(), self.http_method_not_allowed) #反射!!!! else: handler = self.http_method_not_allowed return handler(request, *args, **kwargs) CBV中重写dispatch方法 class LoginView(View): def dispatch(self, request, *args, **kwargs): print(111) # print(request.META) #http所有相关请求头信息 ret = super().dispatch(request, *args, **kwargs) #render(request, 'login.html') print(222) return ret def get(self,request): print('this is get method!!!') return render(request, 'login.html') def post(self,request): uname = request.POST.get('username') pwd = request.POST.get('password') if uname == 'admin' and pwd == 'admin': return redirect('/home/') else: return redirect('/login/') 3.4.2FBV FBV中url写法 url(r'^login/', views.login), #url(r'^login/', views.LoginView.as_view()), FBV中views写法 from django.shortcuts import render,HttpResponse,redirect # Create your views here. def login(request): if request.method == 'GET': return render(request,'login.html') else: uname = request.POST.get('uname') pwd = request.POST.get('pwd') if uname == 'admin' and pwd == 'admin': # return HttpResponse('登陆成功') return redirect('/home/') else: # return HttpResponse('登陆失败') return redirect('/login/') 3.4.3CBV和FBV的装饰器 def func(f): def foo(request): print(111) ret = f(request) print(222) return ret return foo #FBV 模式下,和普通函数加装饰器是一样的写法 @func def home(request): print('home') return HttpResponse('你懂什么是装饰器吗？') CBV加装饰的三个姿势: # @method_decorator(func,name='get') 位置3 class LoginView(View): # @method_decorator(func) #位置2 def dispatch(self, request, *args, **kwargs): print('aaaa') ret = super().dispatch(request, *args, **kwargs) #render(request, 'login.html') print('bbbb') return ret @method_decorator(func) #位置1 def get(self,request): print('this is get method!!!') return render(request, 'login.html') def post(self,request): uname = request.POST.get('username') pwd = request.POST.get('password') if uname == 'admin' and pwd == 'admin': return redirect('/home/') else: return redirect('/login/') 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python进阶/django/4.django 模版渲染.html":{"url":"python/python进阶/django/4.django 模版渲染.html","title":"django基础四 模版渲染","keywords":"","body":" {} [toc] 4.模版渲染 4.1settings文件配置 settings配置文件中的TEMPLATES项是对静态页面的设置，DIRS处需要写上对应的静态文件存放的位置，默认为templates TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR, 'templates')] #别忘了配置这个路径 , 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, }, ] 4.2模版语法 在html文件标签中写 {{ 变量 }} {% 逻辑 %} 4.3万能的点 . html文件 以下代码中dic.name就是.的运用 Title {{ num }} {{ str }} {{ lst }} {{ dic.name }} {# #注意,调用方法时,不能加括号,所有如果方法带参数,就没法用了#} {{ woman.play }} {{ woman.xx }} views文件 from django.shortcuts import render,HttpResponse,redirect from django.views import View #CBV进程的类 # Create your views here. def home(request): # return HttpResponse('登陆成功') # return render(request,'index.html') num = 10 str = 'I am a running 的草泥马' lst = [1,2,3,4,5,6] dic = {'name':'小明','age':20} class A: money = 100 def __init__(self): self.xx = 'oo' def play(self): return '什么价位？' woman = A() return render(request,'模版渲染.html',{'num':num,'str':str,'lst':lst,'dic':dic,'woman':woman}) 运行后的初次效果 4.4过滤器 4.4.1过滤器用法 有参数的过滤器用法 {{ 变量|过滤器名称:'参数' }} 没参数的过滤器用法 {{ 变量|过滤器名称 }} 4.4.2内置过滤器 views文件 from django.shortcuts import render,HttpResponse,redirect from django.views import View #CBV进程的类 # Create your views here. def home(request): # return HttpResponse('登陆成功') # return render(request,'index.html') num = 10 str = 'I am a running 的草泥马' lst = [1,2,3,4,5,6] dic = {'name':'小明','age':20} class A: money = 100 def __init__(self): self.xx = 'oo' def play(self): return '什么价位？' woman = A() return render(request,'模版渲染.html',{'num':num,'str':str,'lst':lst,'dic':dic,'woman':woman}) 4.4.2.1 truncatechars 截断字符串 truncatechars:数字 数字表示要截断的字符数 未截断前 截断后 truncatechars:5表示截取5个字符，其中包括3个. {{ str | truncatechars:5 }} 4.4.2.2 default 如果一个变量是false或者为空，使用给定的默认值。 否则，使用变量的值 在views文件中只有num变量，没有num1变量，因此使用default指定的值 {{ num1 | default:'没有num1这个变量' }} 4.4.2.3 length 获取变量数据长度 {{ lst | length }} 4.4.2.4 filesizeformat 大小按照人类可读方式显示 views文件中定义file_size = 1024，注意还需要在render方法中以字典的形式定义返回 {{ file_size | filesizeformat }} 4.4.2.5 slice 切片(顾头不顾腚) {{ str | slice:'2:9' }} 使用切片截取 4.4.2.6 date 日期格式化显示 views文件 import datetime now = datetime.datetime.now() 注意在render方法中返回 html文件 {{ now | date:'Y-m-d H:i:s' }} 4.4.2.7 safe 关闭HTML的自动转义 safe是防止xss攻击让一些js、html等等代码变成普通字符而不执行，但有的时候我们想要执行这些代码，就需要用到safe Django的模板中在进行模板渲染的时候会对HTML标签和JS等语法标签进行自动转义，原因显而易见，这样是为了安全，django担心这是用户添加的数据，比如如果有人给你评论的时候写了一段js代码，这个评论一提交，js代码就执行啦，这样就可以搞一些坏事儿了，写个弹窗的死循环，浏览器会一直弹窗，这叫做xss攻击，所以浏览器不让你这么搞，给你转义了。但是有的时候我们可能不希望这些HTML元素被转义，比如我们做一个内容管理系统，后台添加的文章中是经过修饰的，这些修饰可能是通过一个类似于FCKeditor编辑加注了HTML修饰符的文本，如果自动转义的话显示的就是保护HTML标签的源文件。为了在Django中关闭HTML的自动转义有两种方式，如果是一个单独的变量我们可以通过 过滤器\"|safe\" 的方式告诉Django这段代码是安全的不必转义。 views文件中定义了一个a_tag，我门现在就想让这个标签成为一个超链接 a_tag = \"百度\" html文件 {{ a_tag | safe }} 没有加safe 加上safe后 4.4.2.8 join 字符串拼接列表 views文件 lst = [1,2,3,4,5,6] html文件 {{ lst | join:\"+\"}} 4.4.2.9 cut 移除变量中所有的与给出的变量相同的字符串 views文件 str = 'I am a running 的草泥马' html文件 {{ str | cut:' ' }} 4.5标签 4.5.1 for标签 for标签语法 {% for foo in 循环的对象 %} {% endfor %} for标签循环列表简单示例 views文件 def home(request): lst = [1,2,3,4,5] return render(request,'home.html',{'lst':lst}) html文件 {% for foo in lst %} {{ foo }} {% endfor %} 结果如下 {% for foo in lst reversed %} {{ foo }} {% endfor %} for标签循环字典简单示例 views文件 def home(request): lst = [1,2,3,4,5] dic = {'name':'小明','age':20,'hobby':'唱、跳、rap、篮球'} return render(request,'home.html',{'lst':lst,'dic':dic}) html文件 {% for foo in dic.items %} //这里可以循环字典的键、值、键和值 {{ foo }} {% endfor %} {% for key,value in dic.items %} {{ key }}--->{{ value }} {% endfor %} for标签中的empty {% for i in lst1 %} #当没有数据时,会生成empty的内容 {{ i }} {% empty %} 啥数据也没有! {% endfor %} for标签中的forloop方法，必须在循环内使用 forloop.counter #当前循环的索引值(从1开始)，forloop是循环器，通过点来使用功能 forloop.counter0 #当前循环的索引值（从0开始） forloop.revcounter #当前循环的倒序索引值（从1开始） forloop.revcounter0 #当前循环的倒序索引值（从0开始） forloop.first #当前循环是不是第一次循环（布尔值） forloop.last #当前循环是不是最后一次循环（布尔值） forloop.parentloop #本层循环的外层循环的对象，再通过上面的几个属性来显示外层循环的计数等 forloop.counter #当前循环的索引值(从1开始)，forloop是循环器，通过点来使用功能 {% for key,value in dic.items %} {{ forloop.counter }}{{ key }}--->{{ value }} {% endfor %} forloop.counter0 #当前循环的索引值（从0开始） {% for key,value in dic.items %} {{ forloop.counter0 }}{{ key }}--->{{ value }} {% endfor %} forloop.first #当前循环是不是第一次循环（布尔值） {% for key,value in dic.items %} {{ forloop.first }} {{ key }}--->{{ value }} {% endfor %} forloop.parentloop #本层循环的外层循环的对象，再通过上面的几个属性来显示外层循环的计数等 views文件 def home(request): lst = [1,2,3,4,5] dic = {'name':'小明','age':20,'hobby':['唱','跳','rap','篮球']} return render(request,'home.html',{'lst':lst,'dic':dic}) html文件 {% for key,value in dic.items %} {{ forloop.last }} {{ key }}--->{{ value }} {% for foo in dic.hobby %} {{ forloop.parentloop.counter }}---{{ forloop.counter }}{{ foo }} {% endfor %} {% endfor %} 4.5.2 if标签 if语句支持 and 、or、==、>、=、in、not in、is、is not判断，注意条件两边都有空格 views文件 def home(request): num = 100 str = 'helow' lst = [1,2,3,4,5] dic = {'name':'小明','age':20,'hobby':'唱、跳、rap、篮球'} return render(request,'home.html',{'lst':lst,'dic':dic,'num':num,'str':str}) html文件 普通判断 {% if num == 100 %} 数字等于100 {% else %} 数字不等于100 {% endif %} #语法 {% if %} {% elif %} {% else %} {% endif %} //以endif结尾 结合过滤器使用 {% if str|length == 5 %} 字符串长度为5 {% endif %} 4.5.3 with标签 用于给比较长的数据调用起别名，只能在with标签中只用 views文件 def home(request): lst = [1,2,{'name':'小明','age':20},4,5] return render(request,'home.html',{'lst':lst} html文件 html文件中想渲染views文件中返回的列表中的元素中的字典的值，如果每一次都写的话会比较长，例如lst.2.name，这个时候我们就可以取一个别名，用于给这个数据的调用，这样的话下次别的地方有引用的时候写起啦会比较简单 //例如，h1标签和a标签中都想引用这个数据的调用，没有起别名之前的写法，这样每次引用都需要在写一遍，比较麻烦 {{ lst.2.name }} {{ lst.2.name }} //这个时候就可以用到with别名的方法 {% with lst.2.name as l %} {{ l }} {% endwith %} 4.5.4 csrf_token 通过csrf认证机制 #写法 {% csrf_token %} csrf_token 　　　　我们以post方式提交表单的时候，会报错，我们在settings里面的中间件配置里面把一个csrf的防御机制给注销了，本身不应该注销的，而是应该学会怎么使用它，并且不让自己的操作被forbiden，通过这个东西就能搞定。 　　　　这个标签用于跨站请求伪造保护， 　　　　在页面的form表单里面（注意是在form表单里面）任何位置写上{% csrf_token %}，这个东西模板渲染的时候替换成了，隐藏的，这个标签的值是个随机字符串，提交的时候，这个东西也被提交了，首先这个东西是我们后端渲染的时候给页面加上的，那么当你通过我给你的form表单提交数据的时候，你带着这个内容我就认识你，不带着，我就禁止你，因为后台我们django也存着这个东西，和你这个值相同的一个值，可以做对应验证是不是我给你的token，就像一个我们后台给这个用户的一个通行证，如果你用户没有按照我给你的这个正常的页面来post提交表单数据，或者说你没有先去请求我这个登陆页面，而是直接模拟请求来提交数据，那么我就能知道，你这个请求是非法的，反爬虫或者恶意攻击我的网站 4.6自定义标签和自定义过滤器 4.6.1自定义标签 1.在应用程序目录中创建一个templatetags目录(名称只能叫templatetags) 2.在templatetabs目录中创建xx.py 3.在xx.py中导入template from django import template #变量名称必须叫register register = template.Library() #自定义标签 @register.simple_tag def tag(v1): return v1 + 'tag' 使用自定义标签 urls文件 url(r'^tags/', views.tags), views文件 def tags(request): name = 'hehe' return render(request,'tags.html',{'name':name}) tags.html文件 //这里表示引入应用程序目录下templatetags目录中自定义的xx.py，这个xx.py中是我们自定义的标签 {% load xx %} {% tag name %} args.html文件中不给自定义标签传参数 views文件中指定给自定义的标签传2个参数，v1是views中tag函数定义的变量name，v2就是args.html中自定义标签传的一个参数 from django import template #变量名称必须叫register register = template.Library() #自定义标签 @register.simple_tag def tag(v1,v2): return v1 + 'tag' + v2 args.html文件中给自定义标签传参 {% tag name '参数1' %} 4.6.2自定义过滤器 1.在应用程序目录中创建一个templatetags目录(名称只能叫templatetags) 2.在templatetabs目录中创建xx.py 3.在xx.py中导入template from django import template #变量名称必须叫register register = template.Library() #自定义过滤器，最多2个参数 这里自定义一个过滤器，接受任意一个参数，给这个参数后边加一个字符串 @register.filter def oo(v1): return v1 + 'oo' 使用自定义过滤器 urls文件 url(r'^tags/', views.tags), views文件 def tags(request): name = 'hehe' return render(request,'tags.html',{'name':name}) tags.html //这里表示引入应用程序目录下templatetags目录中自定义的xx.py，这个xx.py中是我们自定义的过滤器 {% load xx %} {{ name | oo }} 结果如下，可以看到，我们自定义的过滤器，接受任意一个参数，给这个参数后边加一个字符串oo 上述结果是xx.py就是我们自定义的过滤器文件中的函数只传了一个参数，现在我们给这个函数传2个参数(最多传两个参数) xx.py写法 from django import template #变量名称必须叫register register = template.Library() #自定义过滤器，最多2个参数 这里自定义一个过滤器，接受任意一个参数，给这个参数后边加一个字符串 @register.filter def oo(v1,v2): return v2 + 'oo' + v1 上述代码中v1是views中定义的name变量的值，也就是hehe，v2就是tags.html中函数oo后面传的参数，因此打印的结果就是 哈哈oohehe urls文件 url(r'^tags/', views.tags), views文件 def tags(request): name = 'hehe' return render(request,'tags.html',{'name':name}) tags.html //这里表示引入应用程序目录下templatetags目录中自定义的xx.py，这个xx.py中是我们自定义的过滤器 {% load xx %} {{ name | oo:'哈哈' }} 4.6.3 inclusion_tag 修改引入组件的样式（ 非常难理解） inclusion_tag作用如下 接下来开始整个过程 第一步、先在django项目同路径下的templates目录下创建一个菜单静态页面作为一个组件 zujian.html Title .menus{ width: 200px; } .menus .item{ background-color: green; color: white; height: 50px; } 菜单1 菜单2 菜单3 效果如下 第二步、分别在urls和views文件中写上对应的访问路径和函数 这里在后边定义一个xx.html，这个xx.html引入组件文件zujian.html，并且访问路径是xx，视图中的函数也叫xx(返回xx.html) urls文件 url(r'^xx/', views.xx), views文件 def xx(request): return render(request,'xx.html') 第三步、还是在templates目录下创建一个xx.html文件，这个html文件引入刚才创建的组件文件 xx.html文件初始内容如下 Title .nav{ height: 200px; width: 150px; background-color: burlywood; } 我是xx.html，一会我要引入组件中的样式，就是组件中的那个菜单栏，并且我不修改引入的样式，只是修改菜单栏为其他内容 初始xx.html 接下来在xx.html文件中引入组件文件的样式 在div标签下引入zujian.html 我是xx.html，一会我要引入组件中的样式，就是组件中的那个菜单栏，并且我不修改引入的样式，只是修改菜单栏为其他内容 {% include 'zujian.html' %} 引入组件后的样式，绿色为zujian.html文件中的内容 接下来想要将引入的样式中的菜单栏变为其他内容，需要做以下操作，在组件文件zujian.html文件中将想要更改的内容利用for循环便利 zujian.html文件 //利用for循环遍历data变量，foo就是data变量中的所有内容，这样的话就做成了动态的效果，原先的菜单栏div标签现在只需要写一个，因为foo是变量data中所有的值 {% for foo in data %} {{ foo }} {% endfor %} //注释之前的菜单栏，因为我们需要用到动态传入内容 {# 菜单1#} {# 菜单2#} {# 菜单3#} 这里有个问题，现在想把zujian.html文件中的菜单栏做一下动态修改，这里写了从data中取值，但是循环遍历的data是从哪来的？？？这时就引出了inclusion_tag inclusion_tag需要写在templatetags目录下自定义的py文件中，而data就是我们自定义的返回给xx.html文件的动态内容，因为xx.html引入zujian.html的样式是固定的，只有zujian.html文件中定义的内容 第四步、在templatetags目录下创建一个mytags.py templatetags目录是在django项目的应用程序目录下创建的目录，在自定义标签和自定义过滤器的时候必须创建这个目录，且名字只能是这个，在这个templatetags目录下创建的py文件中自定义标签和过滤器 mytags.py @register.inclusion_tag('zujian.html')是固定写法，括号中的参数就是组件文件，这里的data是我们自定义的字典 @register.inclusion_tag('zujian.html') def hehe(v1): return {'data':['唱','跳','rap','篮球']} 第五步、在xx.html文件中引入动态内容 我是xx.html，一会我要引入组件中的样式，就是组件中的那个菜单栏，并且我不修改引入的样式，只是修改菜单栏为其他内容 {#{% include 'zujian.html' %}#} {% load mytags %} //引入自定义函数的那个文件名 {% hehe %} //引入自定义函数，用于返回data 最终运行效果如下，引入的组件的背景颜色绿色没有改变，但是改变了之前的菜单栏 整个过程总结 1.定义urls url(r'^xx/', views.xx), 2.定义视图 def xx(request): return render(request,'xx.html') 3.xx.html文件引入组件文件zujian.html，组件中只定义了一个背景色，但是xx.html文件想要动态的添加一些内容，例如菜单栏，此时需要在组件文件中做如下改动 组件文件中定义的背景色 Title .menus{ width: 200px; } 利用for循环去循环一个变量，例如data，这个变量是在django项目下的应用程序目录下的templatetags下的任意名称py文件中利用@register.inclusion_tag('zujian.html')和自定义函数中的return返回值定义的变量 应用程序下templates目录中定义一个mytags.py(名称任意)，文件内容如下 inclusion_tag中必须跟一个参数，这个参数必须是组件文件，自定义一个函数hehe(名称任意)，return的返回值会返回给组件文件 mytags.py文件内容如下 from django import template register = template.Library() @register.inclusion_tag('zujian.html') def hehe(): return {'data':['唱','跳','rap','篮球']} 4.第3步中的data变量已经返回给了组件文件，因此xx.html文件中需要引入应用程序下的templatetags中定义的那个py文件(这里是mytag.py) 还需要引入这个py文件中自定义的函数的名称(这里是hehe) mytags.py(自定义标签需要在应用程序下的templatetags目录中定义一个任意的py文件，然后在这个文件中写自定义标签内容) xx.html文件内容如下 我是xx.html，一会我要引入组件中的样式，就是组件中的那个菜单栏，并且我不修改引入的样式，只是修改菜单栏为其他内容 {#{% include 'zujian.html' %}#} {% load mytags %} {% hehe %} #参数说明 {% load mytags %} //引入应用程序下的templatetags中定义的那个py文件(这里是mytags.py) {% hehe %} //引入这个py文件(mytags.py)中自定义的函数的名称(这里是hehe) 最终的渲染效果就由应用程序下的templatetags中定义的那个py文件(这里是mytags.py)中定义的函数中自定义的返回值决定 这里定义的data是一个字典，data会返回给组件文件用于动态渲染 from django import template register = template.Library() @register.inclusion_tag('zujian.html') def hehe(): return {'data':['唱','跳','rap','篮球']} 关于动态返回内容的一个问题 写在mytags中是写死的，我们应该写成动态的，所以应该写在views中从数据库中获取数据达到动态返回的效果，因此修改mytags文件如下 mytags文件 from django import template register = template.Library() @register.inclusion_tag('zujian.html') #这里的v1就是给xx.html文件中引入hehe函数能够传一个参数 def hehe(v1): return {'data':v1} views文件 def xx(request): #这里的lst就相当于从数据库中动态获取的数据 lst = ['唱1','跳1','rap1','篮球1'] return render(request,'xx.html',{'lst':lst}) xx.html文件 我是xx.html，一会我要引入组件中的样式，就是组件中的那个菜单栏，并且我不修改引入的样式，只是修改菜单栏为其他内容 {#{% include 'zujian.html' %}#} {% load mytags %} {% hehe lst %} //在这里引入views中定义的lst 最终真正的动态获取内容效果如下 4.7组件 组件类似于python中的模块，组件是把其他html页面引入过来，但是不能修改引入的文件的内容 组件文件 Title 1 2 3 自定义文件 在自定义文件中，我们想引入组件文件中的列表样式，需要如下代码，这个就是在自定义文件中表示引入组件.html文件 #写法 {% include '组件.html' %} Title .c1{ background-color: green; } {% include '组件.html' %} 我是一个引入了组件文件的自定义文件 4.8模版继承 我们在写多个静态页面的时候可能会有一些相同的样式，这样的话每写一个页面就需要复制相同的代码，这样就造成了代码大量重复，此时就用到了模版继承，我们可以把相同样式的页面的代码单独放在一个模版文件中，其余页面继承这个模版文件，然后在设置自定义内容即可，这样就不用重复写相同样式的代码了 模版文件 base.html文件 Title .nav{ background-color: pink; height: 40px; } .left-menu{ display: inline-block; width: 200px; background-color: mediumpurple; } .content{ display: inline-block; height: 200px; width: 600px; background-color: burlywood; color:white; } ul{ padding: 0; margin: 0; } #这里表示预留css样式，子模版继承后可以修改模版的样式 {% block css %} {% endblock %} 个人中心 登录|注册 菜单1 菜单2 菜单3 {% block content %} base页面 xxx {% endblock %} {% block js %} {% endblock %} 模版继承的写法 在模版html文件中需要用到block方法，例如上述示例中，我们想要菜单1和菜单2有自己的自定义界面而不继承模版文件，写法如下 {% block content %} base页面 xxx {% endblock %} 这个标签就是菜单1和菜单2中需要单独设置样式的标签，为了不继承模版文件，需要在这个标签写如下内容 block中包含的a标签就是菜单1和菜单2的html文件中需要重写的 {% block content %} base页面 xxx {% endblock %} 这段代码的意思是，模版文件中的base页面不希望被继承，这里就单独把这个页面预留出来，接下来在菜单1和菜单2的html文件中在单独写这块的样式，写法如下 //这里表示继承模版文件base.html {% extends 'base.html' %} 因为不需要继承模版文件中的 base页面 处的内容，而模版文件中也把这一块内容已经预留出来了，并且定义了预留名称为 content，在菜单1和菜单2的html文件中之需要引入这个模版预留名称，这样就可以自定义自己的样式了，写法如下 {% block content %} 菜单1的内容 {% endblock %} base.html文件如上，现在我们设置菜单1、菜单2的自定义内容，需要做以下操作 urls文件 from app01 import views urlpatterns = [ url(r'^admin/', admin.site.urls), #这个就是模版页面 url(r'^base/', views.base), #下边两个是菜单1和菜单2返回的页面 url(r'^menu01/', views.menu01), url(r'^menu02/', views.menu02), ] views文件 def base(request): return render(request,'base.html') def menu01(request): return render(request,'menu01.html') def menu02(request): return render(request,'menu02.html') menu01.html {% extends 'base.html' %} .content{ display: inline-block; height: 200px; width: 600px; background-color: green; color:white; } {% endblock %} {% block content %} 菜单1的内容 {% endblock %} menu02.html {% extends 'base.html' %} {% block content %} 菜单2的内容 {% endblock %} 访问base页面，点击菜单1，返回页面如下，因为在menu01.html文件中，我门自己指定了样式，因此背景颜色就和模版的不同了 访问base页面，点击菜单2，返回页面如下 block.super() 这个方法就是继承模板预留块的内容的同时在自定义内容 {% block content %} {{ block.super }} #将模版中的content这个名称的块中的内容拿过来 菜单1的内容 {% endblock %} 4.9url别名和反向解析 url别名的作用是当修改了url的访问路径，视图逻辑中返回的url路径也需要改动，其他相关联的地方也可能需要改动，这个时候我们在url中定义一个别名，如果后续更改了url的访问路径，其他地方只需要用到url别名的反向解析就能自动找到别名对应的url路径，这样的话只需要修改url路径而不更改其余地方 写法: url(r'^login/v2/', views.login,name='login'), 视图中反向解析: from django.urls import reverse def login(request): print(reverse('login')) #/login/v2/ if request.method == 'GET': return render(request,'login.html') else: uname = request.POST.get('uname') pwd = request.POST.get('pwd') if uname == 'admin' and pwd == 'admin': return HttpResponse('ok') else: return redirect(reverse('login')) #使用反向解析 html模板渲染时反向解析的语法{% url 别名 %} {% csrf_token %} 用户名: 密码: ⚠️⚠️⚠️进行html模板渲染时反向解析时，如果定义了命名空间，则在html中需要如下写法 html模板渲染时反向解析的语法{% url 命名空间:别名 %} 4.10include路由分发和url命名空间 4.10.1include路由分发 include路由分发就是将不同的url放在不同的应用程序下的urls文件中，先匹配一个应用程序的路径，然后在分别去这两个应用程序对应的urls文件中找下一级目录 例如，现在有app01和app02两个应用程序，现在设置的访问路径如下 /app01/index /app02/index 访问到app01就去app01路径下找index 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python进阶/django/5.django ORM.html":{"url":"python/python进阶/django/5.django ORM.html","title":"django基础五 ORM","keywords":"","body":"[toc] 5.ORM 5.1ORM简介 MVC或者MVC框架中包括一个重要的部分，就是ORM，它实现了数据模型与数据库的解耦，即数据模型的设计不需要依赖于特定的数据库，通过简单的配置就可以轻松更换数据库，这极大的减轻了开发人员的工作量，不需要面对因数据库变更而导致的无效劳动 ORM是“对象-关系-映射”的简称。（Object Relational Mapping，简称ORM） ORM执行的过程 类对象--->sql--->pymysql--->mysql服务端--->磁盘，orm其实就是将类对象的语法翻译成sql语句的一个引擎 ORM与原生SQL对比 5.2ORM操作 5.2.1ORM连接mysql 这里先来一个orm连接mysql并创建一张表的示例 第一步、修改settings配置文件，配置mysql相关信息 #先注释默认项 # DATABASES = { # 'default': { # 'ENGINE': 'django.db.backends.sqlite3', # 'NAME': os.path.join(BASE_DIR, 'db.sqlite3'), # } # } DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', 'NAME': '数据库名', 'HOST': '数据库IP', 'PORT': '数据库端口', 'USER': '用户', 'PASSWORD': '密码' } } 第二步、在项目目录下的同名目录下的__init__.py文件中写上以下内容,来指定pymysql作为连接客户端 import pymysql pymysql.install_as_MySQLdb() 第三步、在应用程序目录下面的models.py文件中写对应的类，这里为创建一张表 from django.db import models #这里表示创建一个userinfo表，需要注意的是，django在创建表的时候会把表名小写并重命名为 应用程序_userinfo class UserInfo(models.Model): id = models.AutoField(primary_key=True) username = models.CharField(max_length=10) password = models.CharField(max_length=32) 第四步: 执行数据库同步指令，在终端中执行，执行的路径是django项目下 #在migrations文件夹下面生成记录文件 python3 manage.py makemigrations #执行记录文件 python3 manage.py migrate 可以看到，我们自定义的表已经创建完成了，但是发现多了好多其他的表，那这些表是从哪来的呢，这些是django从settings文件中循环读取INSTALLED_APPS下的所有子项而默认创建的表 表结构 5.2.2单表操作 5.2.2.1增 应用程序的models.py文件中已经定义了创建一张表的语句 urls文件 from app01 import views urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^index/', views.index), ] views文件 from django.shortcuts import render,HttpResponse from app01 import models #方式1 def index(request): obj = models.UserInfo( username='小明', password='123' ) obj.save() return HttpResponse('ok') #方式2 def index(request): models.UserInfo.objects.create( username='小颖', password='666' ) return HttpResponse('ok') 接下来启动django项目，浏览器访问我们指定的127.0.0.1:8000/index 执行以上任意一种方式都可以插入数据 5.2.2.2删 views文件 def index(request): #删除UserInfo表中id值为1的值 models.UserInfo.objects.filter(id=1).delete() return HttpResponse('ok') 浏览器访问127.0.0.1/index即可成功执行语句 可以看到，id为1的字段已经被删除 5.2.2.3改 views文件 from django.shortcuts import render,HttpResponse from app01 import models # Create your views here. def index(request): # 方式1 models.UserInfo.objects.filter(id=2).update( username='abc', password='abc', ) # 方式2 # obj = models.UserInfo.objects.filter(id=2)[0] # obj.username = 'ggg' # obj.password = 'ggg' # obj.save() return HttpResponse('ok') 浏览器访问127.0.0.1/index 可以看到修改已经生效 批量创建 views文件 from django.shortcuts import render,HttpResponse from app01 import models # Create your views here. def index(request): list_obj = [] for i in range(6): obj = models.UserInfo( username='name%s' %i, password='pwd%s' %i, ) list_obj.append(obj) print(list_obj) models.UserInfo.objects.bulk_create(list_obj) return HttpResponse('ok') print(list_obj)返回的结果如下 [, , , , , ] 浏览器访问127.0.0.1/index 可以看到已经批量插入了数据 update_or_create 有就更新,没有就创建 from django.shortcuts import render,HttpResponse from app01 import models # Create your views here. def index(request): a,b = models.UserInfo.objects.update_or_create( username='name0', defaults={ 'id': 20, 'password': 'abcdef', } ) print(a) #当前更新后的model对象,或者是你新增的记录的model对象 print(b) #新增就是True,查询就False return HttpResponse('ok') #a返回的结果 UserInfo object #b返回的结果 True 5.2.2.4查 最简单的查询 from django.shortcuts import render,HttpResponse from app01 import models # Create your views here. def index(request): ret = models.UserInfo.objects.filter(id=1) print(ret) # ]> -- [] obj = ret[0] print(obj.id, obj.username) return HttpResponse('ok') 表app01_userinfo内容如下 print(ret)结果如下 ]> print(obj.id, obj.username)执行后结果如下 1 aaa 查询非常重要的13种方法 1 all() 查询所有结果，结果是queryset类型 views文件 from django.shortcuts import render,HttpResponse from app01 import models # Create your views here. def index(request): ret = models.UserInfo.objects.all() print(ret) return HttpResponse('ok') #print(ret)结果如下 , ]> 现在我们想看到的是数据库中的数据，需要在models文件中我们创建的UserInfo中写一个类的属性 class UserInfo(models.Model): id = models.AutoField(primary_key=True) username = models.CharField(max_length=10) password = models.CharField(max_length=32) #在models文件中写上这个属性后就能在all()方法中返回数据而不是对戏那个 def __str__(self): return self.username #结果如下 , ]> 2 get() 只能返回一个数据，没有或者数据多就报错，不是queryset类型，是行记录对象 例如，查询一个表中密码为aaa的数据，如果这个表中只有一个用户的密码是aaa，则可以查询成功，如果这个表中有多个用户的密码是aaa这个时候就会报错并且这个表中如果没有用户的密码是aaa同样会报错 from django.shortcuts import render,HttpResponse from app01 import models # Create your views here. def index(request): ret = models.UserInfo.objects.get(password='aaa') print(ret) return HttpResponse('ok') 表app01_userinfo内容如下 浏览器访问127.0.0.1/index 此时返回的结果是可以成功查询的 现在修改表内容如下 再次执行以上代码，查询表中密码为aaa的数据，此时会报错 修改代码为查询表中密码为bbb的数据，同样会报错 get两个报错,但是get请求返回的结果是model对象 #1.UserInfo matching query does not exist. 没有查到的报错 #2 get() returned more than one UserInfo -- it returned 11! 结果多了,不行! 3 filter() 查询数据，返回值是queryset类型 views文件 from django.shortcuts import render,HttpResponse from app01 import models # Create your views here. def index(request): ret = models.UserInfo.objects.filter(password='bbb') print(ret) return HttpResponse('ok') filter查询表中密码为bbb的数据，虽然表中结果没有，但是不会像get报错 print(ret)结果如下 filter查询表中密码为aaa的数据，表中密码为aaa的数据有2条，filter不会像get一样报错 , ]> 4 exclude() 排除，返回值是queryset类型 现有app01_book表如下 查询所有数据，排除bid为4的数据 urls文件 from django.conf.urls import url,include from django.contrib import admin from app01 import views urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^index/', views.index), ] views文件 from django.shortcuts import render,reverse,redirect,HttpResponse from app01 import models def index(request): #查询bid不为4的数据，这里还可以写多个排除条件，以逗号分隔 ret = models.Book.objects.exclude(bid=4) print(ret) return HttpResponse('ok') models文件 class Book(models.Model): bid = models.AutoField(primary_key=True) bname = models.CharField(max_length=20) price = models.DecimalField(max_digits=6,decimal_places=2) publish_date = models.DateField(auto_now_add=False) publish = models.CharField(max_length=20) def __str__(self): return self.bname 浏览器访问127.0.0.1/index，返回结果如下，bid为4的隔壁老王的故事没有返回 , , , , ]> 5 order_by() queryset类型的数据来调用，对查询结果排序,默认是按照id来升序排列的，返回值是queryset类型 按照bid倒序排序，在bid前加个-号即可 viwes文件 def index(request): ret = models.Book.objects.all().order_by('-bid') print(ret) return HttpResponse('ok') 浏览器反问127.0.0.1/index，返回结果如下，结果是倒序 , , , , , ]> order_by()中加多个条件说明 #这段代码的意思是按照id倒叙排序，然后再按照id相同的数据进行价格升序排序 models.Test.objects.all().order_by('-id','money') 现有表app01_test如下 先按照id进行倒叙排序 ret = models.Test.objects.all().order_by('-id') print(ret.values()) 返回结果如下 再加一个条件进行排序，先根据id相同的进行倒序排序，然后再根据money升序排序 ret = models.Test.objects.order_by('-id','money') print(ret.values()) 返回结果如下 6 reverse() queryset类型的数据来调用，对查询结果反向排序，返回值还是queryset类型 以例5中app01_test表为例 ⚠️直接进行reverse进行反向排序是不生效的，必须先进行分组 ret = models.Test.objects.all().reverse() 返回结果如下，不分组进行反向排序是不生效的 先进行分组，然后再进行反向排序才能生效 ret = models.Test.objects.all().order_by('id').reverse() 返回结果如下 7 count() queryset类型的数据来调用，返回数据库中匹配查询(QuerySet)的对象数量 以例5中app01_test表为例 ret = models.Test.objects.all().count() print(ret) 返回结果为5，因为app01_test表中有5条记录 8 first() queryset类型的数据来调用，返回第一条记录，得到的都是model对象，不是queryset 以例5中app01_test表为例 ret = models.Test.objects.all().first() print(ret) #models文件表的类中需要写如下代码 class Test(models.Model): id = models.IntegerField(primary_key=True) name = models.CharField(max_length=10) money = models.IntegerField() def __str__(self): return self.name 返回结果为1-小明 9 last() queryset类型的数据来调用，返回最后一条记录，得到的是model对象 以例5中app01_test表为例 ret = models.Test.objects.all().last() print(ret) #models文件表的类中需要写如下代码 class Test(models.Model): id = models.IntegerField(primary_key=True) name = models.CharField(max_length=10) money = models.IntegerField() def __str__(self): return self.name 查询时遇到的一个问题，使用last方法查询一个没有设置主键的表时，只能返回id相同的记录中的第一个id的记录 例如，一张表中id字段如下：1、2、3、3、3，查询的返回的结果是id=3的第一条记录，即2后边的第一个3 此表中没有设置主键，只能返回3-大强，而理论上是3-哈哈 10 exists() queryset类型的数据来调用，如果QuerySet包含数据，就返回True，否则返回False 空的queryset类型数据也有布尔值True和False，但是一般不用它来判断数据库里面是不是有数据，如果有大量的数据，你用它来判断，那么就需要查询出所有的数据，效率太差了，用count或者exits 例：all_books = models.Book.objects.all().exists() #翻译成的sql是SELECT (1) AS a FROM app01_book LIMIT 1，就是通过limit 1，取一条来看看是不是有数据 以例5中app01_test表为例 test表中没有id=5的数据，因此会返回False ret = models.Test.objects.filter(id=5).exists() 11 values() 用的比较多，queryset类型的数据来调用，返回一ValueQuerySet——一个特殊的QuerySet，运行后得到的并不是一系列model的实例化对象，而是一个可迭代的字典序列,只要是返回的queryset类型，就可以继续连续调用queryset类型的其他的查找方法，其他方法也是一样的。 以例5中app01_test表为例 ret = models.UserInfo.objects.all().values('id','name') queryset调用 ret = models.UserInfo.objects.values('id','name') objects调用--对所有数据进行取值 返回结果如下 12 values_list 它与values()非常相似，它返回的是一个元组序列，values返回的是一个字典序列 以例5中app01_test表为例 ret = models.Test.objects.all().values_list('id','name') 返回结果如下 13 distinct() values和values_list得到的queryset类型的数据来调用，从返回结果中剔除重复纪录 以例5中app01_test表为例 ret = models.UserInfo.objects.all().values('id','name').distinct() ret = models.UserInfo.objects.values('id','name').distinct() 基于双下划线的模糊查询 表app01_book内容如下 #price值等于这三个里面的任意一个的对象 Book.objects.filter(price__in=[10,33,9999]) #大于，大于等于是price__gte=100，别写price>100，这种参数不支持 Book.objects.filter(price__gt=100) #小于，小于等于是price__lte=100，别写price 查询示例 1 查询某某出版社出版过的价格大于100的书籍 ret = models.Book.objects.filter(price__gt=100,publish='老王出版社').values('bname') 2 查询2018年11月出版的所有以Li开头的书籍名称 ret = models.Book.objects.filter(bname__startswith='Linux',publish_date__year=2018,publish_date__month=11).values('bname') 3 查询价格为88,100或者9999的所有书籍名称及其出版社名称 ret = models.Book.objects.filter(price__in=[88,100,9999]).values('bname','publish') 4 查询价格在80到100之间的所有书籍名称及其价格 ret = models.Book.objects.filter(price__range=[80,100]).values('bname','price') 5 查询所有老王出版社出版的书籍的价格（从高到低排序，去重） ret = models.Book.objects.filter(publish='老王出版社').values('price').order_by('-price').distinct() 5.2.3多表操作 表关系及字段设计 urls文件 from django.conf.urls import url from django.contrib import admin from app01 import views urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^query/', views.query), ] settings文件配置数据库连接 DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', 'NAME': 'orm01', 'HOST': '127.0.0.1', 'PORT': '3306', 'USER': 'root', 'PASSWORD': 'xxx' } } models文件 from django.db import models # Create your models here. class Author(models.Model): name=models.CharField(max_length=32) age=models.IntegerField() ad=models.OneToOneField(to=\"AuthorDetail\",to_field=\"id\",on_delete=models.CASCADE) # to_field=\"id\" 可以不写,默认找主键 # to=\"AuthorDetail\", to=可以不用写 # models.SET_NULL 置空 # on_delete=models.CASCADE 默认是级联删除,想做级联更新,直接去数据库修改表结构 class AuthorDetail(models.Model): birthday=models.DateField() telephone=models.CharField(max_length=16) addr=models.CharField(max_length=64) class Publish(models.Model): name=models.CharField(max_length=32) city=models.CharField(max_length=32) class Book(models.Model): title = models.CharField(max_length=32) publishDate=models.DateField() price=models.DecimalField(max_digits=5,decimal_places=2) publishs=models.ForeignKey(to=\"Publish\",to_field=\"id\",on_delete=models.CASCADE) #这行代码的意思是生成第3张表关联book表和author表 authors=models.ManyToManyField(to='Author',) ⚠️如果AuthorDetail写在Author的上边，则ad=models.OneToOneField(to=\"AuthorDetail\"中的AuthorDetail可以不加引号 ⚠️django1.x版本on_delete=models.CASCADE可以不写，默认是级连删除，如果想做级连更新，直接到数据库中操作，但是django2.x必须写级连删除 ⚠️OneToOneField是一对一 ForeignKey是一对多 ManyToManyField是多对多 django项目下同名目录__init__.py文件 import pymysql pymysql.install_as_MySQLdb() 相关配置文件配置完成后开始同步数据库 ⚠️makemigrations执行后必须执行migrate，否则会出问题 //在django项目下 #在migrations目录下面生成记录文件 python3 manage.py makemigrations #执行记录文件 python3 manage.py migrate 关于只执行makemigrations不执行migrate可能出现的问题 1.当执行makemigrations后，会在django项目下的makemigrations目录中创建一个0001_initial.py，这个文件记录的是models文件中的操作(创建表、删除表)，当执行migrate后会执行数据库同步操作，同时在数据库中django会自动创建一张django_migrations表，这个表记录的是models文件中的操作 2.如果只执行了makemigrations而不执行migrate，当修改了models文件再次重复执行makemigrations，django会从数据库中的django_migrations查看0001_initial是否执行过，而此时0001_initial已经执行过了，因此会出现修改了models文件执行migrate提示没有改动的问题 同步后的数据库，可以看到app01_book_authors这张表是orm自动帮我们创建的 5.2.3.1增 views文件 一对一，作者表和作者详细信息表 //给作者详细信息表添加一条记录 models.AuthorDetail.objects.create( birthday='2018-11-11', telephone='18033336789', addr='北京' ) //给作者表添加一条记录 models.Author.objects.create( name='小明', age=1, #方式1 #关联作者详细信息表中id为1的 ad=models.AuthorDetail.objects.get(id=1) #方式2(常用) ad_id = 1 ) 一对多，出版社和书籍表 models.Book.objects.create( title='隔壁老王的故事', publishDate='2011-01-01', price='99', #方式1 publishs=models.Publish.objects.get(id=1) #方式2 publishs_id=2 ) 多对多，书籍表和作者表 //方式一 book_obj = models.Book.objects.get(id=1) author1 =models.Author.objects.get(id=1) author2 =models.Author.objects.get(id=2) #向关联书籍表和作者表的第3张表中插入作者1和作者2，书籍id为1的书的作者有2个，分别为作者id为1的和作者id为2的 #方式1 book_obj.authors.add(author1,author2) #方式2 book_obj.authors.add(*[author1,author2]) //方式二 book_obj = models.Book.objects.get(id=1) #方式1 book_obj.authors.add(1,2) #方式2 book.obj.authors.add(*[1,2]) 查看关联作者表和书籍表的第3张表app01_book_authors，书籍id为1的书作者有2个 5.2.3.2删 一对一、一对多删除方式一样 models.Author.objects.get(id=1).delete() 删除了作者表中id为1的记录，但是不会影响作者详细信息表中作者id为1的记录 作者表的id作为外键关联作者详细信息表中的作者id，可以理解为我抱别人的大腿，删除我自己不影响别人，但是别人如果删除大腿会影响我 作者表是关联者，作者详细信息表是被关联者 多对多 //方式一 remove 移除 #找到书籍id为1的书 book_obj = models.Book.objects.get(id=1) #删除书籍id为1的书的作者id为2和3 book_obj.authors.remove(2,3) //方式二 clear 全部清空 #把书籍id为1的对应的作者全部删除 book_obj = models.Book.objects.get(id=1) book_obj.authors.clear() 5.2.3.3改 一对多和一对一和单表是一样的 #将书籍表中id为1的数据数名修改为python从入门到放弃并且出版社修改为出版社id为3的 book_obj = models.Book.objects.filter(id=1).update( title='python从入门到放弃', #方式1 publishs=models.Publish.objects.get(id=3) #方式2 publishs_id=3 ） 多对多 //set 先清空再追加，set中必须写字符串，删除多个在列表中填写多个数值 #把书籍id为1的作者全部改为2 book_obj = models.Book.objects.get(id=1) book_obj.authors.set(['2',]) 5.2.3.4查 基于对象的跨表查询 一对一 作者表和作者详细信息表 //正向查询 #查询作者小明的住址 author_obj = models.Author.objects.get(name='小明') # author_obj.ad直接拿到authordetail表中的那个记录对 print(author_obj.ad.addr) //反向查询 #查询作者详细信息表中电话号以170开头的作者名字 authordetail_ojb = models.AuthorDetail.objects.filter( telephone__startswith='170').first() print(book_obj) #不加first打印结果 ]> print(book_obj) #加first打印结果 Book object #authordetail_ojb.author中的author是类名小写，直接定位到app01_author这张表 print(authordetail_ojb.author.name) 关于正向查询和反向查询的说明 一对多 出版社表和书籍表 //正向查询 #查询python从入门到放弃这本书的出版社 book_obj = models.Book.objects.filter(title='python从入门到放弃').first() print(book_obj.publishs.name) //反向查询 pub_obj = models.Publish.objects.get(name='南京出版社') #pub_obj.book_set.all()中book_set是结果可能为多个，最后的all是拿到的书籍对象所有信息 books = pub_obj.book_set.all().values('title') print(books) #查询南京出版社中数名包含linux的书 books = pub_obj.book_set.filter(title__contains='linux').values('title') 多对多 书籍表和作者表 //正向查询 #查询书籍表中书名为linux从入门到放弃的作者 book_obj = models.Book.objects.filter(title='linux从入门到放弃').first() authors = book_obj.authors.all().values('name') print(authors) //反向查询 反向查询中用字段存在的表的小写类名，这里为book #查询作者小明写的书 xiaoming_obj = models.Author.objects.get(name='小明') ret = xiaoming_obj.book_set.all().values('title') print(ret) 查询书籍表中书名为linux从入门到放弃的作者 基于双下划线的跨表查询 就是join连表查询 一对一 作者表与作者详细信息表 //查询作者小明的住址 #基于对象的跨表查询写法 正向查询 author_obj = models.Author.objects.get(name='小明') #author_obj.ad直接拿到authordetail表中的那个记录对 print(author_obj.ad.addr) #基于双下划线的跨表查询写法 #正向查询 ret = models.Author.objects.filter(name='小明').values('ad__addr') print(ret) #反向查询 ret = models.AuthorDetail.objects.filter(author__name='小明').values('addr') print(ret) 正向查询中 关联字段__字段 就获得了关联表的数据 反向查询中 表名__字段 一对多 书籍表和出版社表 //查询东京出版社出版的书 #原生sql语句 select app01_book.title from app01_publish inner join app01_book on app01_publish.id = app01_book.publishs_id where app01_publish.name='东京出版社'; #正向查询 ret = models.Book.objects.filter(publishs__name='东京出版社').values('title') #反向查询 ret = models.Publish.objects.filter(name='东京出版社').values('book__title') 多对多 书籍表和作者表 关联字段在书籍表中，从书籍表查询是正向查询 #查询一下小明写了哪些书 #正向查询 关联字段在book表中，现在要查询小明写了哪些书，当前是book表，表中没有作者名字的字段，因此需要先连表然后在取值 ret = models.Book.objects.filter(authors__name='小明').values('title') print(ret) #反向查询 现在要查询小明写了哪些书，当前是author表，表中有作者名字的字段，因此不需要先连表，已知字段在本表的就可以先写条件最后在连表 ret = models.Author.objects.filter(name='小明').values('book__title') print(ret) 关联的方式，正向查询和反向查询都可以使用，关键在于查询的条件 关联字段__字段 就获得了关联表的数据 小写类名__字段 聚合查询 aggregate聚合查询,结果是普通字典,queryset的结束符 #导入相关模块 from django.db.models import Avg,Max,Min,Count,Sum //查询书籍表中价格最高的 obj = models.Book.objects.all().aggregate(a=Max('price')) print(obj) 结果： {'a': Decimal('125.00')} 分组查询 //查询每个出版社出版的书的最高价格 #方式一 ret = models.Book.objects.values('publishs_id').annotate(m=Max('price')) print(ret) 结果： 方式一写法说明 values写在annotate前面,意思是以values括号内的字段作为分组的依据,annotate里面是你要做的统计结果,这样,返回结果为queryset类型数据,里面是字典{'publishs_id':1,'m':100} #方式二 #从出版社表中查询，需要连接book表，因为book表中才有书名和价格，这么写表示直接以publish这张表的id进行分组 ret = models.Publish.objects.annotate(m=Max('book__price')).values('m','name') print(ret) 结果： 方式二写法说明 annotate直接写在了objects后面,意思是按照前面表的所有的数据(默认是id值)作为分组依据,结果返回的是前面这个表的所有models对象(model对象中包含了每个对象自己的统计结果),在通过values来取值,取值时可以直接写字段和统计结果的别名,也是queryset类型,里面是字典{'m':100,'name':'东京出版社'} F和Q查询 在book表中添加连个字段：点赞(dianzan)和评论(pinglun) class Book(models.Model): title = models.CharField(max_length=32) publishDate=models.DateField() dianzan = models.IntegerField(default=100) pinglun = models.IntegerField(default=100) price=models.DecimalField(max_digits=5,decimal_places=2) publishs=models.ForeignKey(to=\"Publish\",to_field=\"id\",on_delete=models.CASCADE) app01_book表中内容 F查询 F查询用于一张表中的两个字段比较之后的符合条件的结果集 //查询book表中点赞数大于评论数的所有书籍 #low版写法 list1 = [] books = models.Book.objects.all() for i in books: if i.dianzan > i.comment: list1.append(i) #F查询写法 ret = models.Book.objects.filter(dianzan__gt=F('pinglun')).values('title') print(ret) 结果： //F还支持四则运算，例如将book表中的价格都上调50元models.Book.objects.all().update( price=F('price')+50 ) Q查询 Q查询用于多个条件有与、或、非时的综合查询 与：& 或：| 非：～ orm执行原生sql语句 //方式一 ret = models.Book.objects.raw/branch('select * from app01_book') for i in ret: print(i.title) 结果： 会把表中所有书名打印出来 //方式二 django自带的连接通道(在项目下同名目录__init__.py中配置的pymysql) from django.db import connection import pymysql def query(request): conn = pymysql.connect() cursor = connection.cursor() cursor.execute('select * from app01_book;') print(cursor.fetchall()) //方式三 def query(request): conn = pymysql.connect( host='127.0.0.1', port=3306, user='root', password='PAPAlichencan!1', database='orm02', charset='utf8' ) cursor = conn.cursor(pymysql.cursors.DictCursor) cursor.execute('select * from app01_book;') print(cursor.fetchall()) 结果： [{'id': 1, 'title': 'python从入门到放弃', 'publishDate': datetime.date(2011, 1, 1), 'price': Decimal('99.00'), 'publishs_id': 3, 'dianzan': 600, 'pinglun': 150}, {'id': 2, 'title': 'linux从入门到放弃', 'publishDate': datetime.date(2019, 12, 12), 'price': Decimal('99.90'), 'publishs_id': 2, 'dianzan': 500, 'pinglun': 300}, {'id': 3, 'title': '隔壁老王的故事', 'publishDate': datetime.date(2019, 11, 7), 'price': Decimal('125.00'), 'publishs_id': 2, 'dianzan': 100, 'pinglun': 100}, {'id': 4, 'title': 'linux安全指南', 'publishDate': datetime.date(2020, 11, 30), 'price': Decimal('66.00'), 'publishs_id': 2, 'dianzan': 300, 'pinglun': 2000}, {'id': 5, 'title': '奔跑的linux内核', 'publishDate': datetime.date(2019, 11, 1), 'price': Decimal('77.00'), 'publishs_id': 2, 'dianzan': 50, 'pinglun': 100}] django外部脚本调用models数据库操作 使用场景：有的时候我们不想运行django项目只想执行一些视图中的逻辑，这个时候就用到了django外部脚本调用models数据库操作 操作方法：在django项目下新建一个xx.py文件，在xx.py文件中写入以下内容，然后直接运行这个xx.py文件就可以在不启动django项目的情况下而单独执行我们想执行的测试逻辑 #导入os模块 import os if __name__ == '__main__': #最后一个参数要写项目名.settings os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"orm02.settings\") #导入django import django django.setup() #导入应用程序的models from app01 import models ret = models.Book.objects.all().values('title') print(ret) ORM事务和锁 锁 models.Book.objects.select_for_update().filter(id=1) 事务 //方式1 全局配置 DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', 'NAME': 'mxshop', 'HOST': '127.0.0.1', 'PORT': '3306', 'USER': 'root', 'PASSWORD': '123', \"ATOMIC_REQUESTS\": True, #全局开启事务，绑定的是http请求响应整个过程当中的sql } } 方式1中，django项目中所有的视图逻辑全部加上了事物 //方式2: 视图函数加装饰器 from django.db import transaction @transaction.atomic def viewfunc(request): # This code executes inside a transaction. 逻辑语句 方式2中，我们只想给某一个视图函数加事物，这个时候就用到了上述的方法 方式3: 上下文加装饰器 from django.db import transaction def viewfunc(request): # This code executes in autocommit mode (Django's default). 逻辑语句1 with transaction.atomic(): #保存点 # This code executes inside a transaction. 逻辑语句2 逻辑语句3 方式3中，我们只想给逻辑语句2添加事物，而其他语句不需要，这个时候就在逻辑语句2的上边写如上语句即可 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python进阶/django/6.cookie和session.html":{"url":"python/python进阶/django/6.cookie和session.html","title":"django基础六 cookie和session","keywords":"","body":"[toc] cookie和session 一、cookie 1.1cookie的由来 http协议是无状态的，无状态的意思就是每次请求都是独立的，它的执行情况和结果与前面的请求和之后的请求都无直接关系，对于服务器来讲，每次都请求都是全新的 状态可以理解为客户端和服务端在某次会话中产生的数据，那无状态的情况下，产生的数据就不会被保存，而会话产生的数据又是需要我们保存的，经典的需求就是客户端登陆服务端后，服务端需要记住客户端是谁，而不用每次都重新登陆，这样就产生了cookie 1.2cookie是什么 cookie是浏览器的技术，cookie具体指的是一段小信息，它是服务器发送出来存储在浏览器上的一组组简直对，可以理解为服务端给客户端的一个小甜点，下次客户端的浏览器访问服务器时就会自动携带这些键值对，一便服务器提取有用信息 1.3cookie的原理 cookie的工作原理是：浏览器访问服务端，带着一个空的cookie，然后由服务器产生内容，浏览器收到内容后保存在本地，当浏览器再次访问服务器时，浏览器会自动带上cookie，这样服务器就能通过cookie的内容来判断请求者是谁了 查看cookie cookie原理示意图 1.4cookie规范 Cookie大小上限为4KB，是总数大小为4KB不是单个 一个服务器最多在客户端浏览器上保存20个Cookie 一个浏览器最多保存300个Cookie，因为一个浏览器可以访问多个服务器 不同浏览器之间是不共享Cookie的 1.5cookie和http头 Cookie是通过HTTP请求和响应头在客户端和服务器端传递的： Cookie：请求头，客户端发送给服务器端； 格式： Cookie: a=A; b=B; c=C。即多个Cookie用分号离开；  Set-Cookie：响应头，服务器端发送给客户端； 一个Cookie对象一个Set-Cookie： Set-Cookie: a=A Set-Cookie: b=B Set-Cookie: c=C 1.6cookie的覆盖 如果服务器端发送重复的Cookie那么会覆盖原有的Cookie，例如客户端的第一个请求服务器端发送的Cookie是：Set-Cookie: a=A；第二请求服务器端发送的是：Set-Cookie: a=AA，那么客户端只留下一个Cookie，即：a=AA。 1.7django中操作cookie 1.7.1django设置cookie 设置cookie #方式一 获取cookie: request.COOKIES.get('xx') 设置cookie: HttpResponse('xx').set_cookie('键','值') #方式二 获取签名cookie: request.get_signed_cookie('is_login',salt='xxx') 设置签名cookie: ret.set_signed_cookie('is_login',True,'xxx') 设置cookie中的参数 key #cookie的键 value='' #cookie的值 max_age=None #超时时间，单位秒，经过多少秒后cookie失效，默认两周 expires=None #超时时间(IE requires expires, so set it if hasn't been already.)，例如，当前时间为1月1日10时10分，设置为7就是1月8日的10时10分失效 path='/' #Cookie生效的路径，/ 表示根路径，特殊的：根路径的cookie可以被任何url的页面访问 domain=None #Cookie生效的域名 secure=False #https传输 httponly=False #只能http协议传输，无法被JavaScript获取（不是绝对，底层抓包可以获取到也可以被覆盖） cookie设置时不要设置中文！！！因为不专业 如果非要设置中文的解决方法 #方式1 def login(request): ret = HttpResponse('ok') ret.set_cookie('k1','你好'.encode('utf-8').decode('iso-8859-1')) #取值：request.COOKIES['k1'].encode('utf-8').decode('iso-8859-1').encode('iso-8859-1').decode('utf-8') return ret #方式2 json def login(request): ret = HttpResponse('ok') import json ret.set_cookie('k1',json.dumps('你好')) #取值 json.loads(request.COOKIES['k1']) return ret 1.7.2django删除cookie 删除cookie: ret = redirect(\"/login/\") ret.delete_cookie('cookie键名称') 1.7.3cookie版登陆校验示例 用户访问login，登陆成功重定向到页面home展示内容 urls文件 from django.conf.urls import url from django.contrib import admin from app01 import views urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^login/', views.login,name='login'), url(r'^home/', views.home,name='home'), ] views文件 from django.shortcuts import render,HttpResponse,redirect # Create your views here. def login(request): if request.method == 'GET': return render(request,'login.html') else: username = request.POST.get('username') password = request.POST.get('password') if username == 'admin' and password == 'admin': ret = redirect('home') #这里是我们自己设置cookie ret.set_cookie('is_login',True) # return redirect('home') return ret else: return redirect('login') def home(request): #这里获取上边我们自己设置的cookie is_login = request.COOKIES.get('is_login') if is_login == 'True': return render(request,'home.html') else: return redirect('login') html文件 #login.html Title 登录页面 {% csrf_token %} 用户名: 密码: #home.html Title 欢迎来到乔杉大保健会所 整体的逻辑是用户登陆成功后返回home页面，直接访问home页面不可以，清除cookie后用户需要重新登陆 查看浏览器中的cookie 二、session 2.1session是什么 Session是服务器端技术，利用这个技术，服务器在运行时可以为每一个用户的浏览器创建一个其独享的session对象，由于session为用户浏览器独享，所以用户在访问服务器的web资源时 ，可以把各自的数据放在各自的session中，当用户再去访问该服务器中的其它web资源时，其它web资源再从用户各自的session中取出数据为用户服务。 session图解 2.2session详细流程 1.当用户登陆之后，服务端生成一个字典{'key':'value'}，并且将字典存入session，key是服务端自动生成的一段字符串标示，返回cookie，value是一个自定义格式的字典，这个字典的内容由我们自己决定 2.在1中生成的字典value中自定义格式来存储用户信息，如用户名、密码等等 3.当我们在django中用到session时，cookie由服务端随机生成，写到浏览器的cookie中，每个浏览器都有自己的cookie值，是session寻找用户信息的唯一标识，每个浏览器请求到后台的接收的request_session等价于1中session字典的key(cookie)对应的value 2.3session规范 借助于cookie进行传输 非明文显示 长度不限 2.4django中操作session 2.4.1django设置session //session可以设置多个 request.session['is_login'] = True request.session['username'] = username //获取session request.session.get('is_login') request.session.get('username') django设置session过程 1.生成随机字符串 2.放到cookie中进行传输 3.将随机字符串和对应数据保存到自己服务端的数据库中 django获取session过程 1.取出cookie中的session随机字符串{'sessionid':'asdfasfpoaijsdgihsdj'} xx = request.COOKIES.get('sessionid') 2.到数据库中查询这个sessionid对应的那条记录 data = select session_data from django_session where session_key = xx; 3.拿出记录中的session_data数据部分进行解密,并取出数据 dic.get('is_login') --> True dic = ss(data) --> {'is_login':True} django删除session过程 1.删除cookie中的sessionid那个键值对 2.删除了数据库中的这条记录 2.4.2django删除session request.session.flush() 2.4.3session版登陆校验示例 用户访问login，登陆成功重定向到页面home展示内容 urls文件 from django.conf.urls import url from django.contrib import admin from app01 import views urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^login/', views.login,name='login'), url(r'^home/', views.home,name='home'), ] views文件 from django.shortcuts import render,HttpResponse,redirect # Create your views here. def login(request): if request.method == 'GET': return render(request,'login.html') else: username = request.POST.get('username') password = request.POST.get('password') if username == 'admin' and password == 'admin': request.session['is_login'] = True request.session['username'] = username \"\"\" 设置session后做的3件事 1.生成随机字符串 2.放到cookie中进行传输 3.将随机字符串和对应数据保存到自己服务端的数据库中 \"\"\" return redirect('home') else: return redirect('login') def home(request): #这里获取上边我们自己设置的session is_login = request.session.get('is_login') \"\"\" request.session.get做的事 1.取出cookie中的session随机字符串{'sessionid':'asdfasfpoaijsdgihsdj'} xx = request.COOKIES.get('sessionid') 2.到数据库中查询这个sessionid对应的那条记录 data = select session_data from django_session where session_key = xx; 3.拿出记录中的session_data数据部分进行解密,并取出数据 dic = sss(data) --> {'is_login':True} dic.get('is_login') --> True \"\"\" if is_login == True: return render(request,'home.html') else: return redirect('login') def logout(request): #删除session request.session.flush() \"\"\" 删除session 1.删除cookie中的sessionid那个键值对 2.删除了数据库中的这条记录 \"\"\" return redirect('login') html文件 #login.html Title 登录页面 {% csrf_token %} 用户名: 密码: #home.html Title 欢迎来到乔杉大保健会所 注销 cookie与session总结 cookie的作用: 保持会话,使用户不需要重复的去登录 1.有大小限制,Cookie总大小上限为4KB； 2.有个数限制 一个服务器最多在客户端浏览器上保存20个Cookie； 一个浏览器最多保存300个Cookie，因为一个浏览器可以访问多个服务器。 session 1.比cookie面上安全一些 2.session没有大小限制 3.可以配置多个存储方案,可以配置到缓存中 django session表中session key标记的是浏览器，不是用户，一个浏览器对应一个服务端 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python进阶/django/7.Ajax.html":{"url":"python/python进阶/django/7.Ajax.html","title":"django基础七 Ajax","keywords":"","body":"6.Ajax 异步提交 局部刷新 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python进阶/django/8.django中间件.html":{"url":"python/python进阶/django/8.django中间件.html","title":"django基础八 django中间件","keywords":"","body":"[toc] django中间件 1.中间件介绍 1.1django中间件简介及定义 django中间件是介于request与response处理之间的一道处理过程，相对比较轻量级，并且在全局上改变django的输入与输出。因为改变的是全局，所以需要谨慎实用，用不好会影响到性能。 Django的中间件的定义： #中间件是一个钩子框架，用于Django的请求/响应处理。这是一个轻量级的、底层的“插件”系统，用于全局地改变Django的输入或输出。 Middleware is a framework of hooks into Django’s request/response processing. It’s a light, low-level “plugin” system for globally altering Django’s input or output. 1.2django中间件的作用 如果你想修改请求，例如被传送到view中的HttpRequest对象。 或者你想修改view返回的HttpResponse对象，这些都可以通过中间件来实现。 可能你还想在view执行之前做一些操作，这种情况就可以用 middleware来实现。 说的直白一点中间件是帮助我们在视图函数执行之前和执行之后都可以做一些额外的操作，它本质上就是一个自定义类，类中定义了几个方法，Django框架会在请求的特定的时间去执行这些方法。 1.3django中间件的配置项 打开django项目的settings.py文件，看到下面的MIDDLEWARE配置项，django默认自带的一些中间件： MIDDLEWARE = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware', ] 2.django生命周期 django生命周期 3.自定义中间件 第一步、在django项目下的应用程序目录下创建一个mymiddleware目录，名称任意，在这个目录中再创建一个middleware文件，名称任意，在这个文件中写入以下内容 #导入MiddlewareMixin from django.utils.deprecation import MiddlewareMixin from django.shortcuts import redirect,render,HttpResponse class Auth(MiddlewareMixin): # 定义一个白名单 white_list = ['/login/', ] def process_request(self,request): if request.path in self.white_list: pass else: is_login = request.session.get('is_login') \"\"\" request.session.get做的事 1.取出cookie中的session随机字符串{'sessionid':'asdfasfpoaijsdgihsdj'} xx = request.COOKIES.get('sessionid') 2.到数据库中查询这个sessionid对应的那条记录 data = select session_data from django_session where session_key = xx; 3.拿出记录中的session_data数据部分进行解密,并取出数据 dic = sss(data) --> {'is_login':True} dic.get('is_login') --> True \"\"\" # 这里写一个pass，就是返回一个None，如果返回是None，则说明通过了请求 if is_login == True: pass else: # return redirect('login') return HttpResponse('中间件不让你登陆') 第二步、在settings文件中MIDDLEWARE项的最后边配置中间件 #应用程序名.自定义中间件的目录.自定义中间件py文件.自定义类 'app01.mymiddleware.middleware.Auth' 第三步、视图文件 from django.shortcuts import render,HttpResponse,redirect from django.urls import reverse # Create your views here. def login(request): if request.method == 'GET': return render(request,'login.html') else: username = request.POST.get('username') password = request.POST.get('password') if username == 'admin' and password == 'admin': request.session['is_login'] = True request.session['username'] = username \"\"\" 设置session后做的3件事 1 生成随机字符串 2 放到cookie中进行传输 3 将随机字符串和对应数据保存到自己服务端的数据库中 \"\"\" return redirect('home') else: return redirect('login') def home(request): #这里获取上边我们自己设置的cookie # is_login = request.COOKIES.get('is_login') is_login = request.session.get('is_login') \"\"\" request.session.get做的事 1.取出cookie中的session随机字符串{'sessionid':'asdfasfpoaijsdgihsdj'} xx = request.COOKIES.get('sessionid') 2.到数据库中查询这个sessionid对应的那条记录 data = select session_data from django_session where session_key = xx; 3.拿出记录中的session_data数据部分进行解密,并取出数据 dic = sss(data) --> {'is_login':True} dic.get('is_login') --> True \"\"\" if is_login == True: return render(request,'home.html') else: return redirect('login') 4.中间件5大方法 4.1process_request(self,request) 3中的自定义中间件用到的就是process_request(self,request)，这是一个对请求进行处理的方法 process_request有一个参数，就是request，这个request和视图函数中的request是一样的。 它的返回值可以是None也可以是HttpResponse对象。返回值是None的话，按正常流程继续走，交给下一个中间件处理，如果是HttpResponse对象，Django将不执行视图函数，而将响应对象返回给浏览器。 4.2process_response(self, request, response) process_response是对视图函数所有的响应进行处理 它有两个参数，一个是request，一个是response，request和视图函数中的request是一样的，response是视图函数返回的HttpResponse对象。该方法的返回值也必须是HttpResponse对象。 例如，在每一个视图函数的响应头中添加一些东西，就可以在process_response方法中添加响应头 def process_response(self,request,response): ''' :param request: :param response: 就是视图函数的返回值(HttpResonse对象) :return: ''' ''' print(response) response返回的是一个HttpResponse对象 ''' #必须返回response这个值，不返回下一个中间件的respnse方法拿不到这个值 return response 4.3process_view(self, request, view_func, view_args, view_kwargs) process_view是在视图函数执行前执行 该方法有四个参数 request是HttpRequest对象。 view_func是Django即将使用的视图函数。（它是实际的函数对象，而不是函数的名称作为字符串。） view_args是将传递给视图的位置参数的列表. view_kwargs是将传递给视图的关键字参数的字典。 view_args和view_kwargs都不包含第一个视图参数（request）。 Django会在调用视图函数之前调用process_view方法。 它应该返回None或一个HttpResponse对象。 如果返回None，Django将继续处理这个请求，执行任何其他中间件的process_view方法，然后再执行相应的视图。 如果它返回一个HttpResponse对象，Django不会调用对应的视图函数。 它将执行中间件的process_response方法并将应用到该HttpResponse并返回结果。 process_view执行顺序示意图 中间件文件 from django.utils.deprecation import MiddlewareMixin from django.shortcuts import redirect,render,HttpResponse class MD1(MiddlewareMixin): def process_request(self,request): print('MD1的process_request方法') def process_response(self,request,response): print('MD1的process_response方法') return response def process_view(self,request,view_func,view_args,view_kwargs): print('MD1的process_view方法',view_func.__name__) class MD2(MiddlewareMixin): def process_request(self, request): print('MD2的process_request方法') def process_response(self, request, response): print('MD2的process_response方法') return response def process_view(self, request, view_func, view_args, view_kwargs): print('MD2的process_view方法', view_func.__name__) 配置settings文件，MIDDLEWARE项 'app01.mymiddleware.xx.MD1', 'app01.mymiddleware.xx.MD2', 视图文件 def login(request): #这里打印一个login视图函数是为了看到执行的顺序，process_request,process_response,process_view print('login视图函数') if request.method == 'GET': return render(request,'login.html') 打印结果 可以看到，先执行process_request方法，然后执行process_view方法，process_view方法是在视图函数执行前执行，之后执行视图函数，最后执行process_response方法，并且process_response是倒序执行 ⚠️⚠️⚠️如果在process_request方法中返回了HttpResponse对象，则只执行自己中间件的process_request方法和process_response方法，如果自己的中间件没有定义process_response方法，则会交给自己类的上一个中间件，如果上一个类也没有，会一直往上推 class MD1(MiddlewareMixin): def process_request(self,request): print('MD1的process_request方法') #这里返回了一个HttpResponse对象，则只执行自己中间件的process_request方法和process_response方法 return HttpResponse('xxx') def process_response(self,request,response): print('MD1的process_response方法') return response def process_view(self,request,view_func,view_args,view_kwargs): print('MD1的process_view方法',view_func.__name__) 打印结果 MD1的process_request方法 MD1的process_response方法 示意图 4.4process_exception(self, request, exception) 该方法两个参数: 一个HttpRequest对象 一个exception是视图函数异常产生的Exception对象。 这个方法只有在视图函数中出现异常了才执行，它返回的值可以是一个None也可以是一个HttpResponse对象。如果是HttpResponse对象，Django将调用模板和中间件中的process_response方法，并返回给浏览器，否则将默认处理异常。如果返回一个None，则交给下一个中间件的process_exception方法来处理异常。它的执行顺序也是按照中间件注册顺序的倒序执行。 process_exception执行示意图 中间件文件 from django.utils.deprecation import MiddlewareMixin from django.shortcuts import redirect,render,HttpResponse class MD1(MiddlewareMixin): def process_request(self,request): print('MD1的process_request方法') # return HttpResponse('xxx') def process_response(self,request,response): print('MD1的process_response方法') return response def process_view(self,request,view_func,view_args,view_kwargs): print('MD1的process_view方法',view_func.__name__) def process_exception(self, request, exception): print(exception) print(\"MD1的process_exception方法\") return HttpResponse(str(exception)) class MD2(MiddlewareMixin): def process_request(self, request): print('MD2的process_request方法') def process_response(self, request, response): print('MD2的process_response方法') return response def process_view(self, request, view_func, view_args, view_kwargs): print('MD2的process_view方法', view_func.__name__) def process_exception(self, request, exception): print(exception) print(\"MD2的process_exception方法\") return HttpResponse(str(exception)) 视图函数文件，在视图函数中抛出一个异常 def login(request): # print('login视图函数') print(\"app01中的login视图函数\") raise ValueError(\"视图函数中出错了才执行process_exception方法\") return HttpResponse(\"OK\") if request.method == 'GET': return render(request,'login.html') else: username = request.POST.get('username') password = request.POST.get('password') if username == 'admin' and password == 'admin': request.session['is_login'] = True request.session['username'] = username \"\"\" 设置session后做的3件事 1 生成随机字符串 2 放到cookie中进行传输 3 将随机字符串和对应数据保存到自己服务端的数据库中 \"\"\" return redirect('home') else: return redirect('login') 打印结果，当md1和md2中的process_exception方法都返回了Httpresponse对象，只执行md2的process_exception方法 当只有md1返回Httpresponse对象时，md1和md2的process_exception方法都执行 4.5process_template_response(self, request, response) 它的参数，一个HttpRequest对象，response是TemplateResponse对象（由视图函数或者中间件产生） process_template_response是在视图函数执行完成后立即执行，但是它有一个前提条件，那就是视图函数返回的对象有一个render()方法（或者表明该对象是一个TemplateResponse对象或等价方法） 中间件文件 from django.utils.deprecation import MiddlewareMixin from django.shortcuts import redirect,render,HttpResponse class MD1(MiddlewareMixin): def process_request(self,request): print('MD1的process_request方法') # return HttpResponse('xxx') def process_response(self,request,response): print('MD1的process_response方法') return response def process_view(self,request,view_func,view_args,view_kwargs): print('MD1的process_view方法',view_func.__name__) def process_exception(self, request, exception): print(exception) print(\"MD1的process_exception方法\") return HttpResponse(str(exception)) def process_template_response(self, request, response): print(\"MD1的process_template_response方法\") return response class MD2(MiddlewareMixin): def process_request(self, request): print('MD2的process_request方法') def process_response(self, request, response): print('MD2的process_response方法') return response def process_view(self, request, view_func, view_args, view_kwargs): print('MD2的process_view方法', view_func.__name__) def process_exception(self, request, exception): print(exception) print(\"MD2的process_exception方法\") # return HttpResponse(str(exception)) def process_template_response(self, request, response): print(\"MD2的process_template_response方法\") return response 视图文件，必须这么写才行 def login(request): def render(): print(\"in index/render\") return HttpResponse(\"OK\") # 返回的将是这个新的对象 rep = HttpResponse(\"OK\") rep.render = render return rep 打印结果 5.中间件执行流程 请求到达中间件之后，先按照正序执行每个注册中间件的process_reques方法，process_request方法返回的值是None，就依次执行，如果返回的值是HttpResponse对象，不再执行后面的process_request方法，而是执行当前对应中间件的process_response方法，将HttpResponse对象返回给浏览器。也就是说：如果MIDDLEWARE中注册了6个中间件，执行过程中，第3个中间件返回了一个HttpResponse对象，那么第4,5,6中间件的process_request和process_response方法都不执行，顺序执行3,2,1中间件的process_response方法 process_request方法都执行完后，匹配路由，找到要执行的视图函数，先不执行视图函数，先执行中间件中的process_view方法，process_view方法返回None，继续按顺序执行，所有process_view方法执行完后执行视图函数。加入中间件3 的process_view方法返回了HttpResponse对象，则4,5,6的process_view以及视图函数都不执行，直接从最后一个中间件，也就是中间件6的process_response方法开始倒序执行 process_template_response和process_exception两个方法的触发是有条件的，执行顺序也是倒序。总结所有的执行流程如下 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python-other/nginx+django+uwsgi部署项目.html":{"url":"python/python-other/nginx+django+uwsgi部署项目.html","title":"nginx+django+uWSGI部署项目","keywords":"","body":"[toc] nginx+django+uwsgi部署项目 一、WSGI、uWSGI、uwsgi概念简述 WSGI WSGI，全称 Web Server Gateway Interface，或者 Python Web Server Gateway Interface ，是为 Python 语言定义的 Web 服务器和 Web 应用程序或框架之间的一种简单而通用的接口 wsgi server (比如uWSGI） 要和 wsgi application（比如django ）交互，uwsgi需要将过来的请求转给django 处理，那么uWSGI 和 django的交互和调用就需要一个统一的规范，这个规范就是WSGI WSGI 的官方定义是，the Python Web Server Gateway Interface。从名字就可以看出来，这东西是一个Gateway，也就是网关。网关的作用就是在协议之间进行转换。 WSGI 是作为 Web 服务器与 Web 应用程序或应用框架之间的一种低级别的接口，以提升可移植 Web 应用开发的共同点。WSGI 是基于现存的 CGI 标准而设计的。 uWSGI uWSGI是一个Web服务器，它实现了WSGI协议、uwsgi、http等协议。Nginx中HttpUwsgiModule的作用是与uWSGI服务器进行交换。 uwsgi uwsgi是服务器和服务端应用程序的通信协议，规定了怎么把请求转发给应用程序和返回 二、处理过程 nginx+django+uWSGI处理过程 1.客户端(浏览器)准备发送请求 2.如果请求的是静态内容则直接返回给客户端，nginx在这里统一收集后端django项目的静态文件 3.如果请求的是动态内容, 就把请求转发给uWSGI ,uWSGI连接Django进入我们的Python程序进行处理，这期间uWSGI会连接数据库获取数据 三、部署过程 3.1环境及角色说明 系统环境 角色 版本 系统 centos7.6 nginx 1.16 django 2.1 mysql 5.7.22 uWSGI 2.0.18 supervisor 4.1.0 角色说明 nginx 提供反向解析功能，将80端口请求转发至django端口，同时还负责处理静态资源 uWSGI+django 启动后端项目，处理动态请求 mysql 数据库，存储数据，django从数据库中获取数据 supervisor 进程管理工具，防止uWSGI突然崩溃，supervisor能自动启动uWSGI 3.2部署过程 3.2.1安装nginx1.16 1.下载nginx [root@django ~]# wget http://nginx.org/download/nginx-1.16.1.tar.gz 2.安装依赖 [root@django ~]# yum -y install gcc gcc-c++ zlib zlib-devel openssl openssl-devel pcre-devel 3.添加ngxin用户和组 [root@django ~]# groupadd nginx && useradd nginx -g nginx -s /sbin/nologin -M 4.解压缩并编译安装 [root@django ~]# tar xf nginx-1.16.1.tar.gz [root@django ~]# cd nginx-1.16.1/ [root@django nginx-1.16.1]# ./configure --user=nginx --group=nginx --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module --with-http_realip_module --with-http_gzip_static_module [root@django nginx-1.16.1]# make -j 2 && make install 5.设置nginx命令环境变量 [root@django ~]# echo \"PATH=/usr/local/nginx/sbin:$PATH\" >/etc/profile.d/nginx.sh [root@django ~]# source /etc/profile 6.启动nginx [root@django ~]# nginx [root@django ~]# ps aux|grep nginx root 6833 0.0 0.2 45968 1132 ? Ss 15:20 0:00 nginx: master process nginx nginx 6834 0.0 0.3 48508 1972 ? S 15:20 0:00 nginx: worker process root 6920 0.0 0.1 112708 988 pts/0 S+ 15:21 0:00 grep --color=auto nginx 3.2.2安装mysql5.7.22 ⚠️⚠️⚠️ 二进制安装mysql的启动脚本和 安装目录/mysql/bin/mysqld_safe 这两个文件中都是默认/usr/local/mysql，如果安装目录不在/usr/local/下，需要修改这两个文件中的路径，即把/usr/local替换为mysql安装目录 sed -i 's#/usr/local#/application#g' /etc/init.d/mysql /application/mysql/bin/mysqld_safe 1.下载MySQL-5.7.22二进制包 [root@django ~]# wget https://cdn.mysql.com/archives/mysql-5.7/mysql-5.7.22-linux-glibc2.12-x86_64.tar.gz MD5值 9ef7a05695f8b4ea29f8d077c3b415e2 2.解压缩mysql二进制包到/usr/local [root@django ~]# tar xf mysql-5.7.22-linux-glibc2.12-x86_64.tar.gz -C /usr/local 3.修改名称、做软连接 [root@django local]# mv /usr/local/mysql-5.7.22-linux-glibc2.12-x86_64 /usr/local/mysql-5.7.22 && \\ ln -s /usr/local/mysql-5.7.22 /usr/local/mysql 4.创建mysql用户和组 [root@django local]# groupadd mysql && useradd -g mysql -s /bin/false mysql 5.编辑主配置文件，myql-5.7.22二进制包默认没有mysql配置文件 #备份/etc/my.cnf [root@django local]# mv /etc/my.cnf /etc/my.cnf.old #以下配置为最精简版，可根据实际情况进行相应设置 [root@django~]# cat >> /etc/my.cnf /etc/profile.d/mysql.sh #使配置生效 [root@django ~]# source /etc/profile 9.配置systemd管理mysql [root@django ~]# cat >> /etc/systemd/system/mysqld.service set password='123'; Query OK, 0 rows affected (0.00 sec) mysql> flush privileges; Query OK, 0 rows affected (0.01 sec) 3.2.3安装python3.6 1.下载python [root@django ~]# wget https://www.python.org/ftp/python/3.6.9/Python-3.6.9.tar.xz 2.安装依赖包 [root@django ~]# yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel libffi-devel gcc gcc-c++ make 3.解压缩python并编译安装 [root@django ~]# tar xf Python-3.6.9.tar.gz [root@django ~]# cd Python-3.6.9/ [root@django Python-3.6.9]# ./configure --prefix=/usr/local/python36 [root@django Python-3.6.9]# make -j 2 && make install 4.导出python命令环境变量 [root@django ~]# echo \"PATH=/usr/local/python36/bin:$PATH\" >/etc/profile.d/python36.sh [root@django ~]# source /etc/profile [root@django ~]# python3 -V Python 3.6.9 5.配置pip国内源 [root@django ~]# mkdir ~/.pip [root@django ~]# cat >~/.pip/pip.conf 3.2.4部署python虚拟环境virtualenv、virtualenvwrappe、uwsgi及django项目 鉴于virtualenv不便于对虚拟环境集中管理,所以推荐直接使用virtualenvwrapper,virtualenvwrapper提供了一系列命令使得和虚拟环境工作变得便利,它把你所有的虚拟环境都放在一个地方 virtualenvwrapper整体工作过程 1.安装virtualenvwrapper 2.在~/.bashrc文件中指定virtualenvwrapper存放虚拟环境总目录,指定virtualenvwrapper.sh存放位置(find查找) 3.mkvirtualenv命令创建python虚拟环境,-p选项指定不同python版本命令路径 4. workon命令切换不同python虚拟环境 第一步、安装virtualenv、virtualenvwrapper [root@django ~]# pip3 install virtualenv virtualenvwrapper 第二步、编辑virtualenvwrapper环境变量 //向~/.bashrc中写入以下内容 [root@django ~]# cat >> ~/.bashrc requirements.txt导出开发环境中所有安装的模块 第六步、修改项目requirements.txt文件中的django版本为2.1 克隆的django博客项目中的requirements.txt文件django版本是2.2.8，结果出了一堆问题，但是作者在github上介绍却是基于django2.1，修改完后安装requirements.txt中的模块 (django) [root@django DjangoBlog]# pip3 install -Ur requirements.txt (django) [root@django DjangoBlog]# pip3 install pymysql 第七步、配置mysql字符集 //修改my.cnf，写入以下内容 [mysqld] character-set-server=utf8mb4 collation-server=utf8mb4_unicode_ci [client] default-character-set = utf8mb4 [mysql] default-character-set = utf8mb4 //重启mysql (django) [root@django DjangoBlog]# systemctl restart mysqld 第八步、建库授权 CREATE USER 'djangoblog'@'localhost' IDENTIFIED BY 'DjAnGoBlOg123!@#'; CREATE DATABASE `djangoblog` /*!40100 DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci */; GRANT all ON djangoblog.* TO 'djangoblog'@'localhost'; FLUSH PRIVILEGES; 第九步、修改django项目mysql配置 //修改DjangoBlog/settings.py中的DATABASES配置，如下所示 DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', 'NAME': 'djangoblog', 'USER': 'djangoblog', 'PASSWORD': 'DjAnGoBlOg123!@#', 'HOST': 'localhost', 'PORT': 3306, 'OPTIONS': {'charset': 'utf8mb4'}, } } 第十步、编辑DjangoBlog/__init__.py，指定pymysql为客户端 cat >DjangoBlog/__init__.py /usr/local/nginx/conf/conf.d/django.blog.conf 配置本机hosts解析后访问django.blog，项目界面如下 3.2.5安装supervisor管理uWSGI 1.安装supervisor (django) [root@django DjangoBlog]# pip3 install supervisor (django) [root@django DjangoBlog]# supervisord -v 4.1.0 2.生成supervisor配置文件，也可以使用命令echo_supervisord_conf > /etc/supervisord.conf cat >> /etc/supervisord.conf /etc/logrotate.d/supervisor > /usr/lib/tmpfiles.d/tmp.conf> /usr/lib/systemd/system/supervisord.service /etc/supervisor/config.d/django.ini update django django: added process group supervisor> status django RUNNING pid 19728, uptime 0:00:17 关于django收集静态文件配置 uwsig默认不解析静态文件，需要统一收集一下，交给nginx去返回给客户端 //settings.py文件中如下参数是配置统一收集所有静态文件 STATIC_ROOT='xxx' //这里克隆的项目作者把目录设置为了如下路径 DjangoBlog/collectedstatic //配置完后需要执行以下命令 ./manage.py collectstatic --no-input #收集静态文件 ./manage.py compress --force #压缩静态文件 nginx对于静态资源的配置 nginx+django+uWSGI中还可以配置nginx专门匹配静态资源路径 location /static { alias /django/DjangoBlog/collectedstatic; } django中配置静态资源别名，这里static是别名，statics是真正的路径 STATIC_URL = '/static/' STATICFILES_DIRS = [ os.path.join(BASE_DIR,'statics'), ] python manage.py runserver 其实是调用wsgiref这个python内置的wsgi 服务器，性能很低，单线程 uWSGI是C写的一个基于uwsgi协议运行的高性能Web服务器，支持多进程、多线程 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python-other/python虚拟环境.html":{"url":"python/python-other/python虚拟环境.html","title":"python虚拟环境","keywords":"","body":"[toc] python虚拟环境 为什么要创建python虚拟环境 python版本众多，部分版本功能差异较大，centOS7.5 默认python版本2.7.5，centOS6.9 默认python版本2.6.6，在使用过程中经常遇到第三方库依赖的python版本和系统python版本不一致的情况，同时又因系统底层需要调用当前版本python，所以不能随意变更当前系统python版本，如此情境下就会有python多版本共存的情况，因此python多环境管理工具应用而生 一、pyenv pyenv github地址 1.1 pyenv简介 pyenv是一个简单的python版本管理工具，以前叫做pythonbrew，这个工具可以方便切换全局python版本，安装多个不同的python版本，设置独立的某个文件夹或者工程目录特异的python版本，同时创建python虚拟环境 1.2 pyenv原理 pyenv作为python的版本管理工具，通过改变shell的环境变量来切换不同的python版本，以达到多版本共存的目的 1) pyenv安装后会在系统PATH中插入shims路径，每次执行python相关的可执行文件，会优先在shims里寻找python路径~/.pyenv/shims:/usr/local/bin:/usr/bin:/bin 2) 系统选择python版本，按照如下顺序选择python脚本 ​ ①shell变量设置(执行 pyenv 查看) ​ ②当前可执行文件目录下的 .python_version 文件里的版本号(执行 pyenv shell 查看) ​ ③上层目录查询找到的第一个 .pyenv-version文件 ​ ④全局的版本号在 ~/.pyenv/version 文件内(执行 pyenv global 查看) 3) 确定版本文件的位置和python版本后，pyenv会根据版本号在 ~/.pyenv/versions/ 目录中查找对应的python版本，执行命令 pyenv versions 可查看系统目前安装的python版本 1.3 部署pyenv # 1.克隆pyenv至root家目录 git clone git://github.com/yyuu/pyenv.git ~/.pyenv # 2.修改环境变量 echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' >> ~/.bashrc echo 'export PATH=\"$PYENV_ROOT/bin:$PATH\"' >> ~/.bashrc echo 'eval \"$(pyenv init -)\"' >> ~/.bashrc $ tail -3 ~/.bashrc export PYENV_ROOT=\"$HOME/.pyenv\" export PATH=\"$PYENV_ROOT/bin:$PATH\" eval \"$(pyenv init -)\" # 3.使配置生效 source ~/.bashrc # 4.测试安装是否正确，返回如下即表明正确 $ pyenv versions * system (set by /root/.pyenv/version) # 5.设置pip国内源 mkdir ~/.pip cat > ~/.pip/pip.conf 1.4 通过oyenv管理多版本python pyenv命令语法 Usage: pyenv [] # 1.查看可安装的版本列表 pyenv install --list 会列出好多不同版本 # 2.安装依赖包 yum -y install python-pip python-devel gcc gcc-c++ zlib-devel libffi-devel bzip2-devel bzip2-libs readline readline-devel readline-static openssl openssl-devel openssl-static sqlite-devel # 3.安装指定的python版本 $ pyenv install 3.7.1 Downloading Python-3.7.1.tar.xz... -> https://www.python.org/ftp/python/3.7.1/Python-3.7.1.tar.xz Installing Python-3.7.1... Installed Python-3.7.1 to /root/.pyenv/versions/3.7.1 # 4.切换当前目录python目录为3.7.1 //未切换前 $ python Python 2.7.5 (default， Aug 7 2019， 00:51:29) [GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] on linux2 Type \"help\"， \"copyright\"， \"credits\" or \"license\" for more information. >>> //切换后 $ pyenv local 3.7.1 $ python Python 3.7.1 (default， Feb 18 2020， 13:20:03) [GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] on linux Type \"help\"， \"copyright\"， \"credits\" or \"license\" for more information. # 5.切换后刷新shims pyenv rehash # 6.切换回系统版本 pyenv global system pyenv遇到的问题 1.pyenv global system切换失败，正常应该是回切换到系统默认的python2.7.5，但是切换失败，原因未知 2.pyenv切换到安装的python版本后，会导致原先编译安装的python环境变量失效，例如，原先编译安装了python3.6，然后通过pyenv安装了python3.7，切换后会导致python3.6的环境变量失效，原因未知！！！ 二、virtualenv 2.1 virtualenv简介 python的第三方包很多，在一个python环境下开发时间越久，安装依赖越多，就越容易出现依赖包冲突问题，为了解决这个问题，virtualenv被开发出来，它可以搭建虚拟且独立的python环境，这样就可以使每个项目环境与其他项目独立开来，保持环境的干净，避免包冲突问题，另外，在开发python应用程序的时候，所有第三方的包都会被pip安装到系统python版本的site-packages目录下，如果要开发多个应用程序，那么这些程序会共用一个python，这意味着所有的包都安装在系统的python目录下，这不仅影响正常开发工作，还有可能因为随意变更系统python版本信息而造成系统不稳定，这种情况下，每个应用可能需要各自拥有一套独立的python运行环境，virtualenv就是用来为一个应用创建一套隔离的python运行环境的，virtualenv是底层基于python开发的python环境隔离工具，其通过虚拟目录的方式来实现多环境的并存 2.2 virtualenv原理 在系统中创建工作目录，该目录类似安装系统的python目录，保留完整的python环境，解释器，标准库和第三方库等，当需要时，切换环境变量激活即可使用 2.3 安装virtualenv # 1.安装python-pip和python-devel程序包 yum -y install python-pip python-devel # 2.安装virtualenv pip install virtualenv 2.4 通过virtualenv管理多python版本 virtualenv不是通过多版本管理的方式来实现系统同时兼容多python环境的，而是通过其在工作目录中虚拟完整的python环境来实现python多环境并存 2.4.1 virtualenv命令用法说明 virtualenv命令语法 virtualenv [options] dest_dir 选项 说明 --version 显示当前版本号 -h，--help 显示帮助信息 -v，--verbose 显示详细信息 -q，--quiet 不显示详细信息 -p PYTHON_EXE，--python=PYTHON_EXE 指定所用的python解析器的版本，比如 --python=python2.5就是使用python2.5版本的解析器创建新的隔离环境，默认使用的是当前系统安装的python解析器(/usr/bin/python) --clear 清空非root用户的安装，并从头开始创建隔离环境 --no-site-packages 令隔离环境不能访问系统全局的site-packages目录 --system-site-packages 令隔离环境可以访问系统全局的site-packages目录 --unzip-setuptools 安装时解压setuptools或distribute --relocatable 重定位某个已存在的隔离环境，使用该选项将修正脚本，并令所有 .pth文件使用相应路径 --distribute 使用distribute代替setuptools，也可以设置环境变量 VIRTUALENV_DISTRIBUTE达到同样效果 --extra-search-dir=SEARCH_DIRS 用于查找setuptools/distribute/pip发布包的目录，可以添加任意数量的 -extra-search-dir路径 --never-dowload 禁止从网上下载任何数据，此时，如果在本地搜索发布包失败，virtualenv就会报错 --prompt==PROMPT 定义隔离环境的命令行前缀 2.4.2 virtualenv创建python虚拟环境示例 ⚠️这里提前已经安装好python3.6 # 1.创建python虚拟工作目录，这里指定使用python3 virtualenv -p /usr/local/python36/bin/python3 /opt/venv1 # 2.加载虚拟环境 $ source /opt/venv1/bin/activate (venv1) [root@test1 ~]# python Python 3.6.9 (default， Feb 18 2020， 19:47:30) [GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] on linux Type \"help\"， \"copyright\"， \"credits\" or \"license\" for more information. >>> # 3.查看当前python环境安装的包 (venv1) [root@test1 ~]# pip3 list Package Version ---------- ------- pip 20.0.2 setuptools 45.2.0 wheel 0.34.2 # 4.退出虚拟环境 (venv1) [root@test ~]# deactivate [root@test2 ~]# 三、virtualenvwrapper 3.1 virtualenvwrapper简介 鉴于virtualenv不便于对虚拟环境集中管理，所以推荐直接使用virtualenvwrapper，virtualenvwrapper提供了一系列命令使得和虚拟环境工作变得便利，它把你所有的虚拟环境都放在一个地方 virtualenvwrapper整体工作过程 1.安装virtualenvwrapper 2.在 ~/.bashrc 文件中指定virtualenvwrapper存放虚拟环境总目录，指定virtualenvwrapper.sh存放位置(find查找) 3. mkvirtualenv 命令创建python虚拟环境，-p 选项指定不同python版本命令路径 4. workon 命令切换不同python虚拟环境 3.2 virtualenvwrapper配置过程 3.2.1 安装virtualenvwrapper(确保virtualenv已安装) pip install virtualenvwrapper 3.2.2 编辑环境变量 virtualenvwrapper.sh 需要find查找一下 find / -name \"virtualenvwrapper.sh\" 一般在 /usr/bin 下或者 /usr/local/bin # 1.向~/.bashrc中写入以下内容 echo \"export WORKON_HOME=/opt/virtualenvwrapper\" >> ~/.bashrc echo \"source /usr/bin/virtualenvwrapper.sh\" >> ~/.bashrc # 2.使配置生效 source ~/.bashrc # 参数说明 # virtualenvwrapper存放虚拟环境目录，这里自定义在/opt/virtualenvwrapper export WORKON_HOME=/opt/virtualenvwrapper # virtrualenvwrapper会安装到python的bin目录下，所以该路径是python安装目录下bin/virtualenvwrapper.sh，本文python安装在了/usr/local/ source /usr/bin/virtualenvwrapper.sh 3.2.3 创建虚拟环境 mkvirtualenv # 1.因为在3.2.2中指定了WORKON_HOME=/opt/virtualenvwrapper，所以创建的python虚拟环境都会在这个目录下 $ mkvirtualenv -p /usr/local/python36/bin/python3 venv1 created virtual environment in 200ms CPython3Posix(dest=/opt/virtualenvwrapper/venv1， clear=False， global=False) with seeder FromAppData pip=latest setuptools=latest wheel=latest app_data_dir=/root/.local/share/virtualenv/seed-v1 via=copy virtualenvwrapper.user_scripts creating /opt/virtualenvwrapper/venv1/bin/predeactivate virtualenvwrapper.user_scripts creating /opt/virtualenvwrapper/venv1/bin/postdeactivate virtualenvwrapper.user_scripts creating /opt/virtualenvwrapper/venv1/bin/preactivate virtualenvwrapper.user_scripts creating /opt/virtualenvwrapper/venv1/bin/postactivate virtualenvwrapper.user_scripts creating /opt/virtualenvwrapper/venv1/bin/get_env_details (venv1) [root@test1 ~]# # 2.查看创建的虚拟环境，venv1就是刚刚创建的虚拟环境 $ ls /opt/virtualenvwrapper/ get_env_details postmkproject predeactivate venv1 initialize postmkvirtualenv premkproject postactivate postrmvirtualenv premkvirtualenv postdeactivate preactivate prermvirtualenv 3.2.4 切换虚拟环境 # 1.先创建两个虚拟环境，并指定python版本 [root@test1 ~]# mkvirtualenv -p /usr/bin/python py2 [root@test1 ~]# mkvirtualenv -p /usr/local/python36/bin/python3 py3 # 2.切换虚拟环境 [root@test1 ~]# workon py2 (py2) [root@test1 ~]# python -V Python 2.7.5 [root@test1 ~]# workon py3 (py3) [root@test1 ~]# python -V Python 3.6.9 3.2.5 其他操作 # 1.查看当前的虚拟环境目录，即通过mkvirtualenv命令创建的venv虚拟环境 [root@test1 ~]# workon py2 py3 # 2.退出虚拟环境 (py3) [root@test1 ~]# deactivate [root@test1 ~]# # 3.删除虚拟环境 [root@test1 ~]# rmvirtualenv py2 Removing py2... 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"python/python安装/centos7.6编译安装python3.6.9.html":{"url":"python/python安装/centos7.6编译安装python3.6.9.html","title":"centos7.6编译安装python3.6","keywords":"","body":"[toc] centos7.6编译安装python3.6.9 python各版本下载地址 1.安装依赖包 yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel libffi-devel gcc gcc-c++ make 2.下载python包 wget https://www.python.org/ftp/python/3.6.9/Python-3.6.9.tar.xz 3.解压缩包并编译安装 tar xf Python-3.6.9.tar.xz && cd Python-3.6.9 ./configure --prefix=/usr/local/python36 --with-ssl make && make install 4.导出python命令环境变量 echo 'PATH=/usr/local/python36/bin:$PATH' >/etc/profile.d/python36.sh && source /etc/profile ⚠️如果把python环境变量写在/etc/profile.d/*.sh，在使用pip3安装的时候可能会报错Caused by SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\",），但是导入到/etc/profile中就没有问题，不知道怎么解决，并且绝对不是网上说的openssl问题 5.配置pip国内源 mkdir ~/.pip cat >~/.pip/pip.conf pip更新 pip install --upgrade pip 或者 python -m pip install --upgrade pip 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"web/html/1.html5基础知识.html":{"url":"web/html/1.html5基础知识.html","title":"html5基础知识","keywords":"","body":"[toc] 前端基础 1.基础结构 泡泡吐肥皂o 一级标题 普通文字 1.1 标签写法分类 全封闭标签 xxx 标签属性 xxx xx:属性名 ss:属性值 自封闭标签 1.2 标签分类 块标签(行外标签):独占一行显示 - **h1-h6(标题标签)** - **p(段落标签)** - **br(换行标签)** - **hr(横线标签)** - **div(普通文本标签)** - **ul(无序列表)** - **ol(有序列表)** - **li(列表中用到的标签，列表中的行)** 内联标签(行内标签):不独占一行显示，内联标签只能包含内联标签，不能包含块级标签 img(图片标签) a(超链接标签) span(普通文本标签) 2.head标签 2.1 meta标签 2.2.1 meta 文档字符编码 我的网页 一级标题 2.2.2 meta 页面刷新 世上最牛逼的页面标题 meta页面刷新测试 运行以上代码，浏览器中就会每隔2秒刷新一次 2.2.3 meta 关键字 meta标签可以设置关键字，用于搜索引擎收录和关键字搜索。 世上最牛逼的页面标题 给搜索引擎交钱就会优先显示上述关键字 例如，在百度搜索的html页面中有上述代码中content中的关键字，搜索框中输入关键字就会有返回结果，给搜索引擎交的钱越多，优先级越大，排在前面的概率也就越大 2.2.4 meta 网站描述 meta标签可以设置网站描述信息，用于在搜索引擎搜索时，显示网站基本描述信息。 大保健会所 搜索引擎搜索到 2.2.5 meta 触屏缩放 meta标签可以设置页面是否支持触屏缩放功能，其中各元素的含义如下： width=device-width ，表示宽度按照设备屏幕的宽度。 initial-scale=1.0，初始显示缩放比例。 minimum-scale=0.5，最小缩放比例。 maximum-scale=1.0，最大缩放比例。 user-scalable=yes，是否支持可缩放比例（触屏缩放） 标题 --> 哈哈哈 2.2 link标签 隔壁老王 隔壁王老汉的幸福生活 隔壁老王 隔壁王老汉的幸福生活 3.body标签 3.1 h1-h6 标题标签 一级标题 二级标题 三级标题 四级标题 五级标题 六级标题 隔壁老王 一级标题 二级标题 三级标题 四级标题 五级标题 六级标题 3.2 br 换行标签 隔壁老王 一级标题 3.3 hr 一行横线标签 隔壁老王 一级标题 一级标题 3.3 a 超链接标签 3.3.1 不加href属性，就是普通文本显示 python从入门到放弃 隔壁老王 一级标题 python从入门到放弃 3.3.2 集上href属性，不加值，文字有颜色效果，还有下划线，并且点击后会刷新当前的html页面 python从入门到放弃 隔壁老王 一级标题 python从入门到放弃 3.3.3 加上href属性，并且加上值，点击后会跳转至对应的网址 python短片 隔壁老王 一级标题 python从入门到放弃 3.3.4 锚点 在页面内容进行跳转 描述:标签设置id属性=值(xx) a标签href属性的值写法:href='#xx',点击这个a标签就能跳转到id属性为xx的那个标签所在位置. Title 这是顶部 第一章 开局(链接) 第二章 捡了一只狗(链接) 第三章 给狗洗澡(链接) 第四章 二手洗澡液有毒(链接) 第五章 大结局(链接) 第一章 开局 没干啥好事儿!! 没干啥好事儿!! 没干啥好事儿!! 第二章 捡了一只狗 草丛有动静!! 走过去看看!! 卧槽，是一条狗!! 把狗捡走!! 第三章 给狗洗澡 狗有点脏!!! 需要给狗洗澡!!! 买了一瓶二手洗澡液!!! 开始给狗洗澡!!! 洗完了!!! 第四章 二手洗澡液有毒 给狗洗完澡了!!! 但是身体有点不适!!! 逐渐开始麻木!!! 糟糕，洗澡液有毒!!! 情况不乐观!!! 卧槽，后悔买二手的了!!! 下次再也不买二手的了!!! 第五章 大结局 中毒身亡!!! 中毒身亡!!! 中毒身亡!!! 中毒身亡!!! 返回顶部 3.4 img 图片标签 参数说明 src属性:图片路径 必须写 alt属性:图片加载失败或者正在加载时提示的内容 title属性:鼠标悬浮时显示的内容 不常用,通过css来控制 width:设置宽度 height:设置高度 设置图片大小 Title 一级标题 img中width(设置宽度，单位:像素)，height(设置高度，单位:像素)设置图片大小 设置鼠标悬浮时图片显示的内容 Title 一级标题 3.5 div和span 普通文本标签 div和span默认是没有任何修饰的文本标签，也可以指定样式，但是一般在css中指定 div默认是普通文本 Title div默认是普通文本 div也可以指定文本颜色，一般写在css中 Title div也可以指定文本颜色，一般写在css中 span默认是普通文本 Title span默认是普通文本 span也可以指定文本颜色，一般写在css中 Title span也可以指定文本颜色，一般写在css中 3.6 列表标签 3.6.1 ul 无序标签 Title 篮球 足球 排球 3.6.2 ol 有序标签 Title 篮球 足球 排球 3.6.3 dl 描述列表标签 Title 河北省 邯郸 石家庄 山西省 太原 运城 3.7 table 表格标签 参数说明 id name hobby 1 小明 看电影 2 小洲 打篮球 没有边框的表格 Title id name hobby 1 小明 看电影 2 小洲 打篮球 3 小颖 逛街 加上边框的表格 Title id name hobby 1 小明 看电影 2 小洲 打篮球 3 小颖 逛街 表格合并(rowspan=\"2\") 枞行合并 Title id name hobby 1 小明 看电影 2 小洲 3 小颖 逛街 表格合并(colspan=\"2\") 横列合并 Title id name hobby 1 小明 看电影 2 小洲 打篮球 3 小颖 3.8 input 输入框标签 3.8.1 用户名、密码、登陆、重置、注册 Title 用户名： 密码： 3.8.2 时间日期输入框 Title 3.8.3 文件选择框 Title 3.8.4 纯数字输入框 Title 3.9 select 下拉框标签 3.9.1 单选 Title 北京 上海 广州 深圳 3.9.2 多选 Title 北京 上海 广州 深圳 杭州 青岛 哈尔滨 成都 3.10 textarea 多行文本输入框标签 Title 3.11 form 表单标签 在网站开发的过程中，用户可以使用用户交互相关的标签让用户输入内容，但如果想要再浏览器上把输入的内容提交到后台，则需要 表单 和 提交按钮 。 form标签内置属性 action=\"/xx/\" ，表示表单要提交的地址 method=\"get\"，表示表单的提交方式（get或post） enctype=\"multipart/form-data\"，如果form内部有文件上传，必须加上此设置 #action属性: 指定提交路径,提交到哪里去 #form表单标签会将嵌套在form标签里面的输入框的数据全部提交到指定路径 form内部【用户交互】相关标签必须设置name，不然提交数据后后端无法获取，例如下方的模拟登陆界面中用户名后的input标签中指定name属性 // 提交表单之后，实际上会将表单中的数据构造成一种特殊的结构，发送给后台，类似于： { user:用户输入的姓名, pwd:用户输入的密码, ... } 简单编辑一个模拟登陆的界面 Title 用户名： 密码： 运行上述代码效果如下 接下来编写一个简单的tcp服务端，服务端绑定本机8080端口 from socket import * tcp_server = socket() tcp_server.bind(('127.0.0.1',8080)) tcp_server.listen() while True: conn, client_addr = tcp_server.accept() from_client_msg = conn.recv(1024) print(from_client_msg.decode('utf-8')) conn.send(b'HTTP/1.1 200 ok\\r\\n\\r\\n') # conn.send(b'lai le laodi!') with open('send.html', 'rb') as f: data = f.read() conn.send(data) conn.close() tcp_server.close() tcp服务端用到的给客户端返回的内容send.html文件内容如下 Title 恭喜登陆成功！！！ 运行模拟登陆界面的html文件，随便输入一个用户名和密码，然后点击登陆 因为模拟登陆界面的html代码中指定了访问本地8080端口，因此tcp服务端会返回指定的send.html文件中的内容 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"web/html/html模版文件.html":{"url":"web/html/html模版文件.html","title":"html模版文件","keywords":"","body":"html模版文件1 参数 说明 src 指定要挂载的内容 path 本地挂载点 fstype 挂载类型 opts 挂载权限 state present 开机挂载，仅将挂载配置写入/etc/fstab mounted 挂载设备，并将配置写入/etc/fstab unmounted 卸载设备，不会清除/etc/fstab写入的配置 absent 卸载设备，会清理/etc/fstab写入的配置 table{ border-collapse:collapse } tr,td{ border:1px solid #333; } 效果 参数 说明 src 指定要挂载的内容 path 本地挂载点 fstype 挂载类型 opts 挂载权限 state present 开机挂载，仅将挂载配置写入/etc/fstab mounted 挂载设备，并将配置写入/etc/fstab unmounted 卸载设备，不会清除/etc/fstab写入的配置 absent 卸载设备，会清理/etc/fstab写入的配置 table{ border-collapse:collapse } tr,td{ border:1px solid #333; } html模版文件2 参数 说明 path 指定远程主机目录或文件信息 recurse 递归授权 state directory touch link absent mode owner group 效果 参数 说明 path 指定远程主机目录或文件信息 recurse 递归授权 state directory touch link absent mode owner group 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"web/css/2.css基础知识.html":{"url":"web/css/2.css基础知识.html","title":"css3基础知识","keywords":"","body":"[toc] css基本知识 1.css样式引入方式 第一种方式 head标签中引入 在body中写的标签在head标签下的style标签中引入样式 Title div{ width: 200px; height: 200px; background-color: greenyellow; } 我是一个div标签 第二种方式 外部css文件引入(常用) /*css代码*/ /*创建一个css文件，在html文件中使用link标签引入*/ div{ /*css注释写法*/ width: 200px; height: 200px; background-color: greenyellow; } Title 我是一个div标签 第三种方式 内联样式，标签中写样式 Title 我是一个div标签 2.css选择器 2.1基本选择器 2.1.1元素选择器 元素选择器写法 标签名称{ css属性:值 } div{ width:100px; } html代码示例 Title div{ width: 200px; height: 200px; background-color: greenyellow; } 我是一个div标签 2.1.2id选择器 Title #d1{ width: 200px; height: 200px; background-color: greenyellow; } 我是一个div标签 2.1.3类选择器 Title .c1{ width: 200px; height: 200px; background-color: greenyellow; } 我是一个div标签 2.2属性选择器 根据自定义属性查找 Title [xx]{ width: 200px; height: 200px; background-color: greenyellow; } 我是第一个div标签 我是第二个div标签 根据自定义属性值查找 Title [xx='x1']{ width: 200px; height: 200px; background-color: greenyellow; } 我是第一个div标签 我是第二个div标签 2.3后代选择器 以下代码中，只有在div标签下的a标签才能生效(有颜色) Title /*只有在div标签下的a标签(多级标签都可以)才能生效*/ div a{ color:orange; /* 字体颜色 */ } 我是div标签的孙子 我是div标签的儿子 我是一个div标签 百度 2.4组合选择器(逗号连接) 必须找到a标签才可以生效 Title #d1 a,#d2 a{ color:orange; } 我是div1 我是div2 我是div3 百度 3.css样式相关 3.1高度、宽度 div1 span1 div{ height: 200px; width: 100px; background-color: pink; } span{ height: 200px; width: 100px; background-color: green; } Title div{ height: 200px; width: 100px; background-color: pink; } span{ height: 200px; width: 100px; background-color: green; } div1 span1 3.2字体相关 Title div{ font-size: 20px; /* 默认字体大小是16px */ color: olivedrab; font-family: '微软雅黑'; /* 字体格式 */ font-weight: 500; /* 字体粗细，100-900，默认是400 */ } 写作日当午， 谁知学生苦。 几本小破书， 一坐一上午。 3.3字体对齐 Title div{ color: #2a8282; /*!*右对齐*!*/ /*text-align: right;*/ /* 水平居中*/ /*text-align: center; */ height: 100px; width: 200px; text-align: left; /*和height高度相同，标签文本垂直居中*/ line-height: 50px; } 只身赴宴鸡毛装！ 都是同学装鸡毛！ 3.4颜色设置 三种方式: 英文单词:red; 十六进制: #ff746d; rgb: rgb(155, 255, 236); ​ 带透明度的: rgba(255, 0, 0,0.3); 单纯的就是颜色透明度 ​ 标签透明度:opacity: 0.3; 0到1的数字,这是整个标签的透明度 rgba是带透明度的，第4个数越小，透明度越高 Title div{ color: rgba(255,0,0,0.3); } 只身赴宴鸡毛装！ 都是同学装鸡毛！ 透明度为0.3 opacity，整个标签的透明度 Title div{ /*单纯的颜色透明度*/ /*color: rgba(255,0,0,0.3);*/ /*整个标签的透明度 */ opacity: 0.3; } 只身赴宴鸡毛装！ 都是同学装鸡毛！ 3.5背景 3.5.1背景图片 Title .c1{ height: 1000px; width: 800px; background-image: url(\"cjk.jpeg\"); } 背景图片，默认会平铺，即设置了样式的高度和宽度，如果图片比设置的小，会继续显示图片的一部分 加参数background-repeat: no-repeat;设置背景图片不平铺 3.5.2背景图片的位置 先截取一个200x200像素的图片，然后背景颜色为600x600像素，效果如下，图片默认的位置是left top center center示例 Title .c1{ height: 600px; width: 600px; background-color: pink; background-image: url(\"200.png\"); background-repeat: no-repeat; /*背景图片居中显示*/ background-position: center center; /* 简写方式 */ background: #ff0000 url(\"200.png\") no-repeat center center; } 3.5.3背景颜色 Title .c1{ height: 200px; width: 200px; background-color: pink; } 200x200像素的背景颜色 3.6边框 border-width 边框宽度 border-style 边框样式(dashed是虚线，solid是实线) border-color 边框颜色 3.6.1对4个边框设置 Title div{ width: 200px; height: 100px; /*边框简写方式,边框粗为1px，solid是实线*/ border: 1px solid red; } 都是同学装鸡毛！ 3.6.2单独对一个边框设置 border-left 左边框 border-right 右边框 border-top 上边框 border-bottom 下边框 Title div{ width: 200px; height: 100px; /*边框简写方式,边框粗为1px，solid是实线*/ border-right: 1px solid red; } 都是同学装鸡毛！ solid 实线边框 dashed 虚线边框 3.7盒子模型 盒子模型的各个值 margin: 外边距 距离其他标签或者自己父级标签的距离 padding: 内边距 内容和边框之间的距离 border: 边框 content: 内容部分 设置的width和height 3.7.1内边距 padding Title #d1{ width: 200px; height: 100px; /*边框4像素，实线，红色*/ border: 4px solid red; /*上下6px 左右8px*/ /*padding: 6px 8px;*/ /*上4右2下6左8*/ /*padding: 4px 2px 6px 8px*/ /*左上右下*/ /*padding-left: 200px;*/ /*padding-top: 20px;*/ /*padding-right: 20px;*/ /*padding-bottom: 20px;*/ } 我是一个div标签 原先效果 padding: 4px 2px 6px 80px效果 3.7.2外边距 margin Title .c1{ background-color: red; height: 100px; width: 100px; /*margin-left: -1000px;*/ /*margin: 10px 15px;*/ } .c2{ background-color: green; height: 20px; width: 20px; /*margin: 10px 15px;*/ margin-left: 20px; } 都是同学装鸡毛! 3.8display属性 display的几个值: inline: 将块级标签变成了内联标签 block:将内联标签变成块级标签 inline-block: 同时具备内联标签和块级标签的属性,也就是不独占一行,但是可以设置高度宽度 none: 设置标签隐藏 Title span{ /*display: block;*/ /*display: none;*/ } .c1{ background-color: red; height: 100px; width: 100px; /*display: inline;*/ display: inline-block; /*display: none;*/ } .c2{ background-color: green; height: 100px; width: 100px; } 我是span标签 鹅鹅鹅,曲项向天歌! 拔毛烧开水,铁锅炖大鹅! display:none隐藏标签 3.9浮动 浮动的元素,不独占一行,并且可以设置高度宽度 Title body{ margin: 0; } .c1{ background-color: red; height: 100px; width: 200px; float: left; } .c2{ background-color: brown; height: 100px; width: 200px; float: right; } .c3{ background-color: pink; height: 100px; width: 100%; } 吟诗作对--> 左右浮动效果 粉色标签设置了宽度100%，但是设置左右浮动后会覆盖这个粉色标签，这种现象就是父级标签塌陷(cc是父级标签，左右浮动的c1和c2是儿子标签)，因为左右两边的标签设置了浮动，已经脱离了文档流，所以粉色的标签(和cc同级的c3)会向上顶，正确情况应该为粉色单独占一行 解决父级标签塌陷问题: 方法一 给父级标签加高度，本示例中是给cc加高度 方法二 清除浮动:clear属性，本示例中给c3添加清除浮动，clear:both的意思是本标签的上边不允许有浮动元素 .c3{ background-color: pink; height: 100px; width: 100%; clear: both; } 方式三(常用)伪元素选择器解决 css样式: .clearfix:after{ content: ''; #这里设置个空值，因为是伪元素，所以无法选中，并且占一行，这样就解决了父级标签塌陷的问题 display: block; #将内联标签变成块级标签，占一行 clear: both; } html代码: 吟诗作对--> 解决后的效果 3.10伪元素选择器 Title div{ background-color: pink; height: 100px; width: 200px; } div:after{ content: '我是无法选中的0.0'; color:white; } 这是c2 都是同学装鸡毛! div:after表示找到div标签后的内容且内容无法选中 3.11伪类选择器 3.11.1hover、pointer hover表示当鼠标悬浮时的效果 pointer表示鼠标悬浮时显示小手 Title .c1{ background-color: red; height: 200px; width: 200px; } .c1:hover{ /*background-color: green;*/ background-image: url(\"200.png\"); cursor: pointer; } 3.11.2其他伪类选择器 /* a标签未访问的时候设置效果 */ a:link{ color:yellow; } /* 鼠标悬浮上去时设置效果 */ a:hover{ color:black; } /* 鼠标左键点击下去的还没有抬起来的时候,设置效果 */ a:active{ color:green; } /* 鼠标抬起后,访问过之后设置效果 */ a:visited{ color:purple; } 完整代码 我不适合学前端 a:link { color: #000000; } /* 未访问链接*/ a:visited { color: #00FF00; } /* 已访问链接 */ a:hover { color: #FF00FF; } /* 鼠标移动到链接上 */ a:active { color: #0000FF; } /* 鼠标点击时 */ 这是一个链接 注意： a:hover 必须在 a:link 和 a:visited 之后，需要严格按顺序才能看到效果。 注意： a:active 必须在 a:hover 之后。 3.12文字装饰 Title a{ text-decoration: none; } 来点我！！！ 没有去除超链接下划线前的效果 去除超链接下划线后的效果 3.13定位postion static: 静态定位,也就是标签默认 relative: 相对定位,按照自己原来的位置进行移动 absolute: 绝对定位,按照父级标签或者祖先辈儿标签设置了相对定位的标签位置进行移动,如果没有找到相对定位标签,会找到整个文档的位置进行移动 fixed: 固定定位, 按照浏览器窗口的位置进行移动 固定定位示例 Title body{ margin: 0; } .c1{ background-color: red; height: 1000px; width: 800px; } .c2{ background-color: green; height: 1000px; width: 800px; } .c3{ background-color: blue; height: 1000px; width: 800px; } .s1{ position: fixed; left: 1000px; bottom: 20px; height: 40px; width: 60px; background-color: aqua; line-height: 40px; text-align: center; } .s1 a{ color:white; text-decoration: none; font-size: 12px; } 这是顶部 返回顶部 定位示例 3.14选择器优先级 /* css属性有继承的概念 权重0*/ /* 标签(元素)选择器 权重1*/ /* 类选择器 权重10*/ /* id选择器 权重100*/ /* 内联样式 权重1000*/ /* color:green!important; 无敌! */ /* 如果优先级相同,按照后面的为准 */ 别忘了,class属性的值可以写多个 简单示例 Title .c1{ height: 100px; width: 100px; background-color: red; } #d1{ height: 100px; width: 100px; background-color: greenyellow; } 我是一个div标签 id选择器优先级大于类选择器优先级，所以显示的颜色是d1中的绿黄色 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"web/js/3.js基础.html":{"url":"web/js/3.js基础.html","title":"javaScript基础知识","keywords":"","body":"[toc] js基础 1.js简介 -- javascript ECMAscript5 ECMAscript6 -- vue.js react .. 由三个部分组成 1 ECMAscript5的核心 js语言 2 BOM 浏览器对象模型 js操作浏览器,做出对应的一些效果 3 DOM 文档对象模型 -- HTML文件 2.js代码引入方式 三种方式 1.head标签的script标签里面(alert('xx'), confirm('xx')) 2.body标签的script标签里面 3.外部文件引入的方式来使用 3.1创建一个.js结尾的文件,写上咱们的js代码 比如:alert('are you ok?'); 3.2在想使用这个js代码的html文件中,head标签或者body标签下面或者上面写下面的内容 第一种方式 head标签中script标签中写js代码 Title alert('are you ok?') 效果图 第二种方式 body标签中的script标签中写js代码 Title alert('are you ok?') 第三种方式 外部文件引入方式 第一步、先创建一个.js结尾的文件，内容如下 alert('are you ok'); 第二步、html文件中引入这个js文件 Title alert('are you ok?') 3.js变量 定义方式一(在函数中是局部变量，在函数外是全局变量) var 变量名 = 值; 定义方式二(都是全局变量) 变量名 = 值; 4.js注释 单行注释 //单行注释内容 // 改变标题： document.getElementById(\"myH\").innerHTML = \"我的第一张页面\"; // 改变段落： document.getElementById(\"myP\").innerHTML = \"我的第一个段落。\"; 多行注释 / 多行注释内容/ /* 下面的代码会改变 网页中 id = \"myH\" 的标题 以及 id = \"myP\" 的段落： */ document.getElementById(\"myH\").innerHTML = \"我的第一张页面\"; document.getElementById(\"myP\").innerHTML = \"我的第一个段落。\"; 5.js数据类型 5.1number类型(整数,浮点数) var n = 11; var n2 = 11.11; 查看数据类型 typeof 变量名; typeof n; -- number类型 变量声明,但没有赋值的时候,变量的值为undefined 5.2string类型(字符串) 示例: var a = 'abcdef'; typeof a; --> \"string\" var a = new String('s'); typeof a; --> \"object\" 字符串的操作方式 定义一个字符串 var s = '好好学习，天天向上'; 索引取值 s[1] --> '好' 移除空格,不能移除字符串中的空格 s.trim(); #去除两端的空格 s.trimLeft(); #去除左边的空格 s.trimRight(); #去除右边的空格 根据索引获取字符 //语法 var value = name.charAt(index) //示例 var s = 'hello'; s.charAt(4); --> 'o' 根据索引获取子序列(切片) //语法 var values = name.substring(开始位置，结束位置) //示例 var s = 'hello'; s.substring(1,3); --> \"el\" 5.3布尔类型(boolean类型) var a = true; var b = false; typeof a; \"boolean\" js的基础数据类型都有布尔值属性, []--false 0,{},'',undefined,null,NaN 字符串转数字: var a = '11'; parseInt(a); var a = '23abc'; parseInt(a); 23 var a = 'asdfabc'; parseInt(a); -- NAN -- not a number typeof NaN; -- \"number\" NaN === NaN; -- false NaN == NaN; -- false 5.4undefined和null类型 undefined 变量声明了,但是没有赋值,此时这个变量是undefined类型 //示例 var b; b undefined typeof b; \"undefined\" null : 变量不用了,就可以给变量赋值为null,--- object类型 //示例 var b = null; undefined b null typeof b; \"object\" 5.5数组(array) var name = [1,2,3]; //括号中的3表示数组有3个元素 names; (3) [1, 2, 3] //⚠️，数组类型也是字符串 typeof names; \"string\" 5.5.1索引取值，从0开始 //数组a中的元素 names = [1,2,3]; (3) [1, 2, 3] //索引取值，从0开始 names[0]; 1 5.5.2尾部追加元素 //数组a中的元素 names = [1,2,3]; (3) [1, 2, 3] //追加数字 names.push(5); 4 names; (4) [1, 2, 3, 5] //追加字符串 names.push('啊'); 5 names; (5) [1, 2, 3, 5, \"啊\"] 5.5.3尾部移除元素 //数组names中的元素 names; (5) [1, 2, 3, 5, \"啊\"] //移除尾部元素 names.pop(); \"啊\" names; (4) [1, 2, 3, 5] 5.5.4头部插入元素 //数组names中的元素 names; (4) [1, 2, 3, 5] //头部插入元素 names.unshift(9); 5 names; (5) [9, 1, 2, 3, 5] 5.5.5头部移除元素 //数组names中的元素 names; (5) [9, 1, 2, 3, 5] //头部移除元素 names.shift(); 9 names; (4) [1, 2, 3, 5] 5.5.6在指定索引位置插入元素 //语法 names.splice(从哪删(索引),删几个(个数),删除位置替换的新元素(可不写,可写多个)) //数组names中的元素 names = [1,2,3,5,6]; (5) [1, 2, 3, 5, 6] names; (5) [1, 2, 3, 5, 6] //在指定索引位置插入元素，这里其实与删除差不多，只不过是第二个参数删除个数为0，不删除元素，插入元素 names.splice(1,0,'呵呵'); [] names; (6) [1, \"呵呵\", 2, 3, 5, 6] 5.5.7在指定索引位置替换元素 //语法 语法和在指定位置插入元素相同，只不过在指定位置插入元素删除的元素个数为0，而在指定索引位置替换元素是删除元素并替换 names.splice(从哪删(索引),删几个(个数),删除位置替换的新元素(可不写,可写多个)) //数组names中的元素 names; (5) [1, 2, 3, 5, 6] //在指定索引位置替换元素,从索引1开始，删除1个元素，并替换为呵呵 names.splice(1,1,'呵呵'); [2] names; (5) [1, \"呵呵\", 3, 5, 6] 5.5.8在指定索引位置删除元素 //语法 names.splice(从哪删(索引),删几个(个数),删除位置替换的新元素(可不写,可写多个)) //数组names中的元素 names; (5) [1, 2, 3, 5, 6] //示例1，只删除指定索引处的元素，不替换 names.splice(1,3); (3) [2, 3, 5] names; (2) [1, 6] //示例2，删除指定索引处的元素，并替换 names.splice(1,3,'呵呵'); (3) [2, 3, 5] names; (3) [1, \"呵呵\", 6] //示例3，删除指定索引处的元素，并替换多个，看效果 names.splice(1,3,'呵呵','哈哈',666); (3) [2, 3, 5] names; (5) [1, \"呵呵\", \"哈哈\", 666, 6] 5.5.9切片 //语法 names.slice(开始位置,结束位置) //数组names中的元素 names; (5) [1, 2, 3, 5, 6] //从索引1开始到索引3结束，顾头不顾尾 names.slice(1,3); (2) [2, 3] 5.5.10数组反转(会直接修改原数组) //数组names中的元素 names; (5) [1, 2, 3, 5, 6] //原数组反转 names.reverse(); (5) [6, 5, 3, 2, 1] 5.5.11数组元素拼接 //数组names中的元素 names; (5) [1, 2, 3, 5, 6] //将数组元素连接起来以构建一个字符串 names.join('滚'); \"1滚2滚3滚5滚6\" 5.5.12连接数组 //数组a中的元素 a = [1,2,3]; (3) [1, 2, 3] //数组b中的元素 b = ['a','b','c']; (3) [\"a\", \"b\", \"c\"] //连接数组a和b a.concat(b); (6) [1, 2, 3, \"a\", \"b\", \"c\"] 5.5.13对原数组进行排序 //数组a中的元素 a = [11,32,2,66,7]; (5) [11, 32, 2, 66, 7] //对原数组进行排序，发现结果并不是正确的，排序并没有按照数字大小进行排序，而是按照数字的第一个数字进行大小排序的 a.sort(); (5) [11, 2, 32, 66, 7] 因此，为了解决上述排序问题，需要自己定义规则 //定义一个函数，使用冒泡排序法进行排序才能得到正确结果 function compare(a,b){ return a - b; /* 当a-b大于0时，两个数互换位置*/ } a.sort(compare); /* a-b是升序排序 */ (5) [2, 7, 11, 32, 66] //倒叙排序 function compare(a,b){ return b - a; } a.sort(compare); (5) [66, 32, 11, 7, 2] 5.6自定义对象(相当于python的中的dict) // 声明一个对象 info = { name:'小明', 'age:18 } info {name: '小明', age: 18} //查看数据类型 typeof info; \"object\" //常用方法 //获取值，通过键获取值必须加引号 info['name']; \"小明\" info.name; \"小明\" //修改值 info.age = 22; 22 info; {name: \"小明\", age: 22} //新增值 info['gender'] = 'man'; \"man\" info; {name: \"小明\", age: 22, gender: \"man\"} //删除值 delete info['age']; true info; {name: \"小明\"} 6.运算符 6.1判断运算符 > = 6.2算术运算符 + - * / % ++ -- ++ 自增 1 -- 自减 1 var a = 2; a++ 先执行逻辑 在+1 ++a 先+1 在执行逻辑 简单示例: if (++a === 4){ console.log('xxx'); } else{ console.log('ooo'); }; 结果： 000 7.流程控制语句 7.1判断语句 7.1.1 if语句 单条件 if (a == 1){ //判断条件写在小括号里面,大括号里面写条件判断成功后的代码内容 console.log('1111'); } else{ console.log('222'); }; 多条件 var a = 0; if(a > 1){ console.log('0'); //浏览器显示结果 //var h = document.getElementById('d1'); //h.innerText = '小明'; }else if(a 7.1.2switch语句 //num只能是数字,case判断的只能是数字 var num = 50; switch(num++){ case 10: console.log('未成年'); break; case 18: console.log('成年'); break; case 35: console.log('大叔'); break; case 40: console.log('中年了'); break; default: console.log('没有前途。。。'); }; 7.2循环语句 7.2.1for循环 //普通循环 for (var i=0;i 7.2.2while循环 var a = 0; while(a 8.函数 8.1普通函数 //定义一个普通函数 function f1(a,b){ return a+b; } 执行: f1(1,2) --> 3 //此写法不能返回多个值 function f1(a,b){ return a,b; }; f1(1,2); 不能返回多个值: --> 2 //返回多个值 function f1(a,b){ return [a,b]; }; f1(1,2); (2) [1, 2] 8.2匿名函数 ⚠️⚠️⚠️匿名函数必须赋值给一个变量，否则会报错 //定义一个匿名函数 var a = function(a,b,c){ return a+b+c; } a(1,2,3) 6 //匿名函数还可以当作自定义对象的值 var d = { 'a':'b','f':function(a,b){ return a+b; } }; d.f(1,2); 3 8.3自执行函数 (function () { alert('自执行函数!') })() 9.序列化 //序列化 JSON.stringify，相当于python中的json.dumps，序列化后会变成json格式，全部替换为双引号 var d = {'a':'aa','b':'bb'}; undefined var d_json = JSON.stringify(d); undefined d_json \"{\"a\":\"aa\",\"b\":\"bb\"}\" //反序列化 JSON.parse d_json \"{\"a\":\"aa\",\"b\":\"bb\"}\" var reverse_json = JSON.parse(d_json); reverse_json {a: \"aa\", b: \"bb\"} 10.BOM对象 浏览器对象模型 10.1弹框 alert('are you ok?'); confirm('are you sure?') alert弹框(只有确定) confirm弹框(有确定和取消) 10.2location对象 //获取当前页面的地址 location.href; \"https://www.baidu.com/\" //跳转到这个网址上 location.href = 'http://www.baidu.com'; //刷新当前页面 location.reload(); 10.3计时器 10.3.1 一段时间后执行某个任务 //写法一 1000是毫秒，1秒后打印呵呵 var t = setTimeout(\"console.log('呵呵')\",1000); //t就是浏览器用来记录你的计时器的标识数字 t 2810 //清除计时器 clearTimeout(t) //写法二 常配合匿名函数使用，5秒后弹框 var t = setTimeout(function(){confirm('你满18岁了吗?')},5000); 10.3.2 每隔一段时间执行某个任务 var t = setInterval(function(){confirm('弹个框!!')},3000); //清除计时器 clearInterval(t); 11.DOM对象 html文档 11.1直接查找选择器 html代码: css代码: .c1{ background-color: green; height: 100px; width: 100px; } .c2{ background-color: red; /*height: 100px;*/ /*width: 100px;*/ color:red; } //按标签名查找: var divEle = document.getElementsByTagName('div'); divEle HTMLCollection(2) [div#d1.c1, div#d2.c1.c2, d1: div#d1.c1, d2: div#d2.c1.c2] //按id值查找: var d1 = document.getElementById('d1'); 示例: d1.style.height = '600px'; 上述操作会将id为d1的标签高度变为600px //按类值查找:var a = document.getElementsByClassName('c1'); a HTMLCollection(2) [div#d1.c1, div#d2.c1.c2, d1: div#d1.c1, d2: div#d2.c1.c2] 11.2间接查找选择器 html文件内容 Title .c1{ border: 1px solid red; height: 100px; width: 100px; } 京东a 百度span div3 修改边框颜色 div1.style.borderColor=\"green\"; div1.nextElementSibling.style.color = 'blue'; 找下一个兄弟标签,并改成蓝色 var div1 = document.getElementsByClassName('c1')[0]; div1.nextElementSibling.style.color = 'blue'; 找下一个兄弟标签,并改了色 div1.previousElementSibling; 找上一个兄弟 div1.firstElementChild; 找第一个儿子 div1.lastElementChild; 找最后一个儿子 div1.children; 找所有儿子,是一个数组 div1.parentElement; 找到自己的父级标签 11.3文本操作 html文件内容 Title .c1{ border: 1px solid red; height: 100px; width: 100px; } 京东a 百度span div3 innerText获取文本: var a = document.getElementById('jd'); a.innerText; 只获取文本内容 innerText设置文本: a.innerText = '呵呵';不能识别标签,单纯的文本内容显示 innerHTML获取文本 var d = document.getElementsByClassName('c1')[0]; d.innerHTML; 获取的内容包含标签 innerHTML设置文本 var d = document.getElementsByClassName('c1')[0]; d.innerHTML = '呵呵'能够识别标签,生成标签效果 //innerText 获取文本: var a = document.getElementById('jd'); a.innerText; 只获取文本内容 设置文本: a.innerText = '呵呵';不能识别标签,单纯的文本内容显示 //innerHTML 获取文本 var d = document.getElementsByClassName('c1')[0]; d.innerHTML; 获取的内容包含标签 设置文本: d.innerHTML = '呵呵'; 能够识别标签,生成标签效果 11.4value值操作 html文件 Title .c1{ border: 1px solid red; height: 100px; width: 100px; } 京东a 百度span div3 找到标签 var inp = document.getElementById('username'); 获取值 inp.value 修改值 inp.value = '哈哈!'; 示例: var inp = document.getElementById('username'); 找到标签 inp.value; 获取值 inp.value = '200块!'; 修改值 使用示例 闪烁效果 html文件 Title .c1{ height: 100px; width: 100px; background-color: red; } .c2{ height: 100px; width: 100px; background-color: green; } js代码 var div1 = document.getElementById('d1'); 找到c2就删除，没有则动态添加，因此动态显示 var t = setInterval(\"div1.classList.toggle('c2')\",100); 用户登陆输入为空触发事件 Title 用户名: 密码: --> 注册 var btnEle = document.getElementById('btn'); btnEle.onclick = function () { var unameEle = document.getElementById('username'); var uname = unameEle.value; var pwdEle = document.getElementById('password'); var pwd = pwdEle.value; if (uname.trim().length === 0){ unameEle.nextElementSibling.innerHTML = '用户名不能为空!'; }else if(pwd.trim().length === 0){ unameEle.nextElementSibling.innerHTML = ''; pwdEle.nextElementSibling.innerHTML = '密码不能为空!'; }else { document.getElementById('ss').innerText = '登录成功!'; unameEle.nextElementSibling.innerHTML=''; pwdEle.nextElementSibling.innerHTML = ''; } } 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"mac使用记录/MAC本添加多个github账号.html":{"url":"mac使用记录/MAC本添加多个github账号.html","title":"mac添加多个github账号","keywords":"","body":"[toc] MAC本添加多个github账号 使用需求 通常情况下，我们会有两个 github 账号：一个是公司的，另一个是私人的。由于 github 是使用 SSH key 的 fingerprint (对应的公钥id_rsa_pub)来判定你是哪个账户，而不是通过用户名，如果是在多台电脑上使用一个账号，可以为该账号添加多个 SSH key，如果是一台电脑使用多个账号，则分别生成多个 SSH key 添加到对应的账户即可。所以本文要实现的是公号和私号在 git 上同时使用，两者互不干扰。 第一步、生成多个sshkey cd ~/.ssh切换到用户家目录，然后生成sshkey，执行以下命令，一路回车即可 ssh-keygen -t rsa -f ~/.ssh/id_rsa_one -C \"one@xxx.com\" ssh-keygen -t rsa -f ~/.ssh/id_rsa_two -C \"two@xxx.com\" 这样会在~/.ssh目录下生成四个文件： id_rsa.one //账号one的私钥 id_rsa.one.pub //账号one的公钥 id_rsa.two //账号two的私钥 id_rsa.two.pub //账号two的公钥 第二步、创建配置文件config 在 ~/.ssh目录下新建 config 文件，令不同 Host 实际映射到同一 HostName，但密钥文件不同，这里举例为one和two，可自行修改为自己使用的用户 # one (first account) Host one.github.com HostName github.com PreferredAuthentications publickey User one IdentityFile ~/.ssh/id_rsa_one # two(second account) Host two.github.com HostName github.com PreferredAuthentications publickey User two IdentityFile ~/.ssh/id_rsa_two 第三步、github添加sshkey及测试 分别登陆两个 github 账号，在 Settings —> SSH and GPG keys 中，点击 “new SSH key”，把 “id_rsa.one.pub” 和 \"id_rsa.two.pub\"这两个公钥的内容分别添加到相应的账号中。 为了确认我们可以通过 SSH 连接 github，可通过输入下面命令来验证 //执行以下命令测试 ssh -T git@one.github.com //返回结果如下说明添加成功 Hi one! You've successfully authenticated, but GitHub does not provide shell access. 第四步、配置git信息 因为一台电脑上配置了多个 github 账号，所以就不能再配置全局的用户名和邮箱了，而是在不同的仓库下，如果需要连接不同的 git 账号，配置相应的局部用户名和邮箱即可，如果之前配置过全局的用户名和邮箱，需要取消配置 //取消之前的全局配置 git config --global --unset user.name git config --global --unset user.email //设置局部用户名和邮箱 git config --local user.name \"xx\" git config --local user.email \"xx@xx.com\" 第五步、使用git git的使用一般是从其他仓库直接clone或本地新建仓库 方式一：clone仓库到本地 原先写法 git clone git@github.com:用户名/仓库.git 现在的写法 //这里距离了one用户的写法，two用户的操作一样 git clone git@one.github.com:one的用户名/仓库.git 如果需要重建origin //清空原有的 git remote rm origin //使用ssh方式添加远程仓库 git remote add origin git@one.github.com:one/仓库名.git ⚠️⚠️⚠️这里有个坑，添加远程仓库时，从github仓库的Clone or download下的Use SSH复制的路径需要修改 这里需要注意的是，使用ssh方式添加远程仓库原先写法是这样的 git@github.com:one/仓库名.git 现在需要修改为如下写法 git remote add origin git@one.github.com:one/仓库名.git 方式二、推送本地仓库 //初始化本地仓库 git init //创建一个文件并提交到本地仓库 touch test git add . git commit -m \"first commit\" //push到github上去 git remote add origin git@one.github.com:one/仓库名.git git push origin master 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"mac使用记录/mac使用gitbook记录.html":{"url":"mac使用记录/mac使用gitbook记录.html","title":"mac使用gitbook记录","keywords":"","body":"[toc] mac使用gitbook记录 1.gitbook搭建 1.1安装node.js 官网下载mac版本的node.js 1.2检测node.js是否安装成功 npm -v 6.12.1 1.3安装gitboot和命令行工具(-g 代表全局安装) //安装 sudo npm install -g gitbook sudo npm install -g gitbook-cli //查看版本 gitbook -V CLI version: 2.3.2 GitBook version: 3.2.3 //更新gitbook命令行工具 sudo npm update gitbook-cli -g //卸载gitbook命令 sudo npm uninstall gitbook-cli -g 2.gitbook的使用 2.1创建gitbook目录 //创建gitbook目录 sudo mkdir /gitbook //初始化gitbook cd /gitbook && sudo gitbook init //初始化完成后会生成两个文件 README.md #项目介绍文件 SUMMARY.md #gitbook目录结构 2.2配置gitbook生成书籍 1.编辑SUMMARY.md，写入以下内容(这里仅做示例) # Summary * [Linux](README.md) * [Linux基础](README.md) * [Linux命令](README.md) * [vim命令](README.md) * [vim命令](linux/linux命令/vim命令.md) ⚠️vim命令.md的路径是/gitbook/linux/linux命令 2.构建书籍(⚠️构建命令必须在SUMMARY.md同路径下) sudo gitbook build 3.启动gitbook sudo gitbook serve & gitbook默认监听4000端口 启动gitbook报错 //gitbook serve启动gitbook报错如下 Error: ENOENT: no such file or directory, stat '/gitbook/_book/gitbook/gitbook-plugin-livereload/plugin.js',Error: ENOENT: no such file or directory, stat '/gitbook/_book/gitbook/gitbook-plugin-livereload' //解决方法 找到copyPluginAssets.js文件，全部替换 将 confirm: true 改为 confirm: false /Users/baixuebing/.gitbook/versions/3.2.3/lib/output/website/copyPluginAssets.js 3.修改gitbook代码框字体大小 prismnode_modules/themes/themes/prism-base16-ateliersulphurpool.light.css 13、14行 font-size: 18px; line-height: 1.6; 4.设置gitbook开机自启 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"mac使用记录/brew安装.html":{"url":"mac使用记录/brew安装.html","title":"brew安装","keywords":"","body":"brew安装 brew官网 国外源 /bin/bash -c \"$(curl -fsSL https://raw/branch.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" 国内源 /bin/zsh -c \"$(curl -fsSL https://gitea.pptfz.cn/cunkai/HomebrewCN/raw/branch/master/Homebrew.sh)\" brew替换国内源 中科大homebrew配置说明文档 替换 brew.git cd \"$(brew --repo)\" git remote set-url origin https://mirrors.ustc.edu.cn/brew.git 替换 homebrew-core.git cd \"$(brew --repo)/Library/Taps/homebrew/homebrew-core\" git remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git 替换 homebrew-bottles echo 'export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles' >> ~/.bash_profile source ~/.bash_profile 应用生效 brew update 重置brew 重置 brew.git cd \"$(brew --repo)\" git remote set-url origin https://github.com/Homebrew/brew.git 重置 homebrew-core.git cd \"$(brew --repo)/Library/Taps/homebrew/homebrew-core\" git remote set-url origin https://github.com/Homebrew/homebrew-core.git 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"mac使用记录/mac配置item2使用rz sz.html":{"url":"mac使用记录/mac配置item2使用rz sz.html","title":"mac配置item2使用rz sz","keywords":"","body":"mac配置item2使用rz sz 1.安装lrzsz 已提前安装好brew brew install lrzsz 2.手动编辑 iterm2-send-zmodem.sh 和 iterm2-recv-zmodem.sh 别尼玛从github下载了，作者都特么把github仓库删除了 编辑2个脚本并放到 /usr/local/bin/ 下，然后授予执行权限 chmod +x /usr/local/bin/iterm2-send-zmodem.sh chmod +x /usr/local/bin/iterm2-recv-zmodem.sh iterm2-send-zmodem.sh 脚本内容 #!/bin/bash # Author: Matt Mastracci (matthew@mastracci.com) # AppleScript from http://stackoverflow.com/questions/4309087/cancel-button-on-osascript-in-a-bash-script # licensed under cc-wiki with attribution required # Remainder of script public domain osascript -e 'tell application \"iTerm2\" to version' > /dev/null 2>&1 && NAME=iTerm2 || NAME=iTerm if [[ $NAME = \"iTerm\" ]]; then FILE=`osascript -e 'tell application \"iTerm\" to activate' -e 'tell application \"iTerm\" to set thefile to choose file with prompt \"Choose a file to send\"' -e \"do shell script (\\\"echo \\\"&(quoted form of POSIX path of thefile as Unicode text)&\\\"\\\")\"` else FILE=`osascript -e 'tell application \"iTerm2\" to activate' -e 'tell application \"iTerm2\" to set thefile to choose file with prompt \"Choose a file to send\"' -e \"do shell script (\\\"echo \\\"&(quoted form of POSIX path of thefile as Unicode text)&\\\"\\\")\"` fi if [[ $FILE = \"\" ]]; then echo Cancelled. # Send ZModem cancel echo -e \\\\x18\\\\x18\\\\x18\\\\x18\\\\x18 sleep 1 echo echo \\# Cancelled transfer else /usr/local/bin/sz \"$FILE\" -e -b sleep 1 echo echo \\# Received $FILE fi iterm2-recv-zmodem.sh 脚本内容 #!/bin/bash # Author: Matt Mastracci (matthew@mastracci.com) # AppleScript from http://stackoverflow.com/questions/4309087/cancel-button-on-osascript-in-a-bash-script # licensed under cc-wiki with attribution required # Remainder of script public domain osascript -e 'tell application \"iTerm2\" to version' > /dev/null 2>&1 && NAME=iTerm2 || NAME=iTerm if [[ $NAME = \"iTerm\" ]]; then FILE=`osascript -e 'tell application \"iTerm\" to activate' -e 'tell application \"iTerm\" to set thefile to choose folder with prompt \"Choose a folder to place received files in\"' -e \"do shell script (\\\"echo \\\"&(quoted form of POSIX path of thefile as Unicode text)&\\\"\\\")\"` else FILE=`osascript -e 'tell application \"iTerm2\" to activate' -e 'tell application \"iTerm2\" to set thefile to choose folder with prompt \"Choose a folder to place received files in\"' -e \"do shell script (\\\"echo \\\"&(quoted form of POSIX path of thefile as Unicode text)&\\\"\\\")\"` fi if [[ $FILE = \"\" ]]; then echo Cancelled. # Send ZModem cancel echo -e \\\\x18\\\\x18\\\\x18\\\\x18\\\\x18 sleep 1 echo echo \\# Cancelled transfer else cd \"$FILE\" /usr/local/bin/rz -E -e -b sleep 1 echo echo echo \\# Sent \\-\\> $FILE fi 3.配置iTerm2 第一步、点击 Preferences 第二步、在 Profiles -> Advanced -> Triggers -> Edit 点击 + 新增2个配置 \\*\\*B010 Run Silent Coprocess /usr/local/bin/iterm2-send-zmodem.sh \\*\\*B00000000000000 Run Silent Coprocess /usr/local/bin/iterm2-recv-zmodem.sh 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"mac使用记录/mac ssh密钥转换问题.html":{"url":"mac使用记录/mac ssh密钥转换问题.html","title":"mac ssh密钥转换","keywords":"","body":"mac ssh密钥转换问题 一、问题描述 背景说明 运维跳板机是通过本机的密钥方式登陆的，即把运维本机的公钥添加到跳板机的 authorized_keys 文件中，然后通过私钥登陆 问题说明 mac使用命令 ssh-keygen 生成的rsa密钥是如下格式的 -----BEGIN OPENSSH PRIVATE KEY----- xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx -----END OPENSSH PRIVATE KEY----- 使用 ZenTermLite 登陆服务器报错如下 并且使用 FinallShell 登陆服务器提示如下 二、密钥格式转换 执行以下命令进行密钥格式转换即可 ssh-keygen -p -m PEM -f 私钥路径 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"mac使用记录/mac安装oh-my-zsh.html":{"url":"mac使用记录/mac安装oh-my-zsh.html","title":"mac安装oh-my-zsh","keywords":"","body":"mac安装oh-my-zsh 参考链接 oh-my-zsh官网 1.安装oh-my-zsh sh -c \"$(curl -fsSL https://raw/branch.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\" 安装完成后界面就会变成如下效果 切换目录效果如下 2.安装插件 2.1 语法高亮插件 zsh-syntax-highlighting zsh-syntax-highlighting github地址 安装前，无论命令正确与否都是白色 安装后，输入的命令正确时是绿色，错误时是红色 下载插件 git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting 编辑 ～/.zshrc 文件增加插件配置 默认内容是 plugins=(git) plugins=( git zsh-syntax-highlighting ) 加载配置生效 source ~/.zshrc 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"mac使用记录/mac安装和配置kubectl.html":{"url":"mac使用记录/mac安装和配置kubectl.html","title":"mac安装和配置kubectl","keywords":"","body":"mac安装和配置kubectl 官方文档 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"git总结/CentOS7编译安装git.html":{"url":"git总结/CentOS7编译安装git.html","title":"git源码安装","keywords":"","body":"CentOS7编译安装git git官网 git github地址 git官方安装文档 git各版本官方下载地址 这个博客写的不错 1.下载源码包 export GIT_VERSION=2.32.0 wget https://mirrors.edge.kernel.org/pub/software/scm/git/git-${GIT_VERSION}.tar.xz 2.安装依赖包 系统环境为最小化安装CentOS7.9 yum -y install curl zlib zlib-devel openssl openssl-devel expat libiconv autoconf gcc gcc-c++ asciidoc xmlto util-linux docbook2X 官方文档中提到 如果你使用 Fedora/RHEL/RHEL衍生版，那么你需要执行以下命令 ln -s /usr/bin/db2x_docbook2texi /usr/bin/docbook2x-texi 以此来解决二进制文件名的不同，db2x_docbook2texi 命令需要安装 docbook2X 包 ln -s /usr/bin/db2x_docbook2texi /usr/bin/docbook2x-texi ln -s /usr/bin/db2x_docbook2texi /usr/bin/docbook2texi ⚠️官方文档中只说明了需要执行 ln -s /usr/bin/db2x_docbook2texi /usr/bin/docbook2x-texi 这个命令，但是实际上还需要执行 ln -s /usr/bin/db2x_docbook2texi /usr/bin/docbook2texi，否则后续make的时候会有如下报错 docbook2texi:/book: no description for directory entry MAKEINFO git.info utf8 \"\\x89\" does not map to Unicode at /usr/share/perl5/vendor_perl/XML/SAX/PurePerl/Reader/Stream.pm line 37. MAKEINFO gitman.info make[1]: Leaving directory `/root/git-2.32.0/Documentation' 3.解压缩包 tar xf git-${GIT_VERSION}.tar.xz && cd git-${GIT_VERSION} 4.编译安装 make configure ./configure --prefix=/usr/local/git make -j`nproc` all doc info make install install-doc install-html install-info 5.导出git命令环境变量 echo 'export PATH=$PATH:/usr/local/git/bin' >/etc/profile.d/git.sh source /etc/profile 6.验证 $ git --version git version 2.32.0 获取git最新版 git clone git://git.kernel.org/pub/scm/git/git.git 7.遇到的问题 ⚠️此问题无解，折腾了半天还是无法解决 $ git clone https://github.com./garabik/grc.git Cloning into 'grc'... git: 'remote-https' is not a git command. See 'git --help'. 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"git总结/git初次使用简单记录.html":{"url":"git总结/git初次使用简单记录.html","title":"git初次使用记录","keywords":"","body":"[toc] 1.git初次使用 第一步、初次使用需要进行全局配置 操作命令 git config --global user.name \"你的用户名\" git config --global user.email \"邮箱\" 第二步、git本地仓库初始化 安装完git后，需要进入到存放代码的目录下，进行初始化(如果再次创建了一个目录，则需要重新初始化) 初始化命令 git init 初始化完成后，会在当前路径下生成一个.git目录 ls -a . .. .git .idea test.py 第三步、提交代码至本地仓库 初始化git仓库完成后，需要将代码目录下的内容提交至本地仓库 git add . #.表示匹配当前代码路径下所有内容 第四步、告知本地仓库提交的内容信息 将代码路径下的内容提交至本地仓库后，需要告知提交至本地仓库的内容信息 git commit -m \"提交的信息内容\" 第五步、与远程仓库建立连接 将代码目录下的文件提交至本地仓库后，需要与远程仓库建立连接从而将本地仓库中的内容提交至远程仓库 git remote add origin 远程仓库地址 第六步、将本地代码仓库文件推送至远程仓库 与远程仓库建立连接后，需要将本地仓库中的文件推送至远程仓库 git push -u origin master #输入远程仓库的用户名和密码即可 2.git非初次使用 第一步、提交代码至本地仓库 初始化git仓库完成后，需要将代码目录下的内容提交至本地仓库 git add . #.表示匹配当前代码路径下所有内容 第二步、告知本地仓库提交的内容信息 将代码路径下的内容提交至本地仓库后，需要告知提交至本地仓库的内容信息 git commit -m \"提交的信息内容\" 第三步、将本地代码仓库文件推送至远程仓库 与远程仓库建立连接后，需要将本地仓库中的文件推送至远程仓库 git push -u origin master #输入远程仓库的用户名和密码即可 3.关于git拉取代码冲突问题 3.1 遵守原则 不删除远程仓库的代码 如需删除代码，则只删除本地的 3.2手贱删除远程仓库文件导致代码冲突恢复演示 3.2.1 远程代码仓库中有以下内容，此时远程仓库和本地仓库中的内容相同 3.2.2 手动删除远程仓库中的test文件，删除后内容为下 3.2.3 手动删除远程仓库文件后，远程仓库和本地仓库中的内容就不同了，此时再次新建文件提交就会有冲突 本地仓库中新建文件test111，尝试提交，报错 解决方法 3.2.4 先拉取代码 git pull origin master 输入拉取代码的命令后会提示如下，意思为请输入一条提交消息来解释为什么需要合并，这里可以选择不输入 3.2.5 加选项 --allow-unrelated-histories (允许合并不相关的历史记录)再次拉取 git pull origin master --allow-unrelated-histories 3.2.6 再次提交代码即可,不会报错 ⚠️因为已经删除了远程仓库中的目录，因此本地的目录在提交之后也会被删除 因此,不要删除远程仓库中的文件！！！ git push origin master 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"git总结/git新建仓库后运行指令说明.html":{"url":"git总结/git新建仓库后运行指令说明.html","title":"git新建仓库后运行指令说明","keywords":"","body":"[toc] Command line instructions（命令行指令） You can also upload existing files from your computer using the instructions below.（您还可以使用下面的说明从您的计算机上载现有文件） Git global setup（git全局配置） git config --global user.name \"Administrator\" git config --global user.email \"admin@example.com\" git本地配置 git config --local user.name \"你的名字\" git config --local user.email \"邮箱\" Create a new repository（创建一个新的仓库） git clone http://gitlab.example.com/root/python-exercise.git cd python-exercise touch README.md git add README.md git commit -m \"add README\" git push -u origin master Push an existing folder（推送现有文件夹） cd existing_folder git init git remote add origin http://gitlab.example.com/root/python-exercise.git git add . git commit -m \"Initial commit\" git push -u origin master Push an existing Git repository（推送现有git仓库） cd existing_repo git remote rename origin old-origin git remote add origin http://gitlab.example.com/root/python-exercise.git git push -u origin --all git push -u origin --tags 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"git总结/git命令总结.html":{"url":"git总结/git命令总结.html","title":"git命令总结","keywords":"","body":"[toc] 1.git简介 1.1 git工作流程 1.2 git四种状态 2.git命令总结 2.1 git工作区域及文件颜色 git status 文件三种颜色的变化 红色 新增文件或者修改的旧文件-->执行命令git add .或者git add 文件名 绿色 git已经管理起来的文件-->执行命令git commit -m '描述信息' 白色 已经生成版本的文件 2.2 git提交数据 1.创建文件 [root@test1 test]# touch aaa bbb 2.查看git文件状态（此时文件是红色的，属于新增文件） [root@test1 test]# git status On branch master Untracked files: (use \"git add ...\" to include in what will be committed) aaa bbb nothing added to commit but untracked files present (use \"git add\" to track) 3.提交文件至暂存区 [root@test1 test]# git add . //此时再查看文件，文件是绿色的，已被git管理起来 [root@test1 test]# git status On branch master Changes to be committed: (use \"git restore --staged ...\" to unstage) new file: aaa new file: bbb 2.3 git删除数据 2.3.1 git删除暂存区中的文件git rm --cached //git删除暂存区中的文件 [root@test1 test]# git rm --cached aaa bbb rm 'aaa' rm 'bbb' //此时文件变回红色 [root@test1 test]# git status On branch master Untracked files: (use \"git add ...\" to include in what will be committed) aaa bbb nothing added to commit but untracked files present (use \"git add\" to track) 2.3.2 git删除工作区和暂存区中的文件git rm -f 文件名 //查看暂存区中的文件，此时是绿色的 [root@test1 test]# git status On branch master Changes to be committed: (use \"git restore --staged ...\" to unstage) new file: aaa new file: bbb //删除工作区的文件同时暂存区中的文件也会同时被删除 [root@test1 test]# git rm -f aaa bbb rm 'aaa' rm 'bbb' [root@test1 test]# ls [root@test1 test]# git status On branch master nothing to commit, working tree clean 2.4 git移动数据 2.4.1 git提交数据至版本库git commit -m '描述信息' //创建文件 [root@test1 test]# touch aaa bbb //提交文件至暂存区 [root@test1 test]# git add . //此时文件是绿色的 [root@test1 test]# git status On branch master Changes to be committed: (use \"git restore --staged ...\" to unstage) new file: aaa new file: bbb //提交文件至版本库 [root@test1 test]# git commit -m 'touch aaa bbb' [master 7215e51] touch aaa bbb 2 files changed, 0 insertions(+), 0 deletions(-) create mode 100644 aaa create mode 100644 bbb //此时再查看文件暂存区中已经没有了，已经被git管理起来了 [root@test1 test]# git status On branch master nothing to commit, working tree clean 2.4.2 git移动数据，有时会将已经添加至暂存区的文件重命名git mv 原文件 新文件 //此时文件是绿色的 [root@test1 test]# git status On branch master Changes to be committed: (use \"git restore --staged ...\" to unstage) new file: aaa new file: bbb //现在想把暂存区中的文件aaa修改为AAA root@test1 test]# git mv aaa AAA [root@test1 test]# git status On branch master Changes to be committed: (use \"git restore --staged ...\" to unstage) new file: AAA new file: bbb //提交文件至git版本库 [root@test1 test]# git commit -m 'change file aaa->AAA' [master 7de2d02] change file aaa->AAA 2 files changed, 0 insertions(+), 0 deletions(-) create mode 100644 AAA create mode 100644 bbb 2.5 git历史数据 2.5.1 git查看历史数据git log //查看全部日志 [root@test1 test]# git log commit 7de2d02e662b1c47cb23085240860bf6a8d0d800 (HEAD -> master) Author: 什么都不会 Date: Sun Feb 23 21:10:03 2020 +0800 change file aaa->AAA commit b32661c0627eb5cdac793c3e80bcc89f3d40a13d (origin/master) Author: 什么都不会 Date: Sun Feb 23 20:00:47 2020 +0800 清空文件 commit 61acb78adac52288805ab59992e9c260866186f0 Author: 什么都不会 Date: Sat Feb 22 22:16:24 2020 +0800 忽略文件测试 。。。。。。。。。 //指定显示日志个数 [root@test1 test]# git log -n 1 commit 7de2d02e662b1c47cb23085240860bf6a8d0d800 (HEAD -> master) Author: 什么都不会 Date: Sun Feb 23 21:10:03 2020 +0800 change file aaa->AAA 2.5.2 git以一行的形式查看日志git log --oneline //但是没有时间显示 [root@test1 test]# git log --oneline 7de2d02 (HEAD -> master) change file aaa->AAA b32661c (origin/master) 清空文件 61acb78 忽略文件测试 4901535 忽略文件测试 1b7cabd 提交忽略文件 a32513a (bug) master清空测试文件 42f05ec 文件内容就是文件名 d66565f caonima 4ae41d2 提交haha hehe test c266f9e touch hehe 7e353d7 增加test文件内容 9f30440 touch test //更长显示commit号 [root@test1 test]# git log --pretty=oneline 7de2d02e662b1c47cb23085240860bf6a8d0d800 (HEAD -> master) change file aaa->AAA b32661c0627eb5cdac793c3e80bcc89f3d40a13d (origin/master) 清空文件 61acb78adac52288805ab59992e9c260866186f0 忽略文件测试 49015353f48daf06f8e14c8d11f692eae795caa1 忽略文件测试 1b7cabd3ee779d68da6cf07241bd8a8dc1542ad4 提交忽略文件 a32513a471688ba24dff4851ce7b2100314c5497 (bug) master清空测试文件 42f05ec321aa987ecf5da2fc303ead235bd59822 文件内容就是文件名 d66565f107148d0827bbed931616a5f21b9bc581 caonima 4ae41d2706308d05fae5ef2183e7f6933bd955e0 提交haha hehe test c266f9ebd1d9bdac4fe8ce265484cf1c58ca6c68 touch hehe 7e353d71a408ec7414e42cbd51b39f208a16d618 增加test文件内容 9f30440387a13fec21d0de2e0e55ce12e32cd5ae touch test 2.5.3 显示具体内容变化git log -p [root@test1 test]# git log -p commit 7de2d02e662b1c47cb23085240860bf6a8d0d800 (HEAD -> master) Author: 什么都不会 Date: Sun Feb 23 21:10:03 2020 +0800 change file aaa->AAA diff --git a/AAA b/AAA new file mode 100644 index 0000000..e69de29 diff --git a/bbb b/bbb new file mode 100644 index 0000000..e69de29 。。。。。。 2.5.4 简要显示文件修改行数git log --stat [root@test1 test]# git log --stat commit 7de2d02e662b1c47cb23085240860bf6a8d0d800 (HEAD -> master) Author: 什么都不会 Date: Sun Feb 23 21:10:03 2020 +0800 change file aaa->AAA AAA | 0 bbb | 0 2 files changed, 0 insertions(+), 0 deletions(-) 。。。。。。 2.5.5 根据不同格式展示历史提交信息git hlog 可以使用format参数来指定具体的输出格式，这样非常便于后期编程的提取分析，常用的格式有： %s 提交说明 %cd 提交日期 %an 作者的名字 %cn 提交者的姓名 %ce 提交者的电子邮件 %H 提交对象的完整SHA-1哈希字串 %h 提交对象的简短SHA-1哈希字串 %T 树对象的完整SHA-1哈希字串 %t 树对象的简短SHA-1哈希字串 %P 父对象的完整SHA-1哈希字串 %p 父对象的简短SHA-1哈希字串 %ad 作者的修订时间 [root@test1 test]# git log --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr)%Creset %cn\"' --abbrev-commit --date=relative * 7de2d02 - (HEAD -> master) change file aaa->AAA (73 minutes ago) 什么都不会\" * b32661c - (origin/master) 清空文件 (2 hours ago) 什么都不会\" * 61acb78 - 忽略文件测试 (24 hours ago) 什么都不会\" * 4901535 - 忽略文件测试 (24 hours ago) 什么都不会\" * 1b7cabd - 提交忽略文件 (24 hours ago) 什么都不会\" * a32513a - (bug) master清空测试文件 (24 hours ago) 什么都不会\" * 42f05ec - 文件内容就是文件名 (25 hours ago) 什么都不会\" * d66565f - caonima (25 hours ago) 什么都不会\" * 4ae41d2 - 提交haha hehe test (25 hours ago) 什么都不会\" * c266f9e - touch hehe (25 hours ago) 什么都不会\" * 7e353d7 - 增加test文件内容 (27 hours ago) 什么都不会\" * 9f30440 - touch test (28 hours ago) 什么都不会\" //设置命令别名，用git hlog代替以上复杂命令 cat >>.git/config 2.6 git恢复数据 2.6.1 恢复历史数据 情况一：修改了本地目录的文件并且提交到了暂存区 1.示例文件aaa原先内容 [root@test1 test]# [root@test1 test]# cat aaa aaa 2.提交文件aaa至暂存区 [root@test1 test]# git add aaa //此时文件是绿色，已提交至暂存区 [root@test1 test]# git status On branch master Changes to be committed: (use \"git restore --staged ...\" to unstage) new file: aaa 3.修改文件内容 [root@test1 test]# echo test >>aaa [root@test1 test]# cat aaa aaa test 4.查看文件状态 [root@test1 test]# git status On branch master Changes to be committed: (use \"git restore --staged ...\" to unstage) new file: aaa Changes not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git restore ...\" to discard changes in working directory) modified: aaa 5.从暂存区覆盖本地目录文件 [root@test1 test]# git checkout -- aaa [root@test1 test]# git status On branch master Changes to be committed: (use \"git restore --staged ...\" to unstage) new file: aaa 6.查看文件，此时文件已经恢复至原先内容 [root@test1 test]# cat aaa aaa 情况二：修改了工作目录文件后提交到了暂存区和本地仓库 1.创建文件bbb [root@test1 test]# touch bbb 2.查看文件状态，此时文件是红色的，还没有提交到暂存区 [root@test1 test]# git status On branch master Untracked files: (use \"git add ...\" to include in what will be committed) bbb nothing added to commit but untracked files present (use \"git add\" to track) 3.往文件bbb中写入内容 [root@test1 test]# echo bbb>bbb [root@test1 test]# cat bbb bbb 4.提交文件bbb至暂存区 [root@test1 test]# git add bbb 5.查看文件状态，此时文件是绿色的 [root@test1 test]# git status On branch master Changes to be committed: (use \"git restore --staged ...\" to unstage) new file: bbb 6.提交文件bbb至本地仓库 [root@test1 test]# git commit -m 'bbb' [master 1dbe8c2] bbb 1 file changed, 1 insertion(+) create mode 100644 bbb 7.查看文件状态，此时暂存区中的文件已经提交到本地仓库了 [root@test1 test]# git status On branch master nothing to commit, working tree clean 8.再次往文件bbb中追加内容，多次追加并提交 第一次，追加内容1到文件中，并提交至暂存区和本地仓库 [root@test1 test]# echo 1 >> bbb [root@test1 test]# cat bbb bbb 1 [root@test1 test]# git add . [root@test1 test]# git commit -m 'echo 1 >> bbb' [master 65d12a7] echo 1 >> bbb 1 file changed, 1 insertion(+) 第二次，追加内容2到文件中，并提交至暂存区和本地仓库 [root@test1 test]# echo 2 >> bbb [root@test1 test]# cat bbb bbb 1 2 [root@test1 test]# git add . [root@test1 test]# git commit -m 'echo 2 >> bbb' [master da5695e] echo 2 >> bbb 1 file changed, 1 insertion(+) 9.此时文件bbb内容如下 [root@test1 test]# cat bbb bbb 1 2 //暂存区中没有内容 [root@test1 test]# git status On branch master nothing to commit, working tree clean 10.查看日志 [root@test1 test]# git log --oneline da5695e (HEAD -> master) echo 2 >> bbb 65d12a7 echo 1 >> bbb 1dbe8c2 bbb 。。。。。。 11.恢复文件内容只有bbb [root@test1 test]# git reset --hard 1dbe8c2 HEAD is now at 1dbe8c2 bbb [root@test1 test]# cat bbb bbb 12.恢复文件内容只有bbb和1 [root@test1 test]# git reset --hard 65d12a7 HEAD is now at 65d12a7 echo 1 >> bbb [root@test1 test]# cat bbb bbb 1 #恢复至一个版本后，通过git log命令查看到的日志就只截止到当前版本，下一个版本的不会记录，因此，如果需要查看全部日志记录，需要用到命令git reflog，此时就可以根据git reflog恢复至任意版本了 [root@test1 test]# git reflog 65d12a7 (HEAD -> master) HEAD@{0}: reset: moving to 65d12a7 1dbe8c2 HEAD@{1}: reset: moving to 1dbe8c2 da5695e HEAD@{2}: commit: echo 2 >> bbb 65d12a7 (HEAD -> master) HEAD@{3}: commit: echo 1 >> bbb 1dbe8c2 HEAD@{4}: commit: bbb git reflog查看全部日志记录 git恢复版本说明 git服务程序中有一个叫做HEAD的版本指针，当用户申请还原数据时，其实就是将HEAD指针指向到某个特定的提交版本，但是因为git是分布式版本控制系统，为了避免历史记录冲突，故使用了SHA-1计算出十六进制的哈希字串来区分每个提交版本，另外默认的HEAD版本指针会指向到最近的一次提交版本记录 git恢复版本重点 1.查看日志，获取对应的操作HEAD指针 2.根据获取到的HEAD指针然后进行 git reset --hard 指针编号 2.7 git分支 2.7.1 git分支命令总结 创建分支 git branch 分支名 切换分支 git checkout 分支名 列出分支 git branch 删除分支 git branch -d 分支名 合并分支 git merge 分支名 创建并切换分支 git checkout -b 分支名(创建分支的同时切换到这个分支) 2.7.2 git分支合并 //master分支创建文件并写入内容 [root@test1 test]# echo 'master分支创建的内容' > txt [root@test1 test]# git add . [root@test1 test]# git commit -m 'master分支创建的内容' //创建切换到dev分支并写入内容 [root@test1 test]# git checkout -b dev Switched to a new branch 'dev' [root@test1 test]# git branch * dev master [root@test1 test]# cat txt master分支创建的内容 [root@test1 test]# echo 'dev分支创建的内容' >> txt [root@test1 test]# cat txt master分支创建的内容 dev分支创建的内容 [root@test1 test]# git add . [root@test1 test]# git commit -m 'dev分支创建的内容' //切换到master分支，可以看到此时文件的内容还没有dev分支写入的内容，需要合并才可以显示 [root@test1 test]# git checkout master Switched to branch 'master' [root@test1 test]# git branch dev * master [root@test1 test]# cat txt master分支创建的内容 //合并分支，可以看到，合并分支后dev分支写入的内容此时已经有了 [root@test1 test]# git merge dev Updating 1f0edf7..a87487f Fast-forward txt | 1 + 1 file changed, 1 insertion(+) [root@test1 test]# cat txt master分支创建的内容 dev分支创建的内容 2.7.3 git合并冲突 合并并不仅仅是简单的文件添加、移除的操作，git 也会合并修改。 //在master分支创建一个空文件test.txt，注意⚠️这里为了演示冲突，不能将文件提交至master分支 [root@test1 test]# git branch * master [root@test1 test]# touch test.txt [root@test1 test]# cat test.txt [root@test1 test]# //创建一个dev分支并切换过去，然后修改test.txt文件的内容，并讲test.txt文件的修改提交到dev分支 [root@test1 test]# git checkout -b dev Switched to a new branch 'dev' [root@test1 test]# cat test.txt [root@test1 test]# echo 'dev分支修改test.txt文件' > test.txt [root@test1 test]# cat test.txt dev分支修改test.txt文件 [root@test1 test]# git add . [root@test1 test]# git commit -m 'dev分支修改test.txt文件' [dev 3d85604] dev分支修改test.txt文件 1 file changed, 1 insertion(+) create mode 100644 test.txt //切换回master分支，此时看不到test.txt文件，因为文件已经被提交到dev分支了，但是这里为了演示冲突手动再次向test.txt文件写入内容 [root@test1 test]# git branch dev * master [root@test1 test]# ls [root@test1 test]# [root@test1 test]# echo 'master分支修改test.txt文件' > test.txt [root@test1 test]# git add . [root@test1 test]# git commit -m 'master分支修改文件' [master 3eefd57] master分支修改文件 1 file changed, 1 insertion(+) create mode 100644 test.txt //合并dev分支，此时会有冲突报错 [root@test1 test]# git merge dev CONFLICT (add/add): Merge conflict in test.txt Auto-merging test.txt Automatic merge failed; fix conflicts and then commit the result. //查看文件，有箭头的地方就是有冲突的地方，删除这部分再合并就可以了 [root@test1 test]# cat test.txt >>>>>> dev //修改后的文件如下 [root@test1 test]# cat test.txt master分支修改test.txt文件 dev分支修改test.txt文件 //git add告诉git文件冲突已解决 [root@test1 test]# git add . [root@test1 test]# git commit -m '解决合并冲突' [master 6fd4fd2] 解决合并冲突 [root@test1 test]# git merge dev Already up to date. 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "},"markdown语法总结/markdown常用语法简记.html":{"url":"markdown语法总结/markdown常用语法简记.html","title":"markdown常用语法简记","keywords":"","body":"[toc] Markdown常规语法 一、标题 # 1个#代表一级标题，mac快捷键 command+1 ## 2个#代表二级标题，mac快捷键 command+2 ### 3个#代表三级标题，mac快捷键 command+3 #### 4个#代表四级标题，mac快捷键 command+4 ##### 5个#代表五级标题，mac快捷键 command+5 ###### 6个#代表六级标题，mac快捷键 command+6 最小到六级标题，#号后边必须有一个空格 一级标题 二级标题 三级标题 四级标题 五级标题 六级标题 二、列表 有序列表 数字1. 空格 无序列表 -号空格 +号空格 *号空格 实心黑圆，有序列表相同位级，按-号然后空格 空心圆，有序列表下一级，按-号然后空格 实心黑方块，空心圆的下一季，按-号然后空格 三、代码块 多行代码块 三个```，一对 多行代码块 多行代码块 。。。 单行代码块 两个``，一对 单行代码块 四、表格 mac快捷键 command+option+T --- --- 五、图片 六、超链接 示例 百度 语法 #中括号中写链接名称，()中写网址，网址形式为http://xxx或者https://xxx mac快捷键 command+K 七、加粗 示例 加粗 语法 **加粗内容** mac快捷键 command+B 八、倾斜 示例 倾斜 语法 *倾斜* mac快捷键 command+I 九、分割线 示例 语法 ---或者+++或者*** mac快捷键 command+option+- 十、颜色（需要借助前端标签实现） 示例 颜色 语法 内容 泡泡吐肥皂o © gitbook.pptfz.cn 2022 all right reserved，powered by Gitbook文件修订时间： 秃笔南波湾！！！ "}}