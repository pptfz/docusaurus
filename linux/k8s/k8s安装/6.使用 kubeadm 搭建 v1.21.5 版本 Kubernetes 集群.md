[toc]



# 使用 kubeadm 搭建 v1.21.5 版本 Kubernetes 集群

# 1.环境准备

**单master架构图**

![iShot2020-07-0410.58.42](https://gitea.pptfz.cn/pptfz/picgo-images/raw/branch/master/img/iShot2020-07-0410.58.42.png)





## 1.1 实验环境

| <span style=color:blue>角色</span> | <span style=color:blue>IP地址</span> | <span style=color:blue>主机名</span> | <span style=color:blue>docker版本</span> | <span style=color:blue>硬件配置</span> | <span style=color:blue>系统</span> | <span style=color:blue>硬盘</span> | <span style=color:blue>内核</span> | <span style=color:blue>安装组件</span>                       |
| ---------------------------------- | ------------------------------------ | ------------------------------------ | ---------------------------------------- | -------------------------------------- | ---------------------------------- | ---------------------------------- | ---------------------------------- | ------------------------------------------------------------ |
| **master**                         | **172.30.100.101**                   | **k8s-master01**                     | **20.10.8**                              | **2C4G**                               | **CentOS7.6**                      | **40g+50g**                        | **5.4.150-1.el7.elrepo.x86_64**    | **kube-apiserver，kube-controller-manager，kube-scheduler，etcd** |
| **node1**                          | **172.30.100.102**                   | **k8s-node01**                       | **20.10.8**                              | **2C4G**                               | **CentOS7.6**                      | **40g+50g**                        | **5.4.150-1.el7.elrepo.x86_64**    | **kubelet，kube-proxy，docker，etcd**                        |
| **node2**                          | **172.30.100.103**                   | **k8s-node02**                       | **20.10.8**                              | **2C4G**                               | **CentOS7.6**                      | **40g+50g**                        | **5.4.150-1.el7.elrepo.x86_64**    | **kubelet，kube-proxy，docker，etcd**                        |



**<span style=color:red>⚠️如无特殊说明，以下所有操作均在master节点</span>**

**<span style=color:red>⚠️已提前配置好master可以免密登陆node节点</span>**

**<span style=color:red>⚠️实验开始前已临时开启root远程登陆，实验结束后关闭root远程登陆，采用sudo用户密钥登陆方式</span>**



## 1.2 编辑环境变量文件

```shell
[ -d /opt/k8s/script ] || mkdir -p /opt/k8s/script && cd /opt/k8s/script
cat > /opt/k8s/script/env.sh <<EOF
export NODE_IPS=(172.30.100.101 172.30.100.102 172.30.100.103)
export NODE_NAMES=(k8s-master01 k8s-node01 k8s-node02)
export SSH_USER=root
export SSH_PORT=2299
export SSH_KEY_FILE=/root/.ssh/id_rsa
export K8S_VERSION=1.21.5
export POD_SUBNET=10.244.0.0/16
export SERVICE_SUBNET=10.96.0.0/12
EOF
```



## 1.3 每个节点配置host信息

```python
# 加载变量文件
source /opt/k8s/script/env.sh

# master节点编辑hosts文件
cat >> /etc/hosts << EOF
${NODE_IPS[0]} ${NODE_NAMES[0]}
${NODE_IPS[1]} ${NODE_NAMES[1]}
${NODE_IPS[2]} ${NODE_NAMES[2]}
EOF

# 拷贝hosts文件到node节点
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    scp -o StrictHostKeyChecking=no -P${SSH_PORT} -i${SSH_KEY_FILE} /etc/hosts ${SSH_USER}@${node_ip}:/etc 
  done  
```



## 1.4 创建 `/etc/sysctl.d/k8s.conf` 文件，添加如下内容

```python
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
vm.swappiness = 0
EOF'
  done  

# 使配置生效  
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'modprobe br_netfilter && sysctl -p /etc/sysctl.d/k8s.conf'
  done    
```



**bridge-nf 说明**

bridge-nf 使得 netfilter 可以对 Linux 网桥上的 IPv4/ARP/IPv6 包过滤。比如，设置 `net.bridge.bridge-nf-call-iptables＝1` 后，二层的网桥在转发包时也会被 iptables的 FORWARD 规则所过滤。常用的选项包括：

- `net.bridge.bridge-nf-call-arptables`：是否在 arptables 的 FORWARD 中过滤网桥的 ARP 包
- `net.bridge.bridge-nf-call-ip6tables`：是否在 ip6tables 链中过滤 IPv6 包
- `net.bridge.bridge-nf-call-iptables`：是否在 iptables 链中过滤 IPv4 包
- `net.bridge.bridge-nf-filter-vlan-tagged`：是否在 iptables/arptables 中过滤打了 vlan 标签的包



## 1.5 安装ipvs

**创建`/etc/sysconfig/modules/ipvs.modules`文件，目的是保证在节点重启后能自动加载所需模块。使用`lsmod | grep -e ip_vs -e nf_conntrack_ipv4`命令查看是否已经正确加载所需的内核模块**

⚠️<span style=color:red>由于内核使用的是最新的版本5.4.150，因此无法加载`nf_conntrack_ipv4` 模块  [google到的答案](https://github.com/easzlab/kubeasz/issues/366)  [看这个isseu](https://github.com/coreos/bugs/issues/2518)</span>

```shell
# 编辑文件
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'cat >> /etc/sysconfig/modules/ipvs.modules << EOF
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
EOF'
  done  

# 使配置生效  
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4'
  done
```



**安装ipset和ipvsadm(便于查看 ipvs 的代理规则)**

```shell
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'yum -y install ipset ipvsadm'
  done
```





## 1.6 安装docker20.10.8

### 1.6.1 添加阿里云yum源

```shell
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo'
  done
```



### 1.6.2  安装docker

使用 `yum list docker-ce --showduplicates | sort -r` 查看可用版本列表

```shell
for node_ip in ${NODE_IPS[@]}
  do
    {
    echo ">>> ${node_ip}"
    ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'export DOCKER_VERSION=20.10.8 && yum -y install docker-ce-${DOCKER_VERSION} docker-ce-cli-${DOCKER_VERSION} containerd.io'
    }&
  done
  wait
```



### 1.6.3 启动docker并设置开机自启

```shell
for node_ip in ${NODE_IPS[@]}
  do
    {
    echo ">>> ${node_ip}"
    ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'systemctl enable docker && systemctl start docker'
    }&
  done
```



### 1.6.4 配置docker镜像加速、修改docker镜像存储路径

3台机器都改，暂时没有批量修改的方法

```shell
[ -d /data/docker-image ] || mkdir /data/docker-image && \
cat > /etc/docker/daemon.json << EOF
{
  "registry-mirrors": ["https://gqk8w9va.mirror.aliyuncs.com"],
  "graph": "/data/docker-image"
}
EOF


for node_ip in ${NODE_IPS[@]}
  do
    {
    echo ">>> ${node_ip}"
    scp -P${SSH_PORT} -i${SSH_KEY_FILE} /etc/docker/daemon.json ${SSH_USER}@${node_ip}:/etc/docker
    }&
  done
```



### 1.6.5 修改docker Cgroup Driver为systemd

> **由于默认情况下 kubelet 使用的 cgroupdriver 是 systemd，所以需要保持 docker 和kubelet 的 cgroupdriver 一致，我们这里修改 docker 的 cgroupdriver=systemd。如果不修改 docker 则需要修改 kubelet 的启动配置，需要保证两者一致。**

```shell
# 修改docker Cgroup Driver为systemd
将/usr/lib/systemd/system/docker.service文件中的这一行 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock

修改为 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd

如果不修改，在添加 worker 节点时可能会碰到如下错误
[WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". 
Please follow the guide at https://kubernetes.io/docs/setup/cri/
```





3台机器都改，暂时没有批量修改的方法

```shell
sed -i.bak 's#^ExecStart=/usr/bin/dockerd.*#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd#g' /usr/lib/systemd/system/docker.service


for node_ip in ${NODE_IPS[@]}
  do
    {
    echo ">>> ${node_ip}"
    scp -P${SSH_PORT} -i${SSH_KEY_FILE} /usr/lib/systemd/system/docker.service ${SSH_USER}@${node_ip}:/usr/lib/systemd/system
    }&
  done
```





## 1.7 安装Kubeadm

### 1.7.1 使用阿里云yum源

```python
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'cat > /etc/yum.repos.d/kubernetes.repo << EOF
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
        http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF' 
  done 
```



### 1.7.2 安装 kubeadm、kubelet、kubectl

**<span style=color:red>⚠️阿里云yum源会随官方更新最新版，因此指定版本</span>**

```python
# 指定版本安装
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'export K8S_VERSION=1.21.5 && yum -y install kubelet-${K8S_VERSION} kubeadm-${K8S_VERSION} kubectl-${K8S_VERSION}'
  done

# 查看版本
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'sudo bash -c "kubeadm version"'
  done
```



### 1.7.3 设置kubelet开机自启

**⚠️<span style=color:red>此时kubelet是无法启动的，因为只有完成master的kubeadm init 的操作，kubelet才能正常启动</span>**

```python
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'systemctl enable kubelet'
  done
```



### 1.7.4 设置k8s命令自动补全

```python
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'yum -y install bash-completion && \
source /usr/share/bash-completion/bash_completion && \
source <(kubectl completion bash) && \
echo "source <(kubectl completion bash)" >> ~/.bashrc'
  done
```



## 1.8 升级内核

[linux内核官网](https://www.kernel.org/)

[centos7内核下载地址](https://elrepo.org/linux/kernel/el7/x86_64/RPMS/)

[linux内核历史版本下载地址](https://mirrors.edge.kernel.org/pub/linux/kernel/)

**可根据自己实际需求下载对应版本**

![iShot2021-09-30 16.46.40](https://gitea.pptfz.cn/pptfz/picgo-images/raw/branch/master/img/iShot2021-09-30%2016.46.40.png)



### 1.8.1 导入elrepo的key并安装elrepo的yum源

```shell
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh -p${SSH_PORT} -i ${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org && 
rpm -Uvh https://elrepo.org/linux/kernel/el7/x86_64/RPMS/elrepo-release-7.0-5.el7.elrepo.noarch.rpm'
  done
```



### 1.8.2 安装最新内核

> 执行此命令为安装最新稳定版
>
> - yum -y install --enablerepo=elrepo-kernel kernel-ml
>
> 执行此命令为安装最新长期维护版
>
> - yum -y install --enablerepo=elrepo-kernel kernel-lt

```shell
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'yum -y install --enablerepo=elrepo-kernel kernel-lt'
  done
```



### 1.8.3 修改内核顺序

```shell
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'grub2-set-default 0 && grub2-mkconfig -o /etc/grub2.cfg'
  done
```



#### 1.8.4 确认是否启动默认内核指向最新安装的内核

```shell
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'grubby --default-kernel'
  done
```



### 1.8.5 重启机器生效

```shell
for node_ip in ${NODE_IPS[@]}
  do
    {
    echo ">>> ${node_ip}"
    ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'reboot'
    }&
  done  
```



### 1.8.6 验证内核版本

```shell
source /opt/k8s/script/env.sh 
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'uname -r'
  done
```



<h3 style=color:red>到此，基本环境安装完成!!!</h3>
---

# 2.初始化集群

## 2.1 master节点操作，配置 kubeadm 初始化文件

**可以通过如下命令导出默认的初始化配置**

```shell
kubeadm config print init-defaults > kubeadm.yaml
```



- **方法一**
  - **命令初始化**

```python
kubeadm init \
--apiserver-advertise-address=172.30.100.101 \
--image-repository registry.aliyuncs.com/google_containers \
--kubernetes-version v2.21.5 \
--service-cidr=10.96.0.0/16 \
--pod-network-cidr=10.20.0.0/16
```



| 参数                                                       | 说明                           |
| ---------------------------------------------------------- | ------------------------------ |
| --apiserver-advertise-address=172.30.100.101               | master节点IP                   |
| --image-repository registry.aliyuncs.com/google_containers | 指定阿里云镜像仓库             |
| --kubernetes-version v2.21.5                               | k8s版本                        |
| --service-cidr=10.96.0.0/16                                | service IP网段                 |
| --pod-network-cidr=10.20.0.0/16                            | pod IP网段，后续网络插件会用到 |





- **方法二**
  - **文件初始化**

[官方清单说明文档](https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2)

```python
[ -d /opt/k8s/yaml ] || mkdir -p /opt/k8s/yaml && cd /opt/k8s/yaml
cat > kubeadm.yaml << EOF
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: ${NODE_IPS[0]}  # apiserver节点内网IP
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: master1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
# registry.aliyuncs.com/k8sxio   # 修改成阿里云镜像源
kind: ClusterConfiguration
kubernetesVersion: v${K8S_VERSION}
networking:
  dnsDomain: cluster.local
  podSubnet: ${POD_SUBNET}  # Pod 网段，flannel插件需要使用这个网段
  serviceSubnet: ${SERVICE_SUBNET}
scheduler: {}
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: ipvs  # kube-proxy 模式
EOF
```





## 2.2 初始化master

**可以先将需要的镜像pull下来**

```shell
$ kubeadm config images pull --config kubeadm.yaml

# 下载的镜像
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-apiserver:v1.21.5
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-controller-manager:v1.21.5
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-scheduler:v1.21.5
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-proxy:v1.21.5
[config/images] Pulled registry.aliyuncs.com/google_containers/pause:3.4.1
[config/images] Pulled registry.aliyuncs.com/google_containers/etcd:3.4.13-0
[config/images] Pulled registry.aliyuncs.com/google_containers/coredns:v1.8.0
```



可以使用如下命令查看kubeadm需要的镜像列表

```shell
# 默认使用的是谷歌的镜像仓库
$ kubeadm config images list
I1001 12:37:55.538500    3567 version.go:254] remote version is much newer: v1.22.2; falling back to: stable-1.21
k8s.gcr.io/kube-apiserver:v1.21.5
k8s.gcr.io/kube-controller-manager:v1.21.5
k8s.gcr.io/kube-scheduler:v1.21.5
k8s.gcr.io/kube-proxy:v1.21.5
k8s.gcr.io/pause:3.4.1
k8s.gcr.io/etcd:3.4.13-0
k8s.gcr.io/coredns/coredns:v1.8.0

# 指定配置文件查看kubeadm需要下载的镜像
$ kubeadm config images list --config kubeadm.yaml
registry.aliyuncs.com/google_containers/kube-apiserver:v1.21.5
registry.aliyuncs.com/google_containers/kube-controller-manager:v1.21.5
registry.aliyuncs.com/google_containers/kube-scheduler:v1.21.5
registry.aliyuncs.com/google_containers/kube-proxy:v1.21.5
registry.aliyuncs.com/google_containers/pause:3.4.1
registry.aliyuncs.com/google_containers/etcd:3.4.13-0
registry.aliyuncs.com/google_containers/coredns:v1.8.0
```



**开始初始化集群**

```shell
kubeadm init --config kubeadm.yaml
```

```python
# 以下为完整输出结果
[init] Using Kubernetes version: v1.21.5
[preflight] Running pre-flight checks
	[WARNING Hostname]: hostname "master1" could not be reached
	[WARNING Hostname]: hostname "master1": lookup master1 on 100.100.2.138:53: no such host
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local master1] and IPs [10.96.0.1 172.30.100.101]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost master1] and IPs [172.30.100.101 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost master1] and IPs [172.30.100.101 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 15.501460 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.21" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node master1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node master1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: abcdef.0123456789abcdef
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 172.30.100.101:6443 --token abcdef.0123456789abcdef \
	--discovery-token-ca-cert-hash sha256:e4a3403c0f8496f53ed90ba5435bd79df40da1804d324a7d97928744c66c5c25 
```

`kubeadm init` 命令执行流程如下图所示

![iShot2020-10-20 16.02.59](https://gitea.pptfz.cn/pptfz/picgo-images/raw/branch/master/img/iShot2020-10-20%2016.02.59.png)





```python
# kubeadm config images pull --config kubeadm.yaml 下载的镜像
REPOSITORY                                                        TAG        IMAGE ID       CREATED         SIZE
registry.aliyuncs.com/google_containers/kube-apiserver            v1.21.5    7b2ac941d4c3   2 weeks ago     126MB
registry.aliyuncs.com/google_containers/kube-proxy                v1.21.5    e08abd2be730   2 weeks ago     104MB
registry.aliyuncs.com/google_containers/kube-controller-manager   v1.21.5    184ef4d127b4   2 weeks ago     120MB
registry.aliyuncs.com/google_containers/kube-scheduler            v1.21.5    8e60ea3644d6   2 weeks ago     50.8MB
registry.aliyuncs.com/google_containers/pause                     3.4.1      0f8457a4c2ec   8 months ago    683kB
registry.aliyuncs.com/google_containers/coredns                   v1.8.0     296a6d5035e2   11 months ago   42.5MB
registry.aliyuncs.com/google_containers/etcd                      3.4.13-0   0369cf4303ff   13 months ago   253MB


# kubeadm init --config kubeadm.yaml启动的容器
613c480c6b7e   e08abd2be730                                          "/usr/local/bin/kube…"   50 seconds ago       Up 50 seconds                 k8s_kube-proxy_kube-proxy-csfth_kube-system_ff2078df-57df-456d-a7c9-c1af4422bf5d_0
e60a9284820f   registry.aliyuncs.com/google_containers/pause:3.4.1   "/pause"                 50 seconds ago       Up 50 seconds                 k8s_POD_kube-proxy-csfth_kube-system_ff2078df-57df-456d-a7c9-c1af4422bf5d_0
ae3d78eb273a   8e60ea3644d6                                          "kube-scheduler --au…"   About a minute ago   Up About a minute             k8s_kube-scheduler_kube-scheduler-master1_kube-system_2fb7c32f95eac5c30e00e43feb43d8c0_0
f127286862b6   184ef4d127b4                                          "kube-controller-man…"   About a minute ago   Up About a minute             k8s_kube-controller-manager_kube-controller-manager-master1_kube-system_3b907e705e4157a55244132169909d32_0
cb172134f689   0369cf4303ff                                          "etcd --advertise-cl…"   About a minute ago   Up About a minute             k8s_etcd_etcd-master1_kube-system_00ba2ed6f0d10f06abb6423169eca485_0
d3808a82f447   7b2ac941d4c3                                          "kube-apiserver --ad…"   About a minute ago   Up About a minute             k8s_kube-apiserver_kube-apiserver-master1_kube-system_92a2138420cba276ed6b4bc76b6a8f1a_0
cd131dbe4fab   registry.aliyuncs.com/google_containers/pause:3.4.1   "/pause"                 About a minute ago   Up About a minute             k8s_POD_kube-scheduler-master1_kube-system_2fb7c32f95eac5c30e00e43feb43d8c0_0
c757028f4427   registry.aliyuncs.com/google_containers/pause:3.4.1   "/pause"                 About a minute ago   Up About a minute             k8s_POD_kube-controller-manager-master1_kube-system_3b907e705e4157a55244132169909d32_0
6619a1b415d4   registry.aliyuncs.com/google_containers/pause:3.4.1   "/pause"                 About a minute ago   Up About a minute             k8s_POD_kube-apiserver-master1_kube-system_92a2138420cba276ed6b4bc76b6a8f1a_0
99e793cea0be   registry.aliyuncs.com/google_containers/pause:3.4.1   "/pause"                 About a minute ago   Up About a minute             k8s_POD_etcd-master1_kube-system_00ba2ed6f0d10f06abb6423169eca485_0
```



**拷贝 kubeconfig 文件**

```python
# $HOME路径为/root
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config
```



## 2.3 master添加节点

**node1和node2相同操作**

**将master节点上的 `/root/.kube/config` 文件拷贝到node节点对应的文件中**

```python
for node_ip in ${NODE_IPS[@]}
  do
    {
    echo ">>> ${node_ip}"
    ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'mkdir -p $HOME/.kube'
    scp -P${SSH_PORT} -i${SSH_KEY_FILE} $HOME/.kube/config ${SSH_USER}@${node_ip}:$HOME/.kube
    ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'chown $(id -u):$(id -g) $HOME/.kube/config' 
    }&  
  done  
```

**将node1和node2加入到集群中**

这里需要用到2.2中初始化master最后生成的token和sha256值

```shell
# 设置node节点数组变量
export NODE_IPS=(172.30.100.102 172.30.100.103)
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh -p${SSH_PORT} -i${SSH_KEY_FILE} ${SSH_USER}@${node_ip} 'kubeadm join 172.30.100.101:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:e4a3403c0f8496f53ed90ba5435bd79df40da1804d324a7d97928744c66c5c25'
  done
```

```python
输出结果  
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

`kubeadm join` 命令执行流程如下所示

![iShot2020-10-20 16.04.22](https://gitea.pptfz.cn/pptfz/picgo-images/raw/branch/master/img/iShot2020-10-20%2016.04.22.png)



**将node节点加入到k8s集群后，node节点会运行kube-proxy**

```shell
# 下载的镜像
registry.aliyuncs.com/google_containers/kube-proxy   v1.21.5   e08abd2be730   2 weeks ago    104MB
registry.aliyuncs.com/google_containers/pause        3.4.1     0f8457a4c2ec   8 months ago   683kB

  
# 启动的容器
7f09c2d3b75f   registry.aliyuncs.com/google_containers/kube-proxy    "/usr/local/bin/kube…"   33 seconds ago   Up 32 seconds             k8s_kube-proxy_kube-proxy-lvcgt_kube-system_e5b226e0-99cf-4164-80b2-7533487322ae_0
091c6e597bbe   registry.aliyuncs.com/google_containers/pause:3.4.1   "/pause"                 37 seconds ago   Up 37 seconds             k8s_POD_kube-proxy-lvcgt_kube-system_e5b226e0-99cf-4164-80b2-7533487322ae_0
```



**如果忘记了token和sha256值，可以在master节点使用如下命令查看**

```shell
# 查看token
$ kubeadm token list
TOKEN                     TTL         EXPIRES                     USAGES                   DESCRIPTION                                                EXTRA GROUPS
abcdef.0123456789abcdef   23h         2021-10-02T14:38:16+08:00   authentication,signing   <none>                                                     system:bootstrappers:kubeadm:default-node-token
            
# 查看sha256            
$ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
e4a3403c0f8496f53ed90ba5435bd79df40da1804d324a7d97928744c66c5c25


# 同时查看token和sha256
$ kubeadm token create --print-join-command
kubeadm join 172.30.100.101:6443 --token 6vpmr9.es2wa9gtromi0aek --discovery-token-ca-cert-hash sha256:e4a3403c0f8496f53ed90ba5435bd79df40da1804d324a7d97928744c66c5c25  
```



**master节点查看node，发现状态都是NotReady，因为还没有安装网络插件，这里我们安装**

[k8s网络插件官方文档](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)

[calio插件官方文档](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)

```shell
$ kubectl get nodes
NAME         STATUS     ROLES                  AGE     VERSION
k8s-node01   NotReady   <none>                 2m22s   v1.21.5
k8s-node02   NotReady   <none>                 2m14s   v1.21.5
master1      NotReady   control-plane,master   15m     v1.21.5
```



## 2.4 master节点安装网络插件flannel

### 2.4.1 下载文件

```shell
cd /opt/k8s/yaml
wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```



### 2.4.2 修改文件内容

搜索 `kube-flannel-ds` ，在 `kube-flannel-ds` 中 `containers` 下的 `args` 添加一行 `- --iface=eth0`，这么做是为了避免机器中是多网卡的情况，这里手动指定一下网卡名称

```yaml
containers:
- name: kube-flannel
  image: quay.io/coreos/flannel:v0.14.0
  command:
  - /opt/bin/flanneld
  args:
  - --ip-masq
  - --kube-subnet-mgr
  - --iface=eth0	# 增加一行
```



### 2.4.3 修改完成后安装flannel网络插件

```shell
kubectl apply -f kube-flannel.yml
```



### 2.4.4 安装完成后稍等一会查看pods状态，全部为running即为正确

```shell
$ kubectl get pods -n kube-system
NAME                              READY   STATUS    RESTARTS   AGE
coredns-59d64cd4d4-9l75q          1/1     Running   0          34m
coredns-59d64cd4d4-plj25          1/1     Running   0          34m
etcd-master1                      1/1     Running   0          34m
kube-apiserver-master1            1/1     Running   0          34m
kube-controller-manager-master1   1/1     Running   0          34m
kube-flannel-ds-4p5kf             1/1     Running   0          16m
kube-flannel-ds-7rb45             1/1     Running   0          16m
kube-flannel-ds-sqt66             1/1     Running   0          16m
kube-proxy-hvrl8                  1/1     Running   0          34m
kube-proxy-lvcgt                  1/1     Running   0          21m
kube-proxy-qdszc                  1/1     Running   0          21m
kube-scheduler-master1            1/1     Running   0          34m
```



### 2.4.5 查看node状态

```shell
$ kubectl get nodes
NAME         STATUS   ROLES                  AGE   VERSION
k8s-node01   Ready    <none>                 22m   v1.21.5
k8s-node02   Ready    <none>                 22m   v1.21.5
master1      Ready    control-plane,master   36m   v1.21.5
```



### 2.4.5 安装flannel网络插件下载的镜像和启动的容器

master节点

```shell
# kubectl apply -f kube-flannel.yml 安装网络插件flannel下载的镜像
quay.io/coreos/flannel                                            v0.14.0    8522d622299c   4 months ago    67.9MB

# kubectl apply -f kube-flannel.yml 安装网络插件flannel启动的容器
fa65c5816d45   8522d622299c                                          "/opt/bin/flanneld -…"   20 minutes ago   Up 20 minutes                         k8s_kube-flannel_kube-flannel-ds-4p5kf_kube-system_49c57fc9-41d9-4c77-a73c-8822ff407c84_0
40b91da4838d   quay.io/coreos/flannel                                "cp -f /etc/kube-fla…"   20 minutes ago   Exited (0) 20 minutes ago             k8s_install-cni_kube-flannel-ds-4p5kf_kube-system_49c57fc9-41d9-4c77-a73c-8822ff407c84_0
f221355e7362   registry.aliyuncs.com/google_containers/pause:3.4.1   "/pause"                 20 minutes ago   Up 20 minutes                         k8s_POD_kube-flannel-ds-4p5kf_kube-system_49c57fc9-41d9-4c77-a73c-8822ff407c84_0
```



node节点

```python
# 下载的镜像
quay.io/coreos/flannel                               v0.14.0   8522d622299c   4 months ago    67.9MB
registry.aliyuncs.com/google_containers/coredns      v1.8.0    296a6d5035e2   11 months ago   42.5MB

# 启动的容器
858dcf4bc524   registry.aliyuncs.com/google_containers/coredns       "/coredns -conf /etc…"   22 minutes ago   Up 22 minutes                         k8s_coredns_coredns-59d64cd4d4-9l75q_kube-system_03015b62-2c0b-4859-b9a5-3dbc49c02877_0
768b0c91b52e   registry.aliyuncs.com/google_containers/pause:3.4.1   "/pause"                 23 minutes ago   Up 22 minutes                         k8s_POD_coredns-59d64cd4d4-9l75q_kube-system_03015b62-2c0b-4859-b9a5-3dbc49c02877_0
7b9ab983793b   8522d622299c                                          "/opt/bin/flanneld -…"   23 minutes ago   Up 23 minutes                         k8s_kube-flannel_kube-flannel-ds-sqt66_kube-system_2dab0071-e15e-4615-90b9-ed121675150d_0
e99be342113a   quay.io/coreos/flannel                                "cp -f /etc/kube-fla…"   23 minutes ago   Exited (0) 23 minutes ago             k8s_install-cni_kube-flannel-ds-sqt66_kube-system_2dab0071-e15e-4615-90b9-ed121675150d_0
26f8110dc18e   registry.aliyuncs.com/google_containers/pause:3.4.1   "/pause"                 23 minutes ago   Up 23 minutes                         k8s_POD_kube-flannel-ds-sqt66_kube-system_2dab0071-e15e-4615-90b9-ed121675150d_0
```





## 2.5 安装Dashboard(可选)

### 2.5.1 下载yaml文件

[这里查看dashboard对应的k8s版本](https://github.com/kubernetes/dashboard/releases)

```shell
cd /opt/k8s/yaml
wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml
mv recommended.yaml dashboard-v2.3.1.yaml
```



###  2.5.2 修改文件

> **修改Service为NodePort类型**

```shell
# 原先内容
spec:
  ports:
    - port: 443
      targetPort: 8443
  selector:
    k8s-app: kubernetes-dashboard


# 修改后内容
spec:
  type: NodePort		# 新增一行，修改类型为nodeport
  ports:
    - port: 443
      targetPort: 8443
      nodePort: 30001   # 新增一行，指定nodeport端口
  selector:
    k8s-app: kubernetes-dashboard
```



### 2.5.3 部署dashboard

```shell
kubectl apply -f dashboard-v2.3.1.yaml
```



`kubectl apply -f dashboard-v2.3.1.yaml` 下载的镜像和启动的容器

```shell
# 下载的镜像
kubernetesui/dashboard                               v2.3.1    e1482a24335a   3 months ago    220MB

# 启动的容器
0c94aae98603   kubernetesui/dashboard                                "/dashboard --insecu…"   About a minute ago   Up About a minute                     k8s_kubernetes-dashboard_kubernetes-dashboard-67484c44f6-lz4r4_kubernetes-dashboard_4139d465-2e82-4048-a983-f3ee1f6b92c7_0
```



### 2.5.4 查看dashboard的运行状态及外网访问端口

```python
# 查看dashboard运行状态
$ kubectl get pods -n kubernetes-dashboard -l k8s-app=kubernetes-dashboard
NAME                                    READY   STATUS    RESTARTS   AGE
kubernetes-dashboard-67484c44f6-lz4r4   1/1     Running   0          2m42s

# 查看dashboard外网访问端口
$ kubectl get svc -n kubernetes-dashboard -l k8s-app=kubernetes-dashboard
NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE
kubernetes-dashboard   NodePort   10.98.151.186   <none>        443:30001/TCP   2m56s
```



### 2.5.5 创建一个具有全局所有权限的用户来登录Dashboard

```yaml
# 编辑admin.yaml文件
cd /opt/k8s/yaml
cat > admin.yaml <<EOF
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: admin
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: admin
  namespace: kubernetes-dashboard
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin
  namespace: kubernetes-dashboard
EOF

# 直接创建
kubectl apply -f admin.yaml
```



### 2.5.6 查看dashboard token

```shell
# 查看token
$ kubectl get secret -n kubernetes-dashboard|grep admin-token
admin-token-852sr                  kubernetes.io/service-account-token   3      103s

# 获取base64解码后的字符串，注意需要用到上边命令查看到的token，会生成很长的一串字符串
kubectl get secret admin-token-zcwfb -o jsonpath={.data.token} -n kubernetes-dashboard |base64 -d

# 直接用这条命令搞定
kubectl get secret `kubectl get secret -n kubernetes-dashboard | grep admin-token|awk '{print $1}'` -o jsonpath={.data.token} -n kubernetes-dashboard |base64 -d && echo
```



### 2.5.6 访问dashboard

**浏览器访问 `https://ip:30001`，<span style=color:red>注意是https</span>**

谷歌浏览器访问会提示如下

![iShot2021-10-01 15.46.33](https://gitea.pptfz.cn/pptfz/picgo-images/raw/branch/master/img/iShot2021-10-01%2015.46.33.png)



通过火狐浏览器访问

![iShot2021-10-01 15.52.45](https://gitea.pptfz.cn/pptfz/picgo-images/raw/branch/master/img/iShot2021-10-01%2015.52.45.png)



**然后粘贴2.5.6步骤中生成的base64字符串登陆dashboard，在登陆页面选择``令牌``一项**

![iShot2021-10-01 15.54.35](https://gitea.pptfz.cn/pptfz/picgo-images/raw/branch/master/img/iShot2021-10-01%2015.54.35.png)



**登陆后的首界面**

![iShot2021-11-14 18.19.11](https://gitee.com/pptfz/picgo-images/raw/master/img/iShot2021-11-14 18.19.11.png)





## 2.6 安装kuboard(可选)

**[kuboard](https://www.kuboard.cn/) 是Kubernetes 的一款图形化管理界面**

### 2.6.1 安装kuboard

> **kuboard可以安装在k8s集群中，也可以单独以容器运行，这里选择单独以容器运行**

```python
docker run -d \
  --restart=unless-stopped \
  --name=kuboard \
  -p 80:80/tcp \
  -p 10081:10081/tcp \
  -e KUBOARD_ENDPOINT="http://172.30.100.101:80" \
  -e KUBOARD_AGENT_SERVER_TCP_PORT="10081" \
  -v /root/kuboard-data:/data \
  eipwork/kuboard:v3
```



启动参数说明

![iShot2021-10-01 16.04.54](https://gitea.pptfz.cn/pptfz/picgo-images/raw/branch/master/img/iShot2021-10-01%2016.04.54.png)





**查看运行状态**

```python
$ docker ps -a|grep kuboard
bcb69ab63a48   eipwork/kuboard:v3                                    "/entrypoint.sh"         2 minutes ago       Up 2 minutes                   443/tcp, 0.0.0.0:10081->10081/tcp, 0.0.0.0:80->80/tcp   kuboard
```





### 2.6.2 访问kuboard

浏览器访问 `ip:80`

用户名： `admin`

密 码： `Kuboard123`

![iShot2021-10-01 16.07.09](https://gitea.pptfz.cn/pptfz/picgo-images/raw/branch/master/img/iShot2021-10-01%2016.07.09.png)



登陆后首界面

![iShot2021-10-01 16.09.43](https://gitea.pptfz.cn/pptfz/picgo-images/raw/branch/master/img/iShot2021-10-01%2016.09.43.png)



### 2.6.3 添加k8s集群

点击 `添加集群`

![iShot2021-10-01 16.10.46](https://gitea.pptfz.cn/pptfz/picgo-images/raw/branch/master/img/iShot2021-10-01%2016.10.46.png)





填写名称和描述

![iShot2021-10-01 16.12.57](https://gitea.pptfz.cn/pptfz/picgo-images/raw/branch/master/img/iShot2021-10-01%2016.12.57.png)



按照提示

![iShot2021-10-01 16.14.35](https://gitea.pptfz.cn/pptfz/picgo-images/raw/branch/master/img/iShot2021-10-01%2016.14.35.png)





```shell
cd /opt/k8s/yaml
curl -k 'http://172.30.100.101:80/kuboard-api/cluster/ops-k8s/kind/KubernetesCluster/ops-k8s/resource/installAgentToKubernetes?token=zSWL5I5l1POTgRVV80FlVDyoWy8HJz0P' > kuboard-agent.yaml
kubectl apply -f ./kuboard-agent.yaml
```





等待几秒，当状态变为已就绪即为成功

![iShot2021-10-01 16.22.47](https://gitea.pptfz.cn/pptfz/picgo-images/raw/branch/master/img/iShot2021-10-01%2016.22.47.png)



## 2.7 安装k8s切换命名空间工具(可选)

```python
# 下载包
git clone https://github.com.cnpmjs.org/ahmetb/kubectx
cp kubectx/kubens /usr/local/bin

# 查看所有命名空间
$ kubens

# 切换到kube-system命名空间
$ kubens kube-system
Context "kubernetes-admin@kubernetes" modified.
Active namespace is "kube-system".
```



<h3 style=color:red>到此，使用kubeadm安装k8s 1.21.5完成！！！</h3>





**k8s 1.21.5master启动的容器及下载的镜像**

下载的镜像

![iShot2021-10-01 16.27.11](https://gitea.pptfz.cn/pptfz/picgo-images/raw/branch/master/img/iShot2021-10-01%2016.27.11.png)



启动的容器

![iShot2021-10-01 16.30.20](https://gitea.pptfz.cn/pptfz/picgo-images/raw/branch/master/img/iShot2021-10-01%2016.30.20.png)





**k8s 1.21.5node启动的容器和下载的镜像**

node下载的镜像

![iShot2021-10-01 16.32.05](https://gitea.pptfz.cn/pptfz/picgo-images/raw/branch/master/img/iShot2021-10-01%2016.32.05.png)



node启动的容器

![iShot2021-10-01 16.32.39](https://gitea.pptfz.cn/pptfz/picgo-images/raw/branch/master/img/iShot2021-10-01%2016.32.39.png)



